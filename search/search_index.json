{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI4EPS: Artificial Intelligence (AI) for Earth and Planetary Science (EPS)","text":""},{"location":"#weiqiang-zhu","title":"Weiqiang Zhu","text":"<p>Assistant Professor (CV)</p> <p>Earth &amp; Planetary Science, University of California, Berkeley</p> <p>Berkeley Seismology Lab</p> <p>Berkeley Institute for Data Science</p> <p>Email: zhuwq@berkeley.edu</p> <p>Github: AI4EPS</p>"},{"location":"#news","title":"News","text":"<p>[05/2022] I will be joining the Department of Earth &amp; Planetary Science at University of California, Berkeley as an Assistant Professor starting from July 2023. I am looking for self-motivated Ph.D. students and PostDoc fellows to join my research group. Candidates interested in earthquake science and machine learning are particularly encouraged to apply. </p>"},{"location":"#education","title":"Education","text":"<ul> <li> <p>Ph.D., Geophysics, Stanford University, 2016-2021</p> <ul> <li> <p>Thesis: Applications of Deep Learning in Seismology</p> </li> <li> <p>Advisor: Greg Beroza</p> </li> </ul> </li> <li> <p>Ph.D. Minor, Computer Science, Stanford University, 2016-2021</p> </li> <li>M.S., Geophysics, Peking University, 2013-2016</li> <li>B.S., Geophysics, Peking University, 2009-2013</li> </ul>"},{"location":"#experience","title":"Experience","text":"<ul> <li>07/2023 - Present, Assistant Professor, University of California, Berkeley</li> <li>10/2021 \u2013 06/2023, Director\u2019s Postdoctoral Fellow, California Institute of Technology</li> <li>06/2019 \u2013 09/2019, Research Intern, Google LLC</li> <li>09/2016 \u2013 06/2021, Graduate Researcher, Stanford University</li> </ul>"},{"location":"#research","title":"Research","text":"<p>My research focuses on understanding earthquake physics and statistics by applying cutting-edge artificial intelligence and scientific computing methods to gain new insights from large seismic datasets.</p>"},{"location":"#machine-learningdeep-learning-for-geophysical-signal-discovery","title":"Machine Learning/Deep Learning for Geophysical Signal Discovery","text":"<ul> <li>Applying deep learning to detecting hidden earthquake signals from large seismic datasets to understand complex earthquake sequences and fault zone structures.</li> <li>Applying cloud computing to large-scale data mining to build high-resolution earthquake catalogs for studying earthquake mechanisms and other geophysical processes, such as subsurface fluid migration, volcanic unrest, and induced seismicity.</li> </ul>"},{"location":"#earthquake-simulation-and-seismic-inversion-for-understanding-earthquake-physics","title":"Earthquake Simulation and Seismic Inversion for Understanding Earthquake Physics","text":"<ul> <li>Applying earthquake simulation to analyze multiphysics couplings within fault zones such as fluid, permeability, friction, and other mechanical properties.</li> <li>Applying automatic differentiation to improve geophysical inversion and constrain key physical parameters such as earthquake source parameters and the Earth\u2019s interior structures.</li> </ul>"},{"location":"#honors-and-awards","title":"Honors and Awards","text":"<ul> <li>2021, Director\u2019s Postdoctoral Fellowship, Caltech Seismological Laboratory </li> <li>2021, Exceptional Thesis, Geophysics, Stanford University</li> <li>2021, Outstanding Student Presentation Award, American Geophysical Union</li> <li>2015, National Scholarship, Peking University </li> <li>2014, Outstanding Student Paper Award, Chinese Geophysical Society </li> <li>2012, National Scholarship, Peking University </li> </ul>"},{"location":"contact/","title":"Opportunities","text":"<p>If you are interested in exploring the interdisciplinary field of seismology and machine learning (and more broadly Earth Science and Data Science) in your undergraduate, graduate, or postdoctoral research, please send me an email. </p>"},{"location":"contact/#phd-program","title":"Ph.D. program","text":"<p>The application deadline of the Ph.D. program of EPS for the 2024-2025 academic year is December 4, 2023.</p> <p>GRE scores are optional. International applicants from countries where English is not the official language are required to take the Test of English as a Foreign Language (TOEFL). </p> <p>Please find more information in this EPS webpage</p> <p>Other links about EPS and Berkeley:</p> <ul> <li>http://eps.berkeley.edu/graduate-students</li> <li>https://grad.berkeley.edu/admissions/steps-to-apply</li> </ul>"},{"location":"contact/#postdoc","title":"Postdoc","text":"<ul> <li>Miller Fellowship (Deadline September each year)</li> <li>NSF Postdoctoral Fellowships</li> </ul>"},{"location":"job_announcement/","title":"Job announcement","text":"<p>Job Announcement Text: </p> <p>Assistant Professor - Data Science, Departments of Astronomy &amp; Earth and Planetary Science - Division of Mathematical and Physical Sciences - UC Berkeley</p> <p>The Departments of Astronomy and Earth &amp; Planetary Science at the University of California, Berkeley invite applications for an Assistant Professor Faculty position as part of a campus-wide initiative to promote scientific discovery through data-driven science. We seek candidates from all areas of astronomy, astrophysics, earth and planetary science, with an emphasis on individuals who develop or apply modern data-science techniques or tools to make progress in their respective field of research. Candidates whose research crosses disciplinary boundaries are especially encouraged to apply. UC Berkeley is committed to fostering a diverse, equitable, and inclusive environment. We are looking for an outstanding scientist who has the potential for innovation and leadership in research, commits to teaching and mentoring undergraduate and graduate students, and shares our core values:</p> <p>https://astro.berkeley.edu/about/diversity-and-climate https://eps.berkeley.edu/diversity-equity-inclusion-and-accessibility-deia</p> <p>The Departments are committed to addressing the family needs of faculty, including dual career couples and single parents. We are also interested in candidates who have had non-traditional career paths or who have taken time off for family reasons (e.g., caring for children, the disabled, or the elderly), or who have achieved excellence in careers outside academia (e.g., in professional or industry service).</p> <p>For information about potential relocation to Berkeley, or career needs of accompanying partners and spouses, please visit: http://ofew.berkeley.edu/new-faculty</p> <p>Departments: https://astro.berkeley.edu and https://eps.berkeley.edu/</p> <p>Division: https://ls.berkeley.edu/ls-divisions/mathematical-physical-sciences</p> <p>To apply, visit https://apptrkr.com/2542051</p>"},{"location":"presentation/","title":"Presentations","text":""},{"location":"presentation/#invited-talks","title":"Invited Talks","text":"<ul> <li> <p>Keynote Speaker of GMG Fall 2022 Meeting (2022/09): Deep Learning for Earthquake Monitoring using Seismic Networks and Distributed Acoustic Sensing</p> </li> <li> <p>Plenary Speaker of 2022 SCEC Annual Meeting (2022/09): Deep Learning and Cloud Computing for Earthquake Science</p> </li> </ul>"},{"location":"presentation/#conferences","title":"Conferences","text":""},{"location":"publication/","title":"Publications","text":"<p>Google Scholar Page</p> <p>If you cannot find PDFs from your library, please send me a paper request on ResearchGate</p>"},{"location":"publication/#2022","title":"2022","text":"<ul> <li>Zhu, W., Hou, A. B., Yang, R., Datta, A., Mousavi, S. M., Ellsworth, W. L., &amp; Beroza, G. C. (2022). QuakeFlow: A Scalable Machine-learning-based Earthquake Monitoring Workflow with Cloud Computing. Geophysical Journal International.</li> <li>Zhu, W., Tai, K. S., Mousavi, S. M., Bailis, P., &amp; Beroza, G. C. (2022). An end-to-end earthquake detection method for joint phase picking and association using deep learning. Journal of Geophysical Research: Solid Earth.</li> <li>Zhu, W., McBrearty, I. W., Mousavi, S. M., Ellsworth, W. L., &amp; Beroza, G. C. (2022). Earthquake phase association using a bayesian gaussian mixture model. Journal of Geophysical Research: Solid Earth.</li> <li>Xu, K., Zhu, W., &amp; Darve, E. (2022). Learning generative neural networks with physics knowledge. Research in the Mathematical Sciences.</li> <li>Yang, L., Liu, X., Zhu, W., Zhao, L., &amp; Beroza, G. C. (2022). Toward improved urban earthquake monitoring through deep-learning-based noise suppression. Science advances.</li> <li>Datta, A., Wu, D. J., Zhu, W., Cai, M., &amp; Ellsworth, W. L. (2022). Deepshake: Shaking intensity prediction using deep spatiotemporal RNNs for earthquake early warning. Seismological Society of America.</li> </ul>"},{"location":"publication/#2021","title":"2021","text":"<ul> <li>Zhu, W. (2021). Ph.D. Thesis: Applications of Deep Learning in Seismology. pdf</li> <li>Zhu, W., Xu, K., Darve, E., &amp; Beroza, G. C. (2021). A General Approach to Seismic Inversion with Automatic Differentiation. Computers &amp; Geosciences.</li> <li>Zhu, W., Xu, K., Darve, E., Biondi, B., &amp; Beroza, G. C. (2021). Integrating deep neural networks with full-waveform inversion: Reparametrization, regularization, and uncertainty quantification. Geophysics.</li> <li>Retailleau, L., Saurel, J.-M., Zhu, W., Satriano, C., Beroza, G. C., Issartel, S., ... Team, O. (2021). PhaseWorm: A real-time machine-learning-based algorithm for volcano-tectonic earthquake monitoring. Seismological Research Letters.</li> <li>Tan, Y. J., Waldhauser, F., Ellsworth, W. L., Zhang, M., Zhu, W., Michele, M., ... Segou, M. (2021). Machine-learning-based high-resolution earthquake catalog reveals how complex fault structures were activated during the 2016\u20132017 central italy sequence. The Seismic Record.</li> </ul>"},{"location":"publication/#2020","title":"2020","text":"<ul> <li>Zhu, W., Mousavi, S. M., &amp; Beroza, G. C. (2020). Seismic Signal Augmentation to Improve Generalization of Deep Neural Networks. Advances in Geophysics. pdf</li> <li>Zhu, W., Allison, K. L., Dunham, E. M., &amp; Yang, Y. (2020). Fault Valving and Pore Pressure Evolution in Simulations of Earthquake Sequences and Aseismic Slip. Nature Communications.</li> <li>Xu, K., Zhu, W., &amp; Darve, E. (2020). Distributed machine learning for computational engineering using MPI. arXiv preprint.</li> <li>Mousavi, S. M., Ellsworth, W. L., Zhu, W., Chuang, L. Y., &amp; Beroza, G. C. (2020). Earthquake transformer\u2014an attentive deep-learning model for simultaneous earthquake detection and phase picking. Nature Communications.</li> <li>Chai, C., Maceira, M., Santos-Villalobos, H. J., Venkatakrishnan, S. V., Schoenball, M., Zhu, W., ... Team, E. C. (2020). Using a deep neural network and transfer learning to bridge scales for seismic phase picking. Geophysical Research Letters.</li> <li>Liu, M., Zhang, M., Zhu, W., Ellsworth, W. L., &amp; Li, H. (2020). Rapid characterization of the july 2019 Ridgecrest, california, earthquake sequence from raw seismic data using machine-learning phase picker. Geophysical Research Letters.</li> <li>Park, Y., Mousavi, S. M., Zhu, W., Ellsworth, W. L., &amp; Beroza, G. C. (2020). Machine-learning-based analysis of the Guy-Greenbrier, arkansas earthquakes: A tale of two sequences. Geophysical Research Letters.</li> </ul>"},{"location":"publication/#2019","title":"2019","text":"<ul> <li>Zhu, W., Mousavi, S. M., &amp; Beroza, G. C. (2019). Seismic Signal Denoising and Decomposition using Deep Neural Networks. IEEE Transactions on Geoscience and Remote Sensing.</li> <li>Mousavi, S. M., Sheng, Y., Zhu, W., &amp; Beroza, G. C. (2019). STanford EArthquake Dataset (STEAD): A global data set of seismic signals for AI. IEEE Access.</li> <li>Mousavi, S. M., Zhu, W., Ellsworth, W., &amp; Beroza, G. (2019). Unsupervised clustering of seismic signals using deep convolutional autoencoders. IEEE Geoscience and Remote Sensing Letters.</li> <li>Mousavi, S. M., Zhu, W., Sheng, Y., &amp; Beroza, G. C. (2019). CRED: A deep residual network of convolutional and recurrent units for earthquake signal detection. Scientific Reports.</li> </ul>"},{"location":"publication/#2018","title":"2018","text":"<ul> <li>Zhu, W., &amp; Beroza, G. C. (2018). PhaseNet: a deep-neural-network-based seismic arrival-time picking method. arXiv preprint.</li> </ul>"},{"location":"research/","title":"Research","text":"<p>My research focuses on understanding earthquake physics and statistics by applying cutting-edge artificial intelligence and scientific computing methods to gain new insights from large seismic datasets.</p>"},{"location":"research/#machine-learningdeep-learning-for-geophysical-signal-discovery","title":"Machine Learning/Deep Learning for Geophysical Signal Discovery","text":"<ul> <li>Applying deep learning to detecting hidden earthquake signals from large seismic datasets to understand complex earthquake sequences and fault zone structures.</li> <li>Applying cloud computing to large-scale data mining to build high-resolution earthquake catalogs for studying earthquake ricks, volcanic activities, glacier evolution, etc.</li> </ul>"},{"location":"research/#earthquake-simulation-and-seismic-inversion-for-understanding-earthquake-physics","title":"Earthquake Simulation and Seismic Inversion for Understanding Earthquake Physics","text":"<ul> <li>Applying earthquake simulation to analyze multiphysics couplings within fault zones such as fluid, permeability, friction, and other mechanical properties.</li> <li>Applying automatic differentiation to improve geophysical inversion and constrain key physical parameters such as earthquake source parameters and the Earth\u2019s interior structures.</li> </ul>"},{"location":"teaching/","title":"Teaching","text":""},{"location":"teaching/#observational-seismology","title":"Observational Seismology","text":""},{"location":"teaching/#earthquake-catalog-workshop","title":"Earthquake Catalog Workshop","text":""},{"location":"teaching/#pyearth-a-python-introduction-to-earth-science","title":"PyEarth: A Python Introduction to Earth Science","text":""},{"location":"teaching/#ml4earth","title":"ML4Earth","text":""},{"location":"teaching/#playground","title":"Playground","text":"<p>Jupyterhub: https://datahub.berkeley.edu</p> <p>Colab: https://colab.research.google.com/</p>"},{"location":"ADSeismic/","title":"Overview","text":""},{"location":"ADSeismic/#adseismic-an-open-source-high-performance-package-for-automatic-differentiation-ad-based-general-seismic-inversion","title":"ADSeismic: An Open Source High Performance Package for Automatic Differentiation (AD) based General Seismic Inversion","text":"Documentation Build Status <p>ADSeismic is built for general seismic inversion problems, such as estimating velocity model, source location and time function. The package implements the forward FDTD (finite difference time domain) simulation of acoustic and elastic wavefields and enables flexible inversions of parameters in the wave equations using automatic differentiation. Several features of this package includes:</p> <ul> <li>Battery included: No adjoint state method codes required for building your own inversion models; Automatic differentiation is the work horse, making it unified approach to various seismic inversion problems such as full waveform inversion (FWI), earthquake location inversion, and source rupture inversion.</li> <li>Neural Networks: Easy to integrate Neural Networks (NN) into seismic inversion.</li> <li>High performance: Computational graph optimization using Tensorflow; Parallel computing using MPI.</li> <li>(Multi-)GPU support: Support for GPU acceleration as well as multi-GPU (in theory TPUs are also supported).</li> </ul> <p>Examples can be found int these papers:</p> <p>A General Approach to Seismic Inversion with Automatic Differentiation </p> <p>Integrating Deep Neural Networks with Full-waveform Inversion: Reparametrization, Regularization, and Uncertainty Quantification</p> <p>Distributed Machine Learning for Computational Engineering using MPI</p> <p>Learning generative neural networks with physics knowledge</p>"},{"location":"ADSeismic/#installation","title":"Installation","text":"<p>ADSeismic.jl requires proper installation of ADCME.jl.</p> <pre><code>using Pkg\nPkg.add(\"ADCME\")\n</code></pre> <p>This might take up to 20 minutes for complete installation. </p> <p>Then install this package (ADSeismic.jl) by </p> <pre><code>using Pkg\nPkg.add(\"ADSeismic\")\n</code></pre> <p>To enable GPU support, make sure <code>nvcc</code> is available on your machine, and then</p> <pre><code>using ADCME\nuse_gpu()\n</code></pre> <p>Use has_gpu() to check GPUs exist.</p>"},{"location":"ADSeismic/#demo","title":"Demo","text":"<ul> <li>Acoustic Wave</li> </ul> <p>Script: AcousticWave.jl</p> <p>Result: </p> <ul> <li>Elastic Wave</li> </ul> <p>Script: ElasticWave.jl</p> <p>Result: </p>"},{"location":"ADSeismic/src/","title":"Getting Started","text":"<p>An Open Source High Performance Package for General Seismic Inversion Problems</p> <p></p> <p>ADSeismic is suitable for general inversion problems in seismic imaging. The packages implements the forward simulation of acoustic and elastic wave fields and allows inversion of physical properties such as media densities, Lam\u00e9 parameters, shear modulus, etc. by means of automatic differentiation. For example, the following problems fall into the scope of this framework</p> <ul> <li>Full waveform inversion (FWI)</li> <li>Rupture inversion</li> <li>Source-time inversion</li> </ul> <p>The package provides a unified interface for both acoustic and elastic wave simulation. Users only need to specify the geometries and known parameters of the physical simulation. Gradients (traditionally derived by adjoint methods) are computed automatically. Some notable features of this package are</p> <ul> <li>Battery included: unified approach to various seismic inversion problems such as full waveform inversion (FWI), rupture inversion and source-time inversion.</li> <li>High performance: computational graph optimization and parallel computing. </li> <li>(Multi-)GPU support: support for GPU acceleration as well as multi-GPU (in theory TPUs are also supported).</li> <li>Easy-to-use: no adjoint state method codes required for building your own inversion models; automatic differentiation is the workhorce.</li> </ul> <p>ADSeismic is built on ADCME.jl, an automatic differentiation library for computational mathematics and engineering. The former is fully compatible with ADCME.jl, which indicates that this package (ADSeismic.jl) can serve as a acoustic/elastic simulation solver in a system inverse modeling problem. </p>"},{"location":"ADSeismic/src/#installation","title":"Installation","text":"<p><code>ADCME</code> is a dependency for using this package. Install <code>ADCME</code> with</p> <pre><code>using Pkg; Pkg.add(\"ADCME\")\n</code></pre> <p>Then install the lastest ADSeismic.jl by</p> <pre><code>using Pkg; Pkg.add(\"https://github.com/kailaix/ADSeismic.jl#master\")\n</code></pre> <p>or the stable version by </p> <pre><code>using Pkg; Pkg.add(\"ADSeismic\")\n</code></pre> <p>If you want to use GPU for ADSeismic, you need to install the GPU-capable ADCME. See instructions [here](https://kailaix.github.io/ADCME.jl/latest/tu_customop/#Install-GPU-enabled-TensorFlow-(Linux-and-Windows).</p>"},{"location":"ADSeismic/src/#simple-example-acoustic-wave-equation","title":"Simple Example: Acoustic Wave Equation","text":"<p>We consider the acoustic wave equation </p> \\[\\frac{\\partial^2 u }{\\partial t^2} = \\nabla \\cdot (c^2 \\nabla u) + f\\] <p>where \\(u\\) is the displacement, \\(f\\) is the source term, and \\(c\\) is the spatially varying acoustic velocity. In numerical simulations, we apply the perfect matched layer (PML) boundary conditions so that the outgoing waves are damped near the boundary. This enables us to emulate an infinite space using a finite computational domain. A description of the numerical scheme could be found in this paper. <sup>1</sup></p> <p>In the forward simulation, we consider a layer model as the ground truth for \\(c\\), which consists of three layers with different values: </p> <p>We consider a delta source function \\(f(\\mathbf{x}, t) = \\delta(\\mathbf{x}-\\mathbf{x}_0)g(t)\\), where \\(g(t)\\) is a Ricker wavelet. </p> <p>Using this source function (left plot), we are able to generate the wavefield (right plot)</p> Source Function Wavefield <p>The code for forward simulation is as follows:</p> <pre><code>using ADCME\nusing ADSeismic\nusing PyPlot\nusing DelimitedFiles\nusing JLD2 \n\nscale = 201\n\nparam = AcousticPropagatorParams(NX=scale, NY=scale, \n    NSTEP=1000, DELTAT=1e-4,  DELTAX=1.0, DELTAY=1.0,\n    PropagatorKernel = 0, Rcoef = 1e-8)\n\nrc = Ricker(param, 30.0, 200.0, 1e6)\nsrci = [div(param.NX,2)]\nsrcj = [div(param.NY,5)]\nsrcv = reshape(rc, :, 1)\nsrc = AcousticSource(srci, srcj, srcv)\n\nlayers = ones(param.NX+2, param.NY+2)\nn_piece = div(param.NX + 1, 3) + 1\nfor k = 1:3\n    i_interval = (k-1)*n_piece+1:min(k*n_piece, param.NX+2)\n    layers[:, i_interval] .= 0.5 + (k-1)*0.25\nend\n\nC = placeholder(3300*layers)\nmodel = AcousticPropagatorSolver(param, src, C)\n\nsess = Session(); init(sess)\nu = run(sess, model.u)\n</code></pre> <p>In the inverse problem, we assume we can observe the wavefield \\(u(x, 40, t)\\) for all \\(x\\) and \\(t\\). The unknown is the velocity model \\(c\\). We can find the value of \\(c\\) by solving the PDE-constrained optimization problem </p> \\[\\begin{aligned} \\min_c &amp;\\; \\sum_{} (u_h(x_i, 40, t_j) - u(x_i, 40, t_j))^2 \\\\ \\text{s.t.} &amp;\\; F_h(u_h, c) = 0\\end{aligned}\\] <p>Here \\(F_h(u_h, c)=0\\) is the numerical discretization of the wave equation and \\(u_h\\) is the numerical solution for all time steps. The following plot shows the signal we received:</p> <p></p> <p>We can apply automatic differentiation with the following code to calculate the gradients of the loss function with respect to \\(c\\)</p> <pre><code>using ADCME\nusing ADSeismic\nusing PyPlot\nusing DelimitedFiles\nusing JLD2 \n\nscale = 201\n\nparam = AcousticPropagatorParams(NX=scale, NY=scale, \n    NSTEP=1000, DELTAT=1e-4,  DELTAX=1.0, DELTAY=1.0,\n    PropagatorKernel = 0, Rcoef = 1e-8)\n\nrc = Ricker(param, 30.0, 200.0, 1e6)\nsrci = [div(param.NX,2)]\nsrcj = [div(param.NY,5)]\nsrcv = reshape(rc, :, 1)\nsrc = AcousticSource(srci, srcj, srcv)\n\n\nC = Variable(3300*ones(param.NX+2, param.NY+2))\nmodel = AcousticPropagatorSolver(param, src, C)\n@load \"data.jld2\" u \n\nU = model.u\nloss = sum((u[:, :, 40] - U[:,:,40])^2)\ng = gradients(loss, C)\n\nsess = Session(); init(sess)\nG = run(sess, g)\n</code></pre> <p>The gradient at the first step is shown below:</p> <p></p> <p>The codes can be found here.</p> <ol> <li> <p>Grote, Marcus J., and Imbo Sim. \"Efficient PML for the wave equation.\" arXiv preprint arXiv:1001.0319 (2010).\u00a0\u21a9</p> </li> </ol>"},{"location":"ADSeismic/src/NNFWI/","title":"NNFWI","text":"<p>Integrating Deep Neural Networks with Full-waveform Inversion: Reparametrization, Regularization, and Uncertainty Quantification</p>"},{"location":"ADSeismic/src/NNFWI/#architecture","title":"Architecture","text":""},{"location":"ADSeismic/src/NNFWI/#forward-simulation","title":"Forward Simulation","text":"Marmousi model Inital 1D model BP2004 model Inital 1D model"},{"location":"ADSeismic/src/NNFWI/#inversion-based-on-automatic-differentiation","title":"Inversion based on Automatic Differentiation","text":""},{"location":"ADSeismic/src/NNFWI/#loss-function","title":"Loss function","text":"Noise level Marmousi model BP2004 model \\(\\sigma=0\\) \\(\\sigma=0.5\\)"},{"location":"ADSeismic/src/NNFWI/#marmousi-model","title":"Marmousi model","text":"Noise level Traditional  FWI NNFWI \\(\\sigma=0\\) \\(\\sigma=0.5\\) \\(\\sigma=1\\)"},{"location":"ADSeismic/src/NNFWI/#bp2004-model","title":"BP2004 model","text":"Noise level Traditional  FWI NNFWI \\(\\sigma=0\\) \\(\\sigma=0.5\\) \\(\\sigma=1\\)"},{"location":"ADSeismic/src/NNFWI/#uncertainty-quantification-using-dropout","title":"Uncertainty Quantification using Dropout","text":"Inverted \\(V_p\\) std(\\(V_p\\)) std(\\(V_p\\))/\\(V_p\\) \\(\\cdot\\) 100%"},{"location":"ADSeismic/src/ad/","title":"Inverse Modeling","text":"<p>Inverse modeling (IM) identifies a certain set of parameters or functions with which the outputs of the forward analysis matches the desired result or measurement. IM can usually be solved by formulating it as an optimization problem. But the major difference is that IM aims at getting information not accessible to forward analysis, instead of obtaining an optimal value of a fixed objective function and set of constraints. In IM, the objective function and constraints can be adjusted, and prior information of the unknown parameters or functions can be imposed in the form of regularizers, to better reflect the physical laws. </p> <p>For example, given an image \\(x\\in\\mathbb{R}^{1024\\times 1024}\\), the forward analysis is given by \\(y = F(x) = \\sum_{i,j} x_{i,j}\\), i.e., the summation of all pixel values. One possible IM problem requires you to estimate \\(x\\) given the measurement \\(y\\). It can be formulated an optimization problem \\(\\min_x (F(x)-y)^2\\), which is underdetermined. However, if we have the prior that the image is a pure color image, then the inverse problem is well-defined and has a unique solution. There are many ways to impose this prior as contraints to the optimization problem, but the IM problem itself may not be described as an optimization problem. </p> <p></p>"},{"location":"ADSeismic/src/ad/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>One powerful tool in inverse modeling is automatic differentiation (AD). Automatic differentiation is a general way to compute gradients based on the chain rule. By tracing the forward-pass computation, the gradient at the final step can propagate back to every operator and every parameter in a computational graph. </p> <p>As an example, a neural network model mainly consists of a sequence of linear transforms and non-linear activation functions. The goal of the training process is to minimize the error between its prediction and the label of ground truth. Automatic differentiation is used to calculate the gradients of every variable by back-propagating the gradients from the loss function to the trainable parameters, i.e., the weights and biases of neural networks. The gradients are then used in a gradient-based optimizer such as gradient descent methods to update the parameters. </p> <p>For another example, the physical forward simulation is similar to the neural network model in that they are both sequences of linear/non-linear transforms. One popular method in physical simulation, the FDTD (Finite-Difference Time-Domain) method, applies a finite difference operator to a consecutive time steps to solve time-dependent partial differential equations (PDEs). In seismic problems, we can specify parameters such as earthquake source functions and earth media properties to simulate the received seismic signals. In seismic inversion problems, those parameters are unknown and we can invert the underlining source characteristic and media property by minimizing the difference between the simulated seismic signals and the observed ones. In the framework of automatic differentiation, the gradients of the difference can be computed automatically and thus used in a gradient-based optimizer. </p> <p></p>"},{"location":"ADSeismic/src/ad/#ad-implementation-in-adcme","title":"AD Implementation in ADCME","text":"<p>ADCME uses TensorFlow as the backend for automatic differentiation. However, one major difference of ADCME compared with TensorFlow is that it provides a friendly syntax for scientific computing (essentially the same syntax as native Julia). This substantially reduces development time. In addition, ADCME augments TensorFlow libraries by adding missing features that are useful for scientific computing, such as sparse matrix solve, sparse least square, sparse assembling, etc. Additionally, Julia interfaces make it possible for directly implementing efficient numerical computation parts of the simulation (requires no automatic differentiation), for interacting with other languages (MATLAB, C/C++, R, etc.) and for built-in Julia parallelism. </p> <p>As an example, we show how a convoluted acoustic wave equation simulation with PML boundary condition can be translated to Julia codes with AD feature very neatly. </p> <p></p>"},{"location":"ADSeismic/src/ad/#forward-operator-types","title":"Forward Operator Types","text":"<p>All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. </p> <p></p> <p>In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called input, \\(x\\), and turns out a parameter called ouput, \\(y=f(x)\\). For reverse mode automatic differentiation, besides evaluating \\(f(x)\\), we need also to compute \\(\\frac{\\partial J}{\\partial x}\\) given \\(\\frac{\\partial J}{\\partial y}\\) where \\(J\\) is a functional of \\(y\\). </p> <p>Note  the operator \\(y=f(x)\\) may be implicit in the sense that \\(f\\) is not given directly. In general, we can write the relationship between \\(x\\) and \\(y\\) as \\(F(x,y)=0\\). The operator is well-defined if for given \\(x\\), there exists one and only one \\(y\\) such that \\(F(x,y)=0\\). </p> <p>For automatic differentiation, besides the well-definedness of \\(F\\), we also require that we can compute \\(\\frac{\\partial J}{\\partial x}\\) given \\(\\frac{\\partial J}{\\partial y}\\). It is easy to see that</p> <pre><code>\\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}F_y^{-1}F_x\n</code></pre> <p>Therefore, we call an operator \\(F\\) is well-posed if \\(F_y^{-1}\\) exists. </p> <p>All operators can be classified into four types based on the linearity and explicitness.</p> <p>Linear and explicit</p> <p>This type of operators has the form </p> <pre><code>y = Ax\n</code></pre> <p>where \\(A\\) is a matrix. In this case, </p> <pre><code>F(x,y) = Ax-y\n</code></pre> <p>and therefore </p> <pre><code>\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}A\n</code></pre> <p>In Tensorflow, such an operator can be implemented as (assuming <code>A</code> is )</p> <pre><code>import tensorflow as tf\n@tf.custom_gradient\ndef F(x):\n\u200b      u = tf.linalg.matvec(A, x)\n\u200b      def grad(dy):\n\u200b          return tf.linalg.matvec(tf.transpose(A), dy)\n\u200b      return u, grad\n</code></pre> <p>Nonlinear and explicit</p> <p>In this case, we have </p> <pre><code>y = F(x)\n</code></pre> <p>where \\(F\\) is explicitly given. We have</p> <pre><code>F(x,y) = F(x)-y\\Rightarrow \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y} F_x(x)\n</code></pre> <p>One challenge here is we need to implement the matrix vector production \\(\\frac{\\partial J}{\\partial y} F_x(x)\\) for <code>grad</code>. </p> <p>Linear and implicit</p> <p>In this case </p> <pre><code>Ay = x\n</code></pre> <p>We have \\(F(x,y) = x-Ay\\) and </p> <pre><code>\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}A^{-1}\n</code></pre> <p>Nonlinear and implicit</p> <p>In this case \\(F(x,y)=0\\) and the corresponding gradient is </p> <pre><code>\\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}F_y^{-1}F_x\n</code></pre> <p>This case is the most challenging of the four but widely seen in scientific computing code. In many numerical simulation code, \\(F_y\\) is usually sparse and therefore it is rewarding to exploit the sparse structure for computation acceleration in practice.</p>"},{"location":"ADSeismic/src/api/","title":"API Reference","text":""},{"location":"ADSeismic/src/api/#data-structure","title":"Data Structure","text":"<p>```@autodocs Modules = [ADSeismic] Pages   = [\"Struct.jl\"]</p> <pre><code>\n## Simulation\n```@autodocs\nModules = [ADSeismic]\nPages   = [\"Core.jl\"]\n</code></pre>"},{"location":"ADSeismic/src/api/#io","title":"I/O","text":"<p>```@autodocs Modules = [ADSeismic] Pages   = [\"IO.jl\"]</p> <pre><code>\n## Utilities\n```@autodocs\nModules = [ADSeismic]\nPages   = [\"Utils.jl\"]\n</code></pre>"},{"location":"ADSeismic/src/api/#optimization","title":"Optimization","text":"<p><code>@autodocs Modules = [ADSeismic] Pages   = [\"Optim.jl\"]</code></p>"},{"location":"ADSeismic/src/backward_inversion/","title":"Inversioin using Automatic Differentiation","text":"<p>In this section, we describe three inversion problems in seismic imaging: inverting velocity, inverting source location and time function, and inverting rupture process. </p> <p>Despite the method described here is also applicable to elastic wave equation, we consider the simpler the acoustic wave equation \\(\\begin{aligned} {u_{tt}} - {c^2}\\Delta u =&amp; f \\\\ u =&amp; {u_0} \\\\ {u_t} =&amp; {v_0} \\end{aligned}\\)</p> <p>Here \\(f\\) is the source function, \\(u_0\\) and \\(v_0\\) are initial conditions. For numerical simulation, the perfect matched layer boundary condition is used to truncate the computational domain. See Acoustic Wave Simulation for details. </p>"},{"location":"ADSeismic/src/backward_inversion/#inverting-velocity-model","title":"Inverting Velocity Model","text":"<p>In this model, \\(c(x, y)\\) is unknown. Red dots represent \\(n\\) sources and triangles represent \\(m\\) receivers at locations \\(\\{\\mathbf{x}_i\\}\\). We activate the sources one by one and collect the time serious signals \\(u(t, \\mathbf{x}_i)\\) at the receivers for different source functions. Therefore, the observations are \\(m\\) \\(\\mathbb{R}^{n_t\\times n}\\) matrices, where \\(n_t\\) is the size of the time series. This inversion problem is called full waveform inversion, or FWI for short. </p> <p>The idea of FWI is updating \\(c(x, y)\\) until the observations and the estimations--which are obtained from solving the wave equation--match. We do benchmarks on two classical models for \\(c(x,y)\\): Marmousi model and Layer model. </p>"},{"location":"ADSeismic/src/backward_inversion/#marmousi-model","title":"Marmousi model","text":"Ground Truth Inversion"},{"location":"ADSeismic/src/backward_inversion/#layer-model","title":"Layer model","text":"Ground Truth Inversion <p>To invert the acoustic wave velocity, we first create three containers for storing simulation results, source functions and receiver data</p> <pre><code>ap_sim = load_acoustic_model(\"models/layer-model-smooth.mat\"; inv_vp=true, IT_DISPLAY=0)\nsrc = load_acoustic_source(\"models/layer-model-smooth.mat\")\nrcv_sim = load_acoustic_receiver(\"models/layer-model-smooth.mat\")\n</code></pre> <p>Then we load ground truth receiver data</p> <pre><code>Rs = Array{Array{Float64,2}}(undef, length(src))\nfor i = 1:length(src)\n    Rs[i] = readdlm(joinpath(output_dir, \"layermodel-r$i.txt\"))\nend\n</code></pre> <p>Since we want to run the model on multi-GPU, we instruct ADSeismic to compute the gradients and loss on GPUs and then assemble them on CPU</p> <pre><code>vp = get_collection()[1]\nlosses, gs = compute_loss_and_grads_GPU(ap_sim, src, rcv_sim, Rs, vp) # losses and gs are computed on GPUs\ng = sum(gs); loss = sum(losses) # g and loss are assembled on CPU\n</code></pre> <p>Finally, the optimization can be triggered by <code>LBFGS!</code>, a built-in L-BFGS optimizer</p> <pre><code>sess = Session(); init(sess)\nLBFGS!(sess, loss, g, vp)\n</code></pre>"},{"location":"ADSeismic/src/backward_inversion/#inverting-source-location-and-time-function","title":"Inverting Source Location and Time Function","text":"<p>In this case, the location and amplitude function of the source (we assume there is one single source) is not known. The receivers are placed on the earth surface. We want to estimate the source term \\(f\\) based on the receiver information (which are \\(m\\) time series in \\(\\mathbb{R}^{n_t}\\)).</p> <p>Mathematically, \\(f(t, \\mathbf{x})\\) is a Delta function in \\(\\mathbf{x}\\); to make the inversion problem continuous, we use </p> <pre><code>f_{\\theta}(t, \\mathbf{x}) = g(t) \\frac{1}{2\\pi\\sigma^2}\\exp(-\\frac{\\|\\mathbf{x}-\\theta\\|^2}{2\\sigma^2})\n</code></pre> <p>to approximate \\(f(t, \\mathbf{x})\\); here \\(\\theta\\in\\mathbb{R}^2\\) and \\(g(t)\\) are unknown. </p> Source/Receiver Location Forward Simulation Source Time Function Inversion Source Location Inversion <p></p>"},{"location":"ADSeismic/src/backward_inversion/#inverting-rupture-process","title":"Inverting Rupture Process","text":""},{"location":"ADSeismic/src/backward_inversion/#1-invert-the-rupture-process-directly","title":"1. Invert the rupture process directly","text":"Source/Receiver Location Forward Simulation Ground Truth Inversion"},{"location":"ADSeismic/src/backward_inversion/#2-inverte-the-slip-time-and-amplitude","title":"2. Inverte the slip time and amplitude","text":"<p>In this case, the functional form of the source function is known (see <code>Gauss</code>)</p> <pre><code>f(t) = \\frac{2A}{\\pi^2 a^2} \\exp\\left( -\\frac{(t-t_0)^2}{\\pi^2 a^2} \\right)\n</code></pre> <p>where \\(A\\) is the amplitude, \\(a\\) is the parameter that controls width of the Gaussian, and \\(t_0\\) is the shift of the parameter. </p> Ground Truth Inversion <p></p>"},{"location":"ADSeismic/src/contents/","title":"Script Reference","text":"Script Description BenchmarkAcoustic.jl Benchmarking CPU/GPU Performance for Acoustic Simulation BenchmarkElastic.jl Benchmarking CPU/GPU Performance for Elastic Simulation AcousticMultiGPU.jl Acoustic wave simulation using multi-GPUs AcousticFWIMultiGPU.jl Acoustic wave full waveform inversion using multi-GPUs"},{"location":"ADSeismic/src/forward_simulation/","title":"Forward Simulation","text":"<p>In this section, we describe forward simulation for acoustic and elastic equations. </p>"},{"location":"ADSeismic/src/forward_simulation/#acoustic-wave-simulation","title":"Acoustic Wave Simulation","text":"<p>We consider a time dependent wave field \\(u\\) propagating through an unbounded domain \\(\\mathbb{R}^2\\). In \\(\\Omega\\), the wave field \\(u\\) satisfies </p> <p>\\(\\begin{aligned} {u_{tt}} - {c^2}\\Delta u =&amp; f \\\\ u =&amp; {u_0} \\\\ {u_t} =&amp; {v_0} \\end{aligned}\\)</p> <p>Here \\(f\\) is the source function, \\(u_0\\) and \\(v_0\\) are initial conditions. </p> <p>We truncate the computational domain to a bounded 2D domain \\(\\Omega\\in \\mathbb{R}^2\\). To prevent the wave field to reflect from the boundaries, perfect matched layer (PML) are usually used to absorb the waves. Mathematically, the governing equation can be described by a modified partial differential equation (PDE) system with auxilliary variables \\(\\phi\\)</p> <p>\\(\\begin{aligned}{u_{tt}} + ({\\xi _1} + {\\xi _2}){u_t} + {\\xi _1}{\\xi _2}u =&amp; {c^2}\\Delta u + \\nabla  \\cdot \\phi   \\\\ {\\phi _t} =&amp; {\\Gamma _1}\\phi  + {c^2}{\\Gamma _2}\\nabla u\\end{aligned}\\)</p> <p>and \\(\\Gamma_1\\) and \\(\\Gamma_2\\) are defined by</p> \\[{\\Gamma _1} = \\left[ {\\matrix{ { - {\\xi _1}} &amp; {}  \\cr    {} &amp; { - {\\xi _2}}  \\cr  } } \\right],\\quad{\\Gamma _2} = \\left[ {\\matrix{{{\\xi _2} - {\\xi _1}} &amp; {}  \\cr    {} &amp; {{\\xi _1} - {\\xi _2}}  \\cr  } }\\right]\\] <p>The terms \\(\\xi_i(x)\\geq 0\\), \\(i=1,2\\) are the damping profiles and only depend on \\(x_i\\) (\\(x=(x_1,x_2)\\)). In our example, we use</p> \\[{\\xi _i}(x) = \\left\\{ {\\matrix{{0\\quad x\\not  \\in \\tilde \\Omega }  \\cr    {{{\\bar \\xi }_i}\\left( {{{d(x,\\tilde \\Omega )} \\over {{L_i}}} - {{\\sin \\left( {{{2\\pi d(x,\\tilde \\Omega )} \\over {{L_i}}}} \\right)} \\over {2\\pi }}} \\right)\\quad x \\in\\tilde \\Omega }  \\cr  } } \\right.\\] <p>Here \\(L_i\\) is the PML width in \\(x\\) and \\(y\\) direction and \\(\\tilde \\Omega_i\\) is the domain excluding the PML layer in the \\(x\\) (i=1) or \\(y\\) (i=2) direction. \\(d(x, \\tilde\\Omega_i)\\) is the distance between \\(x\\) and \\(\\tilde\\Omega_i\\). See the following figure for illustration. </p> <p></p> <p>The profiling coefficient \\(\\bar\\xi_i\\) is given by the relative reflection \\(R\\) (we always use \\(R=0.001\\)), the (estimated) velocity of acoustic waves \\(c\\) and the PML width \\(L_i\\)</p> <pre><code>\\bar\\xi_i = \\frac{c}{L_i}\\log\\left(\\frac{1}{R}\\right)\n</code></pre> <p>In what follows, we assume the velocity is given by the elastic Marmousi model. </p> <p></p> <p>The following code snippet shows how we can run the forward simulation with the 13-th source function. For how to generate the dataset <code>marmousi2_model_big_true.mat</code>, we refer you to the notebooks. </p> <pre><code>using ADSeismic\nusing ADCME\nusing PyPlot\n\nmatplotlib.use(\"agg\")\nmodel = load_acoustic_model(\"models/marmousi2_model_big_true.mat\", IT_DISPLAY=50)\nsrc = load_acoustic_source(\"models/marmousi2_model_big_true.mat\")\nrcv = load_acoustic_receiver(\"models/marmousi2_model_big_true.mat\") # assume there is only one receiver\n\n@assert length(src)==length(rcv)\nfor i = 1:length(src)\n    SimulatedObservation!(model(src[i]), rcv[i])\nend\n\nsess = Session(); init(sess)\nu = run(sess, model(src[13]).u)\nvisualize_file(u, model.param, dirs=\"$(@__DIR__)/../docs/src/asset/Acoustic\")\n</code></pre> <p>The following animation shows the simulation result. </p> <p></p>"},{"location":"ADSeismic/src/forward_simulation/#elastic-wave-simulation","title":"Elastic Wave Simulation","text":"<p>The governing equation for the elastic wave propagation is</p> \\[\\begin{aligned}\\rho \\frac{\\partial v_z}{\\partial t} =&amp; \\frac{\\partial \\sigma_{zz}}{\\partial z} + \\frac{\\partial \\sigma_{xz}}{\\partial x}  \\\\\\rho \\frac{\\partial v_x}{\\partial t} =&amp; \\frac{\\partial \\sigma_{xx}}{\\partial x} + \\frac{\\partial \\sigma_{xz}}{\\partial z}  \\\\  \\frac{\\partial \\sigma_{zz}}{\\partial t} =&amp; (\\lambda + 2\\mu)\\frac{\\partial v_z}{\\partial z} + \\lambda\\frac{\\partial v_x}{\\partial x}  \\\\  \\frac{\\partial \\sigma_{xx}}{\\partial t} =&amp; (\\lambda + 2\\mu)\\frac{\\partial v_x}{\\partial x} + \\lambda\\frac{\\partial v_z}{\\partial z}  \\\\  \\frac{\\partial \\sigma_{xz}}{\\partial t} =&amp; \\mu \\frac{\\partial v_z}{\\partial x} + \\lambda\\frac{\\partial v_x}{\\partial z}\\end{aligned}\\] <p>Here </p> Variable Description \\(v_x, v_z\\) Velocity of the wave field in the \\(x\\) and \\(z\\) direction \\(\\sigma_{xx}, \\sigma_{zz}, \\sigma_{xz}\\) Stress tensor \\(\\rho\\) Density of the media \\(\\lambda\\) Lam\u00e9's first parameter \\(\\mu\\) Shear modulus <p>The following animation shows the simulation result for \\(v_z\\). </p> <p></p>"},{"location":"ADSeismic/src/forward_simulation/#benchmark-of-acoustic-wave-simulation-on-cpu-and-gpu","title":"Benchmark of Acoustic Wave Simulation on CPU and GPU","text":"<p>We benchmark the simulation on CPU and GPU. The configuration for GPU and CPU are </p> CPU GPU Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz NVIDIA Tesla V100-PCIE-32GB <p>The benchmark script can be found in BenchmarkAcoustic.jl and BenchmarkElastic.jl.  </p> <p>The following shows the comparison of the simulation time for CPU and GPU. </p> Acoustic Simulation Elastic Simulation"},{"location":"ADSeismic/src/forward_simulation/#multi-gpus","title":"Multi-GPUs","text":"<p>With only a few modification to the original simulation codes, the model can also be run on multi-GPUs (see AcousticMultiGPU.jl for the full script). In this example, we compute the gradients of the loss function (the discrepancy between the observed seismic data and the simulated seismic data) with respect to the acoustic velocity. </p> <p>In the following code snippets, we explicitly place different source functions to different GPUs. This can be done with the  <code>@pywith tf.device(\"/gpu:0\")</code> syntax, and the enclosed codes are executed on 0-th GPU.  </p> <pre><code>using ADSeismic\nusing ADCME\nusing PyPlot\nusing PyCall\nusing DelimitedFiles\nreset_default_graph()\n\nsrc = load_acoustic_source(\"models/marmousi2_model_initial.mat\")\nep_sim = load_acoustic_model(\"models/marmousi2_model_initial.mat\"; inv_vp=true, IT_DISPLAY=0)\nrcv_sim = load_acoustic_receiver(\"models/marmousi2_model_initial.mat\")\n\n\nRs = Array{Array{Float64,2}}(undef, length(src))\nfor i = 1:length(src)\n    Rs[i] = readdlm(\"models/AcousticData/Acoustic-R$i.txt\")\nend\n\n\nfunction run_on_gpu_device(batch_id)\n    local loss, g\n    @pywith tf.device(\"/gpu:$(batch_id-1)\") begin\n        [SimulatedObservation!(ep_sim(src[i]), rcv_sim[i]) for i = batch_id]\n        loss = sum([sum((rcv_sim[i].rcvv-Rs[i])^2) for i = batch_id]) \n        g = gradients(loss, get_collection()[1])\n    end\n    return loss, g\nend\n\nlosses = Array{PyObject}(undef, 8)\ngs = Array{PyObject}(undef, 8)\nfor i = 1:8\n    losses[i], gs[i] = run_on_gpu_device(i)\nend\ng = sum(gs)\nsess = Session(); init(sess)\nrun(sess, g)\n</code></pre> <p>The following plot shows the dataflow of the above computation. </p> <p></p> <p>We take the snapshot of <code>nvidia-smi</code> and it shows the memory and computation utilization of GPUs on a DGX server with 8 Tesla V100s </p> <p></p>"},{"location":"ADSeismic/src/test_contents/","title":"Description of Contents","text":""},{"location":"DeepDenoiser/","title":"DeepDenoiser: Seismic Signal Denoising and Decomposition Using Deep Neural Networks","text":""},{"location":"DeepDenoiser/#1-install-miniconda-and-requirements","title":"1.  Install miniconda and requirements","text":"<ul> <li>Download DeepDenoiser repository</li> </ul> <pre><code>git clone https://github.com/wayneweiqiang/DeepDenoiser.git\ncd DeepDenoiser\n</code></pre> <ul> <li>Install to default environment</li> </ul> <pre><code>conda env update -f=env.yml -n base\n</code></pre> <ul> <li>Install to \"deepdenoiser\" virtual envirionment</li> </ul> <pre><code>conda env create -f env.yml\nconda activate deepdenoiser\n</code></pre>"},{"location":"DeepDenoiser/#2-pre-trained-model","title":"2. Pre-trained model","text":"<p>Located in directory: model/190614-104802</p>"},{"location":"DeepDenoiser/#3-related-papers","title":"3. Related papers","text":"<ul> <li>Zhu, Weiqiang, S. Mostafa Mousavi, and Gregory C. Beroza. \"Seismic Signal Denoising and Decomposition Using Deep Neural Networks.\" arXiv preprint arXiv:1811.02695 (2018).</li> </ul>"},{"location":"DeepDenoiser/#4-interactive-example","title":"4. Interactive example","text":"<p>See details in the notebook: example_interactive.ipynb</p>"},{"location":"DeepDenoiser/#5-batch-prediction","title":"5. Batch prediction","text":"<p>See details in the notebook: example_batch_prediction.ipynb</p>"},{"location":"DeepDenoiser/#6-train","title":"6. Train","text":""},{"location":"DeepDenoiser/#data-format","title":"Data format","text":"<p>Required: two csv files for signal and noise, corresponding directories of the npz files.</p> <p>The csv file contains four columns: \"fname\", \"itp\", \"channels\"</p> <p>The npz file contains four variable: \"data\", \"itp\",  \"channels\"</p> <p>The shape of \"data\" variables has a shape of 9001 x 3</p> <p>The variables \"itp\" is the data points of first P arrival times.</p> <p>Note: In the demo data, for simplicity we use the waveform before itp as noise samples, so the train_noise_list is same as train_signal_list here.</p> <pre><code>python deepdenoiser/train.py --mode=train --train_signal_dir=./Dataset/train --train_signal_list=./Dataset/train.csv --train_noise_dir=./Dataset/train --train_noise_list=./Dataset/train.csv --batch_size=20\n</code></pre> <p>Please let us know of any bugs found in the code. Suggestions and collaborations are welcomed</p>"},{"location":"DeepDenoiser/example_batch_prediction/","title":"Batch Prediction","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nPROJECT_ROOT = os.path.realpath(os.path.join(os.path.abspath(''), \"..\"))\n</pre> import numpy as np import matplotlib.pyplot as plt import os import glob PROJECT_ROOT = os.path.realpath(os.path.join(os.path.abspath(''), \"..\")) In\u00a0[2]: Copied! <pre>plt.close(\"all\")\nfor i, fp in enumerate(sorted(glob.glob(os.path.join(PROJECT_ROOT, \"output/results/*npz\")))):\n    signal = np.load(fp)[\"data\"][5000:8000,-1,-1]\n    raw_signal = np.load(os.path.join(PROJECT_ROOT, \"test_data/npz/\", fp.split(\"/\")[-1]))[\"data\"][5000:8000,-1]\n    plt.figure(figsize=(6,2))\n    plt.subplot(121)\n    plt.plot(raw_signal, 'k', linewidth=0.5)\n    ylim = plt.ylim()\n    plt.subplot(122)\n    plt.plot(signal, 'k', linewidth=0.5)\n    plt.ylim(ylim)\n    plt.suptitle(fp.split(\"/\")[-1])\n    plt.tight_layout()\n    plt.show()\n    if i &gt;= 3:\n        break\n    \n</pre> plt.close(\"all\") for i, fp in enumerate(sorted(glob.glob(os.path.join(PROJECT_ROOT, \"output/results/*npz\")))):     signal = np.load(fp)[\"data\"][5000:8000,-1,-1]     raw_signal = np.load(os.path.join(PROJECT_ROOT, \"test_data/npz/\", fp.split(\"/\")[-1]))[\"data\"][5000:8000,-1]     plt.figure(figsize=(6,2))     plt.subplot(121)     plt.plot(raw_signal, 'k', linewidth=0.5)     ylim = plt.ylim()     plt.subplot(122)     plt.plot(signal, 'k', linewidth=0.5)     plt.ylim(ylim)     plt.suptitle(fp.split(\"/\")[-1])     plt.tight_layout()     plt.show()     if i &gt;= 3:         break      In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepDenoiser/example_batch_prediction/#batch-prediction","title":"Batch Prediction\u00b6","text":""},{"location":"DeepDenoiser/example_batch_prediction/#1-download-demo-data","title":"1. Download demo data\u00b6","text":"<pre><code>cd DeepDenoiser\nwget https://github.com/wayneweiqiang/PhaseNet/releases/download/test_data/test_data.zip\nunzip test_data.zip\n</code></pre>"},{"location":"DeepDenoiser/example_batch_prediction/#2-run-batch-prediction","title":"2. Run batch prediction\u00b6","text":"<p>DeepDenoiser currently supports three data formats: numpy, and mseed</p> <ul> <li>For numpy format:</li> </ul> <pre>python deepdenoiser/predict.py --model_dir=model/190614-104802 --data_list=test_data/npz.csv --data_dir=test_data/npz --format=numpy --save_signal --plot_figure\n</pre> <ul> <li>For mseed format:</li> </ul> <pre>python deepdenoiser/predict.py --model_dir=model/190614-104802 --data_list=test_data/mseed.csv --data_dir=test_data/mseed --format=mseed --save_signal --plot_figure\n</pre> <p>Optional arguments:</p> <pre><code>usage: predict.py [-h] [--format FORMAT]\n                  [--batch_size BATCH_SIZE] [--output_dir OUTPUT_DIR]\n                  [--model_dir MODEL_DIR] [--sampling_rate SAMPLING_RATE]\n                  [--data_dir DATA_DIR] [--data_list DATA_LIST]\n                  [--plot_figure] [--save_signal] [--save_noise]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --format FORMAT       Input data format: numpy or mseed\n  --batch_size BATCH_SIZE\n                        Batch size\n  --output_dir OUTPUT_DIR\n                        Output directory (default: output)\n  --model_dir MODEL_DIR\n                        Checkpoint directory (default: None)\n  --sampling_rate SAMPLING_RATE\n                        sampling rate of pred data\n  --data_dir DATA_DIR   Input file directory\n  --data_list DATA_LIST\n                        Input csv file\n  --plot_figure         If plot figure\n  --save_signal         If save denoised signal\n  --save_noise          If save denoised noise\n</code></pre>"},{"location":"DeepDenoiser/example_batch_prediction/#3-read-denoised-signals","title":"3. Read denoised signals\u00b6","text":""},{"location":"DeepDenoiser/example_interactive/","title":"Interactive Example","text":"In\u00a0[1]: Copied! <pre>import os, sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy\nimport requests\nsys.path.insert(0, os.path.abspath(\"../\"))\n</pre> import os, sys import numpy as np import matplotlib.pyplot as plt import obspy import requests sys.path.insert(0, os.path.abspath(\"../\")) In\u00a0[2]: Copied! <pre># DEEPDENOISER_API_URL = \"http://127.0.0.1:8002\"\n# DEEPDENOISER_API_URL = \"http://deepdenoiser.quakeflow.com\" # gcp\n# DEEPDENOISER_API_URL = \"http://test.quakeflow.com:8003\" # gcp\nDEEPDENOISER_API_URL = \"https://ai4eps-deepdenoiser.hf.space\"\n# DEEPDENOISER_API_URL = \"http://131.215.74.195:8003\" # local machine\n</pre> # DEEPDENOISER_API_URL = \"http://127.0.0.1:8002\" # DEEPDENOISER_API_URL = \"http://deepdenoiser.quakeflow.com\" # gcp # DEEPDENOISER_API_URL = \"http://test.quakeflow.com:8003\" # gcp DEEPDENOISER_API_URL = \"https://ai4eps-deepdenoiser.hf.space\" # DEEPDENOISER_API_URL = \"http://131.215.74.195:8003\" # local machine In\u00a0[3]: Copied! <pre>import obspy\nstream = obspy.read()\nstream.plot();\n</pre> import obspy stream = obspy.read() stream.plot(); In\u00a0[4]: Copied! <pre>## Extract 3-component data\nstream = stream.sort()\nassert(len(stream) == 3)\ndata = []\nfor trace in stream:\n    data.append(trace.data)\ndata = np.array(data).T\nassert(data.shape[-1] == 3)\ndata_id = stream[0].get_id()[:-1]\ntimestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n\n## Add some noise\nnoisy_data = data + np.random.randn(*data.shape)*np.max(data)/20\n</pre> ## Extract 3-component data stream = stream.sort() assert(len(stream) == 3) data = [] for trace in stream:     data.append(trace.data) data = np.array(data).T assert(data.shape[-1] == 3) data_id = stream[0].get_id()[:-1] timestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  ## Add some noise noisy_data = data + np.random.randn(*data.shape)*np.max(data)/20 In\u00a0[5]: Copied! <pre>req = {\"id\": [data_id],\n       \"timestamp\": [timestamp],\n       \"vec\": [noisy_data.tolist()]}\n\nresp = requests.post(f'{DEEPDENOISER_API_URL}/predict', json=req)\nprint(resp)\n\ndenoised_data = np.array(resp.json()[\"vec\"])\n\nplt.figure(figsize=(10,4))\nplt.subplot(331)\nplt.plot(data[:,0], 'k', linewidth=0.5, label=\"E\")\nplt.legend()\nplt.title(\"Raw signal\")\nplt.subplot(332)\nplt.plot(noisy_data[:,0], 'k', linewidth=0.5, label=\"E\")\nplt.title(\"Nosiy signal\")\nplt.subplot(333)\nplt.plot(denoised_data[0, :,0], 'k', linewidth=0.5, label=\"E\")\nplt.title(\"Denoised signal\")\nplt.subplot(334)\nplt.plot(data[:,1], 'k', linewidth=0.5, label=\"N\")\nplt.legend()\nplt.subplot(335)\nplt.plot(noisy_data[:,1], 'k', linewidth=0.5, label=\"N\")\nplt.subplot(336)\nplt.plot(denoised_data[0,:,1], 'k', linewidth=0.5, label=\"N\")\nplt.subplot(337)\nplt.plot(data[:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.legend()\nplt.subplot(338)\nplt.plot(noisy_data[:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.subplot(339)\nplt.plot(denoised_data[0,:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.tight_layout()\nplt.show();\n</pre> req = {\"id\": [data_id],        \"timestamp\": [timestamp],        \"vec\": [noisy_data.tolist()]}  resp = requests.post(f'{DEEPDENOISER_API_URL}/predict', json=req) print(resp)  denoised_data = np.array(resp.json()[\"vec\"])  plt.figure(figsize=(10,4)) plt.subplot(331) plt.plot(data[:,0], 'k', linewidth=0.5, label=\"E\") plt.legend() plt.title(\"Raw signal\") plt.subplot(332) plt.plot(noisy_data[:,0], 'k', linewidth=0.5, label=\"E\") plt.title(\"Nosiy signal\") plt.subplot(333) plt.plot(denoised_data[0, :,0], 'k', linewidth=0.5, label=\"E\") plt.title(\"Denoised signal\") plt.subplot(334) plt.plot(data[:,1], 'k', linewidth=0.5, label=\"N\") plt.legend() plt.subplot(335) plt.plot(noisy_data[:,1], 'k', linewidth=0.5, label=\"N\") plt.subplot(336) plt.plot(denoised_data[0,:,1], 'k', linewidth=0.5, label=\"N\") plt.subplot(337) plt.plot(data[:,2], 'k', linewidth=0.5, label=\"Z\") plt.legend() plt.subplot(338) plt.plot(noisy_data[:,2], 'k', linewidth=0.5, label=\"Z\") plt.subplot(339) plt.plot(denoised_data[0,:,2], 'k', linewidth=0.5, label=\"Z\") plt.tight_layout() plt.show(); <pre>&lt;Response [200]&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepDenoiser/example_interactive/#interactive-example","title":"Interactive Example\u00b6","text":""},{"location":"DeepDenoiser/example_interactive/#1-run-deepdenoiser-in-terminal-or-use-quakeflow-api","title":"1. Run DeepDenoiser in terminal or use QuakeFlow API\u00b6","text":""},{"location":"DeepDenoiser/example_interactive/#2-prepare-seismic-waveforms","title":"2. Prepare seismic waveforms\u00b6","text":"<p>Find more details in obspy's tutorials:</p> <p>FDSN web service client for ObsPy</p> <p>Mass Downloader for FDSN Compliant Web Services</p>"},{"location":"DeepDenoiser/example_interactive/#3-get-denoised-waveforms-using-deepdenoiser","title":"3. Get denoised waveforms using DeepDenoiser\u00b6","text":""},{"location":"GaMMA/","title":"GaMMA: Gaussian Mixture Model Associator","text":""},{"location":"GaMMA/#1-install","title":"1. Install","text":"<pre><code>pip install git+https://github.com/wayneweiqiang/GaMMA.git\n</code></pre> <p>The implementation is based on the Gaussian mixture models in scikit-learn</p>"},{"location":"GaMMA/#2-related-papers","title":"2. Related papers","text":"<ul> <li>Zhu, Weiqiang et al. \"Earthquake Phase Association using a Bayesian Gaussian Mixture Model.\" (2021)</li> <li>Zhu, Weiqiang, and Gregory C. Beroza. \"PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking Method.\" arXiv preprint arXiv:1803.03211 (2018). </li> </ul>"},{"location":"GaMMA/#3-examples","title":"3. Examples","text":"<ul> <li>Hyperparameters:</li> <li>use_amplitude (default = True): If using amplitude information.</li> <li>vel (default = {\"p\": 6.0, \"s\": 6.0 / 1.75}): velocity for P and S phases.</li> <li>use_dbscan: If using dbscan to cut a long sequence of picks into segments. Using DBSCAN can significantly speed up associaiton using small windows. </li> <li>dbscan_eps (default = 10.0s): The maximum time between two picks for one to be considered as a neighbor of the other. See details in DBSCAN</li> <li>dbscan_min_samples (default = 3): The number of samples in a neighborhood for a point to be considered as a core point. See details in DBSCAN</li> <li>oversampling_factor (default = 10): The initial number of clusters is determined by (Number of picks)/(Number of stations)/(Inital points) * (oversampling factor).</li> <li>initial_points (default=[1,1,1] for (x, y, z) directions): Initial earthquake locations (cluster centers). For a large area over 10 degrees, more initial points are helpful, such as [2,2,1].</li> <li>covariance_prior (default = (5, 5)): covariance prior of time and amplitude residuals. Because current code only uses an uniform velocity model, a large covariance prior can be used to avoid splitting one event into multiple events.</li> <li>Filtering low quality association<ul> <li>min_picks_per_eq: Minimum picks for associated earthquakes. We can also specify minimum P or S picks:</li> <li>min_p_picks_per_eq: Minimum P-picks for associated earthquakes.</li> <li>min_s_picks_per_eq: Minimum S-picks for associated earthquakes.</li> <li>max_sigma11: Max phase time residual (s)</li> <li>max_sigma22: Max phase amplitude residual (in log scale)</li> <li>max_sigma12: Max covariance term. (Usually not used)</li> </ul> </li> </ul> <p>Note the association speed is controlled by dbscan_eps and oversampling_factor. Larger values are preferred, but at the expense of a slower association speed.</p> <ul> <li>Synthetic Example</li> </ul> <p></p> <ul> <li>Example using PhaseNet picks</li> </ul> <p>See details in the notebook: example_phasenet.ipynb</p> <ul> <li>Example using Seisbench</li> </ul> <p>See details in the notebook: example_seisbench.ipynb</p> <p></p> <p>More examples can be found in the earthquake detection workflow -- QuakeFlow</p>"},{"location":"GaMMA/example_fastapi/","title":"Example fastapi","text":"In\u00a0[1]: Copied! <pre>import requests\nimport json\nimport pandas as pd\nimport os\nimport warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\n%load_ext autoreload\n%autoreload 2\n</pre> import requests import json import pandas as pd import os import warnings import numpy as np import matplotlib.pyplot as plt %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># GAMMA_API_URL = \"http://127.0.0.1:8000\"\nGAMMA_API_URL = \"https://ai4eps-gamma.hf.space\"\n</pre> # GAMMA_API_URL = \"http://127.0.0.1:8000\" GAMMA_API_URL = \"https://ai4eps-gamma.hf.space\"  In\u00a0[3]: Copied! <pre># !wget https://github.com/wayneweiqiang/GMMA/releases/download/test_data/test_data.zip\n# !unzip test_data.zip\n</pre> # !wget https://github.com/wayneweiqiang/GMMA/releases/download/test_data/test_data.zip # !unzip test_data.zip In\u00a0[4]: Copied! <pre>data_dir = lambda x: os.path.join(\"test_data\", x)\nstation_csv = data_dir(\"stations.csv\")\npick_json = data_dir(\"picks.json\")\ncatalog_csv = data_dir(\"catalog_gamma.csv\")\npicks_csv = data_dir(\"picks_gamma.csv\")\nif not os.path.exists(\"figures\"):\n    os.makedirs(\"figures\")\nfigure_dir = lambda x: os.path.join(\"figures\", x)\n\n## set config\nconfig = {'xlim_degree': [-118.004, -117.004], \n          'ylim_degree': [35.205, 36.205],\n          'z(km)': [0, 41]}\n\n## read stations\nstations = pd.read_csv(station_csv, delimiter=\"\\t\")\nstations = stations.rename(columns={\"station\":\"station_id\"})\n\n## read picks\npicks = pd.read_json(pick_json).sort_values(\"timestamp\").iloc[:200]\npicks[\"timestamp\"] = pd.to_datetime(picks[\"timestamp\"])\n</pre> data_dir = lambda x: os.path.join(\"test_data\", x) station_csv = data_dir(\"stations.csv\") pick_json = data_dir(\"picks.json\") catalog_csv = data_dir(\"catalog_gamma.csv\") picks_csv = data_dir(\"picks_gamma.csv\") if not os.path.exists(\"figures\"):     os.makedirs(\"figures\") figure_dir = lambda x: os.path.join(\"figures\", x)  ## set config config = {'xlim_degree': [-118.004, -117.004],            'ylim_degree': [35.205, 36.205],           'z(km)': [0, 41]}  ## read stations stations = pd.read_csv(station_csv, delimiter=\"\\t\") stations = stations.rename(columns={\"station\":\"station_id\"})  ## read picks picks = pd.read_json(pick_json).sort_values(\"timestamp\").iloc[:200] picks[\"timestamp\"] = pd.to_datetime(picks[\"timestamp\"])   In\u00a0[5]: Copied! <pre>picks\n</pre> picks Out[5]: id timestamp prob amp type 51368 CI.WCS2..HH 2019-07-04 17:00:00.004 0.371902 1.576248e-06 p 50738 CI.WBM..BH 2019-07-04 17:00:00.004 0.429425 4.883445e-07 p 51254 CI.WCS2..BH 2019-07-04 17:00:00.004 0.570932 1.388111e-06 p 51643 CI.WMF..BH 2019-07-04 17:00:00.004 0.325480 1.930339e-07 p 51727 CI.WMF..HH 2019-07-04 17:00:00.004 0.499070 1.795238e-07 p ... ... ... ... ... ... 52664 CI.WRV2..EH 2019-07-04 17:05:13.634 0.565324 6.402773e-07 p 53676 PB.B918..EH 2019-07-04 17:05:13.754 0.621534 6.146262e-07 s 53083 CI.WVP2..HN 2019-07-04 17:05:13.894 0.891210 4.123632e-06 p 46727 CI.CCC..HN 2019-07-04 17:05:14.004 0.791852 5.821601e-06 s 46543 CI.CCC..HH 2019-07-04 17:05:14.004 0.827445 5.868008e-06 s <p>200 rows \u00d7 5 columns</p> In\u00a0[6]: Copied! <pre>stations\n</pre> stations Out[6]: station_id longitude latitude elevation(m) unit component response 0 CI.CCC..BH -117.365 35.525 670.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 1 CI.CCC..HH -117.365 35.525 670.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 2 CI.CCC..HN -117.365 35.525 670.0 m/s**2 E,N,Z 213979.00,214322.00,213808.00 3 CI.CLC..BH -117.598 35.816 775.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 4 CI.CLC..HH -117.598 35.816 775.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 5 CI.CLC..HN -117.598 35.816 775.0 m/s**2 E,N,Z 213945.00,213808.00,213740.00 6 CI.DTP..BH -117.846 35.267 908.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 7 CI.DTP..HH -117.846 35.267 908.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 8 CI.DTP..HN -117.846 35.267 908.0 m/s**2 E,N,Z 214399.00,213971.00,214484.00 9 CI.JRC2..BH -117.809 35.982 1469.0 m/s E,N,Z 784866000.00,784866000.00,790478000.00 10 CI.JRC2..HH -117.809 35.982 1469.0 m/s E,N,Z 784866000.00,784866000.00,790478000.00 11 CI.JRC2..HN -117.809 35.982 1469.0 m/s**2 E,N,Z 213808.00,213945.00,214185.00 12 CI.LRL..BH -117.682 35.480 1340.0 m/s E,N,Z 628306000.00,629984000.00,627467000.00 13 CI.LRL..HH -117.682 35.480 1340.0 m/s E,N,Z 628306000.00,629984000.00,627467000.00 14 CI.LRL..HN -117.682 35.480 1340.0 m/s**2 E,N,Z 213757.00,213671.00,213201.00 15 CI.LRL.2C.HN -117.682 35.480 1340.0 m/s**2 E,N,Z 213757.00,213671.00,213201.00 16 CI.MPM..BH -117.489 36.058 1839.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 17 CI.MPM..HH -117.489 36.058 1839.0 m/s E,N,Z 627368000.00,627368000.00,627368000.00 18 CI.MPM..HN -117.489 36.058 1839.0 m/s**2 E,N,Z 213911.00,214219.00,213911.00 19 CI.Q0072.01.HN -117.667 35.610 695.0 m/s**2 E,N,Z 256354.00,256354.00,256354.00 20 CI.SLA..BH -117.283 35.891 1174.0 m/s E,N,Z 622338000.00,618992000.00,616482000.00 21 CI.SLA..HH -117.283 35.891 1174.0 m/s E,N,Z 622338000.00,618992000.00,616482000.00 22 CI.SLA..HN -117.283 35.891 1174.0 m/s**2 E,N,Z 214253.00,213671.00,213979.00 23 CI.SRT..BH -117.751 35.692 667.0 m/s E,N,Z 629145000.00,629145000.00,629145000.00 24 CI.SRT..HH -117.751 35.692 667.0 m/s E,N,Z 629145000.00,629145000.00,629145000.00 25 CI.SRT..HN -117.751 35.692 667.0 m/s**2 E,N,Z 214056.00,213628.00,213842.00 26 CI.TOW2..BH -117.765 35.809 685.0 m/s E,N,Z 626910000.00,626910000.00,626838000.00 27 CI.TOW2..HH -117.765 35.809 685.0 m/s E,N,Z 626910000.00,626910000.00,626838000.00 28 CI.TOW2..HN -117.765 35.809 685.0 m/s**2 E,N,Z 213800.00,214142.00,214356.00 29 CI.WBM..BH -117.890 35.608 892.0 m/s E,N,Z 314573000.00,314573000.00,314573000.00 30 CI.WBM..HH -117.890 35.608 892.0 m/s E,N,Z 314573000.00,314573000.00,314573000.00 31 CI.WBM..HN -117.890 35.608 892.0 m/s**2 E,N,Z 213550.00,214064.00,213550.00 32 CI.WBM.2C.HN -117.890 35.608 892.0 m/s**2 E,N,Z 213550.00,214064.00,213550.00 33 CI.WCS2..BH -117.765 36.025 1143.0 m/s E,N,Z 626910000.00,626910000.00,626838000.00 34 CI.WCS2..HH -117.765 36.025 1143.0 m/s E,N,Z 626910000.00,626910000.00,626838000.00 35 CI.WCS2..HN -117.765 36.025 1143.0 m/s**2 E,N,Z 213757.00,213329.00,213415.00 36 CI.WMF..BH -117.855 36.118 1537.4 m/s E,N,Z 625790000.00,627467000.00,625790000.00 37 CI.WMF..HH -117.855 36.118 1537.4 m/s E,N,Z 625790000.00,627467000.00,625790000.00 38 CI.WMF..HN -117.855 36.118 1537.4 m/s**2 E,N,Z 213842.00,213842.00,213842.00 39 CI.WMF.2C.HN -117.855 36.118 1537.4 m/s**2 E,N,Z 213842.00,213842.00,213842.00 40 CI.WNM..EH -117.906 35.842 974.3 m/s Z 69328700.00 41 CI.WNM..HN -117.906 35.842 974.3 m/s**2 E,N,Z 214021.00,213892.00,214021.00 42 CI.WNM.2C.HN -117.906 35.842 974.3 m/s**2 E,N,Z 214039.00,213911.00,214039.00 43 CI.WRC2..BH -117.650 35.948 943.0 m/s E,N,Z 629145000.00,629145000.00,629145000.00 44 CI.WRC2..HH -117.650 35.948 943.0 m/s E,N,Z 629145000.00,629145000.00,629145000.00 45 CI.WRC2..HN -117.650 35.948 943.0 m/s**2 E,N,Z 214227.00,213970.00,214056.00 46 CI.WRV2..EH -117.890 36.008 1070.0 m/s Z 71450700.00 47 CI.WRV2..HN -117.890 36.008 1070.0 m/s**2 E,N,Z 213850.00,235188.00,235102.00 48 CI.WRV2.2C.HN -117.890 36.008 1070.0 m/s**2 E,N,Z 213868.00,235208.00,235122.00 49 CI.WVP2..EH -117.818 35.949 1465.0 m/s Z 68041300.00 50 CI.WVP2..HN -117.818 35.949 1465.0 m/s**2 E,N,Z 213764.00,213550.00,213721.00 51 CI.WVP2.2C.HN -117.818 35.949 1465.0 m/s**2 E,N,Z 213782.00,213569.00,213740.00 52 NP.1809..HN -117.957 36.110 1092.0 m/s**2 E,N,Z 429497.00,429497.00,426141.00 53 NP.5419..HN -117.662 35.649 689.0 m/s**2 E,N,Z 426141.00,429497.00,429497.00 54 PB.B916..EH -117.668 36.193 1859.9 m/s 1,2,Z 781398000.00,781398000.00,781398000.00 55 PB.B917..EH -117.259 35.405 1192.0 m/s 1,2,Z 781398000.00,781398000.00,781398000.00 56 PB.B918..EH -117.602 35.936 1042.6 m/s 1,2,Z 781398000.00,781398000.00,781398000.00 57 PB.B921..EH -117.462 35.587 694.5 m/s 1,2,Z 781398000.00,781398000.00,781398000.00 In\u00a0[7]: Copied! <pre>picks.rename(columns={\"id\": \"station_id\", \"timestamp\": \"phase_time\", \"prob\": \"phase_score\", \"amp\": \"phase_amplitude\", \"type\": \"phase_type\"}, inplace=True)\nstations.rename(columns={\"id\": \"station_id\", \"elevation(m)\": \"elevation_m\"}, inplace=True)\nstations.drop(columns=[\"unit\", \"component\", \"response\"], inplace=True, errors=\"ignore\")\n</pre> picks.rename(columns={\"id\": \"station_id\", \"timestamp\": \"phase_time\", \"prob\": \"phase_score\", \"amp\": \"phase_amplitude\", \"type\": \"phase_type\"}, inplace=True) stations.rename(columns={\"id\": \"station_id\", \"elevation(m)\": \"elevation_m\"}, inplace=True) stations.drop(columns=[\"unit\", \"component\", \"response\"], inplace=True, errors=\"ignore\") In\u00a0[8]: Copied! <pre>picks[\"phase_type\"] = picks[\"phase_type\"].str.upper()\n</pre> picks[\"phase_type\"] = picks[\"phase_type\"].str.upper() In\u00a0[9]: Copied! <pre>picks = picks.merge(stations[[\"station_id\", \"latitude\", \"longitude\", \"elevation_m\"]], on=\"station_id\", how=\"left\")\n</pre> picks = picks.merge(stations[[\"station_id\", \"latitude\", \"longitude\", \"elevation_m\"]], on=\"station_id\", how=\"left\") In\u00a0[10]: Copied! <pre>picks\n</pre> picks Out[10]: station_id phase_time phase_score phase_amplitude phase_type latitude longitude elevation_m 0 CI.WCS2..HH 2019-07-04 17:00:00.004 0.371902 1.576248e-06 P 36.025 -117.765 1143.0 1 CI.WBM..BH 2019-07-04 17:00:00.004 0.429425 4.883445e-07 P 35.608 -117.890 892.0 2 CI.WCS2..BH 2019-07-04 17:00:00.004 0.570932 1.388111e-06 P 36.025 -117.765 1143.0 3 CI.WMF..BH 2019-07-04 17:00:00.004 0.325480 1.930339e-07 P 36.118 -117.855 1537.4 4 CI.WMF..HH 2019-07-04 17:00:00.004 0.499070 1.795238e-07 P 36.118 -117.855 1537.4 ... ... ... ... ... ... ... ... ... 195 CI.WRV2..EH 2019-07-04 17:05:13.634 0.565324 6.402773e-07 P 36.008 -117.890 1070.0 196 PB.B918..EH 2019-07-04 17:05:13.754 0.621534 6.146262e-07 S 35.936 -117.602 1042.6 197 CI.WVP2..HN 2019-07-04 17:05:13.894 0.891210 4.123632e-06 P 35.949 -117.818 1465.0 198 CI.CCC..HN 2019-07-04 17:05:14.004 0.791852 5.821601e-06 S 35.525 -117.365 670.0 199 CI.CCC..HH 2019-07-04 17:05:14.004 0.827445 5.868008e-06 S 35.525 -117.365 670.0 <p>200 rows \u00d7 8 columns</p> In\u00a0[11]: Copied! <pre>picks = picks[(picks[\"phase_time\"] &gt; pd.to_datetime(\"2019-07-04T17:02:00.000\")) &amp; (picks[\"phase_time\"] &lt; pd.to_datetime(\"2019-07-04T17:04:00.000\"))]\n</pre> picks = picks[(picks[\"phase_time\"] &gt; pd.to_datetime(\"2019-07-04T17:02:00.000\")) &amp; (picks[\"phase_time\"] &lt; pd.to_datetime(\"2019-07-04T17:04:00.000\"))] In\u00a0[12]: Copied! <pre>plt.figure()\ncolor = {\"P\": \"red\", \"S\": \"blue\"}\nplt.scatter(picks[\"phase_time\"], picks[\"latitude\"], c=picks[\"phase_type\"].apply(lambda x: color[x]), s=10)\nplt.scatter([], [], c=\"red\", label=\"P\")\nplt.scatter([], [], c=\"blue\", label=\"S\")\nplt.legend()\nplt.show()\n</pre> plt.figure() color = {\"P\": \"red\", \"S\": \"blue\"} plt.scatter(picks[\"phase_time\"], picks[\"latitude\"], c=picks[\"phase_type\"].apply(lambda x: color[x]), s=10) plt.scatter([], [], c=\"red\", label=\"P\") plt.scatter([], [], c=\"blue\", label=\"S\") plt.legend() plt.show() In\u00a0[13]: Copied! <pre>from app import run_gamma\n\nconfig[\"region\"] = \"Ridgecrest\"\nconfig[\"event_index\"] = 1\n\npicks_ = picks.copy()\nevengts_, picks_ = run_gamma(picks, stations, config)\n\nplt.figure()\nmapping_color = lambda x: f\"C{x}\" if x!= -1 else \"black\"\nplt.scatter(picks_[\"phase_time\"], picks_[\"latitude\"], c=picks_[\"event_index\"].apply(mapping_color), s=10)\n</pre> from app import run_gamma  config[\"region\"] = \"Ridgecrest\" config[\"event_index\"] = 1  picks_ = picks.copy() evengts_, picks_ = run_gamma(picks, stations, config)  plt.figure() mapping_color = lambda x: f\"C{x}\" if x!= -1 else \"black\" plt.scatter(picks_[\"phase_time\"], picks_[\"latitude\"], c=picks_[\"event_index\"].apply(mapping_color), s=10) <pre>Associating 108 picks with 1 CPUs\n.</pre> Out[13]: <pre>&lt;matplotlib.collections.PathCollection at 0x320f95d80&gt;</pre> In\u00a0[18]: Copied! <pre>picks_  = picks.copy()\npicks_[\"phase_time\"] = picks_[\"phase_time\"].apply(lambda x: x.isoformat())\nstations_ = stations.copy()\n\npicks_ = picks_.to_dict(orient=\"records\")\nstations_ = stations.to_dict(orient=\"records\")\n\nresponse = requests.post(f\"{GAMMA_API_URL}/predict/\", json={\"picks\": {\"data\":picks_}, \"stations\": {\"data\": stations_}, \"config\": config})\n\nif response.status_code == 200:\n    result = response.json()\n    # Process the result as needed\nelse:\n    print(f\"Request failed with status code: {response.status_code}\")\n    print(f\"Error message: {response.text}\")\n</pre> picks_  = picks.copy() picks_[\"phase_time\"] = picks_[\"phase_time\"].apply(lambda x: x.isoformat()) stations_ = stations.copy()  picks_ = picks_.to_dict(orient=\"records\") stations_ = stations.to_dict(orient=\"records\")  response = requests.post(f\"{GAMMA_API_URL}/predict/\", json={\"picks\": {\"data\":picks_}, \"stations\": {\"data\": stations_}, \"config\": config})  if response.status_code == 200:     result = response.json()     # Process the result as needed else:     print(f\"Request failed with status code: {response.status_code}\")     print(f\"Error message: {response.text}\") In\u00a0[19]: Copied! <pre>events = pd.DataFrame(result[\"events\"])\npicks_ = pd.DataFrame(result[\"picks\"])\npicks_[\"phase_time\"] = pd.to_datetime(picks_[\"phase_time\"])\n\nplt.figure()\nmapping_color = lambda x: f\"C{x}\" if x!= -1 else \"black\"\nplt.scatter(picks_[\"phase_time\"], picks_[\"latitude\"], c=picks_[\"event_index\"].apply(mapping_color), s=10)\n</pre> events = pd.DataFrame(result[\"events\"]) picks_ = pd.DataFrame(result[\"picks\"]) picks_[\"phase_time\"] = pd.to_datetime(picks_[\"phase_time\"])  plt.figure() mapping_color = lambda x: f\"C{x}\" if x!= -1 else \"black\" plt.scatter(picks_[\"phase_time\"], picks_[\"latitude\"], c=picks_[\"event_index\"].apply(mapping_color), s=10) Out[19]: <pre>&lt;matplotlib.collections.PathCollection at 0x325197a00&gt;</pre> In\u00a0[20]: Copied! <pre>events\n</pre> events Out[20]: time magnitude sigma_time sigma_amp cov_time_amp gamma_score num_picks num_p_picks num_s_picks event_index longitude latitude depth_km 0 2019-07-04T17:02:55.008 4.339750 0.326146 0.328015 0.050179 98.0 98 49 49 1 -117.495965 35.710353 16.201632 1 2019-07-04T17:03:20.759 2.352815 0.671548 0.509110 0.054984 6.0 6 6 0 2 -117.964128 35.615117 0.000000 In\u00a0[21]: Copied! <pre>picks_\n</pre> picks_ Out[21]: station_id phase_time phase_score phase_amplitude phase_type latitude longitude elevation_m event_index gamma_score 0 CI.WCS2..HN 2019-07-04 17:02:24.474 0.345736 0.000003 S 36.025 -117.765 1143.0 -1 -1.000000 1 PB.B921..EH 2019-07-04 17:02:58.304 0.945370 0.000961 P 35.587 -117.462 694.5 1 0.000156 2 CI.CLC..BH 2019-07-04 17:02:58.494 0.976444 0.002152 P 35.816 -117.598 775.0 1 0.230913 3 CI.CLC..HN 2019-07-04 17:02:58.504 0.968991 0.002354 P 35.816 -117.598 775.0 1 0.748433 4 CI.CLC..HH 2019-07-04 17:02:58.504 0.979083 0.002593 P 35.816 -117.598 775.0 1 0.788957 ... ... ... ... ... ... ... ... ... ... ... 103 CI.WCS2..HN 2019-07-04 17:03:28.354 0.496533 0.000056 P 36.025 -117.765 1143.0 2 0.824170 104 CI.JRC2..HN 2019-07-04 17:03:28.714 0.383873 0.000084 P 35.982 -117.809 1469.0 2 0.979752 105 CI.WNM..HN 2019-07-04 17:03:40.234 0.313130 0.000019 P 35.842 -117.906 974.3 -1 -1.000000 106 CI.MPM..HN 2019-07-04 17:03:42.394 0.373462 0.000028 P 36.058 -117.489 1839.0 -1 -1.000000 107 CI.DTP..HN 2019-07-04 17:03:43.354 0.401861 0.000014 P 35.267 -117.846 908.0 -1 -1.000000 <p>108 rows \u00d7 10 columns</p>"},{"location":"GaMMA/example_fastapi/#interactive-example","title":"Interactive Example\u00b6","text":""},{"location":"GaMMA/example_fastapi/#2-prepare-test-data","title":"2. Prepare test data\u00b6","text":"<ul> <li>Download test data: PhaseNet picks of the 2019 Ridgecrest earthquake sequence</li> </ul> <ol> <li>picks file: picks.json</li> <li>station information: stations.csv</li> <li>events in SCSN catalog: events.csv</li> <li>config file: config.pkl</li> </ol> <pre>wget https://github.com/wayneweiqiang/GMMA/releases/download/test_data/test_data.zip\nunzip test_data.zip\n</pre>"},{"location":"GaMMA/example_phasenet/","title":"PhaseNet Example","text":"In\u00a0[1]: Copied! <pre>!pip install git+https://github.com/wayneweiqiang/GaMMA.git\n</pre> !pip install git+https://github.com/wayneweiqiang/GaMMA.git In\u00a0[2]: Copied! <pre>import pandas as pd\nfrom gamma.utils import association, estimate_eps\nimport numpy as np\nimport os\nfrom pyproj import Proj\n</pre> import pandas as pd from gamma.utils import association, estimate_eps import numpy as np import os from pyproj import Proj  In\u00a0[3]: Copied! <pre># !if [ -f demo.tar ]; then rm demo.tar; fi\n# !if [ -d test_data ]; then rm -rf test_data; fi\n!wget -q https://github.com/AI4EPS/GaMMA/releases/download/test_data/demo.tar\n!tar -xf demo.tar\n</pre> # !if [ -f demo.tar ]; then rm demo.tar; fi # !if [ -d test_data ]; then rm -rf test_data; fi !wget -q https://github.com/AI4EPS/GaMMA/releases/download/test_data/demo.tar !tar -xf demo.tar In\u00a0[4]: Copied! <pre>region = \"ridgecrest\"\n# region = \"chile\"\ndata_path = lambda x: os.path.join(f\"test_data/{region}\", x)\nresult_path = f\"results/{region}\"\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nresult_path = lambda x: os.path.join(f\"results/{region}\", x)\nstation_csv = data_path(\"stations.csv\")\npicks_csv = data_path(\"picks.csv\")\nif not os.path.exists(\"figures\"):\n    os.makedirs(\"figures\")\nfigure_dir = lambda x: os.path.join(\"figures\", x)\n</pre> region = \"ridgecrest\" # region = \"chile\" data_path = lambda x: os.path.join(f\"test_data/{region}\", x) result_path = f\"results/{region}\" if not os.path.exists(result_path):     os.makedirs(result_path) result_path = lambda x: os.path.join(f\"results/{region}\", x) station_csv = data_path(\"stations.csv\") picks_csv = data_path(\"picks.csv\") if not os.path.exists(\"figures\"):     os.makedirs(\"figures\") figure_dir = lambda x: os.path.join(\"figures\", x) In\u00a0[5]: Copied! <pre>## read picks\npicks = pd.read_csv(picks_csv, parse_dates=[\"phase_time\"])\npicks.rename(columns={\"station_id\": \"id\", \"phase_time\": \"timestamp\", \"phase_type\": \"type\", \"phase_score\": \"prob\", \"phase_amplitude\": \"amp\"}, inplace=True)\nprint(\"Pick format:\", picks.iloc[:10])\n\n## read stations\nstations = pd.read_csv(station_csv)\nstations.rename(columns={\"station_id\": \"id\"}, inplace=True)\nprint(\"Station format:\", stations.iloc[:10])\n\n## Automatic region; you can also specify a region\nx0 = stations[\"longitude\"].median()\ny0 = stations[\"latitude\"].median()\nxmin = stations[\"longitude\"].min()\nxmax = stations[\"longitude\"].max()\nymin = stations[\"latitude\"].min()\nymax = stations[\"latitude\"].max()\nconfig = {}\nconfig[\"center\"] = (x0, y0)\nconfig[\"xlim_degree\"] = (2 * xmin - x0, 2 * xmax - x0)\nconfig[\"ylim_degree\"] = (2 * ymin - y0, 2 * ymax - y0)\n\n## projection to km\nproj = Proj(f\"+proj=aeqd +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\nstations[[\"x(km)\", \"y(km)\"]] = stations.apply(lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1)\nstations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x/1e3)\n\n### setting GMMA configs\nconfig[\"use_dbscan\"] = True\nif region == \"chile\":\n    config[\"use_amplitude\"] = False\nelse:\n    config[\"use_amplitude\"] = True\nconfig[\"method\"] = \"BGMM\"  \nif config[\"method\"] == \"BGMM\": ## BayesianGaussianMixture\n    config[\"oversample_factor\"] = 5\nif config[\"method\"] == \"GMM\": ## GaussianMixture\n    config[\"oversample_factor\"] = 1\n#!!Prior for (time and amplitude): set a larger value to prevent splitting events. A too large value will cause merging events.\n# config[\"covariance_prior\"] = [5.0, 2.0] \n\n# earthquake location\nconfig[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\nconfig[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\nconfig[\"x(km)\"] = proj(longitude=config[\"xlim_degree\"], latitude=[config[\"center\"][1]] * 2)[0]\nconfig[\"y(km)\"] = proj(longitude=[config[\"center\"][0]] * 2, latitude=config[\"ylim_degree\"])[1]\nif region == \"ridgecrest\":\n    config[\"z(km)\"] = (0, 20)\nelif region == \"chile\":\n    config[\"z(km)\"] = (0, 250)\nelse:\n    print(\"Please specify z(km) for your region\")\n    raise NotImplementedError\nconfig[\"bfgs_bounds\"] = (\n    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n    (0, config[\"z(km)\"][1] + 1),  # z\n    (None, None),  # t\n)\n\n# DBSCAN: \n##!!Truncate the picks into segments: change the dbscan_eps to balance speed and event splitting. A larger eps prevent spliting events but can take longer time in the preprocessing step.\nconfig[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"]) \nconfig[\"dbscan_min_samples\"] = 3\n\n## using Eikonal for 1D velocity model\nif region == \"ridgecrest\":\n    zz = [0.0, 5.5, 16.0, 32.0]\n    vp = [5.5, 5.5,  6.7,  7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    h = 1.0\n    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\nelif region == \"chile\":\n    velocity_model = pd.read_csv(data_path(\"iasp91.csv\"), names=[\"zz\", \"rho\", \"vp\", \"vs\"])\n    velocity_model = velocity_model[velocity_model[\"zz\"] &lt;= config[\"z(km)\"][1]]\n    vel = {\"z\": velocity_model[\"zz\"].values, \"p\": velocity_model[\"vp\"].values, \"s\": velocity_model[\"vs\"].values}\n    h = 1.0\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\nelse:\n    print(\"Using uniform velocity model\")\n\n\n# set number of cpus\nconfig[\"ncpu\"] = 32\n\n##!!Post filtering (indepent of gmm): change these parameters to filter out associted picks with large errors\nconfig[\"min_picks_per_eq\"] = 5\nconfig[\"min_p_picks_per_eq\"] = 0\nconfig[\"min_s_picks_per_eq\"] = 0\nconfig[\"max_sigma11\"] = 3.0 # second\nconfig[\"max_sigma22\"] = 1.0 # log10(m/s)\nconfig[\"max_sigma12\"] = 1.0 # covariance\n\n## filter picks without amplitude measurements\nif config[\"use_amplitude\"]:\n    picks = picks[picks[\"amp\"] != -1]\n\nfor k, v in config.items():\n    print(f\"{k}: {v}\")\n</pre> ## read picks picks = pd.read_csv(picks_csv, parse_dates=[\"phase_time\"]) picks.rename(columns={\"station_id\": \"id\", \"phase_time\": \"timestamp\", \"phase_type\": \"type\", \"phase_score\": \"prob\", \"phase_amplitude\": \"amp\"}, inplace=True) print(\"Pick format:\", picks.iloc[:10])  ## read stations stations = pd.read_csv(station_csv) stations.rename(columns={\"station_id\": \"id\"}, inplace=True) print(\"Station format:\", stations.iloc[:10])  ## Automatic region; you can also specify a region x0 = stations[\"longitude\"].median() y0 = stations[\"latitude\"].median() xmin = stations[\"longitude\"].min() xmax = stations[\"longitude\"].max() ymin = stations[\"latitude\"].min() ymax = stations[\"latitude\"].max() config = {} config[\"center\"] = (x0, y0) config[\"xlim_degree\"] = (2 * xmin - x0, 2 * xmax - x0) config[\"ylim_degree\"] = (2 * ymin - y0, 2 * ymax - y0)  ## projection to km proj = Proj(f\"+proj=aeqd +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\") stations[[\"x(km)\", \"y(km)\"]] = stations.apply(lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1) stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x/1e3)  ### setting GMMA configs config[\"use_dbscan\"] = True if region == \"chile\":     config[\"use_amplitude\"] = False else:     config[\"use_amplitude\"] = True config[\"method\"] = \"BGMM\"   if config[\"method\"] == \"BGMM\": ## BayesianGaussianMixture     config[\"oversample_factor\"] = 5 if config[\"method\"] == \"GMM\": ## GaussianMixture     config[\"oversample_factor\"] = 1 #!!Prior for (time and amplitude): set a larger value to prevent splitting events. A too large value will cause merging events. # config[\"covariance_prior\"] = [5.0, 2.0]   # earthquake location config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75} config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)'] config[\"x(km)\"] = proj(longitude=config[\"xlim_degree\"], latitude=[config[\"center\"][1]] * 2)[0] config[\"y(km)\"] = proj(longitude=[config[\"center\"][0]] * 2, latitude=config[\"ylim_degree\"])[1] if region == \"ridgecrest\":     config[\"z(km)\"] = (0, 20) elif region == \"chile\":     config[\"z(km)\"] = (0, 250) else:     print(\"Please specify z(km) for your region\")     raise NotImplementedError config[\"bfgs_bounds\"] = (     (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x     (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y     (0, config[\"z(km)\"][1] + 1),  # z     (None, None),  # t )  # DBSCAN:  ##!!Truncate the picks into segments: change the dbscan_eps to balance speed and event splitting. A larger eps prevent spliting events but can take longer time in the preprocessing step. config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  config[\"dbscan_min_samples\"] = 3  ## using Eikonal for 1D velocity model if region == \"ridgecrest\":     zz = [0.0, 5.5, 16.0, 32.0]     vp = [5.5, 5.5,  6.7,  7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     h = 1.0     vel = {\"z\": zz, \"p\": vp, \"s\": vs}     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]} elif region == \"chile\":     velocity_model = pd.read_csv(data_path(\"iasp91.csv\"), names=[\"zz\", \"rho\", \"vp\", \"vs\"])     velocity_model = velocity_model[velocity_model[\"zz\"] &lt;= config[\"z(km)\"][1]]     vel = {\"z\": velocity_model[\"zz\"].values, \"p\": velocity_model[\"vp\"].values, \"s\": velocity_model[\"vs\"].values}     h = 1.0     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]} else:     print(\"Using uniform velocity model\")   # set number of cpus config[\"ncpu\"] = 32  ##!!Post filtering (indepent of gmm): change these parameters to filter out associted picks with large errors config[\"min_picks_per_eq\"] = 5 config[\"min_p_picks_per_eq\"] = 0 config[\"min_s_picks_per_eq\"] = 0 config[\"max_sigma11\"] = 3.0 # second config[\"max_sigma22\"] = 1.0 # log10(m/s) config[\"max_sigma12\"] = 1.0 # covariance  ## filter picks without amplitude measurements if config[\"use_amplitude\"]:     picks = picks[picks[\"amp\"] != -1]  for k, v in config.items():     print(f\"{k}: {v}\")  <pre>Pick format:            id               timestamp      prob       amp type\n0  CI.CCC..BH 2019-07-04 22:00:06.084  0.939738  0.000017    P\n1  CI.CCC..BH 2019-07-04 22:00:31.934  0.953992  0.000006    P\n2  CI.CCC..BH 2019-07-04 22:00:38.834  0.837302  0.000006    P\n3  CI.CCC..BH 2019-07-04 22:01:15.654  0.881130  0.000005    P\n4  CI.CCC..BH 2019-07-04 22:01:32.054  0.869447  0.000004    P\n5  CI.CCC..BH 2019-07-04 22:01:45.574  0.921890  0.000007    P\n6  CI.CCC..BH 2019-07-04 22:02:39.534  0.973068  0.000860    P\n7  CI.CCC..BH 2019-07-04 22:03:36.054  0.972033  0.000030    P\n8  CI.CCC..BH 2019-07-04 22:03:44.014  0.532193  0.000038    P\n9  CI.CCC..BH 2019-07-04 22:04:13.174  0.953794  0.000026    P\nStation format:             id  longitude  latitude  elevation_m\n0   CI.CCC..BH   -117.365    35.525        670.0\n1   CI.CCC..HH   -117.365    35.525        670.0\n2   CI.CCC..HN   -117.365    35.525        670.0\n3   CI.CLC..BH   -117.598    35.816        775.0\n4   CI.CLC..HH   -117.598    35.816        775.0\n5   CI.CLC..HN   -117.598    35.816        775.0\n6   CI.DTP..BH   -117.846    35.267        908.0\n7   CI.DTP..HH   -117.846    35.267        908.0\n8   CI.DTP..HN   -117.846    35.267        908.0\n9  CI.JRC2..BH   -117.809    35.982       1469.0\ncenter: (-117.765, 35.842)\nxlim_degree: (-118.14899999999999, -116.753)\nylim_degree: (34.69200000000001, 36.544)\nuse_dbscan: True\nuse_amplitude: True\nmethod: BGMM\noversample_factor: 5\nvel: {'p': 6.0, 's': 3.4285714285714284}\ndims: ['x(km)', 'y(km)', 'z(km)']\nx(km): (-34.691783955538526, 91.4272532846083)\ny(km): (-127.59157172611789, 77.89670610884579)\nz(km): (0, 20)\nbfgs_bounds: ((-35.691783955538526, 92.4272532846083), (-128.59157172611788, 78.89670610884579), (0, 21), (None, None))\ndbscan_eps: 6.985571894222694\ndbscan_min_samples: 3\neikonal: {'vel': {'z': [0.0, 5.5, 16.0, 32.0], 'p': [5.5, 5.5, 6.7, 7.8], 's': [3.179190751445087, 3.179190751445087, 3.8728323699421967, 4.508670520231214]}, 'h': 1.0, 'xlim': (-34.691783955538526, 91.4272532846083), 'ylim': (-127.59157172611789, 77.89670610884579), 'zlim': (0, 20)}\nncpu: 32\nmin_picks_per_eq: 5\nmin_p_picks_per_eq: 0\nmin_s_picks_per_eq: 0\nmax_sigma11: 3.0\nmax_sigma22: 1.0\nmax_sigma12: 1.0\n</pre> In\u00a0[6]: Copied! <pre>event_idx0 = 0 ## current earthquake index\nassignments = []\nevents, assignments = association(picks, stations, config, event_idx0, config[\"method\"])\nevent_idx0 += len(events)\n\n## create catalog\nevents = pd.DataFrame(events)\nevents[[\"longitude\",\"latitude\"]] = events.apply(lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1)\nevents[\"depth_km\"] = events[\"z(km)\"]\nevents.to_csv(result_path(\"gamma_events.csv\"), index=False, \n                float_format=\"%.3f\",\n                date_format='%Y-%m-%dT%H:%M:%S.%f')\n\n## add assignment to picks\nassignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\npicks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({'event_index': int})\npicks.rename(columns={\"id\": \"station_id\", \"timestamp\": \"phase_time\", \"type\": \"phase_type\", \"prob\": \"phase_score\", \"amp\": \"phase_amplitude\"}, inplace=True)\npicks.to_csv(result_path(\"gamma_picks.csv\"), index=False, \n                date_format='%Y-%m-%dT%H:%M:%S.%f')\n</pre> event_idx0 = 0 ## current earthquake index assignments = [] events, assignments = association(picks, stations, config, event_idx0, config[\"method\"]) event_idx0 += len(events)  ## create catalog events = pd.DataFrame(events) events[[\"longitude\",\"latitude\"]] = events.apply(lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1) events[\"depth_km\"] = events[\"z(km)\"] events.to_csv(result_path(\"gamma_events.csv\"), index=False,                  float_format=\"%.3f\",                 date_format='%Y-%m-%dT%H:%M:%S.%f')  ## add assignment to picks assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"]) picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({'event_index': int}) picks.rename(columns={\"id\": \"station_id\", \"timestamp\": \"phase_time\", \"type\": \"phase_type\", \"prob\": \"phase_score\", \"amp\": \"phase_amplitude\"}, inplace=True) picks.to_csv(result_path(\"gamma_picks.csv\"), index=False,                  date_format='%Y-%m-%dT%H:%M:%S.%f') <pre>Eikonal Solver: \nIter 0, error = 999.818\nIter 1, error = 0.000\nTime: 3.300\nEikonal Solver: \nIter 0, error = 999.685\nIter 1, error = 0.000\nTime: 0.005\nAssociating 553 clusters with 16 CPUs\n....................................................................\nAssociated 100 events\n..............................\nAssociated 200 events\n..................\nAssociated 300 events\n...................\nAssociated 400 events\n................\nAssociated 500 events\n.............................\nAssociated 600 events\n......................\nAssociated 700 events.\n.....................................\nAssociated 800 events\n.................................\nAssociated 900 events\n.........................\nAssociated 1000 events\n..............................\nAssociated 1100 events\n..............................\nAssociated 1200 events\n..............................\nAssociated 1300 events\n..................................\nAssociated 1400 events\n..................................\nAssociated 1500 events\n....................\nAssociated 1600 events\n.....................\nAssociated 1700 events\n..........................\nAssociated 1800 events\n........................\nAssociated 1900 events\n......\nAssociated 2000 events\n\nAssociated 2100 events\n</pre> In\u00a0[7]: Copied! <pre># %%\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm import tqdm\nimport pandas as pd\n\nevents = pd.read_csv(result_path(\"gamma_events.csv\"))\npicks = pd.read_csv(result_path(\"gamma_picks.csv\"))\nstations = pd.read_csv(data_path(\"stations.csv\"))\n\nMIN_NEAREST_STATION_RATIO = 0.3\n\n# %%\nstations = stations[stations[\"station_id\"].isin(picks[\"station_id\"].unique())]\n\nneigh = NearestNeighbors(n_neighbors=min(len(stations), 10))\nneigh.fit(stations[[\"longitude\", \"latitude\"]].values)\n\n# %%\npicks = picks.merge(events[[\"event_index\", \"longitude\", \"latitude\"]], on=\"event_index\", suffixes=(\"\", \"_event\"))\npicks = picks.merge(stations[[\"station_id\", \"longitude\", \"latitude\"]], on=\"station_id\", suffixes=(\"\", \"_station\"))\n\n# %%\nfiltered_events = []\nfor i, event in tqdm(events.iterrows(), total=len(events)):\n    sid = neigh.kneighbors([[event[\"longitude\"], event[\"latitude\"]]])[1][0]\n    picks_ = picks[picks[\"event_index\"] == event[\"event_index\"]]\n    # longitude, latitude = picks_[[\"longitude\", \"latitude\"]].mean().values\n    # sid = neigh.kneighbors([[longitude, latitude]])[1][0]\n    stations_neigh = stations.iloc[sid][\"station_id\"].values\n    picks_neigh = picks_[picks_[\"station_id\"].isin(stations_neigh)]\n    stations_with_picks = picks_neigh[\"station_id\"].unique()\n    if len(stations_with_picks) / len(stations_neigh) &gt; MIN_NEAREST_STATION_RATIO:\n        filtered_events.append(event)\n\n# %%\nprint(f\"Events before filtering: {len(events)}\")\nprint(f\"Events after filtering: {len(filtered_events)}\")\nfiltered_events = pd.DataFrame(filtered_events)\nos.system(f\"mv {result_path('gamma_events.csv')} {result_path('gamma_events_raw.csv')}\")\nfiltered_events.to_csv(result_path(\"gamma_events.csv\"), index=False)\n</pre> # %% from sklearn.neighbors import NearestNeighbors from tqdm import tqdm import pandas as pd  events = pd.read_csv(result_path(\"gamma_events.csv\")) picks = pd.read_csv(result_path(\"gamma_picks.csv\")) stations = pd.read_csv(data_path(\"stations.csv\"))  MIN_NEAREST_STATION_RATIO = 0.3  # %% stations = stations[stations[\"station_id\"].isin(picks[\"station_id\"].unique())]  neigh = NearestNeighbors(n_neighbors=min(len(stations), 10)) neigh.fit(stations[[\"longitude\", \"latitude\"]].values)  # %% picks = picks.merge(events[[\"event_index\", \"longitude\", \"latitude\"]], on=\"event_index\", suffixes=(\"\", \"_event\")) picks = picks.merge(stations[[\"station_id\", \"longitude\", \"latitude\"]], on=\"station_id\", suffixes=(\"\", \"_station\"))  # %% filtered_events = [] for i, event in tqdm(events.iterrows(), total=len(events)):     sid = neigh.kneighbors([[event[\"longitude\"], event[\"latitude\"]]])[1][0]     picks_ = picks[picks[\"event_index\"] == event[\"event_index\"]]     # longitude, latitude = picks_[[\"longitude\", \"latitude\"]].mean().values     # sid = neigh.kneighbors([[longitude, latitude]])[1][0]     stations_neigh = stations.iloc[sid][\"station_id\"].values     picks_neigh = picks_[picks_[\"station_id\"].isin(stations_neigh)]     stations_with_picks = picks_neigh[\"station_id\"].unique()     if len(stations_with_picks) / len(stations_neigh) &gt; MIN_NEAREST_STATION_RATIO:         filtered_events.append(event)  # %% print(f\"Events before filtering: {len(events)}\") print(f\"Events after filtering: {len(filtered_events)}\") filtered_events = pd.DataFrame(filtered_events) os.system(f\"mv {result_path('gamma_events.csv')} {result_path('gamma_events_raw.csv')}\") filtered_events.to_csv(result_path(\"gamma_events.csv\"), index=False)  <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2107/2107 [00:02&lt;00:00, 801.78it/s]</pre> <pre>Events before filtering: 2107\nEvents after filtering: 1404\n</pre> <pre>\n</pre> In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nresult_label=\"GaMMA\"\ncatalog_label=\"Standard\"\n</pre> import matplotlib.pyplot as plt import matplotlib.dates as mdates result_label=\"GaMMA\" catalog_label=\"Standard\" In\u00a0[9]: Copied! <pre>stations = pd.read_csv(data_path(\"stations.csv\"))\ngamma_events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"])\n\nif os.path.exists(data_path(\"standard_catalog.csv\")):\n    standard_catalog = pd.read_csv(data_path(\"standard_catalog.csv\"), parse_dates=[\"time\"])\n    starttime = standard_catalog[\"time\"].min()\n    endtime = standard_catalog[\"time\"].max()\nelse:\n    standard_catalog = None\n    starttime = gamma_events[\"time\"].min()\n    endtime = gamma_events[\"time\"].max()\n\n\nplt.figure()\nplt.hist(gamma_events[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=1.0, linewidth=0.5, label=f\"{result_label}: {len(gamma_events['time'])}\")\nif standard_catalog is not None:\n    plt.hist(standard_catalog[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=0.6, linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['time'])}\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Date\")\nplt.gca().autoscale(enable=True, axis='x', tight=True)\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\nplt.gcf().autofmt_xdate()\nplt.legend()\nplt.savefig(figure_dir(\"earthquake_number.png\"), bbox_inches=\"tight\", dpi=300)\nplt.savefig(figure_dir(\"earthquake_number.pdf\"), bbox_inches=\"tight\")\nplt.show();\n</pre> stations = pd.read_csv(data_path(\"stations.csv\")) gamma_events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"])  if os.path.exists(data_path(\"standard_catalog.csv\")):     standard_catalog = pd.read_csv(data_path(\"standard_catalog.csv\"), parse_dates=[\"time\"])     starttime = standard_catalog[\"time\"].min()     endtime = standard_catalog[\"time\"].max() else:     standard_catalog = None     starttime = gamma_events[\"time\"].min()     endtime = gamma_events[\"time\"].max()   plt.figure() plt.hist(gamma_events[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=1.0, linewidth=0.5, label=f\"{result_label}: {len(gamma_events['time'])}\") if standard_catalog is not None:     plt.hist(standard_catalog[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=0.6, linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['time'])}\") plt.ylabel(\"Frequency\") plt.xlabel(\"Date\") plt.gca().autoscale(enable=True, axis='x', tight=True) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H')) plt.gcf().autofmt_xdate() plt.legend() plt.savefig(figure_dir(\"earthquake_number.png\"), bbox_inches=\"tight\", dpi=300) plt.savefig(figure_dir(\"earthquake_number.pdf\"), bbox_inches=\"tight\") plt.show(); In\u00a0[10]: Copied! <pre>fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([1.5,1]))\nbox = dict(boxstyle='round', facecolor='white', alpha=1)\ntext_loc = [0.05, 0.92]\ngrd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1,1])\nfig.add_subplot(grd[:, 0])\nplt.plot(gamma_events[\"longitude\"], gamma_events[\"latitude\"], '.',markersize=2, alpha=1.0)\nif standard_catalog is not None:\n    plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"latitude\"], '.', markersize=2, alpha=0.6)\nplt.axis(\"scaled\")\nplt.xlim(np.array(config[\"xlim_degree\"]))\nplt.ylim(np.array(config[\"ylim_degree\"]))\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Longitude\")\nplt.gca().set_prop_cycle(None)\nplt.plot([], [], '.', markersize=10, label=f\"{result_label}\", rasterized=True)\nplt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\", rasterized=True)\nplt.plot(stations[\"longitude\"], stations[\"latitude\"], 'k^', markersize=5, alpha=0.7, label=\"Stations\")\nplt.legend(loc=\"lower right\")\nplt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n\nfig.add_subplot(grd[0, 1])\nplt.plot(gamma_events[\"longitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True)\nif standard_catalog is not None:\n    plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True)\nplt.xlim(np.array(config[\"xlim_degree\"])+np.array([0.2,-0.27]))\nplt.ylim(config[\"z(km)\"])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Depth (km)\")\nplt.gca().set_prop_cycle(None)\nplt.plot([], [], '.', markersize=10, label=f\"{result_label}\")\nplt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\")\nplt.legend(loc=\"lower right\")\nplt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n\nfig.add_subplot(grd[1, 1])\nplt.plot(gamma_events[\"latitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True)\nif standard_catalog is not None:\n    plt.plot(standard_catalog[\"latitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True)\nplt.xlim(np.array(config[\"ylim_degree\"])+np.array([0.2,-0.27]))\nplt.ylim(config[\"z(km)\"])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Depth (km)\")\nplt.gca().set_prop_cycle(None)\nplt.plot([], [], '.', markersize=10, label=f\"{result_label}\")\nplt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\")\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.savefig(figure_dir(\"earthquake_location.png\"), bbox_inches=\"tight\", dpi=300)\nplt.savefig(figure_dir(\"earthquake_location.pdf\"), bbox_inches=\"tight\", dpi=300)\nplt.show();\n</pre> fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([1.5,1])) box = dict(boxstyle='round', facecolor='white', alpha=1) text_loc = [0.05, 0.92] grd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1,1]) fig.add_subplot(grd[:, 0]) plt.plot(gamma_events[\"longitude\"], gamma_events[\"latitude\"], '.',markersize=2, alpha=1.0) if standard_catalog is not None:     plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"latitude\"], '.', markersize=2, alpha=0.6) plt.axis(\"scaled\") plt.xlim(np.array(config[\"xlim_degree\"])) plt.ylim(np.array(config[\"ylim_degree\"])) plt.xlabel(\"Latitude\") plt.ylabel(\"Longitude\") plt.gca().set_prop_cycle(None) plt.plot([], [], '.', markersize=10, label=f\"{result_label}\", rasterized=True) plt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\", rasterized=True) plt.plot(stations[\"longitude\"], stations[\"latitude\"], 'k^', markersize=5, alpha=0.7, label=\"Stations\") plt.legend(loc=\"lower right\") plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)  fig.add_subplot(grd[0, 1]) plt.plot(gamma_events[\"longitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True) if standard_catalog is not None:     plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True) plt.xlim(np.array(config[\"xlim_degree\"])+np.array([0.2,-0.27])) plt.ylim(config[\"z(km)\"]) plt.gca().invert_yaxis() plt.xlabel(\"Longitude\") plt.ylabel(\"Depth (km)\") plt.gca().set_prop_cycle(None) plt.plot([], [], '.', markersize=10, label=f\"{result_label}\") plt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\") plt.legend(loc=\"lower right\") plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)  fig.add_subplot(grd[1, 1]) plt.plot(gamma_events[\"latitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True) if standard_catalog is not None:     plt.plot(standard_catalog[\"latitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True) plt.xlim(np.array(config[\"ylim_degree\"])+np.array([0.2,-0.27])) plt.ylim(config[\"z(km)\"]) plt.gca().invert_yaxis() plt.xlabel(\"Latitude\") plt.ylabel(\"Depth (km)\") plt.gca().set_prop_cycle(None) plt.plot([], [], '.', markersize=10, label=f\"{result_label}\") plt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\") plt.legend(loc=\"lower right\") plt.tight_layout() plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.savefig(figure_dir(\"earthquake_location.png\"), bbox_inches=\"tight\", dpi=300) plt.savefig(figure_dir(\"earthquake_location.pdf\"), bbox_inches=\"tight\", dpi=300) plt.show(); In\u00a0[11]: Copied! <pre>if standard_catalog is not None:\n    range = (0, standard_catalog[\"magnitude\"].max())\nelse:\n    range = (-1, gamma_events[\"magnitude\"].max())\nif (gamma_events[\"magnitude\"] != 999).any():\n    plt.figure()\n    plt.hist(gamma_events[\"magnitude\"], range=range, bins=25, alpha=1.0,  edgecolor=\"k\", linewidth=0.5, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")\n    if standard_catalog is not None:\n        plt.hist(standard_catalog[\"magnitude\"], range=range, bins=25, alpha=0.6,  edgecolor=\"k\", linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")\n    plt.legend()\n    plt.xlim([-1,standard_catalog[\"magnitude\"].max()])\n    plt.xlabel(\"Magnitude\")\n    plt.ylabel(\"Frequency\")\n    plt.gca().set_yscale('log')\n    plt.savefig(figure_dir(\"earthquake_magnitude_frequency.png\"), bbox_inches=\"tight\", dpi=300)\n    plt.savefig(figure_dir(\"earthquake_magnitude_frequency.pdf\"), bbox_inches=\"tight\")\n    plt.show();\n</pre> if standard_catalog is not None:     range = (0, standard_catalog[\"magnitude\"].max()) else:     range = (-1, gamma_events[\"magnitude\"].max()) if (gamma_events[\"magnitude\"] != 999).any():     plt.figure()     plt.hist(gamma_events[\"magnitude\"], range=range, bins=25, alpha=1.0,  edgecolor=\"k\", linewidth=0.5, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")     if standard_catalog is not None:         plt.hist(standard_catalog[\"magnitude\"], range=range, bins=25, alpha=0.6,  edgecolor=\"k\", linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")     plt.legend()     plt.xlim([-1,standard_catalog[\"magnitude\"].max()])     plt.xlabel(\"Magnitude\")     plt.ylabel(\"Frequency\")     plt.gca().set_yscale('log')     plt.savefig(figure_dir(\"earthquake_magnitude_frequency.png\"), bbox_inches=\"tight\", dpi=300)     plt.savefig(figure_dir(\"earthquake_magnitude_frequency.pdf\"), bbox_inches=\"tight\")     plt.show(); In\u00a0[12]: Copied! <pre>if (gamma_events[\"magnitude\"] != 999).any():\n    plt.figure()\n    plt.plot(gamma_events[\"time\"], gamma_events[\"magnitude\"], '.', markersize=5, alpha=1.0, rasterized=True)\n    if standard_catalog is not None:\n        plt.plot(standard_catalog[\"time\"], standard_catalog[\"magnitude\"], '.', markersize=5, alpha=0.8, rasterized=True)\n    plt.xlim([starttime, endtime])\n    ylim = plt.ylim()\n    plt.ylabel(\"Magnitude\")\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\n    plt.gcf().autofmt_xdate()\n    plt.gca().set_prop_cycle(None)\n    plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")\n    plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")\n    plt.legend()\n    plt.ylim(ylim)\n    plt.grid()\n    plt.savefig(figure_dir(\"earthquake_magnitude_time.png\"), bbox_inches=\"tight\", dpi=300)\n    plt.savefig(figure_dir(\"earthquake_magnitude_time.pdf\"), bbox_inches=\"tight\", dpi=300)\n    plt.show();\n</pre> if (gamma_events[\"magnitude\"] != 999).any():     plt.figure()     plt.plot(gamma_events[\"time\"], gamma_events[\"magnitude\"], '.', markersize=5, alpha=1.0, rasterized=True)     if standard_catalog is not None:         plt.plot(standard_catalog[\"time\"], standard_catalog[\"magnitude\"], '.', markersize=5, alpha=0.8, rasterized=True)     plt.xlim([starttime, endtime])     ylim = plt.ylim()     plt.ylabel(\"Magnitude\")     plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))     plt.gcf().autofmt_xdate()     plt.gca().set_prop_cycle(None)     plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")     plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")     plt.legend()     plt.ylim(ylim)     plt.grid()     plt.savefig(figure_dir(\"earthquake_magnitude_time.png\"), bbox_inches=\"tight\", dpi=300)     plt.savefig(figure_dir(\"earthquake_magnitude_time.pdf\"), bbox_inches=\"tight\", dpi=300)     plt.show(); In\u00a0[13]: Copied! <pre>fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([0.8,1.1]))\nbox = dict(boxstyle='round', facecolor='white', alpha=1)\ntext_loc = [0.05, 0.90]\nplt.subplot(311)\nplt.plot(gamma_events[\"time\"], gamma_events[\"sigma_time\"], '.', markersize=3.0, label=\"Travel-time\")\nplt.ylabel(r\"$\\sigma_{11}$ (s)\")\nplt.legend(loc=\"upper right\")\nplt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.subplot(312)\nplt.plot(gamma_events[\"time\"], gamma_events[\"sigma_amp\"], '.', markersize=3.0, label=\"Amplitude\")\nplt.ylabel(r\"$\\sigma_{22}$ ($\\log10$ m/s)\")\nplt.legend(loc=\"upper right\")\nplt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.subplot(313)\nplt.plot(gamma_events[\"time\"], gamma_events[\"cov_time_amp\"], '.', markersize=3.0, label=\"Travel-time vs. Amplitude\")\nplt.ylabel(r\"$\\Sigma_{12}$\")\nplt.ylim([-0.5, 0.5])\nplt.legend(loc=\"upper right\")\nplt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\nplt.gcf().autofmt_xdate()\nplt.tight_layout()\nplt.gcf().align_labels()\nplt.savefig(figure_dir(\"covariance.png\"), bbox_inches=\"tight\", dpi=300)\nplt.savefig(figure_dir(\"covariance.pdf\"), bbox_inches=\"tight\")\nplt.show();\n</pre> fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([0.8,1.1])) box = dict(boxstyle='round', facecolor='white', alpha=1) text_loc = [0.05, 0.90] plt.subplot(311) plt.plot(gamma_events[\"time\"], gamma_events[\"sigma_time\"], '.', markersize=3.0, label=\"Travel-time\") plt.ylabel(r\"$\\sigma_{11}$ (s)\") plt.legend(loc=\"upper right\") plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.subplot(312) plt.plot(gamma_events[\"time\"], gamma_events[\"sigma_amp\"], '.', markersize=3.0, label=\"Amplitude\") plt.ylabel(r\"$\\sigma_{22}$ ($\\log10$ m/s)\") plt.legend(loc=\"upper right\") plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.subplot(313) plt.plot(gamma_events[\"time\"], gamma_events[\"cov_time_amp\"], '.', markersize=3.0, label=\"Travel-time vs. Amplitude\") plt.ylabel(r\"$\\Sigma_{12}$\") plt.ylim([-0.5, 0.5]) plt.legend(loc=\"upper right\") plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H')) plt.gcf().autofmt_xdate() plt.tight_layout() plt.gcf().align_labels() plt.savefig(figure_dir(\"covariance.png\"), bbox_inches=\"tight\", dpi=300) plt.savefig(figure_dir(\"covariance.pdf\"), bbox_inches=\"tight\") plt.show();"},{"location":"GaMMA/example_phasenet/#phasenet-example","title":"PhaseNet Example\u00b6","text":"<p>Applying GaMMA to associate PhaseNet picks</p>"},{"location":"GaMMA/example_phasenet/#1-download-demo-data","title":"1. Download demo data\u00b6","text":"<p>There are two examples in the demo data: Ridgecrest, CA and Chile.</p> <p>Phase Picks:</p> <ul> <li>test_data/ridgecrest<ul> <li>picks.csv</li> <li>stations.csv</li> <li>standard_catalog.csv</li> </ul> </li> <li>test_data/chile<ul> <li>picks.csv</li> <li>stations.csv</li> <li>iasp91.csv</li> </ul> </li> </ul> <p>Results:</p> <ul> <li>test_data/ridgecrest<ul> <li>gamma_events.csv</li> <li>gamma_picks.csv</li> </ul> </li> <li>test_data/chile<ul> <li>gamma_events.csv</li> <li>gamma_picks.csv</li> </ul> </li> </ul>"},{"location":"GaMMA/example_phasenet/#2-associaiton-with-gamma","title":"2. Associaiton with GaMMA\u00b6","text":""},{"location":"GaMMA/example_phasenet/#3-filtering-based-on-nearest-station-ratio-not-required","title":"3. Filtering based on Nearest Station Ratio (Not required)\u00b6","text":""},{"location":"GaMMA/example_phasenet/#4-visualize-results","title":"4. Visualize results\u00b6","text":"<p>Note that the location and magnitude are estimated during associaiton, which are not expected to have high accuracy.</p>"},{"location":"GaMMA/example_phasenet_ncedc/","title":"Example phasenet ncedc","text":"In\u00a0[1]: Copied! <pre># !pip install git+https://github.com/wayneweiqiang/GaMMA.git\n</pre> # !pip install git+https://github.com/wayneweiqiang/GaMMA.git In\u00a0[2]: Copied! <pre>import pandas as pd\nfrom gamma.utils import association, estimate_eps\nimport numpy as np\nimport os\nfrom pyproj import Proj\nimport json\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd from gamma.utils import association, estimate_eps import numpy as np import os from pyproj import Proj import json import matplotlib.pyplot as plt  In\u00a0[3]: Copied! <pre># !if [ -f demo.tar ]; then rm demo.tar; fi\n# !if [ -d test_data ]; then rm -rf test_data; fi\n# !wget -q https://github.com/AI4EPS/GaMMA/releases/download/test_data/demo.tar\n# !tar -xf demo.tar\n</pre> # !if [ -f demo.tar ]; then rm demo.tar; fi # !if [ -d test_data ]; then rm -rf test_data; fi # !wget -q https://github.com/AI4EPS/GaMMA/releases/download/test_data/demo.tar # !tar -xf demo.tar In\u00a0[4]: Copied! <pre># region = \"ridgecrest\"\n# region = \"chile\"\nregion = \"ncedc\"\n# test_name = \"geyser\"\ntest_name = \"mtj\"\ndata_path = lambda x: os.path.join(f\"test_data/{region}\", x)\nresult_path = f\"results/{region}\"\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nresult_path = lambda x: os.path.join(f\"results/{region}\", x)\nstation_csv = data_path(\"stations.csv\")\nstation_json = data_path(\"stations.json\")\nif test_name == \"geyser\":\n    picks_csv = data_path(\"picks_geyser.csv\")\nif test_name == \"mtj\":\n    picks_csv = data_path(\"picks_mtj.csv\")\nif not os.path.exists(\"figures\"):\n    os.makedirs(\"figures\")\nfigure_dir = lambda x: os.path.join(\"figures\", x)\n</pre> # region = \"ridgecrest\" # region = \"chile\" region = \"ncedc\" # test_name = \"geyser\" test_name = \"mtj\" data_path = lambda x: os.path.join(f\"test_data/{region}\", x) result_path = f\"results/{region}\" if not os.path.exists(result_path):     os.makedirs(result_path) result_path = lambda x: os.path.join(f\"results/{region}\", x) station_csv = data_path(\"stations.csv\") station_json = data_path(\"stations.json\") if test_name == \"geyser\":     picks_csv = data_path(\"picks_geyser.csv\") if test_name == \"mtj\":     picks_csv = data_path(\"picks_mtj.csv\") if not os.path.exists(\"figures\"):     os.makedirs(\"figures\") figure_dir = lambda x: os.path.join(\"figures\", x) In\u00a0[5]: Copied! <pre>## read picks\npicks = pd.read_csv(picks_csv, parse_dates=[\"phase_time\"])\npicks = picks[['station_id', 'phase_time', 'phase_type', 'phase_score', 'phase_amplitude']]\nif test_name == \"geyser\":\n    picks = picks[(picks[\"phase_time\"] &gt;= \"2023-03-02 21:15:00\") &amp; (picks[\"phase_time\"] &lt;= \"2023-03-02 21:30:00\")]\n    # picks = picks[(picks[\"phase_time\"] &gt;= \"2023-03-02 21:21:00\") &amp; (picks[\"phase_time\"] &lt;= \"2023-03-02 21:24:00\")]\n\npicks.rename(columns={\"station_id\": \"id\", \"phase_time\": \"timestamp\", \"phase_type\": \"type\", \"phase_score\": \"prob\", \"phase_amplitude\": \"amp\"}, inplace=True)\nprint(\"Pick format:\", picks.iloc[:3])\n\n## read stations\n# stations = pd.read_csv(station_csv)\n# stations.rename(columns={\"station_id\": \"id\"}, inplace=True)\nstations = pd.read_json(station_json, orient=\"index\")\nstations[\"id\"] = stations.index\nprint(\"Station format:\", stations.iloc[:3])\n\n## Automatic region; you can also specify a region\nwith open(data_path(\"config.json\"), \"r\") as f:\n    config = json.load(f)\n    config[\"center\"] = ((config[\"minlongitude\"] + config[\"maxlongitude\"]) / 2, (config[\"minlatitude\"] + config[\"maxlatitude\"]) / 2)\n    config[\"xlim_degree\"] = (config[\"minlongitude\"], config[\"maxlongitude\"])\n    config[\"ylim_degree\"] = (config[\"minlatitude\"], config[\"maxlatitude\"])\n\n# x0 = stations[\"longitude\"].median()\n# y0 = stations[\"latitude\"].median()\n# xmin = stations[\"longitude\"].min()\n# xmax = stations[\"longitude\"].max()\n# ymin = stations[\"latitude\"].min()\n# ymax = stations[\"latitude\"].max()\n# config = {}\n# config[\"center\"] = (x0, y0)\n# config[\"xlim_degree\"] = (2 * xmin - x0, 2 * xmax - x0)\n# config[\"ylim_degree\"] = (2 * ymin - y0, 2 * ymax - y0)\n\n## projection to km\nproj = Proj(f\"+proj=aeqd +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\nstations[[\"x(km)\", \"y(km)\"]] = stations.apply(lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1)\nstations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x/1e3)\nstations = stations[(stations[\"longitude\"] &gt; config[\"minlongitude\"]) &amp; (stations[\"longitude\"] &lt; config[\"maxlongitude\"]) &amp; (stations[\"latitude\"] &gt; config[\"minlatitude\"]) &amp; (stations[\"latitude\"] &lt; config[\"maxlatitude\"])]\npicks = picks[picks[\"id\"].isin(stations[\"id\"])]\n\n### setting GMMA configs\nconfig[\"use_dbscan\"] = True\nif region == \"chile\":\n    config[\"use_amplitude\"] = False\nelse:\n    config[\"use_amplitude\"] = True\nconfig[\"method\"] = \"BGMM\"  \nif config[\"method\"] == \"BGMM\": ## BayesianGaussianMixture\n    config[\"oversample_factor\"] = 5\nif config[\"method\"] == \"GMM\": ## GaussianMixture\n    config[\"oversample_factor\"] = 1\n\n# config[\"covariance_prior\"] = [1000.0, 1000.0]\n\n# earthquake location\nconfig[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\nconfig[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\nconfig[\"x(km)\"] = proj(longitude=config[\"xlim_degree\"], latitude=[config[\"center\"][1]] * 2)[0]\nconfig[\"y(km)\"] = proj(longitude=[config[\"center\"][0]] * 2, latitude=config[\"ylim_degree\"])[1]\n\nif region == \"ridgecrest\":\n    config[\"z(km)\"] = (0, 20)\nelif region == \"chile\":\n    config[\"z(km)\"] = (0, 250)\nelse:\n    config[\"z(km)\"] = (0, 30)\n    print(\"Please specify z(km) for your region\")\n    # raise NotImplementedError\nconfig[\"bfgs_bounds\"] = (\n    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n    (0, config[\"z(km)\"][1] + 1),  # z\n    (None, None),  # t\n)\n\n# DBSCAN\nconfig[\"dbscan_eps\"] = 10 #estimate_eps(stations, config[\"vel\"][\"p\"])\nconfig[\"dbscan_min_samples\"] = 3\n\n## using Eikonal for 1D velocity model\nif region == \"ridgecrest\":\n    zz = [0.0, 5.5, 16.0, 32.0]\n    vp = [5.5, 5.5,  6.7,  7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    h = 1.0\n    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\nelif region == \"chile\":\n    velocity_model = pd.read_csv(data_path(\"iasp91.csv\"), names=[\"zz\", \"rho\", \"vp\", \"vs\"])\n    velocity_model = velocity_model[velocity_model[\"zz\"] &lt;= config[\"z(km)\"][1]]\n    vel = {\"z\": velocity_model[\"zz\"].values, \"p\": velocity_model[\"vp\"].values, \"s\": velocity_model[\"vs\"].values}\n    h = 1.0\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\nelse:\n    print(\"Using uniform velocity model\")\n\nif region == \"chile\":\n    config[\"initial_points\"] = [1, 1, 1] # x, y, z\n\n# set number of cpus\nconfig[\"ncpu\"] = 32\n\n# filtering\nconfig[\"min_picks_per_eq\"] = 5\nconfig[\"min_p_picks_per_eq\"] = 0\nconfig[\"min_s_picks_per_eq\"] = 0\nconfig[\"max_sigma11\"] = 3.0*5 # second\nconfig[\"max_sigma22\"] = 1.0*3 # log10(m/s)\nconfig[\"max_sigma12\"] = 1.0*3 # covariance\n\n## filter picks without amplitude measurements\nif config[\"use_amplitude\"]:\n    picks = picks[picks[\"amp\"] != -1]\n\nfor k, v in config.items():\n    print(f\"{k}: {v}\")\n</pre> ## read picks picks = pd.read_csv(picks_csv, parse_dates=[\"phase_time\"]) picks = picks[['station_id', 'phase_time', 'phase_type', 'phase_score', 'phase_amplitude']] if test_name == \"geyser\":     picks = picks[(picks[\"phase_time\"] &gt;= \"2023-03-02 21:15:00\") &amp; (picks[\"phase_time\"] &lt;= \"2023-03-02 21:30:00\")]     # picks = picks[(picks[\"phase_time\"] &gt;= \"2023-03-02 21:21:00\") &amp; (picks[\"phase_time\"] &lt;= \"2023-03-02 21:24:00\")]  picks.rename(columns={\"station_id\": \"id\", \"phase_time\": \"timestamp\", \"phase_type\": \"type\", \"phase_score\": \"prob\", \"phase_amplitude\": \"amp\"}, inplace=True) print(\"Pick format:\", picks.iloc[:3])  ## read stations # stations = pd.read_csv(station_csv) # stations.rename(columns={\"station_id\": \"id\"}, inplace=True) stations = pd.read_json(station_json, orient=\"index\") stations[\"id\"] = stations.index print(\"Station format:\", stations.iloc[:3])  ## Automatic region; you can also specify a region with open(data_path(\"config.json\"), \"r\") as f:     config = json.load(f)     config[\"center\"] = ((config[\"minlongitude\"] + config[\"maxlongitude\"]) / 2, (config[\"minlatitude\"] + config[\"maxlatitude\"]) / 2)     config[\"xlim_degree\"] = (config[\"minlongitude\"], config[\"maxlongitude\"])     config[\"ylim_degree\"] = (config[\"minlatitude\"], config[\"maxlatitude\"])  # x0 = stations[\"longitude\"].median() # y0 = stations[\"latitude\"].median() # xmin = stations[\"longitude\"].min() # xmax = stations[\"longitude\"].max() # ymin = stations[\"latitude\"].min() # ymax = stations[\"latitude\"].max() # config = {} # config[\"center\"] = (x0, y0) # config[\"xlim_degree\"] = (2 * xmin - x0, 2 * xmax - x0) # config[\"ylim_degree\"] = (2 * ymin - y0, 2 * ymax - y0)  ## projection to km proj = Proj(f\"+proj=aeqd +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\") stations[[\"x(km)\", \"y(km)\"]] = stations.apply(lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1) stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x/1e3) stations = stations[(stations[\"longitude\"] &gt; config[\"minlongitude\"]) &amp; (stations[\"longitude\"] &lt; config[\"maxlongitude\"]) &amp; (stations[\"latitude\"] &gt; config[\"minlatitude\"]) &amp; (stations[\"latitude\"] &lt; config[\"maxlatitude\"])] picks = picks[picks[\"id\"].isin(stations[\"id\"])]  ### setting GMMA configs config[\"use_dbscan\"] = True if region == \"chile\":     config[\"use_amplitude\"] = False else:     config[\"use_amplitude\"] = True config[\"method\"] = \"BGMM\"   if config[\"method\"] == \"BGMM\": ## BayesianGaussianMixture     config[\"oversample_factor\"] = 5 if config[\"method\"] == \"GMM\": ## GaussianMixture     config[\"oversample_factor\"] = 1  # config[\"covariance_prior\"] = [1000.0, 1000.0]  # earthquake location config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75} config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)'] config[\"x(km)\"] = proj(longitude=config[\"xlim_degree\"], latitude=[config[\"center\"][1]] * 2)[0] config[\"y(km)\"] = proj(longitude=[config[\"center\"][0]] * 2, latitude=config[\"ylim_degree\"])[1]  if region == \"ridgecrest\":     config[\"z(km)\"] = (0, 20) elif region == \"chile\":     config[\"z(km)\"] = (0, 250) else:     config[\"z(km)\"] = (0, 30)     print(\"Please specify z(km) for your region\")     # raise NotImplementedError config[\"bfgs_bounds\"] = (     (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x     (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y     (0, config[\"z(km)\"][1] + 1),  # z     (None, None),  # t )  # DBSCAN config[\"dbscan_eps\"] = 10 #estimate_eps(stations, config[\"vel\"][\"p\"]) config[\"dbscan_min_samples\"] = 3  ## using Eikonal for 1D velocity model if region == \"ridgecrest\":     zz = [0.0, 5.5, 16.0, 32.0]     vp = [5.5, 5.5,  6.7,  7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     h = 1.0     vel = {\"z\": zz, \"p\": vp, \"s\": vs}     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]} elif region == \"chile\":     velocity_model = pd.read_csv(data_path(\"iasp91.csv\"), names=[\"zz\", \"rho\", \"vp\", \"vs\"])     velocity_model = velocity_model[velocity_model[\"zz\"] &lt;= config[\"z(km)\"][1]]     vel = {\"z\": velocity_model[\"zz\"].values, \"p\": velocity_model[\"vp\"].values, \"s\": velocity_model[\"vs\"].values}     h = 1.0     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]} else:     print(\"Using uniform velocity model\")  if region == \"chile\":     config[\"initial_points\"] = [1, 1, 1] # x, y, z  # set number of cpus config[\"ncpu\"] = 32  # filtering config[\"min_picks_per_eq\"] = 5 config[\"min_p_picks_per_eq\"] = 0 config[\"min_s_picks_per_eq\"] = 0 config[\"max_sigma11\"] = 3.0*5 # second config[\"max_sigma22\"] = 1.0*3 # log10(m/s) config[\"max_sigma12\"] = 1.0*3 # covariance  ## filter picks without amplitude measurements if config[\"use_amplitude\"]:     picks = picks[picks[\"amp\"] != -1]  for k, v in config.items():     print(f\"{k}: {v}\")  <pre>Pick format:            id               timestamp type   prob       amp\n0  BG.AL1..DP 2023-01-01 18:35:33.390    P  0.349  0.000007\n1  BG.AL1..DP 2023-01-01 18:50:19.400    P  0.374  0.000001\n2  BG.AL1..DP 2023-01-01 18:50:23.780    S  0.519  0.000001\nStation format:            network station location instrument component  \\\nAZ.KNW..BH      AZ     KNW                  BH     12ENZ   \nAZ.KNW..EH      AZ     KNW                  EH       12Z   \nAZ.KNW..HH      AZ     KNW                  HH     12ENZ   \n\n                                                  sensitivity  latitude  \\\nAZ.KNW..BH  [839091000.0, 209773000.0, 839091000.0, 209773...   33.7141   \nAZ.KNW..EH  [31432900.0, 15716500.0, 31432900.0, 15716500....   33.7141   \nAZ.KNW..HH  [157649000.0, 39412300.0, 840484000.0, 2101210...   33.7141   \n\n            longitude  elevation_m  depth_km     x_km     y_km   z_km  \\\nAZ.KNW..BH  -116.7119       1507.0    -1.507  258.718 -416.537 -1.507   \nAZ.KNW..EH  -116.7119       1507.0    -1.507  258.718 -416.537 -1.507   \nAZ.KNW..HH  -116.7119       1507.0    -1.507  258.718 -416.537 -1.507   \n\n           provider          id  \nAZ.KNW..BH       NC  AZ.KNW..BH  \nAZ.KNW..EH       NC  AZ.KNW..EH  \nAZ.KNW..HH       NC  AZ.KNW..HH  \nPlease specify z(km) for your region\nUsing uniform velocity model\nminlatitude: 32\nmaxlatitude: 43\nminlongitude: -125\nmaxlongitude: -114.0\nnum_nodes: 1\nsampling_rate: 100\ndegree2km: 111.1949\nchannel: HH*,BH*,EH*,HN*\nlevel: response\ngamma: {'zmin_km': 0, 'zmax_km': 60}\ncctorch: {'sampling_rate': 100, 'time_before': 0.25, 'time_after': 1.0, 'min_pair_dist_km': 10, 'components': 'ENZ123', 'component_mapping': {'3': 0, '2': 1, '1': 2, 'E': 0, 'N': 1, 'Z': 2}}\ncenter: (-119.5, 37.5)\nxlim_degree: (-125, -114.0)\nylim_degree: (32, 43)\nuse_dbscan: True\nuse_amplitude: True\nmethod: BGMM\noversample_factor: 5\nvel: {'p': 6.0, 's': 3.4285714285714284}\ndims: ['x(km)', 'y(km)', 'z(km)']\nx(km): (-486.29893938501783, 486.2989393850155)\ny(km): (-610.6165417479444, 611.185297582399)\nz(km): (0, 30)\nbfgs_bounds: ((-487.29893938501783, 487.2989393850155), (-611.6165417479444, 612.185297582399), (0, 31), (None, None))\ndbscan_eps: 10\ndbscan_min_samples: 3\nncpu: 32\nmin_picks_per_eq: 5\nmin_p_picks_per_eq: 0\nmin_s_picks_per_eq: 0\nmax_sigma11: 15.0\nmax_sigma22: 3.0\nmax_sigma12: 3.0\n</pre> In\u00a0[6]: Copied! <pre>from sklearn.cluster import DBSCAN\nfrom tqdm import tqdm\n</pre> from sklearn.cluster import DBSCAN from tqdm import tqdm In\u00a0[7]: Copied! <pre>## Merge different instrument types\npicks[\"id\"] = picks[\"id\"].apply(lambda x: \".\".join(x.split(\".\")[:3])) # network.station.location.channel -&gt; network.station.location\nstations[\"id\"] = stations[\"id\"].apply(lambda x: \".\".join(x.split(\".\")[:3])) # network.station.location.channel -&gt; network.station.location\nstations = stations.groupby(\"id\").first().reset_index()\n</pre> ## Merge different instrument types picks[\"id\"] = picks[\"id\"].apply(lambda x: \".\".join(x.split(\".\")[:3])) # network.station.location.channel -&gt; network.station.location stations[\"id\"] = stations[\"id\"].apply(lambda x: \".\".join(x.split(\".\")[:3])) # network.station.location.channel -&gt; network.station.location stations = stations.groupby(\"id\").first().reset_index()  In\u00a0[8]: Copied! <pre>picks[\"t\"] = (picks[\"timestamp\"] - picks[\"timestamp\"].min()).dt.total_seconds()\npicks_filt = []\nMIN_SECOND = 5 # minimum seconds between picks\nfor id, group in tqdm(picks.groupby(\"id\")):\n    db = DBSCAN(eps=MIN_SECOND, min_samples=1).fit(group[[\"t\"]])\n    labels = db.labels_\n    for la in np.unique(labels):\n        picks_filt.append(group[labels == la].sort_values(\"prob\", ascending=False).iloc[0])\n</pre> picks[\"t\"] = (picks[\"timestamp\"] - picks[\"timestamp\"].min()).dt.total_seconds() picks_filt = [] MIN_SECOND = 5 # minimum seconds between picks for id, group in tqdm(picks.groupby(\"id\")):     db = DBSCAN(eps=MIN_SECOND, min_samples=1).fit(group[[\"t\"]])     labels = db.labels_     for la in np.unique(labels):         picks_filt.append(group[labels == la].sort_values(\"prob\", ascending=False).iloc[0]) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 652/652 [00:03&lt;00:00, 185.13it/s]\n</pre> In\u00a0[9]: Copied! <pre>picks_filt = pd.DataFrame(picks_filt).reset_index(drop=True)\npicks = picks_filt\nstations = stations[[\"latitude\", \"longitude\", \"elevation_m\", \"id\", \"x(km)\", \"y(km)\", \"z(km)\"]].reset_index(drop=True)\n</pre> picks_filt = pd.DataFrame(picks_filt).reset_index(drop=True) picks = picks_filt stations = stations[[\"latitude\", \"longitude\", \"elevation_m\", \"id\", \"x(km)\", \"y(km)\", \"z(km)\"]].reset_index(drop=True) In\u00a0[10]: Copied! <pre>from sklearn.cluster import DBSCAN\nimport matplotlib.dates as mdates\n</pre> from sklearn.cluster import DBSCAN import matplotlib.dates as mdates In\u00a0[11]: Copied! <pre>picks_ = picks.merge(stations[[\"id\", \"x(km)\", \"y(km)\", \"z(km)\", \"longitude\", \"latitude\"]], on=\"id\")\npicks_[\"t\"] = (picks_[\"timestamp\"] - picks_[\"timestamp\"].min()).dt.total_seconds()\ndata = picks_[[\"t\", \"x(km)\", \"y(km)\", \"z(km)\"]].values\n\nvel = np.average(config[\"vel\"][\"p\"])\ndb = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(data[:, :3] / np.array([1, vel, vel]))\nlabels = db.labels_\nunique_labels = set(labels)\nunique_labels = unique_labels.difference([-1])\n\n# lat0, lon0 = 40.409, -123.971  #MTJ\n# lat0, lon0 = 38.821, -122.805\nif test_name == \"geyser\":\n    lat0, lon0 = 38.821, -122.805\nif test_name == \"mtj\":\n    lat0, lon0 = 40.409, -123.971\n\npicks_[\"dist_km\"] = np.sqrt((picks_[\"latitude\"] - lat0)**2 + (picks_[\"longitude\"] - lon0)**2) * 111.32\npicks_[\"label\"] = labels \ncenter = picks_.groupby(\"label\").agg({\"x(km)\": \"mean\", \"y(km)\": \"mean\", \"z(km)\": \"mean\", \"timestamp\": \"min\", \"latitude\": \"mean\", \"longitude\": \"mean\"}).reset_index()\ncenter[\"dist_km\"] = np.sqrt((center[\"latitude\"] - lat0)**2 + (center[\"longitude\"] - lon0)**2) * 111.32\n</pre> picks_ = picks.merge(stations[[\"id\", \"x(km)\", \"y(km)\", \"z(km)\", \"longitude\", \"latitude\"]], on=\"id\") picks_[\"t\"] = (picks_[\"timestamp\"] - picks_[\"timestamp\"].min()).dt.total_seconds() data = picks_[[\"t\", \"x(km)\", \"y(km)\", \"z(km)\"]].values  vel = np.average(config[\"vel\"][\"p\"]) db = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(data[:, :3] / np.array([1, vel, vel])) labels = db.labels_ unique_labels = set(labels) unique_labels = unique_labels.difference([-1])  # lat0, lon0 = 40.409, -123.971  #MTJ # lat0, lon0 = 38.821, -122.805 if test_name == \"geyser\":     lat0, lon0 = 38.821, -122.805 if test_name == \"mtj\":     lat0, lon0 = 40.409, -123.971  picks_[\"dist_km\"] = np.sqrt((picks_[\"latitude\"] - lat0)**2 + (picks_[\"longitude\"] - lon0)**2) * 111.32 picks_[\"label\"] = labels  center = picks_.groupby(\"label\").agg({\"x(km)\": \"mean\", \"y(km)\": \"mean\", \"z(km)\": \"mean\", \"timestamp\": \"min\", \"latitude\": \"mean\", \"longitude\": \"mean\"}).reset_index() center[\"dist_km\"] = np.sqrt((center[\"latitude\"] - lat0)**2 + (center[\"longitude\"] - lon0)**2) * 111.32   In\u00a0[12]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], picks_[\"latitude\"], c=picks_[\"label\"].apply(mapping_color), s=1)\nplt.scatter(center[\"timestamp\"], center[\"latitude\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\n# plt set x time format to %Y-%m-%d %H:%M:%S\nax = plt.gca()  # Get the current Axes instance\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\nplt.gcf().autofmt_xdate()\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], picks_[\"latitude\"], c=picks_[\"label\"].apply(mapping_color), s=1) plt.scatter(center[\"timestamp\"], center[\"latitude\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") # plt set x time format to %Y-%m-%d %H:%M:%S ax = plt.gca()  # Get the current Axes instance ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S')) plt.gcf().autofmt_xdate() In\u00a0[13]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1)\nplt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1) plt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") Out[13]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e31184d60&gt;</pre> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1)\nplt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\nif test_name == \"mtj\":\n    plt.xlim(pd.to_datetime(\"2023-01-01 18:35:00\"), pd.to_datetime(\"2023-01-01 18:40:00\"))\nif test_name == \"geyser\":\n    plt.xlim(pd.to_datetime(\"2023-03-02 21:15:00\"), pd.to_datetime(\"2023-03-02 21:30:00\"))\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1) plt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") if test_name == \"mtj\":     plt.xlim(pd.to_datetime(\"2023-01-01 18:35:00\"), pd.to_datetime(\"2023-01-01 18:40:00\")) if test_name == \"geyser\":     plt.xlim(pd.to_datetime(\"2023-03-02 21:15:00\"), pd.to_datetime(\"2023-03-02 21:30:00\")) In\u00a0[15]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], np.log10(picks_[\"amp\"]), c=picks_[\"label\"].apply(mapping_color), s=1)\n# plt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], np.log10(picks_[\"amp\"]), c=picks_[\"label\"].apply(mapping_color), s=1) # plt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") Out[15]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e311beb30&gt;</pre> In\u00a0[16]: Copied! <pre>for k, v in config.items():\n    print(f\"{k}: {v}\")\n</pre> for k, v in config.items():     print(f\"{k}: {v}\") <pre>minlatitude: 32\nmaxlatitude: 43\nminlongitude: -125\nmaxlongitude: -114.0\nnum_nodes: 1\nsampling_rate: 100\ndegree2km: 111.1949\nchannel: HH*,BH*,EH*,HN*\nlevel: response\ngamma: {'zmin_km': 0, 'zmax_km': 60}\ncctorch: {'sampling_rate': 100, 'time_before': 0.25, 'time_after': 1.0, 'min_pair_dist_km': 10, 'components': 'ENZ123', 'component_mapping': {'3': 0, '2': 1, '1': 2, 'E': 0, 'N': 1, 'Z': 2}}\ncenter: (-119.5, 37.5)\nxlim_degree: (-125, -114.0)\nylim_degree: (32, 43)\nuse_dbscan: True\nuse_amplitude: True\nmethod: BGMM\noversample_factor: 5\nvel: {'p': 6.0, 's': 3.4285714285714284}\ndims: ['x(km)', 'y(km)', 'z(km)']\nx(km): (-486.29893938501783, 486.2989393850155)\ny(km): (-610.6165417479444, 611.185297582399)\nz(km): (0, 30)\nbfgs_bounds: ((-487.29893938501783, 487.2989393850155), (-611.6165417479444, 612.185297582399), (0, 31), (None, None))\ndbscan_eps: 10\ndbscan_min_samples: 3\nncpu: 32\nmin_picks_per_eq: 5\nmin_p_picks_per_eq: 0\nmin_s_picks_per_eq: 0\nmax_sigma11: 15.0\nmax_sigma22: 3.0\nmax_sigma12: 3.0\n</pre> In\u00a0[17]: Copied! <pre>event_idx0 = 0 ## current earthquake index\nassignments = []\nevents, assignments = association(picks, stations, config, event_idx0, config[\"method\"])\nevent_idx0 += len(events)\n\n## create catalog\nevents = pd.DataFrame(events)\nevents[[\"longitude\",\"latitude\"]] = events.apply(lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1)\nevents[\"depth_km\"] = events[\"z(km)\"]\nevents.to_csv(result_path(\"gamma_events.csv\"), index=False, \n                float_format=\"%.3f\",\n                date_format='%Y-%m-%dT%H:%M:%S.%f')\n\n## add assignment to picks\nassignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\npicks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({'event_index': int})\npicks.rename(columns={\"id\": \"station_id\", \"timestamp\": \"phase_time\", \"type\": \"phase_type\", \"prob\": \"phase_score\", \"amp\": \"phase_amplitude\"}, inplace=True)\npicks.to_csv(result_path(\"gamma_picks.csv\"), index=False, \n                date_format='%Y-%m-%dT%H:%M:%S.%f')\n</pre> event_idx0 = 0 ## current earthquake index assignments = [] events, assignments = association(picks, stations, config, event_idx0, config[\"method\"]) event_idx0 += len(events)  ## create catalog events = pd.DataFrame(events) events[[\"longitude\",\"latitude\"]] = events.apply(lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1) events[\"depth_km\"] = events[\"z(km)\"] events.to_csv(result_path(\"gamma_events.csv\"), index=False,                  float_format=\"%.3f\",                 date_format='%Y-%m-%dT%H:%M:%S.%f')  ## add assignment to picks assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"]) picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({'event_index': int}) picks.rename(columns={\"id\": \"station_id\", \"timestamp\": \"phase_time\", \"type\": \"phase_type\", \"prob\": \"phase_score\", \"amp\": \"phase_amplitude\"}, inplace=True) picks.to_csv(result_path(\"gamma_picks.csv\"), index=False,                  date_format='%Y-%m-%dT%H:%M:%S.%f') <pre>Associating 198 clusters with 16 CPUs\n..................................................................................................................................................................................\nAssociated 100 events.\n...................</pre> In\u00a0[18]: Copied! <pre>if test_name == \"mtj\":\n    lat0, lon0 = 40.409, -123.971\nif test_name == \"geyser\":\n    lat0, lon0 = 38.821, -122.805\npicks = pd.read_csv(result_path(\"gamma_picks.csv\"), parse_dates=[\"phase_time\"]) \nevents = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"])\npicks[\"id\"] = picks[\"station_id\"]\npicks = picks.merge(stations[[\"id\", \"latitude\", \"longitude\"]], on=\"id\")\npicks[\"dist_km\"] = np.sqrt((picks[\"latitude\"] - lat0)**2 + (picks[\"longitude\"] - lon0)**2) * 111.32\nevents[\"dist_km\"] = np.sqrt((events[\"latitude\"] - lat0)**2 + (events[\"longitude\"] - lon0)**2) * 111.32\n</pre> if test_name == \"mtj\":     lat0, lon0 = 40.409, -123.971 if test_name == \"geyser\":     lat0, lon0 = 38.821, -122.805 picks = pd.read_csv(result_path(\"gamma_picks.csv\"), parse_dates=[\"phase_time\"])  events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"]) picks[\"id\"] = picks[\"station_id\"] picks = picks.merge(stations[[\"id\", \"latitude\", \"longitude\"]], on=\"id\") picks[\"dist_km\"] = np.sqrt((picks[\"latitude\"] - lat0)**2 + (picks[\"longitude\"] - lon0)**2) * 111.32 events[\"dist_km\"] = np.sqrt((events[\"latitude\"] - lat0)**2 + (events[\"longitude\"] - lon0)**2) * 111.32  In\u00a0[19]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks[\"phase_time\"], picks[\"latitude\"], c=picks[\"event_index\"].apply(mapping_color), s=1)\nplt.scatter(events[\"time\"], events[\"latitude\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks[\"phase_time\"], picks[\"latitude\"], c=picks[\"event_index\"].apply(mapping_color), s=1) plt.scatter(events[\"time\"], events[\"latitude\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") Out[19]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e2f8ff340&gt;</pre> In\u00a0[20]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks[\"phase_time\"], np.log10(picks[\"phase_amplitude\"]), c=picks[\"event_index\"].apply(mapping_color), s=1)\n# plt.scatter(events[\"time\"], events[\"latitude\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks[\"phase_time\"], np.log10(picks[\"phase_amplitude\"]), c=picks[\"event_index\"].apply(mapping_color), s=1) # plt.scatter(events[\"time\"], events[\"latitude\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") Out[20]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e310ba0b0&gt;</pre> In\u00a0[21]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1)\nplt.scatter(events[\"time\"], events[\"dist_km\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1) plt.scatter(events[\"time\"], events[\"dist_km\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") Out[21]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e30a3d270&gt;</pre> In\u00a0[22]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1)\nplt.scatter(events[\"time\"], events[\"dist_km\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\nif test_name == \"mtj\":\n    plt.xlim(pd.to_datetime(\"2023-01-01 18:35:00\"), pd.to_datetime(\"2023-01-01 18:40:00\"))\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1) plt.scatter(events[\"time\"], events[\"dist_km\"], c=events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") if test_name == \"mtj\":     plt.xlim(pd.to_datetime(\"2023-01-01 18:35:00\"), pd.to_datetime(\"2023-01-01 18:40:00\")) In\u00a0[23]: Copied! <pre># %%\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm import tqdm\nimport pandas as pd\n\nevents = pd.read_csv(result_path(\"gamma_events.csv\"))\npicks = pd.read_csv(result_path(\"gamma_picks.csv\"))\n# stations = pd.read_csv(data_path(\"stations.csv\"))\nstations = pd.read_json(station_json, orient=\"index\")\nstations[\"station_id\"] = stations.index\n\nMIN_NEAREST_STATION_RATIO = 0.1\n\n# %%\nstations[\"station_id\"] = stations[\"station_id\"].apply(lambda x: \".\".join(x.split(\".\")[:3]))\nstations = stations[stations[\"station_id\"].isin(picks[\"station_id\"].unique())]\n\nneigh = NearestNeighbors(n_neighbors=min(len(stations), 10))\nneigh.fit(stations[[\"longitude\", \"latitude\"]].values)\n\n# %%\npicks = picks.merge(events[[\"event_index\", \"longitude\", \"latitude\"]], on=\"event_index\", suffixes=(\"\", \"_event\"))\npicks = picks.merge(stations[[\"station_id\", \"longitude\", \"latitude\"]], on=\"station_id\", suffixes=(\"\", \"_station\"))\n\n# %%\nfiltered_events = []\nfor i, event in tqdm(events.iterrows(), total=len(events)):\n    sid = neigh.kneighbors([[event[\"longitude\"], event[\"latitude\"]]])[1][0]\n    picks_ = picks[picks[\"event_index\"] == event[\"event_index\"]]\n    # longitude, latitude = picks_[[\"longitude\", \"latitude\"]].mean().values\n    # sid = neigh.kneighbors([[longitude, latitude]])[1][0]\n    stations_neigh = stations.iloc[sid][\"station_id\"].values\n    picks_neigh = picks_[picks_[\"station_id\"].isin(stations_neigh)]\n    stations_with_picks = picks_neigh[\"station_id\"].unique()\n    if len(stations_with_picks) / len(stations_neigh) &gt; MIN_NEAREST_STATION_RATIO:\n        filtered_events.append(event)\n\n# %%\nprint(f\"Events before filtering: {len(events)}\")\nprint(f\"Events after filtering: {len(filtered_events)}\")\nfiltered_events = pd.DataFrame(filtered_events)\nos.system(f\"mv {result_path('gamma_events.csv')} {result_path('gamma_events_raw.csv')}\")\nfiltered_events.to_csv(result_path(\"gamma_events.csv\"), index=False)\n</pre> # %% from sklearn.neighbors import NearestNeighbors from tqdm import tqdm import pandas as pd  events = pd.read_csv(result_path(\"gamma_events.csv\")) picks = pd.read_csv(result_path(\"gamma_picks.csv\")) # stations = pd.read_csv(data_path(\"stations.csv\")) stations = pd.read_json(station_json, orient=\"index\") stations[\"station_id\"] = stations.index  MIN_NEAREST_STATION_RATIO = 0.1  # %% stations[\"station_id\"] = stations[\"station_id\"].apply(lambda x: \".\".join(x.split(\".\")[:3])) stations = stations[stations[\"station_id\"].isin(picks[\"station_id\"].unique())]  neigh = NearestNeighbors(n_neighbors=min(len(stations), 10)) neigh.fit(stations[[\"longitude\", \"latitude\"]].values)  # %% picks = picks.merge(events[[\"event_index\", \"longitude\", \"latitude\"]], on=\"event_index\", suffixes=(\"\", \"_event\")) picks = picks.merge(stations[[\"station_id\", \"longitude\", \"latitude\"]], on=\"station_id\", suffixes=(\"\", \"_station\"))  # %% filtered_events = [] for i, event in tqdm(events.iterrows(), total=len(events)):     sid = neigh.kneighbors([[event[\"longitude\"], event[\"latitude\"]]])[1][0]     picks_ = picks[picks[\"event_index\"] == event[\"event_index\"]]     # longitude, latitude = picks_[[\"longitude\", \"latitude\"]].mean().values     # sid = neigh.kneighbors([[longitude, latitude]])[1][0]     stations_neigh = stations.iloc[sid][\"station_id\"].values     picks_neigh = picks_[picks_[\"station_id\"].isin(stations_neigh)]     stations_with_picks = picks_neigh[\"station_id\"].unique()     if len(stations_with_picks) / len(stations_neigh) &gt; MIN_NEAREST_STATION_RATIO:         filtered_events.append(event)  # %% print(f\"Events before filtering: {len(events)}\") print(f\"Events after filtering: {len(filtered_events)}\") filtered_events = pd.DataFrame(filtered_events) os.system(f\"mv {result_path('gamma_events.csv')} {result_path('gamma_events_raw.csv')}\") filtered_events.to_csv(result_path(\"gamma_events.csv\"), index=False)  <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 145/145 [00:00&lt;00:00, 814.08it/s]</pre> <pre>Events before filtering: 145\nEvents after filtering: 31\n</pre> <pre>\n</pre> In\u00a0[24]: Copied! <pre>if test_name == \"mtj\":\n    lat0, lon0 = 40.409, -123.971\nif test_name == \"geyser\":\n    lat0, lon0 = 38.821, -122.805\npicks = pd.read_csv(result_path(\"gamma_picks.csv\"), parse_dates=[\"phase_time\"])\npicks[\"id\"] = picks[\"station_id\"]\npicks = picks.merge(stations[[\"station_id\", \"latitude\", \"longitude\"]], on=\"station_id\")\nfiltered_events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"])\npicks[\"dist_km\"] = np.sqrt((picks[\"latitude\"] - lat0)**2 + (picks[\"longitude\"] - lon0)**2) * 111.32\nevents[\"dist_km\"] = np.sqrt((events[\"latitude\"] - lat0)**2 + (events[\"longitude\"] - lon0)**2) * 111.32\nfiltered_events[\"dist_km\"] = np.sqrt((filtered_events[\"latitude\"] - lat0)**2 + (filtered_events[\"longitude\"] - lon0)**2) * 111.32\n# picks = picks[picks[\"event_index\"].isin(filtered_events[\"event_index\"])]\n# picks[\"event_index\"] = picks[\"event_index\"].apply(lambda x: -1 if x not in filtered_events[\"event_index\"].values else x)\nfiltered_picks = picks[picks[\"event_index\"].isin(filtered_events[\"event_index\"])]\n</pre> if test_name == \"mtj\":     lat0, lon0 = 40.409, -123.971 if test_name == \"geyser\":     lat0, lon0 = 38.821, -122.805 picks = pd.read_csv(result_path(\"gamma_picks.csv\"), parse_dates=[\"phase_time\"]) picks[\"id\"] = picks[\"station_id\"] picks = picks.merge(stations[[\"station_id\", \"latitude\", \"longitude\"]], on=\"station_id\") filtered_events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"]) picks[\"dist_km\"] = np.sqrt((picks[\"latitude\"] - lat0)**2 + (picks[\"longitude\"] - lon0)**2) * 111.32 events[\"dist_km\"] = np.sqrt((events[\"latitude\"] - lat0)**2 + (events[\"longitude\"] - lon0)**2) * 111.32 filtered_events[\"dist_km\"] = np.sqrt((filtered_events[\"latitude\"] - lat0)**2 + (filtered_events[\"longitude\"] - lon0)**2) * 111.32 # picks = picks[picks[\"event_index\"].isin(filtered_events[\"event_index\"])] # picks[\"event_index\"] = picks[\"event_index\"].apply(lambda x: -1 if x not in filtered_events[\"event_index\"].values else x) filtered_picks = picks[picks[\"event_index\"].isin(filtered_events[\"event_index\"])] In\u00a0[25]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1)\nplt.scatter(filtered_events[\"time\"], filtered_events[\"dist_km\"], c=filtered_events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1) plt.scatter(filtered_events[\"time\"], filtered_events[\"dist_km\"], c=filtered_events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") Out[25]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e30fe1210&gt;</pre> In\u00a0[26]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(filtered_picks[\"phase_time\"], filtered_picks[\"dist_km\"], c=filtered_picks[\"event_index\"].apply(mapping_color), s=1)\nplt.scatter(filtered_events[\"time\"], filtered_events[\"dist_km\"], c=filtered_events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(filtered_picks[\"phase_time\"], filtered_picks[\"dist_km\"], c=filtered_picks[\"event_index\"].apply(mapping_color), s=1) plt.scatter(filtered_events[\"time\"], filtered_events[\"dist_km\"], c=filtered_events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") Out[26]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3e30fcf4f0&gt;</pre> In\u00a0[27]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1)\nplt.scatter(filtered_events[\"time\"], filtered_events[\"dist_km\"], c=filtered_events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\")\nif test_name == \"mtj\":\n    plt.xlim(pd.to_datetime(\"2023-01-01 18:34:30\"), pd.to_datetime(\"2023-01-01 18:40:00\"))\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks[\"phase_time\"], picks[\"dist_km\"], c=picks[\"event_index\"].apply(mapping_color), s=1) plt.scatter(filtered_events[\"time\"], filtered_events[\"dist_km\"], c=filtered_events[\"event_index\"].apply(mapping_color), s=50, marker=\"x\") if test_name == \"mtj\":     plt.xlim(pd.to_datetime(\"2023-01-01 18:34:30\"), pd.to_datetime(\"2023-01-01 18:40:00\"))  In\u00a0[28]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nresult_label=\"GaMMA\"\ncatalog_label=\"Standard\"\n</pre> import matplotlib.pyplot as plt import matplotlib.dates as mdates result_label=\"GaMMA\" catalog_label=\"Standard\" In\u00a0[29]: Copied! <pre># stations = pd.read_csv(data_path(\"stations.csv\"))\nstations = pd.read_json(station_json, orient=\"index\")\nstations[\"id\"] = stations.index\ngamma_events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"])\n\nif os.path.exists(data_path(\"standard_catalog.csv\")):\n    standard_catalog = pd.read_csv(data_path(\"standard_catalog.csv\"), parse_dates=[\"time\"])\n    starttime = standard_catalog[\"time\"].min()\n    endtime = standard_catalog[\"time\"].max()\nelse:\n    standard_catalog = None\n    starttime = gamma_events[\"time\"].min()\n    endtime = gamma_events[\"time\"].max()\n\n\nplt.figure()\nplt.hist(gamma_events[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=1.0, linewidth=0.5, label=f\"{result_label}: {len(gamma_events['time'])}\")\nif standard_catalog is not None:\n    plt.hist(standard_catalog[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=0.6, linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['time'])}\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Date\")\nplt.gca().autoscale(enable=True, axis='x', tight=True)\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\nplt.gcf().autofmt_xdate()\nplt.legend()\nplt.savefig(figure_dir(\"earthquake_number.png\"), bbox_inches=\"tight\", dpi=300)\nplt.savefig(figure_dir(\"earthquake_number.pdf\"), bbox_inches=\"tight\")\nplt.show();\n</pre> # stations = pd.read_csv(data_path(\"stations.csv\")) stations = pd.read_json(station_json, orient=\"index\") stations[\"id\"] = stations.index gamma_events = pd.read_csv(result_path(\"gamma_events.csv\"), parse_dates=[\"time\"])  if os.path.exists(data_path(\"standard_catalog.csv\")):     standard_catalog = pd.read_csv(data_path(\"standard_catalog.csv\"), parse_dates=[\"time\"])     starttime = standard_catalog[\"time\"].min()     endtime = standard_catalog[\"time\"].max() else:     standard_catalog = None     starttime = gamma_events[\"time\"].min()     endtime = gamma_events[\"time\"].max()   plt.figure() plt.hist(gamma_events[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=1.0, linewidth=0.5, label=f\"{result_label}: {len(gamma_events['time'])}\") if standard_catalog is not None:     plt.hist(standard_catalog[\"time\"], range=(starttime, endtime), bins=24, edgecolor=\"k\", alpha=0.6, linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['time'])}\") plt.ylabel(\"Frequency\") plt.xlabel(\"Date\") plt.gca().autoscale(enable=True, axis='x', tight=True) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H')) plt.gcf().autofmt_xdate() plt.legend() plt.savefig(figure_dir(\"earthquake_number.png\"), bbox_inches=\"tight\", dpi=300) plt.savefig(figure_dir(\"earthquake_number.pdf\"), bbox_inches=\"tight\") plt.show(); In\u00a0[30]: Copied! <pre>fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([1.5,1]))\nbox = dict(boxstyle='round', facecolor='white', alpha=1)\ntext_loc = [0.05, 0.92]\ngrd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1,1])\nfig.add_subplot(grd[:, 0])\nplt.plot(gamma_events[\"longitude\"], gamma_events[\"latitude\"], '.',markersize=2, alpha=1.0)\nif standard_catalog is not None:\n    plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"latitude\"], '.', markersize=5, alpha=0.6)\nplt.axis(\"scaled\")\nplt.xlim(np.array(config[\"xlim_degree\"]))\nplt.ylim(np.array(config[\"ylim_degree\"]))\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Longitude\")\nplt.gca().set_prop_cycle(None)\nplt.plot([], [], '.', markersize=10, label=f\"{result_label}\", rasterized=True)\nplt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\", rasterized=True)\nplt.plot(stations[\"longitude\"], stations[\"latitude\"], 'k^', markersize=5, alpha=0.01, label=\"Stations\")\nplt.legend(loc=\"lower right\")\nplt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n\nfig.add_subplot(grd[0, 1])\nplt.plot(gamma_events[\"longitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True)\nif standard_catalog is not None:\n    plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True)\nplt.xlim(np.array(config[\"xlim_degree\"])+np.array([0.2,-0.27]))\nplt.ylim(config[\"z(km)\"])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Depth (km)\")\nplt.gca().set_prop_cycle(None)\nplt.plot([], [], '.', markersize=10, label=f\"{result_label}\")\nplt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\")\nplt.legend(loc=\"lower right\")\nplt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n\nfig.add_subplot(grd[1, 1])\nplt.plot(gamma_events[\"latitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True)\nif standard_catalog is not None:\n    plt.plot(standard_catalog[\"latitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True)\nplt.xlim(np.array(config[\"ylim_degree\"])+np.array([0.2,-0.27]))\nplt.ylim(config[\"z(km)\"])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Depth (km)\")\nplt.gca().set_prop_cycle(None)\nplt.plot([], [], '.', markersize=10, label=f\"{result_label}\")\nplt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\")\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.savefig(figure_dir(\"earthquake_location.png\"), bbox_inches=\"tight\", dpi=300)\nplt.savefig(figure_dir(\"earthquake_location.pdf\"), bbox_inches=\"tight\", dpi=300)\nplt.show();\n</pre> fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([1.5,1])) box = dict(boxstyle='round', facecolor='white', alpha=1) text_loc = [0.05, 0.92] grd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1,1]) fig.add_subplot(grd[:, 0]) plt.plot(gamma_events[\"longitude\"], gamma_events[\"latitude\"], '.',markersize=2, alpha=1.0) if standard_catalog is not None:     plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"latitude\"], '.', markersize=5, alpha=0.6) plt.axis(\"scaled\") plt.xlim(np.array(config[\"xlim_degree\"])) plt.ylim(np.array(config[\"ylim_degree\"])) plt.xlabel(\"Latitude\") plt.ylabel(\"Longitude\") plt.gca().set_prop_cycle(None) plt.plot([], [], '.', markersize=10, label=f\"{result_label}\", rasterized=True) plt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\", rasterized=True) plt.plot(stations[\"longitude\"], stations[\"latitude\"], 'k^', markersize=5, alpha=0.01, label=\"Stations\") plt.legend(loc=\"lower right\") plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)  fig.add_subplot(grd[0, 1]) plt.plot(gamma_events[\"longitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True) if standard_catalog is not None:     plt.plot(standard_catalog[\"longitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True) plt.xlim(np.array(config[\"xlim_degree\"])+np.array([0.2,-0.27])) plt.ylim(config[\"z(km)\"]) plt.gca().invert_yaxis() plt.xlabel(\"Longitude\") plt.ylabel(\"Depth (km)\") plt.gca().set_prop_cycle(None) plt.plot([], [], '.', markersize=10, label=f\"{result_label}\") plt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\") plt.legend(loc=\"lower right\") plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)  fig.add_subplot(grd[1, 1]) plt.plot(gamma_events[\"latitude\"], gamma_events[\"depth_km\"], '.', markersize=2, alpha=1.0, rasterized=True) if standard_catalog is not None:     plt.plot(standard_catalog[\"latitude\"], standard_catalog[\"depth_km\"], '.', markersize=2, alpha=0.6, rasterized=True) plt.xlim(np.array(config[\"ylim_degree\"])+np.array([0.2,-0.27])) plt.ylim(config[\"z(km)\"]) plt.gca().invert_yaxis() plt.xlabel(\"Latitude\") plt.ylabel(\"Depth (km)\") plt.gca().set_prop_cycle(None) plt.plot([], [], '.', markersize=10, label=f\"{result_label}\") plt.plot([], [], '.', markersize=10, label=f\"{catalog_label}\") plt.legend(loc=\"lower right\") plt.tight_layout() plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.savefig(figure_dir(\"earthquake_location.png\"), bbox_inches=\"tight\", dpi=300) plt.savefig(figure_dir(\"earthquake_location.pdf\"), bbox_inches=\"tight\", dpi=300) plt.show(); In\u00a0[31]: Copied! <pre>if standard_catalog is not None:\n    range = (0, standard_catalog[\"magnitude\"].max())\nelse:\n    range = (-1, gamma_events[\"magnitude\"].max())\nif (gamma_events[\"magnitude\"] != 999).any():\n    plt.figure()\n    plt.hist(gamma_events[\"magnitude\"], range=range, bins=25, alpha=1.0,  edgecolor=\"k\", linewidth=0.5, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")\n    if standard_catalog is not None:\n        plt.hist(standard_catalog[\"magnitude\"], range=range, bins=25, alpha=0.6,  edgecolor=\"k\", linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")\n    plt.legend()\n    if standard_catalog is not None:\n        plt.xlim([0, standard_catalog[\"magnitude\"].max()])\n    else:\n        plt.xlim([-1,gamma_events[\"magnitude\"].max()])\n    plt.xlabel(\"Magnitude\")\n    plt.ylabel(\"Frequency\")\n    plt.gca().set_yscale('log')\n    plt.savefig(figure_dir(\"earthquake_magnitude_frequency.png\"), bbox_inches=\"tight\", dpi=300)\n    plt.savefig(figure_dir(\"earthquake_magnitude_frequency.pdf\"), bbox_inches=\"tight\")\n    plt.show();\n</pre> if standard_catalog is not None:     range = (0, standard_catalog[\"magnitude\"].max()) else:     range = (-1, gamma_events[\"magnitude\"].max()) if (gamma_events[\"magnitude\"] != 999).any():     plt.figure()     plt.hist(gamma_events[\"magnitude\"], range=range, bins=25, alpha=1.0,  edgecolor=\"k\", linewidth=0.5, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")     if standard_catalog is not None:         plt.hist(standard_catalog[\"magnitude\"], range=range, bins=25, alpha=0.6,  edgecolor=\"k\", linewidth=0.5, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")     plt.legend()     if standard_catalog is not None:         plt.xlim([0, standard_catalog[\"magnitude\"].max()])     else:         plt.xlim([-1,gamma_events[\"magnitude\"].max()])     plt.xlabel(\"Magnitude\")     plt.ylabel(\"Frequency\")     plt.gca().set_yscale('log')     plt.savefig(figure_dir(\"earthquake_magnitude_frequency.png\"), bbox_inches=\"tight\", dpi=300)     plt.savefig(figure_dir(\"earthquake_magnitude_frequency.pdf\"), bbox_inches=\"tight\")     plt.show(); In\u00a0[32]: Copied! <pre>if (gamma_events[\"magnitude\"] != 999).any():\n    plt.figure()\n    plt.plot(gamma_events[\"time\"], gamma_events[\"magnitude\"], '.', markersize=5, alpha=1.0, rasterized=True)\n    if standard_catalog is not None:\n        plt.plot(standard_catalog[\"time\"], standard_catalog[\"magnitude\"], '.', markersize=5, alpha=0.8, rasterized=True)\n    plt.xlim([starttime, endtime])\n    ylim = plt.ylim()\n    plt.ylabel(\"Magnitude\")\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\n    plt.gcf().autofmt_xdate()\n    plt.gca().set_prop_cycle(None)\n    plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")\n    if standard_catalog is not None:\n        plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")\n    plt.legend()\n    plt.ylim(ylim)\n    plt.grid()\n    plt.savefig(figure_dir(\"earthquake_magnitude_time.png\"), bbox_inches=\"tight\", dpi=300)\n    plt.savefig(figure_dir(\"earthquake_magnitude_time.pdf\"), bbox_inches=\"tight\", dpi=300)\n    plt.show();\n</pre> if (gamma_events[\"magnitude\"] != 999).any():     plt.figure()     plt.plot(gamma_events[\"time\"], gamma_events[\"magnitude\"], '.', markersize=5, alpha=1.0, rasterized=True)     if standard_catalog is not None:         plt.plot(standard_catalog[\"time\"], standard_catalog[\"magnitude\"], '.', markersize=5, alpha=0.8, rasterized=True)     plt.xlim([starttime, endtime])     ylim = plt.ylim()     plt.ylabel(\"Magnitude\")     plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))     plt.gcf().autofmt_xdate()     plt.gca().set_prop_cycle(None)     plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{result_label}: {len(gamma_events['magnitude'])}\")     if standard_catalog is not None:         plt.plot([],[], '.', markersize=15, alpha=1.0, label=f\"{catalog_label}: {len(standard_catalog['magnitude'])}\")     plt.legend()     plt.ylim(ylim)     plt.grid()     plt.savefig(figure_dir(\"earthquake_magnitude_time.png\"), bbox_inches=\"tight\", dpi=300)     plt.savefig(figure_dir(\"earthquake_magnitude_time.pdf\"), bbox_inches=\"tight\", dpi=300)     plt.show(); In\u00a0[33]: Copied! <pre>fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([0.8,1.1]))\nbox = dict(boxstyle='round', facecolor='white', alpha=1)\ntext_loc = [0.05, 0.90]\nplt.subplot(311)\nplt.plot(gamma_events[\"time\"], gamma_events[\"sigma_time\"], '.', markersize=3.0, label=\"Travel-time\")\nplt.ylabel(r\"$\\sigma_{11}$ (s)\")\nplt.legend(loc=\"upper right\")\nplt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.subplot(312)\nplt.plot(gamma_events[\"time\"], gamma_events[\"sigma_amp\"], '.', markersize=3.0, label=\"Amplitude\")\nplt.ylabel(r\"$\\sigma_{22}$ ($\\log10$ m/s)\")\nplt.legend(loc=\"upper right\")\nplt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.subplot(313)\nplt.plot(gamma_events[\"time\"], gamma_events[\"cov_time_amp\"], '.', markersize=3.0, label=\"Travel-time vs. Amplitude\")\nplt.ylabel(r\"$\\Sigma_{12}$\")\n# plt.ylim([-0.5, 0.5])\nplt.legend(loc=\"upper right\")\nplt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\", \n         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\nplt.gcf().autofmt_xdate()\nplt.tight_layout()\nplt.gcf().align_labels()\nplt.savefig(figure_dir(\"covariance.png\"), bbox_inches=\"tight\", dpi=300)\nplt.savefig(figure_dir(\"covariance.pdf\"), bbox_inches=\"tight\")\nplt.show();\n</pre> fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([0.8,1.1])) box = dict(boxstyle='round', facecolor='white', alpha=1) text_loc = [0.05, 0.90] plt.subplot(311) plt.plot(gamma_events[\"time\"], gamma_events[\"sigma_time\"], '.', markersize=3.0, label=\"Travel-time\") plt.ylabel(r\"$\\sigma_{11}$ (s)\") plt.legend(loc=\"upper right\") plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.subplot(312) plt.plot(gamma_events[\"time\"], gamma_events[\"sigma_amp\"], '.', markersize=3.0, label=\"Amplitude\") plt.ylabel(r\"$\\sigma_{22}$ ($\\log10$ m/s)\") plt.legend(loc=\"upper right\") plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.subplot(313) plt.plot(gamma_events[\"time\"], gamma_events[\"cov_time_amp\"], '.', markersize=3.0, label=\"Travel-time vs. Amplitude\") plt.ylabel(r\"$\\Sigma_{12}$\") # plt.ylim([-0.5, 0.5]) plt.legend(loc=\"upper right\") plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\",           transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H')) plt.gcf().autofmt_xdate() plt.tight_layout() plt.gcf().align_labels() plt.savefig(figure_dir(\"covariance.png\"), bbox_inches=\"tight\", dpi=300) plt.savefig(figure_dir(\"covariance.pdf\"), bbox_inches=\"tight\") plt.show(); In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"GaMMA/example_phasenet_ncedc/#phasenet-example","title":"PhaseNet Example\u00b6","text":"<p>Applying GaMMA to associate PhaseNet picks</p>"},{"location":"GaMMA/example_phasenet_ncedc/#1-download-demo-data","title":"1. Download demo data\u00b6","text":"<p>There are two examples in the demo data: Ridgecrest, CA and Chile.</p> <p>Phase Picks:</p> <ul> <li>test_data/ridgecrest<ul> <li>picks.csv</li> <li>stations.csv</li> <li>standard_catalog.csv</li> </ul> </li> <li>test_data/chile<ul> <li>picks.csv</li> <li>stations.csv</li> <li>iasp91.csv</li> </ul> </li> </ul> <p>Results:</p> <ul> <li>test_data/ridgecrest<ul> <li>gamma_events.csv</li> <li>gamma_picks.csv</li> </ul> </li> <li>test_data/chile<ul> <li>gamma_events.csv</li> <li>gamma_picks.csv</li> </ul> </li> </ul>"},{"location":"GaMMA/example_phasenet_ncedc/#2-associaiton-with-gamma","title":"2. Associaiton with GaMMA\u00b6","text":""},{"location":"GaMMA/example_phasenet_ncedc/#3-filtering-based-on-nearest-station-ratio-not-required","title":"3. Filtering based on Nearest Station Ratio (Not required)\u00b6","text":""},{"location":"GaMMA/example_phasenet_ncedc/#4-visualize-results","title":"4. Visualize results\u00b6","text":"<p>Note that the location and magnitude are estimated during associaiton, which are not expected to have high accuracy.</p>"},{"location":"GaMMA/example_phasenet_ransac/","title":"Example phasenet ransac","text":"In\u00a0[1]: Copied! <pre># !pip install git+https://github.com/wayneweiqiang/GaMMA.git\n</pre> # !pip install git+https://github.com/wayneweiqiang/GaMMA.git In\u00a0[2]: Copied! <pre>import pandas as pd\nfrom gamma.utils import association, estimate_eps\nimport numpy as np\nimport os\nfrom pyproj import Proj\nimport json\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd from gamma.utils import association, estimate_eps import numpy as np import os from pyproj import Proj import json import matplotlib.pyplot as plt <pre>/tmp/ipykernel_3870/2885422005.py:1: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n</pre> In\u00a0[3]: Copied! <pre>!if [ -f demo.tar ]; then rm demo.tar; fi\n!if [ -d test_data ]; then rm -rf test_data; fi\n!wget -q https://github.com/AI4EPS/GaMMA/releases/download/test_data/demo.tar\n!tar -xf demo.tar\n</pre> !if [ -f demo.tar ]; then rm demo.tar; fi !if [ -d test_data ]; then rm -rf test_data; fi !wget -q https://github.com/AI4EPS/GaMMA/releases/download/test_data/demo.tar !tar -xf demo.tar <pre>tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.provenance'\n</pre> In\u00a0[4]: Copied! <pre># region = \"ridgecrest\"\n# region = \"chile\"\nregion = \"ncedc\"\ndata_path = lambda x: os.path.join(f\"test_data/{region}\", x)\nresult_path = f\"results/{region}\"\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nresult_path = lambda x: os.path.join(f\"results/{region}\", x)\nstation_csv = data_path(\"stations.csv\")\nstation_json = data_path(\"stations.json\")\npicks_csv = data_path(\"picks.csv\")\nif not os.path.exists(\"figures\"):\n    os.makedirs(\"figures\")\nfigure_dir = lambda x: os.path.join(\"figures\", x)\n</pre> # region = \"ridgecrest\" # region = \"chile\" region = \"ncedc\" data_path = lambda x: os.path.join(f\"test_data/{region}\", x) result_path = f\"results/{region}\" if not os.path.exists(result_path):     os.makedirs(result_path) result_path = lambda x: os.path.join(f\"results/{region}\", x) station_csv = data_path(\"stations.csv\") station_json = data_path(\"stations.json\") picks_csv = data_path(\"picks.csv\") if not os.path.exists(\"figures\"):     os.makedirs(\"figures\") figure_dir = lambda x: os.path.join(\"figures\", x) In\u00a0[5]: Copied! <pre>## read picks\npicks = pd.read_csv(picks_csv, parse_dates=[\"phase_time\"])\npicks.rename(columns={\"station_id\": \"id\", \"phase_time\": \"timestamp\", \"phase_type\": \"type\", \"phase_score\": \"prob\", \"phase_amplitude\": \"amp\"}, inplace=True)\nprint(\"Pick format:\", picks.iloc[:3])\n\n## read stations\n# stations = pd.read_csv(station_csv)\n# stations.rename(columns={\"station_id\": \"id\"}, inplace=True)\nstations = pd.read_json(station_json, orient=\"index\")\nstations[\"id\"] = stations.index\nprint(\"Station format:\", stations.iloc[:3])\n\n## Automatic region; you can also specify a region\nwith open(data_path(\"config.json\"), \"r\") as f:\n    config = json.load(f)\n    config[\"center\"] = ((config[\"minlongitude\"] + config[\"maxlongitude\"]) / 2, (config[\"minlatitude\"] + config[\"maxlatitude\"]) / 2)\n    config[\"xlim_degree\"] = (config[\"minlongitude\"], config[\"maxlongitude\"])\n    config[\"ylim_degree\"] = (config[\"minlatitude\"], config[\"maxlatitude\"])\n\n# x0 = stations[\"longitude\"].median()\n# y0 = stations[\"latitude\"].median()\n# xmin = stations[\"longitude\"].min()\n# xmax = stations[\"longitude\"].max()\n# ymin = stations[\"latitude\"].min()\n# ymax = stations[\"latitude\"].max()\n# config = {}\n# config[\"center\"] = (x0, y0)\n# config[\"xlim_degree\"] = (2 * xmin - x0, 2 * xmax - x0)\n# config[\"ylim_degree\"] = (2 * ymin - y0, 2 * ymax - y0)\n\n## projection to km\nproj = Proj(f\"+proj=aeqd +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\nstations[[\"x(km)\", \"y(km)\"]] = stations.apply(lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1)\nstations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x/1e3)\nstations = stations[(stations[\"longitude\"] &gt; config[\"minlongitude\"]) &amp; (stations[\"longitude\"] &lt; config[\"maxlongitude\"]) &amp; (stations[\"latitude\"] &gt; config[\"minlatitude\"]) &amp; (stations[\"latitude\"] &lt; config[\"maxlatitude\"])]\npicks = picks[picks[\"id\"].isin(stations[\"id\"])]\n\n### setting GMMA configs\nconfig[\"use_dbscan\"] = True\nif region == \"chile\":\n    config[\"use_amplitude\"] = False\nelse:\n    config[\"use_amplitude\"] = True\nconfig[\"method\"] = \"BGMM\"  \nif config[\"method\"] == \"BGMM\": ## BayesianGaussianMixture\n    config[\"oversample_factor\"] = 5\nif config[\"method\"] == \"GMM\": ## GaussianMixture\n    config[\"oversample_factor\"] = 1\n\n# config[\"covariance_prior\"] = [1000.0, 1000.0]\n\n# earthquake location\nconfig[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\nconfig[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\nconfig[\"x(km)\"] = proj(longitude=config[\"xlim_degree\"], latitude=[config[\"center\"][1]] * 2)[0]\nconfig[\"y(km)\"] = proj(longitude=[config[\"center\"][0]] * 2, latitude=config[\"ylim_degree\"])[1]\n\nif region == \"ridgecrest\":\n    config[\"z(km)\"] = (0, 20)\nelif region == \"chile\":\n    config[\"z(km)\"] = (0, 250)\nelse:\n    config[\"z(km)\"] = (0, 30)\n    print(\"Please specify z(km) for your region\")\n    # raise NotImplementedError\nconfig[\"bfgs_bounds\"] = (\n    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n    (0, config[\"z(km)\"][1] + 1),  # z\n    (None, None),  # t\n)\n\n# DBSCAN\nconfig[\"dbscan_eps\"] = 10 #estimate_eps(stations, config[\"vel\"][\"p\"])\nconfig[\"dbscan_min_samples\"] = 3\n\n## using Eikonal for 1D velocity model\nif region == \"ridgecrest\":\n    zz = [0.0, 5.5, 16.0, 32.0]\n    vp = [5.5, 5.5,  6.7,  7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    h = 1.0\n    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\nelif region == \"chile\":\n    velocity_model = pd.read_csv(data_path(\"iasp91.csv\"), names=[\"zz\", \"rho\", \"vp\", \"vs\"])\n    velocity_model = velocity_model[velocity_model[\"zz\"] &lt;= config[\"z(km)\"][1]]\n    vel = {\"z\": velocity_model[\"zz\"].values, \"p\": velocity_model[\"vp\"].values, \"s\": velocity_model[\"vs\"].values}\n    h = 1.0\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\nelse:\n    print(\"Using uniform velocity model\")\n\nif region == \"chile\":\n    config[\"initial_points\"] = [1, 1, 1] # x, y, z\n\n# set number of cpus\nconfig[\"ncpu\"] = 32\n\n# filtering\nconfig[\"min_picks_per_eq\"] = 5\nconfig[\"min_p_picks_per_eq\"] = 0\nconfig[\"min_s_picks_per_eq\"] = 0\nconfig[\"max_sigma11\"] = 3.0*5 # second\nconfig[\"max_sigma22\"] = 1.0*3 # log10(m/s)\nconfig[\"max_sigma12\"] = 1.0*3 # covariance\n\n## filter picks without amplitude measurements\nif config[\"use_amplitude\"]:\n    picks = picks[picks[\"amp\"] != -1]\n\nfor k, v in config.items():\n    print(f\"{k}: {v}\")\n</pre> ## read picks picks = pd.read_csv(picks_csv, parse_dates=[\"phase_time\"]) picks.rename(columns={\"station_id\": \"id\", \"phase_time\": \"timestamp\", \"phase_type\": \"type\", \"phase_score\": \"prob\", \"phase_amplitude\": \"amp\"}, inplace=True) print(\"Pick format:\", picks.iloc[:3])  ## read stations # stations = pd.read_csv(station_csv) # stations.rename(columns={\"station_id\": \"id\"}, inplace=True) stations = pd.read_json(station_json, orient=\"index\") stations[\"id\"] = stations.index print(\"Station format:\", stations.iloc[:3])  ## Automatic region; you can also specify a region with open(data_path(\"config.json\"), \"r\") as f:     config = json.load(f)     config[\"center\"] = ((config[\"minlongitude\"] + config[\"maxlongitude\"]) / 2, (config[\"minlatitude\"] + config[\"maxlatitude\"]) / 2)     config[\"xlim_degree\"] = (config[\"minlongitude\"], config[\"maxlongitude\"])     config[\"ylim_degree\"] = (config[\"minlatitude\"], config[\"maxlatitude\"])  # x0 = stations[\"longitude\"].median() # y0 = stations[\"latitude\"].median() # xmin = stations[\"longitude\"].min() # xmax = stations[\"longitude\"].max() # ymin = stations[\"latitude\"].min() # ymax = stations[\"latitude\"].max() # config = {} # config[\"center\"] = (x0, y0) # config[\"xlim_degree\"] = (2 * xmin - x0, 2 * xmax - x0) # config[\"ylim_degree\"] = (2 * ymin - y0, 2 * ymax - y0)  ## projection to km proj = Proj(f\"+proj=aeqd +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\") stations[[\"x(km)\", \"y(km)\"]] = stations.apply(lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1) stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x/1e3) stations = stations[(stations[\"longitude\"] &gt; config[\"minlongitude\"]) &amp; (stations[\"longitude\"] &lt; config[\"maxlongitude\"]) &amp; (stations[\"latitude\"] &gt; config[\"minlatitude\"]) &amp; (stations[\"latitude\"] &lt; config[\"maxlatitude\"])] picks = picks[picks[\"id\"].isin(stations[\"id\"])]  ### setting GMMA configs config[\"use_dbscan\"] = True if region == \"chile\":     config[\"use_amplitude\"] = False else:     config[\"use_amplitude\"] = True config[\"method\"] = \"BGMM\"   if config[\"method\"] == \"BGMM\": ## BayesianGaussianMixture     config[\"oversample_factor\"] = 5 if config[\"method\"] == \"GMM\": ## GaussianMixture     config[\"oversample_factor\"] = 1  # config[\"covariance_prior\"] = [1000.0, 1000.0]  # earthquake location config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75} config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)'] config[\"x(km)\"] = proj(longitude=config[\"xlim_degree\"], latitude=[config[\"center\"][1]] * 2)[0] config[\"y(km)\"] = proj(longitude=[config[\"center\"][0]] * 2, latitude=config[\"ylim_degree\"])[1]  if region == \"ridgecrest\":     config[\"z(km)\"] = (0, 20) elif region == \"chile\":     config[\"z(km)\"] = (0, 250) else:     config[\"z(km)\"] = (0, 30)     print(\"Please specify z(km) for your region\")     # raise NotImplementedError config[\"bfgs_bounds\"] = (     (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x     (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y     (0, config[\"z(km)\"][1] + 1),  # z     (None, None),  # t )  # DBSCAN config[\"dbscan_eps\"] = 10 #estimate_eps(stations, config[\"vel\"][\"p\"]) config[\"dbscan_min_samples\"] = 3  ## using Eikonal for 1D velocity model if region == \"ridgecrest\":     zz = [0.0, 5.5, 16.0, 32.0]     vp = [5.5, 5.5,  6.7,  7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     h = 1.0     vel = {\"z\": zz, \"p\": vp, \"s\": vs}     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]} elif region == \"chile\":     velocity_model = pd.read_csv(data_path(\"iasp91.csv\"), names=[\"zz\", \"rho\", \"vp\", \"vs\"])     velocity_model = velocity_model[velocity_model[\"zz\"] &lt;= config[\"z(km)\"][1]]     vel = {\"z\": velocity_model[\"zz\"].values, \"p\": velocity_model[\"vp\"].values, \"s\": velocity_model[\"vs\"].values}     h = 1.0     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]} else:     print(\"Using uniform velocity model\")  if region == \"chile\":     config[\"initial_points\"] = [1, 1, 1] # x, y, z  # set number of cpus config[\"ncpu\"] = 32  # filtering config[\"min_picks_per_eq\"] = 5 config[\"min_p_picks_per_eq\"] = 0 config[\"min_s_picks_per_eq\"] = 0 config[\"max_sigma11\"] = 3.0*5 # second config[\"max_sigma22\"] = 1.0*3 # log10(m/s) config[\"max_sigma12\"] = 1.0*3 # covariance  ## filter picks without amplitude measurements if config[\"use_amplitude\"]:     picks = picks[picks[\"amp\"] != -1]  for k, v in config.items():     print(f\"{k}: {v}\")  <pre>Pick format:            id               timestamp   prob type       amp  \\\n0  BG.AL1..DP 2023-01-01 18:35:33.390  0.349    P  0.000007   \n1  BG.AL1..DP 2023-01-01 18:50:19.400  0.374    P  0.000001   \n2  BG.AL1..DP 2023-01-01 18:50:23.780  0.519    S  0.000001   \n\n                begin_time  phase_index    dt  \n0  2023-01-01T00:00:00.000      6693339  0.01  \n1  2023-01-01T00:00:00.000      6781940  0.01  \n2  2023-01-01T00:00:00.000      6782378  0.01  \nStation format:            network station location instrument component  \\\nAZ.KNW..BH      AZ     KNW                  BH     12ENZ   \nAZ.KNW..EH      AZ     KNW                  EH       12Z   \nAZ.KNW..HH      AZ     KNW                  HH     12ENZ   \n\n                                                  sensitivity  latitude  \\\nAZ.KNW..BH  [839091000.0, 209773000.0, 839091000.0, 209773...   33.7141   \nAZ.KNW..EH  [31432900.0, 15716500.0, 31432900.0, 15716500....   33.7141   \nAZ.KNW..HH  [157649000.0, 39412300.0, 840484000.0, 2101210...   33.7141   \n\n            longitude  elevation_m  depth_km     x_km     y_km   z_km  \\\nAZ.KNW..BH  -116.7119       1507.0    -1.507  258.718 -416.537 -1.507   \nAZ.KNW..EH  -116.7119       1507.0    -1.507  258.718 -416.537 -1.507   \nAZ.KNW..HH  -116.7119       1507.0    -1.507  258.718 -416.537 -1.507   \n\n           provider          id  \nAZ.KNW..BH       NC  AZ.KNW..BH  \nAZ.KNW..EH       NC  AZ.KNW..EH  \nAZ.KNW..HH       NC  AZ.KNW..HH  \nPlease specify z(km) for your region\nUsing uniform velocity model\nminlatitude: 32\nmaxlatitude: 43\nminlongitude: -125\nmaxlongitude: -114.0\nnum_nodes: 1\nsampling_rate: 100\ndegree2km: 111.1949\nchannel: HH*,BH*,EH*,HN*\nlevel: response\ngamma: {'zmin_km': 0, 'zmax_km': 60}\ncctorch: {'sampling_rate': 100, 'time_before': 0.25, 'time_after': 1.0, 'min_pair_dist_km': 10, 'components': 'ENZ123', 'component_mapping': {'3': 0, '2': 1, '1': 2, 'E': 0, 'N': 1, 'Z': 2}}\ncenter: (-119.5, 37.5)\nxlim_degree: (-125, -114.0)\nylim_degree: (32, 43)\nuse_dbscan: True\nuse_amplitude: True\nmethod: BGMM\noversample_factor: 5\nvel: {'p': 6.0, 's': 3.4285714285714284}\ndims: ['x(km)', 'y(km)', 'z(km)']\nx(km): (-486.29893938501783, 486.2989393850155)\ny(km): (-610.6165417479444, 611.185297582399)\nz(km): (0, 30)\nbfgs_bounds: ((-487.29893938501783, 487.2989393850155), (-611.6165417479444, 612.185297582399), (0, 31), (None, None))\ndbscan_eps: 10\ndbscan_min_samples: 3\nncpu: 32\nmin_picks_per_eq: 5\nmin_p_picks_per_eq: 0\nmin_s_picks_per_eq: 0\nmax_sigma11: 15.0\nmax_sigma22: 3.0\nmax_sigma12: 3.0\n</pre> In\u00a0[6]: Copied! <pre>from sklearn.cluster import DBSCAN\nimport matplotlib.dates as mdates\n</pre> from sklearn.cluster import DBSCAN import matplotlib.dates as mdates In\u00a0[7]: Copied! <pre>picks_ = picks.merge(stations[[\"id\", \"x(km)\", \"y(km)\", \"z(km)\", \"longitude\", \"latitude\"]], on=\"id\")\npicks_[\"t\"] = (picks_[\"timestamp\"] - picks_[\"timestamp\"].min()).dt.total_seconds()\ndata = picks_[[\"t\", \"x(km)\", \"y(km)\", \"z(km)\"]].values\n\nvel = np.average(config[\"vel\"][\"p\"])\ndb = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(data[:, :3] / np.array([1, vel, vel]))\nlabels = db.labels_\nunique_labels = set(labels)\nunique_labels = unique_labels.difference([-1])\n\nlat0, lon0 = 40.409, -123.971\npicks_[\"dist_km\"] = np.sqrt((picks_[\"latitude\"] - lat0)**2 + (picks_[\"longitude\"] - lon0)**2) * 111.32\npicks_[\"label\"] = labels \ncenter = picks_.groupby(\"label\").agg({\"x(km)\": \"mean\", \"y(km)\": \"mean\", \"z(km)\": \"mean\", \"timestamp\": \"min\", \"latitude\": \"mean\", \"longitude\": \"mean\"}).reset_index()\ncenter[\"dist_km\"] = np.sqrt((center[\"latitude\"] - lat0)**2 + (center[\"longitude\"] - lon0)**2) * 111.32\n</pre> picks_ = picks.merge(stations[[\"id\", \"x(km)\", \"y(km)\", \"z(km)\", \"longitude\", \"latitude\"]], on=\"id\") picks_[\"t\"] = (picks_[\"timestamp\"] - picks_[\"timestamp\"].min()).dt.total_seconds() data = picks_[[\"t\", \"x(km)\", \"y(km)\", \"z(km)\"]].values  vel = np.average(config[\"vel\"][\"p\"]) db = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(data[:, :3] / np.array([1, vel, vel])) labels = db.labels_ unique_labels = set(labels) unique_labels = unique_labels.difference([-1])  lat0, lon0 = 40.409, -123.971 picks_[\"dist_km\"] = np.sqrt((picks_[\"latitude\"] - lat0)**2 + (picks_[\"longitude\"] - lon0)**2) * 111.32 picks_[\"label\"] = labels  center = picks_.groupby(\"label\").agg({\"x(km)\": \"mean\", \"y(km)\": \"mean\", \"z(km)\": \"mean\", \"timestamp\": \"min\", \"latitude\": \"mean\", \"longitude\": \"mean\"}).reset_index() center[\"dist_km\"] = np.sqrt((center[\"latitude\"] - lat0)**2 + (center[\"longitude\"] - lon0)**2) * 111.32  In\u00a0[8]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], picks_[\"latitude\"], c=picks_[\"label\"].apply(mapping_color), s=1)\nplt.scatter(center[\"timestamp\"], center[\"latitude\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\n# plt set x time format to %Y-%m-%d %H:%M:%S\nax = plt.gca()  # Get the current Axes instance\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\nplt.gcf().autofmt_xdate()\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], picks_[\"latitude\"], c=picks_[\"label\"].apply(mapping_color), s=1) plt.scatter(center[\"timestamp\"], center[\"latitude\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") # plt set x time format to %Y-%m-%d %H:%M:%S ax = plt.gca()  # Get the current Axes instance ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S')) plt.gcf().autofmt_xdate() In\u00a0[9]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1)\nplt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1) plt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") Out[9]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f200f5a6ce0&gt;</pre> In\u00a0[10]: Copied! <pre>plt.figure(figsize=(20, 5))\nmapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\"\nplt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1)\nplt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\")\nplt.xlim(pd.to_datetime(\"2023-01-01 18:35:00\"), pd.to_datetime(\"2023-01-01 18:40:00\"))\n</pre> plt.figure(figsize=(20, 5)) mapping_color = lambda x: f\"C{x}\" if x &gt;= 0 else \"k\" plt.scatter(picks_[\"timestamp\"], picks_[\"dist_km\"], c=picks_[\"label\"].apply(mapping_color), s=1) plt.scatter(center[\"timestamp\"], center[\"dist_km\"], c=center[\"label\"].apply(mapping_color), s=100, marker=\"x\") plt.xlim(pd.to_datetime(\"2023-01-01 18:35:00\"), pd.to_datetime(\"2023-01-01 18:40:00\")) Out[10]: <pre>(19358.774305555555, 19358.777777777777)</pre> In\u00a0[11]: Copied! <pre>picks_window = picks_[(picks_[\"timestamp\"] &gt; pd.to_datetime(\"2023-01-01 18:35:00\")) &amp; (picks_[\"timestamp\"] &lt; pd.to_datetime(\"2023-01-01 18:37:00\"))]\n</pre> picks_window = picks_[(picks_[\"timestamp\"] &gt; pd.to_datetime(\"2023-01-01 18:35:00\")) &amp; (picks_[\"timestamp\"] &lt; pd.to_datetime(\"2023-01-01 18:37:00\"))] In\u00a0[12]: Copied! <pre>plt.figure()\nplt.plot(picks_window[\"timestamp\"], picks_window[\"dist_km\"], \"k.\")\n</pre> plt.figure() plt.plot(picks_window[\"timestamp\"], picks_window[\"dist_km\"], \"k.\") Out[12]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f2011d5a890&gt;]</pre> In\u00a0[13]: Copied! <pre>picks_window\n</pre> picks_window Out[13]: id timestamp prob type amp begin_time phase_index dt x(km) y(km) z(km) longitude latitude t dist_km label 0 BK.AASB.00.HH 2023-01-01 18:35:49.170 0.476 P 0.000007 2023-01-01T00:00:00.000 6694917 0.01 -140.566236 104.472114 -0.0658 -121.109750 38.430260 348.180 387.261842 0 5 BK.AONC.00.HH 2023-01-01 18:35:29.073 0.799 P 0.000043 2023-01-01T00:00:00.003 6692907 0.01 -218.961038 273.946479 -0.0669 -122.060800 39.939810 328.083 218.964015 0 8 BK.AONC.00.HH 2023-01-01 18:35:01.173 0.389 S 0.000001 2023-01-01T00:00:00.003 6690117 0.01 -218.961038 273.946479 -0.0669 -122.060800 39.939810 300.183 218.964015 -1 9 BK.AONC.00.HH 2023-01-01 18:35:48.293 0.390 S 0.000165 2023-01-01T00:00:00.003 6694829 0.01 -218.961038 273.946479 -0.0669 -122.060800 39.939810 347.303 218.964015 5 14 BK.BAKR.00.HH 2023-01-01 18:35:24.040 0.706 P 0.000133 2023-01-01T00:00:00.000 6692404 0.01 -254.611447 312.892997 -0.1901 -122.492261 40.280351 323.050 165.235017 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5219 WR.SLD.10.HN 2023-01-01 18:36:04.280 0.790 P 0.000003 2023-01-01T00:00:00.000 6696428 0.01 -153.059809 -45.808513 -0.1360 -121.221222 37.074699 363.290 481.114199 0 5220 WR.SPR.10.HN 2023-01-01 18:35:20.390 0.678 P 0.000008 2023-01-01T00:00:00.000 6692039 0.01 -138.874745 -45.338183 -0.1150 -121.061836 37.081154 319.400 492.051956 -1 5227 WR.THER.00.HN 2023-01-01 18:35:34.470 0.410 P 0.000008 2023-01-01T00:00:00.000 6693447 0.01 -188.311791 222.507700 -0.0367 -121.688221 39.484103 333.480 274.184446 0 5228 WR.THER.00.HN 2023-01-01 18:35:35.050 0.495 P 0.000026 2023-01-01T00:00:00.000 6693505 0.01 -188.311791 222.507700 -0.0367 -121.688221 39.484103 334.060 274.184446 0 5229 WR.THER.00.HN 2023-01-01 18:35:56.770 0.338 S 0.000107 2023-01-01T00:00:00.000 6695677 0.01 -188.311791 222.507700 -0.0367 -121.688221 39.484103 355.780 274.184446 263 <p>766 rows \u00d7 16 columns</p> In\u00a0[14]: Copied! <pre>stations\n</pre> stations Out[14]: network station location instrument component sensitivity latitude longitude elevation_m depth_km x_km y_km z_km provider id x(km) y(km) z(km) AZ.KNW..BH AZ KNW BH 12ENZ [839091000.0, 209773000.0, 839091000.0, 209773... 33.714100 -116.711900 1507.0 -1.5070 258.718 -416.537 -1.507 NC AZ.KNW..BH 258.718033 -416.537133 -1.5070 AZ.KNW..EH AZ KNW EH 12Z [31432900.0, 15716500.0, 31432900.0, 15716500.... 33.714100 -116.711900 1507.0 -1.5070 258.718 -416.537 -1.507 NC AZ.KNW..EH 258.718033 -416.537133 -1.5070 AZ.KNW..HH AZ KNW HH 12ENZ [157649000.0, 39412300.0, 840484000.0, 2101210... 33.714100 -116.711900 1507.0 -1.5070 258.718 -416.537 -1.507 NC AZ.KNW..HH 258.718033 -416.537133 -1.5070 AZ.KNW..HN AZ KNW HN ENZ [536936.0, 536936.0, 536936.0, 536936.0, 26846... 33.714100 -116.711900 1507.0 -1.5070 258.718 -416.537 -1.507 NC AZ.KNW..HN 258.718033 -416.537133 -1.5070 AZ.LVA2..BH AZ LVA2 BH ENZ [839091000.0, 209773000.0, 789683000.0, 789683... 33.351601 -116.561501 1435.0 -1.4350 273.875 -456.385 -1.435 NC AZ.LVA2..BH 273.874617 -456.384630 -1.4350 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... WR.STNI.10.HN WR STNI 10 HN ENZ [213669.0, 213584.0, 214781.0] 38.119679 -121.540173 -0.7 0.0007 -178.905 70.734 0.001 NC WR.STNI.10.HN -178.904657 70.733524 0.0007 WR.THER.00.HN WR THER 00 HN ENZ [856798.0, 856798.0, 856798.0, 857141.0, 85714... 39.484103 -121.688221 36.7 -0.0367 -188.312 222.508 -0.037 NC WR.THER.00.HN -188.311791 222.507700 -0.0367 WR.THER.01.HN WR THER 01 HN ENZ [857141.0, 857141.0, 857141.0, 860562.0, 86056... 39.484142 -121.687941 44.0 -0.0440 -188.288 222.511 -0.044 NC WR.THER.01.HN -188.287593 222.511458 -0.0440 WR.THER.02.HN WR THER 02 HN ENZ [851324.0, 851324.0, 851324.0, 851324.0, 85132... 39.484135 -121.688224 21.4 -0.0214 -188.312 222.511 -0.021 NC WR.THER.02.HN -188.311964 222.511260 -0.0214 WR.THER.03.HN WR THER 03 HN ENZ [851324.0, 851324.0, 851324.0, 427031.0, 42703... 39.484069 -121.688224 -24.3 0.0243 -188.312 222.504 0.024 NC WR.THER.03.HN -188.312139 222.503931 0.0243 <p>6317 rows \u00d7 18 columns</p> In\u00a0[15]: Copied! <pre>import numpy as np\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.linear_model import RANSACRegressor\n\n\nclass EQLoc(BaseEstimator):\n    def __init__(self, config, event=None):\n        \"\"\"\n        event: [t, x, y, z]\n        \"\"\"\n        xlim = config[\"xlim_km\"]\n        ylim = config[\"ylim_km\"]\n        zlim = config[\"zlim_km\"]\n        vel = config[\"vel\"]\n        self.config = config\n        self.vel = vel\n\n        if event is not None:\n            self.event = event\n        else:\n            self.event = np.array([0, (xlim[0] + xlim[1]) / 2, (ylim[0] + ylim[1]) / 2, (zlim[0] + zlim[1]) / 2])\n        # self.vel = {\"P\": 6.0, \"S\": 6.0 / 1.75}\n        # self.vel = {0: 6.0, 1: 6.0 / 1.75}\n        \n\n    @staticmethod\n    def loss(event, X, y, vel={0: 6.0, 1: 6.0 / 1.75}):\n        \"\"\"\n        X: data_frame with columns [\"timestamp\", \"x_km\", \"y_km\", \"z_km\", \"type\"] \n        \"\"\"\n        # dataframe\n        # xyz = X[[\"x_km\", \"y_km\", \"z_km\"]].values\n        # v = X[\"vel\"].values\n        # numpy\n        xyz = X[:, :3]\n        type = X[:, 3]\n        v = np.array([vel[t] for t in type])\n\n        event_t = event[0]\n        event_loc = event[1:]\n\n        tt = np.linalg.norm(xyz - event_loc, axis=-1) / v + event_t\n        loss = 0.5 * np.sum((tt - y)**2)\n\n        J = np.zeros((len(X), 4))\n        diff = tt - y\n        J[:, 0] = diff\n        J[:, 1:] = (event_loc - xyz) /  np.linalg.norm(xyz - event_loc, axis=-1, keepdims=True) / v[:,np.newaxis] * diff[:,np.newaxis]\n\n        J = np.sum(J, axis=0)\n        return loss, J\n\n    def fit(self, X, y=None):\n        \n        opt = scipy.optimize.minimize(\n            self.loss,\n            x0 = self.event,\n            method=\"L-BFGS-B\",\n            jac=True,\n            args=(X, y, self.vel), \n            # args=(phase_time, phase_type, station_loc, weight, vel, 1, eikonal),\n            # bounds=bounds,\n            # options={\"maxiter\": max_iter, \"gtol\": convergence, \"iprint\": -1},\n        )\n\n        self.event = opt.x\n        self.is_fitted_ = True\n        \n        return self\n    \n    def predict(self, X):\n        \"\"\"\n        X: data_frame with columns [\"timestamp\", \"x_km\", \"y_km\", \"z_km\", \"type\"] \n        \"\"\"\n        # dataframe\n        # xyz = X[[\"x_km\", \"y_km\", \"z_km\"]].values\n        # v = X[\"vel\"].values\n        # numpy\n        xyz = X[:, :3]\n        type = X[:, 3]\n        v = np.array([self.vel[t] for t in type])\n        event_t = self.event[0]\n        event_loc = self.event[1:]\n        tt = np.linalg.norm(xyz - event_loc, axis=-1) / v + event_t\n    \n        return tt\n    \n    def score(self, X, y=None):\n        \"\"\"\n        X: data_frame with columns [\"timestamp\", \"x_km\", \"y_km\", \"z_km\", \"type\"] \n        \"\"\"\n        if len(X) == 0:\n            return 0\n        tt = self.predict(X)\n        R2 = 1 - np.sum((y - tt)**2) / np.sum((y - np.mean(y))**2)\n        print(f\"{R2=}\")\n        return R2\n        \n</pre> import numpy as np import scipy from sklearn.base import BaseEstimator from sklearn.linear_model import RANSACRegressor   class EQLoc(BaseEstimator):     def __init__(self, config, event=None):         \"\"\"         event: [t, x, y, z]         \"\"\"         xlim = config[\"xlim_km\"]         ylim = config[\"ylim_km\"]         zlim = config[\"zlim_km\"]         vel = config[\"vel\"]         self.config = config         self.vel = vel          if event is not None:             self.event = event         else:             self.event = np.array([0, (xlim[0] + xlim[1]) / 2, (ylim[0] + ylim[1]) / 2, (zlim[0] + zlim[1]) / 2])         # self.vel = {\"P\": 6.0, \"S\": 6.0 / 1.75}         # self.vel = {0: 6.0, 1: 6.0 / 1.75}               @staticmethod     def loss(event, X, y, vel={0: 6.0, 1: 6.0 / 1.75}):         \"\"\"         X: data_frame with columns [\"timestamp\", \"x_km\", \"y_km\", \"z_km\", \"type\"]          \"\"\"         # dataframe         # xyz = X[[\"x_km\", \"y_km\", \"z_km\"]].values         # v = X[\"vel\"].values         # numpy         xyz = X[:, :3]         type = X[:, 3]         v = np.array([vel[t] for t in type])          event_t = event[0]         event_loc = event[1:]          tt = np.linalg.norm(xyz - event_loc, axis=-1) / v + event_t         loss = 0.5 * np.sum((tt - y)**2)          J = np.zeros((len(X), 4))         diff = tt - y         J[:, 0] = diff         J[:, 1:] = (event_loc - xyz) /  np.linalg.norm(xyz - event_loc, axis=-1, keepdims=True) / v[:,np.newaxis] * diff[:,np.newaxis]          J = np.sum(J, axis=0)         return loss, J      def fit(self, X, y=None):                  opt = scipy.optimize.minimize(             self.loss,             x0 = self.event,             method=\"L-BFGS-B\",             jac=True,             args=(X, y, self.vel),              # args=(phase_time, phase_type, station_loc, weight, vel, 1, eikonal),             # bounds=bounds,             # options={\"maxiter\": max_iter, \"gtol\": convergence, \"iprint\": -1},         )          self.event = opt.x         self.is_fitted_ = True                  return self          def predict(self, X):         \"\"\"         X: data_frame with columns [\"timestamp\", \"x_km\", \"y_km\", \"z_km\", \"type\"]          \"\"\"         # dataframe         # xyz = X[[\"x_km\", \"y_km\", \"z_km\"]].values         # v = X[\"vel\"].values         # numpy         xyz = X[:, :3]         type = X[:, 3]         v = np.array([self.vel[t] for t in type])         event_t = self.event[0]         event_loc = self.event[1:]         tt = np.linalg.norm(xyz - event_loc, axis=-1) / v + event_t              return tt          def score(self, X, y=None):         \"\"\"         X: data_frame with columns [\"timestamp\", \"x_km\", \"y_km\", \"z_km\", \"type\"]          \"\"\"         if len(X) == 0:             return 0         tt = self.predict(X)         R2 = 1 - np.sum((y - tt)**2) / np.sum((y - np.mean(y))**2)         print(f\"{R2=}\")         return R2          In\u00a0[16]: Copied! <pre># https://earthquake.usgs.gov/earthquakes/eventpage/nc73827571/executive\nlat0, lon0, z0 = 40.409, -123.971, 30.6\norigin_time = pd.to_datetime(\"2023-01-01 18:35:04\")\nx0, y0 = proj(lon0, lat0)\nprint(f\"{x0 = }, {y0 = }\")\n</pre> # https://earthquake.usgs.gov/earthquakes/eventpage/nc73827571/executive lat0, lon0, z0 = 40.409, -123.971, 30.6 origin_time = pd.to_datetime(\"2023-01-01 18:35:04\") x0, y0 = proj(lon0, lat0) print(f\"{x0 = }, {y0 = }\") <pre>x0 = -379.7164208407868, y0 = 332.33303405425085\n</pre> In\u00a0[17]: Copied! <pre># picks_window = picks_[(picks_[\"timestamp\"] &gt; pd.to_datetime(\"2023-01-01 18:35:00\")) &amp; (picks_[\"timestamp\"] &lt; pd.to_datetime(\"2023-01-01 18:37:00\"))]\npicks_window = picks_[(picks_[\"timestamp\"] &gt; pd.to_datetime(\"2023-01-01 18:43:30\")) &amp; (picks_[\"timestamp\"] &lt; pd.to_datetime(\"2023-01-01 18:45:00\"))]\n\nX = picks_window[[\"timestamp\", \"x(km)\", \"y(km)\", \"z(km)\", \"type\"]].copy()\nX.rename(columns={\"x(km)\": \"x_km\", \"y(km)\": \"y_km\", \"z(km)\": \"z_km\"}, inplace=True)\nt0 = X[\"timestamp\"].min()\nconfig[\"vel\"] = {\"p\": 7.5, \"s\": 7.5 / 1.75}\nmapping = {\"p\": 0, \"s\": 1}\nconfig[\"vel\"] = {mapping[k]: v for k, v in config[\"vel\"].items()}\nX[\"type\"] = X[\"type\"].apply(lambda x: mapping[x.lower()])\nX[\"t_s\"] = (X[\"timestamp\"] - t0).dt.total_seconds().copy()\n</pre> # picks_window = picks_[(picks_[\"timestamp\"] &gt; pd.to_datetime(\"2023-01-01 18:35:00\")) &amp; (picks_[\"timestamp\"] &lt; pd.to_datetime(\"2023-01-01 18:37:00\"))] picks_window = picks_[(picks_[\"timestamp\"] &gt; pd.to_datetime(\"2023-01-01 18:43:30\")) &amp; (picks_[\"timestamp\"] &lt; pd.to_datetime(\"2023-01-01 18:45:00\"))]  X = picks_window[[\"timestamp\", \"x(km)\", \"y(km)\", \"z(km)\", \"type\"]].copy() X.rename(columns={\"x(km)\": \"x_km\", \"y(km)\": \"y_km\", \"z(km)\": \"z_km\"}, inplace=True) t0 = X[\"timestamp\"].min() config[\"vel\"] = {\"p\": 7.5, \"s\": 7.5 / 1.75} mapping = {\"p\": 0, \"s\": 1} config[\"vel\"] = {mapping[k]: v for k, v in config[\"vel\"].items()} X[\"type\"] = X[\"type\"].apply(lambda x: mapping[x.lower()]) X[\"t_s\"] = (X[\"timestamp\"] - t0).dt.total_seconds().copy()  In\u00a0[18]: Copied! <pre>plt.figure()\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1)\n</pre> plt.figure() plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1) Out[18]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f2011cdd570&gt;</pre> In\u00a0[19]: Copied! <pre>plt.figure()\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"dist_km\"], c=picks_window[\"label\"].apply(mapping_color), s=1)\n</pre> plt.figure() plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"dist_km\"], c=picks_window[\"label\"].apply(mapping_color), s=1) Out[19]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f2011a772b0&gt;</pre> In\u00a0[20]: Copied! <pre>config_ = {\"xlim_km\": config[\"x(km)\"], \"ylim_km\": config[\"y(km)\"], \"zlim_km\": config[\"z(km)\"], \"vel\": config[\"vel\"]}\n# estimator = EQLoc(config_, event_loc=np.array([x0, y0, z0]), event_t=(origin_time - t0).total_seconds())\nestimator = EQLoc(config_)\ntt = estimator.predict(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values)\nestimator.score(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, y=X[\"t_s\"].values)\n</pre> config_ = {\"xlim_km\": config[\"x(km)\"], \"ylim_km\": config[\"y(km)\"], \"zlim_km\": config[\"z(km)\"], \"vel\": config[\"vel\"]} # estimator = EQLoc(config_, event_loc=np.array([x0, y0, z0]), event_t=(origin_time - t0).total_seconds()) estimator = EQLoc(config_) tt = estimator.predict(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values) estimator.score(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, y=X[\"t_s\"].values) <pre>R2=-3.107563041066297\n</pre> Out[20]: <pre>-3.107563041066297</pre> In\u00a0[21]: Copied! <pre>estimator\n</pre> estimator Out[21]: <pre>EQLoc(config={'vel': {0: 7.5, 1: 4.285714285714286},\n              'xlim_km': (-486.29893938501783, 486.2989393850155),\n              'ylim_km': (-610.6165417479444, 611.185297582399),\n              'zlim_km': (0, 30)},\n      event=array([ 0.00000000e+00, -1.16529009e-12,  2.84377917e-01,  1.50000000e+01]))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0EQLociNot fitted<pre>EQLoc(config={'vel': {0: 7.5, 1: 4.285714285714286},\n              'xlim_km': (-486.29893938501783, 486.2989393850155),\n              'ylim_km': (-610.6165417479444, 611.185297582399),\n              'zlim_km': (0, 30)},\n      event=array([ 0.00000000e+00, -1.16529009e-12,  2.84377917e-01,  1.50000000e+01]))</pre> In\u00a0[22]: Copied! <pre>plt.figure()\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=\"gray\", s=1)\nplt.scatter(tt, picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1)\n</pre> plt.figure() plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=\"gray\", s=1) plt.scatter(tt, picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1) Out[22]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f2011a034f0&gt;</pre> In\u00a0[23]: Copied! <pre>estimator.fit(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, y=X[\"t_s\"].values)\nestimator.score(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, y=X[\"t_s\"].values)\ntt = estimator.predict(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values)\n</pre> estimator.fit(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, y=X[\"t_s\"].values) estimator.score(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, y=X[\"t_s\"].values) tt = estimator.predict(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values) <pre>R2=-1.0992846369968086\n</pre> In\u00a0[24]: Copied! <pre>estimator\n</pre> estimator Out[24]: <pre>EQLoc(config={'vel': {0: 7.5, 1: 4.285714285714286},\n              'xlim_km': (-486.29893938501783, 486.2989393850155),\n              'ylim_km': (-610.6165417479444, 611.185297582399),\n              'zlim_km': (0, 30)},\n      event=array([ -18.40470069, -106.27319428,  147.30408598,  179.00896123]))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0EQLociFitted<pre>EQLoc(config={'vel': {0: 7.5, 1: 4.285714285714286},\n              'xlim_km': (-486.29893938501783, 486.2989393850155),\n              'ylim_km': (-610.6165417479444, 611.185297582399),\n              'zlim_km': (0, 30)},\n      event=array([ -18.40470069, -106.27319428,  147.30408598,  179.00896123]))</pre> In\u00a0[25]: Copied! <pre>plt.figure()\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=\"gray\", s=1)\nplt.scatter(tt, picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1)\n</pre> plt.figure() plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=\"gray\", s=1) plt.scatter(tt, picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1) Out[25]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f2011a733a0&gt;</pre> In\u00a0[26]: Copied! <pre># reg = RANSACRegressor(estimator=EQLoc(config_, event_loc=np.array([[x0, y0, 0]]), event_t=0), random_state=0, min_samples=10, residual_threshold=4.0).fit(X[[\"x_km\", \"y_km\", \"z_km\", \"vel\"]].values, X[\"t_s\"].values)\n# reg = RANSACRegressor(estimator=EQLoc(config_), random_state=0, min_samples=5, residual_threshold=1.0).fit(X[[\"x_km\", \"y_km\", \"z_km\", \"vel\"]].values, X[\"t_s\"].values)\nreg = RANSACRegressor(estimator=EQLoc(config_), random_state=0, min_samples=4, residual_threshold=1.0).fit(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, X[\"t_s\"].values)\nmask = reg.inlier_mask_\n</pre> # reg = RANSACRegressor(estimator=EQLoc(config_, event_loc=np.array([[x0, y0, 0]]), event_t=0), random_state=0, min_samples=10, residual_threshold=4.0).fit(X[[\"x_km\", \"y_km\", \"z_km\", \"vel\"]].values, X[\"t_s\"].values) # reg = RANSACRegressor(estimator=EQLoc(config_), random_state=0, min_samples=5, residual_threshold=1.0).fit(X[[\"x_km\", \"y_km\", \"z_km\", \"vel\"]].values, X[\"t_s\"].values) reg = RANSACRegressor(estimator=EQLoc(config_), random_state=0, min_samples=4, residual_threshold=1.0).fit(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values, X[\"t_s\"].values) mask = reg.inlier_mask_ <pre>R2=0.9993119923582199\nR2=0.999109936172186\nR2=0.9990283747975219\nR2=0.998868125087602\nR2=0.9993438148380437\nR2=0.9984155618858801\nR2=0.9986155026229359\n</pre> In\u00a0[27]: Copied! <pre>reg.estimator_\n</pre> reg.estimator_ Out[27]: <pre>EQLoc(config={'vel': {0: 7.5, 1: 4.285714285714286},\n              'xlim_km': (-486.29893938501783, 486.2989393850155),\n              'ylim_km': (-610.6165417479444, 611.185297582399),\n              'zlim_km': (0, 30)},\n      event=array([  18.56194543, -393.91449824,  327.36059711,   47.86903248]))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0EQLociFitted<pre>EQLoc(config={'vel': {0: 7.5, 1: 4.285714285714286},\n              'xlim_km': (-486.29893938501783, 486.2989393850155),\n              'ylim_km': (-610.6165417479444, 611.185297582399),\n              'zlim_km': (0, 30)},\n      event=array([  18.56194543, -393.91449824,  327.36059711,   47.86903248]))</pre> In\u00a0[28]: Copied! <pre>tt = reg.predict(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values)\n</pre> tt = reg.predict(X[[\"x_km\", \"y_km\", \"z_km\", \"type\"]].values) In\u00a0[29]: Copied! <pre>plt.figure()\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=\"gray\", s=1)\nplt.scatter(tt, picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1)\nplt.scatter(tt, picks_window[\"y(km)\"], c=mask, s=1)\n</pre> plt.figure() plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"y(km)\"], c=\"gray\", s=1) plt.scatter(tt, picks_window[\"y(km)\"], c=picks_window[\"label\"].apply(mapping_color), s=1) plt.scatter(tt, picks_window[\"y(km)\"], c=mask, s=1) Out[29]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f200c9c5180&gt;</pre> In\u00a0[30]: Copied! <pre>plt.figure()\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"dist_km\"], c=\"gray\", s=1)\nplt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"dist_km\"], c=mask, s=1)\n\n# plt.scatter(tt, picks_window[\"dist_km\"], c=picks_window[\"label\"].apply(mapping_color), s=1)\n# plt.scatter(tt, picks_window[\"dist_km\"], c=mask, s=1)\n</pre> plt.figure() plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"dist_km\"], c=\"gray\", s=1) plt.scatter((picks_window[\"timestamp\"]-t0).dt.total_seconds(), picks_window[\"dist_km\"], c=mask, s=1)  # plt.scatter(tt, picks_window[\"dist_km\"], c=picks_window[\"label\"].apply(mapping_color), s=1) # plt.scatter(tt, picks_window[\"dist_km\"], c=mask, s=1) Out[30]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f2011a73460&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"GaMMA/example_phasenet_ransac/#phasenet-example","title":"PhaseNet Example\u00b6","text":"<p>Applying GaMMA to associate PhaseNet picks</p>"},{"location":"GaMMA/example_phasenet_ransac/#1-download-demo-data","title":"1. Download demo data\u00b6","text":"<p>There are two examples in the demo data: Ridgecrest, CA and Chile.</p> <p>Phase Picks:</p> <ul> <li>test_data/ridgecrest<ul> <li>picks.csv</li> <li>stations.csv</li> <li>standard_catalog.csv</li> </ul> </li> <li>test_data/chile<ul> <li>picks.csv</li> <li>stations.csv</li> <li>iasp91.csv</li> </ul> </li> </ul> <p>Results:</p> <ul> <li>test_data/ridgecrest<ul> <li>gamma_events.csv</li> <li>gamma_picks.csv</li> </ul> </li> <li>test_data/chile<ul> <li>gamma_events.csv</li> <li>gamma_picks.csv</li> </ul> </li> </ul>"},{"location":"GaMMA/example_seisbench/","title":"Seisbench Example","text":"<p>This code is necessary on colab to install SeisBench and the other required dependencies. If all dependencies are already installed on your machine, you can skip this.</p> In\u00a0[4]: Copied! <pre>!pip install seisbench pyproj seaborn\n!pip install git+https://github.com/wayneweiqiang/GaMMA.git\n</pre> !pip install seisbench pyproj seaborn !pip install git+https://github.com/wayneweiqiang/GaMMA.git <p>This cell is required to circumvent an issue with colab and obspy. For details, check this issue in the obspy documentation: https://github.com/obspy/obspy/issues/2547</p> In\u00a0[5]: Copied! <pre>try:\n    import obspy\n    obspy.read()\nexcept TypeError:\n    # Needs to restart the runtime once, because obspy only works properly after restart.\n    print('Stopping RUNTIME. If you run this code for the first time, this is expected. Colaboratory will restart automatically. Please run again.')\n    exit()\n</pre> try:     import obspy     obspy.read() except TypeError:     # Needs to restart the runtime once, because obspy only works properly after restart.     print('Stopping RUNTIME. If you run this code for the first time, this is expected. Colaboratory will restart automatically. Please run again.')     exit() In\u00a0[6]: Copied! <pre>import obspy\nfrom obspy.clients.fdsn import Client\nfrom obspy import UTCDateTime\nfrom pyproj import CRS, Transformer, Proj\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\nfrom gamma.utils import association\nimport seisbench.models as sbm\n\nsns.set(font_scale=1.2)\nsns.set_style(\"ticks\")\n</pre> import obspy from obspy.clients.fdsn import Client from obspy import UTCDateTime from pyproj import CRS, Transformer, Proj import pandas as pd import numpy as np from collections import Counter from tqdm import tqdm import matplotlib.pyplot as plt import seaborn as sns import torch  from gamma.utils import association import seisbench.models as sbm  sns.set(font_scale=1.2) sns.set_style(\"ticks\") In\u00a0[7]: Copied! <pre># Projections\nwgs84 = CRS.from_epsg(4326)\nlocal_crs = CRS.from_epsg(9155)  # SIRGAS-Chile 2016 / UTM zone 19S\ntransformer = Transformer.from_crs(wgs84, local_crs)\n\n# Gamma\nconfig = {}\nconfig[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\nconfig[\"use_dbscan\"] = True\nconfig[\"use_amplitude\"] = False\nconfig[\"x(km)\"] = (250, 600)\nconfig[\"y(km)\"] = (7200, 8000)\nconfig[\"z(km)\"] = (0, 150)\nconfig[\"vel\"] = {\"p\": 7.0, \"s\": 7.0 / 1.75}  # We assume rather high velocities as we expect deeper events\nconfig[\"method\"] = \"BGMM\"\nif config[\"method\"] == \"BGMM\":\n    config[\"oversample_factor\"] = 4\nif config[\"method\"] == \"GMM\":\n    config[\"oversample_factor\"] = 1\n\n# DBSCAN\nconfig[\"bfgs_bounds\"] = (\n    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n    (0, config[\"z(km)\"][1] + 1),  # x\n    (None, None),  # t\n)\nconfig[\"dbscan_eps\"] = 25  # seconds\nconfig[\"dbscan_min_samples\"] = 3\n\n# Filtering\nconfig[\"min_picks_per_eq\"] = 5\nconfig[\"max_sigma11\"] = 2.0\nconfig[\"max_sigma22\"] = 1.0\nconfig[\"max_sigma12\"] = 1.0\n</pre> # Projections wgs84 = CRS.from_epsg(4326) local_crs = CRS.from_epsg(9155)  # SIRGAS-Chile 2016 / UTM zone 19S transformer = Transformer.from_crs(wgs84, local_crs)  # Gamma config = {} config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)'] config[\"use_dbscan\"] = True config[\"use_amplitude\"] = False config[\"x(km)\"] = (250, 600) config[\"y(km)\"] = (7200, 8000) config[\"z(km)\"] = (0, 150) config[\"vel\"] = {\"p\": 7.0, \"s\": 7.0 / 1.75}  # We assume rather high velocities as we expect deeper events config[\"method\"] = \"BGMM\" if config[\"method\"] == \"BGMM\":     config[\"oversample_factor\"] = 4 if config[\"method\"] == \"GMM\":     config[\"oversample_factor\"] = 1  # DBSCAN config[\"bfgs_bounds\"] = (     (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x     (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y     (0, config[\"z(km)\"][1] + 1),  # x     (None, None),  # t ) config[\"dbscan_eps\"] = 25  # seconds config[\"dbscan_min_samples\"] = 3  # Filtering config[\"min_picks_per_eq\"] = 5 config[\"max_sigma11\"] = 2.0 config[\"max_sigma22\"] = 1.0 config[\"max_sigma12\"] = 1.0 In\u00a0[8]: Copied! <pre>client = Client(\"GFZ\")\n\nt0 = UTCDateTime(\"2014/05/01 00:00:00\")\nt1 = t0 + 60 * 60\n# t1 = t0 + 12 * 60 * 60   # Full day, requires more memory\nstream = client.get_waveforms(network=\"CX\", station=\"*\", location=\"*\", channel=\"HH?\", starttime=t0, endtime=t1)\n\ninv = client.get_stations(network=\"CX\", station=\"*\", location=\"*\", channel=\"HH?\", starttime=t0, endtime=t1)\n</pre> client = Client(\"GFZ\")  t0 = UTCDateTime(\"2014/05/01 00:00:00\") t1 = t0 + 60 * 60 # t1 = t0 + 12 * 60 * 60   # Full day, requires more memory stream = client.get_waveforms(network=\"CX\", station=\"*\", location=\"*\", channel=\"HH?\", starttime=t0, endtime=t1)  inv = client.get_stations(network=\"CX\", station=\"*\", location=\"*\", channel=\"HH?\", starttime=t0, endtime=t1) In\u00a0[9]: Copied! <pre>picker = sbm.EQTransformer.from_pretrained(\"instance\")\n\nif torch.cuda.is_available():\n    picker.cuda()\n\n# We tuned the thresholds a bit - Feel free to play around with these values\npicks, _ = picker.classify(stream, batch_size=256, P_threshold=0.075, S_threshold=0.1, parallelism=1)\n\nCounter([p.phase for p in picks])  # Output number of P and S picks\n</pre> picker = sbm.EQTransformer.from_pretrained(\"instance\")  if torch.cuda.is_available():     picker.cuda()  # We tuned the thresholds a bit - Feel free to play around with these values picks, _ = picker.classify(stream, batch_size=256, P_threshold=0.075, S_threshold=0.1, parallelism=1)  Counter([p.phase for p in picks])  # Output number of P and S picks <pre>Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.52M/1.52M [00:00&lt;00:00, 2.14MB/s]\n</pre> Out[9]: <pre>Counter({'S': 302, 'P': 453})</pre> <p>We now convert the picks and station metadata into pandas dataframes in the format required for the GaMMA associator.</p> In\u00a0[10]: Copied! <pre>pick_df = []\nfor p in picks:\n    pick_df.append({\n        \"id\": p.trace_id,\n        \"timestamp\": p.peak_time.datetime,\n        \"prob\": p.peak_value,\n        \"type\": p.phase.lower()\n    })\npick_df = pd.DataFrame(pick_df)\n\nstation_df = []\nfor station in inv[0]:\n    station_df.append({\n        \"id\": f\"CX.{station.code}.\",\n        \"longitude\": station.longitude,\n        \"latitude\": station.latitude,\n        \"elevation(m)\": station.elevation\n    })\nstation_df = pd.DataFrame(station_df)\n\nstation_df[\"x(km)\"] = station_df.apply(lambda x: transformer.transform(x[\"latitude\"], x[\"longitude\"])[0] / 1e3, axis=1)\nstation_df[\"y(km)\"] = station_df.apply(lambda x: transformer.transform(x[\"latitude\"], x[\"longitude\"])[1] / 1e3, axis=1)\nstation_df[\"z(km)\"] = - station_df[\"elevation(m)\"] / 1e3\n\nnorthing = {station: y for station, y in zip(station_df[\"id\"], station_df[\"y(km)\"])}\nstation_dict = {station: (x, y) for station, x, y in zip(station_df[\"id\"], station_df[\"x(km)\"], station_df[\"y(km)\"])}\n</pre> pick_df = [] for p in picks:     pick_df.append({         \"id\": p.trace_id,         \"timestamp\": p.peak_time.datetime,         \"prob\": p.peak_value,         \"type\": p.phase.lower()     }) pick_df = pd.DataFrame(pick_df)  station_df = [] for station in inv[0]:     station_df.append({         \"id\": f\"CX.{station.code}.\",         \"longitude\": station.longitude,         \"latitude\": station.latitude,         \"elevation(m)\": station.elevation     }) station_df = pd.DataFrame(station_df)  station_df[\"x(km)\"] = station_df.apply(lambda x: transformer.transform(x[\"latitude\"], x[\"longitude\"])[0] / 1e3, axis=1) station_df[\"y(km)\"] = station_df.apply(lambda x: transformer.transform(x[\"latitude\"], x[\"longitude\"])[1] / 1e3, axis=1) station_df[\"z(km)\"] = - station_df[\"elevation(m)\"] / 1e3  northing = {station: y for station, y in zip(station_df[\"id\"], station_df[\"y(km)\"])} station_dict = {station: (x, y) for station, x, y in zip(station_df[\"id\"], station_df[\"x(km)\"], station_df[\"y(km)\"])} <p>Let's have a look at the picks generated by the model. Note that we retained the probability from the deep learning model. It will be used by the associator later on.</p> In\u00a0[11]: Copied! <pre>pick_df.sort_values(\"timestamp\")\n</pre> pick_df.sort_values(\"timestamp\") Out[11]: id timestamp prob type 1 CX.PB11. 2014-05-01 00:00:06.189998 0.821354 p 2 CX.PB01. 2014-05-01 00:00:06.788391 0.887685 p 0 CX.PATCX. 2014-05-01 00:00:07.240000 0.394103 s 3 CX.PB07. 2014-05-01 00:00:07.908393 0.396942 p 6 CX.PB08. 2014-05-01 00:00:10.578393 0.501345 p ... ... ... ... ... 751 CX.PB11. 2014-05-01 00:59:42.179998 0.105164 p 307 CX.MNMCX. 2014-05-01 00:59:42.630000 0.196548 s 752 CX.PB12. 2014-05-01 00:59:44.180000 0.797832 p 753 CX.PSGCX. 2014-05-01 00:59:45.300000 0.342246 s 754 CX.PB16. 2014-05-01 00:59:54.999998 0.300616 p <p>755 rows \u00d7 4 columns</p> In\u00a0[12]: Copied! <pre>catalogs, assignments = association(pick_df, station_df, config, method=config[\"method\"])\n\ncatalog = pd.DataFrame(catalogs)\nassignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n</pre> catalogs, assignments = association(pick_df, station_df, config, method=config[\"method\"])  catalog = pd.DataFrame(catalogs) assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"]) <pre>Associating 36 clusters with 15 CPUs\n....................................</pre> In\u00a0[13]: Copied! <pre>catalog\n</pre> catalog Out[13]: time magnitude sigma_time sigma_amp cov_time_amp gamma_score number_picks number_p_picks number_s_picks event_index x(km) y(km) z(km) 0 2014-05-01T00:28:24.136 999 0.489847 0 0 6.000000 6 4 2 0 292.300870 7808.356924 0.687653 1 2014-05-01T00:12:11.046 999 0.349988 0 0 7.000000 6 4 2 1 470.051181 7785.504979 99.394047 2 2014-05-01T00:03:56.853 999 0.496083 0 0 12.901254 13 7 6 2 371.498155 7793.702568 39.713205 3 2014-05-01T00:18:44.591 999 0.433984 0 0 13.685868 14 7 7 3 487.879766 7674.490727 107.048313 4 2014-05-01T00:10:25.243 999 1.220064 0 0 6.447093 7 4 3 4 364.991866 7541.558002 0.000000 5 2014-05-01T00:29:15.684 999 0.496311 0 0 7.000000 6 3 3 5 343.282066 7858.856656 27.411413 6 2014-05-01T00:06:34.050 999 0.656182 0 0 4.665885 5 4 1 6 491.873384 7595.226655 0.000000 7 2014-05-01T00:26:02.264 999 0.751507 0 0 15.004951 6 4 2 7 533.950933 7648.899353 0.000000 8 2014-05-01T00:39:59.880 999 0.033031 0 0 5.000000 5 0 5 8 510.687757 7663.709439 107.088551 9 2014-05-01T00:12:40.804 999 0.902238 0 0 6.397226 6 2 4 9 412.695895 7503.368963 73.042818 10 2014-05-01T00:12:40.929 999 0.423287 0 0 22.598363 23 14 9 10 540.283910 7523.216113 133.849184 11 2014-05-01T00:41:40.281 999 1.124938 0 0 30.000000 23 13 10 11 530.000686 7606.522966 145.237274 12 2014-05-01T00:44:15.323 999 0.527026 0 0 9.996045 10 5 5 12 509.841945 7665.438005 108.386367 13 2014-05-01T00:47:09.921 999 0.752921 0 0 17.000000 17 10 7 13 302.446151 7775.492644 4.004625 14 2014-05-01T00:45:43.622 999 0.771175 0 0 16.881866 16 12 4 14 288.477806 7804.164706 1.766920 15 2014-05-01T00:06:52.555 999 1.131202 0 0 26.779469 5 2 3 15 249.000000 7678.146061 151.000000 16 2014-05-01T00:29:30.036 999 0.944306 0 0 36.414809 25 14 11 16 517.199569 7551.219313 126.011577 17 2014-05-01T00:08:38.225 999 0.508971 0 0 4.583490 5 3 2 17 420.326704 7630.634609 0.007491 18 2014-05-01T00:38:05.434 999 0.640808 0 0 17.999516 18 10 8 18 313.549825 7587.831439 0.000000 19 2014-05-01T00:17:16.430 999 1.150991 0 0 8.227512 8 4 4 19 450.147797 7619.579021 0.000000 20 2014-05-01T00:17:38.915 999 0.853361 0 0 9.385856 10 8 2 20 447.446001 7624.385194 14.134843 21 2014-05-01T00:17:46.959 999 0.877146 0 0 8.223502 7 2 5 21 461.900645 7646.898056 0.160134 22 2014-05-01T00:14:58.491 999 0.218517 0 0 2.979169 5 3 2 22 370.148112 7581.212855 0.000000 23 2014-05-01T00:32:05.537 999 0.742171 0 0 5.446704 7 5 2 23 591.498708 7585.501128 0.502048 24 2014-05-01T00:39:19.815 999 0.599434 0 0 14.506541 15 8 7 24 515.670572 7675.214531 120.706285 25 2014-05-01T00:49:32.219 999 0.500908 0 0 26.000000 5 0 5 25 395.278636 8001.000000 146.937561 26 2014-05-01T00:58:54.875 999 0.827728 0 0 21.000000 13 8 5 26 312.601911 7745.440257 38.513134 27 2014-04-30T23:59:42.492 999 0.628810 0 0 8.823824 9 6 3 27 322.281323 7719.412150 95.769563 28 2014-04-30T23:59:49.742 999 0.581883 0 0 16.885276 18 8 10 28 502.573409 7652.139451 122.592592 29 2014-05-01T00:00:40.536 999 0.486267 0 0 7.237385 6 4 2 29 408.383875 7501.140088 0.000000 30 2014-05-01T00:23:10.981 999 0.932767 0 0 12.886966 5 3 2 30 278.184277 7625.038674 3.886796 31 2014-05-01T00:23:35.367 999 1.014327 0 0 5.543328 6 2 4 31 356.565410 7570.481704 0.000000 32 2014-05-01T00:24:14.996 999 0.376849 0 0 5.802843 6 3 3 32 430.151900 7626.813197 0.000000 33 2014-05-01T00:24:17.391 999 0.045699 0 0 4.785834 5 3 2 33 504.765554 7544.874126 0.000000 34 2014-05-01T00:25:15.728 999 0.526167 0 0 6.321563 7 5 2 34 297.411182 7799.804126 0.284809 35 2014-05-01T00:52:18.614 999 0.907981 0 0 50.000000 5 4 1 35 414.547258 7819.978281 29.561588 <p>We can also plot the catalog. Conveniently, we are already in a local transverse mercator projection, so need for further thought in the plotting here.</p> <p>We use the <code>scatter</code> function and encode the depth of the events using color. The plot nicely resolves the intense shallow seismicity in the Iquique area (offshore, Northing arong 7800 km, Easting around 300 km). It also shows the seismicity along the Slap (West-East dipping).</p> In\u00a0[14]: Copied! <pre>fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111)\nax.set_aspect(\"equal\")\ncb = ax.scatter(catalog[\"x(km)\"], catalog[\"y(km)\"], c=catalog[\"z(km)\"], s=8, cmap=\"viridis\")\ncbar = fig.colorbar(cb)\ncbar.ax.set_ylim(cbar.ax.get_ylim()[::-1])\ncbar.set_label(\"Depth[km]\")\n\nax.plot(station_df[\"x(km)\"], station_df[\"y(km)\"], \"r^\", ms=10, mew=1, mec=\"k\")\nax.set_xlabel(\"Easting [km]\")\nax.set_ylabel(\"Northing [km]\")\nplt.show()\n</pre> fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111) ax.set_aspect(\"equal\") cb = ax.scatter(catalog[\"x(km)\"], catalog[\"y(km)\"], c=catalog[\"z(km)\"], s=8, cmap=\"viridis\") cbar = fig.colorbar(cb) cbar.ax.set_ylim(cbar.ax.get_ylim()[::-1]) cbar.set_label(\"Depth[km]\")  ax.plot(station_df[\"x(km)\"], station_df[\"y(km)\"], \"r^\", ms=10, mew=1, mec=\"k\") ax.set_xlabel(\"Easting [km]\") ax.set_ylabel(\"Northing [km]\") plt.show() <p>As a last check, we manually inspect some events. The code block below selects a random event and plots the waveforms, together with the P (solid black lines) and S (dashed black lines). The x axis denotes the time, the y axis the distance between station and estimated event location. Therefore, we should see roughly a hyperbolic moveout. Run the cell a few times to see a few example events.</p> In\u00a0[15]: Copied! <pre>event_idx = np.random.randint(len(catalog))\nevent_picks = [picks[i] for i in assignments[assignments[\"event_index\"] == event_idx][\"pick_index\"]]\nevent = catalog.iloc[event_idx]\n\nfirst, last = min(pick.peak_time for pick in event_picks), max(pick.peak_time for pick in event_picks)\n\nsub = obspy.Stream()\n\nfor station in np.unique([pick.trace_id for pick in event_picks]):\n    sub.append(stream.select(station=station[3:-1], channel=\"HHZ\")[0])\n\nsub = sub.slice(first - 5, last + 5)\n\nsub = sub.copy()\nsub.detrend()\nsub.filter(\"highpass\", freq=2)\n\nfig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(111)\n\nfor i, trace in enumerate(sub):\n    normed = trace.data - np.mean(trace.data)\n    normed = normed / np.max(np.abs(normed))\n    station_x, station_y = station_dict[trace.id[:-4]]\n    y = np.sqrt((station_x - event[\"x(km)\"]) ** 2 + (station_y - event[\"y(km)\"]) ** 2 + event[\"z(km)\"] ** 2)\n    ax.plot(trace.times(), 10 * normed + y)\n    \nfor pick in event_picks:\n    station_x, station_y = station_dict[pick.trace_id]\n    y = np.sqrt((station_x - event[\"x(km)\"]) ** 2 + (station_y - event[\"y(km)\"]) ** 2 + event[\"z(km)\"] ** 2)\n    x = pick.peak_time - trace.stats.starttime\n    if pick.phase == \"P\":\n        ls = '-'\n    else:\n        ls = '--'\n    ax.plot([x, x], [y - 10, y + 10], 'k', ls=ls)\n    \nax.set_ylim(0)\nax.set_xlim(0, np.max(trace.times()))\nax.set_ylabel(\"Hypocentral distance [km]\")\nax.set_xlabel(\"Time [s]\")\n\nprint(\"Event information\")\nprint(event)\n</pre> event_idx = np.random.randint(len(catalog)) event_picks = [picks[i] for i in assignments[assignments[\"event_index\"] == event_idx][\"pick_index\"]] event = catalog.iloc[event_idx]  first, last = min(pick.peak_time for pick in event_picks), max(pick.peak_time for pick in event_picks)  sub = obspy.Stream()  for station in np.unique([pick.trace_id for pick in event_picks]):     sub.append(stream.select(station=station[3:-1], channel=\"HHZ\")[0])  sub = sub.slice(first - 5, last + 5)  sub = sub.copy() sub.detrend() sub.filter(\"highpass\", freq=2)  fig = plt.figure(figsize=(15, 10)) ax = fig.add_subplot(111)  for i, trace in enumerate(sub):     normed = trace.data - np.mean(trace.data)     normed = normed / np.max(np.abs(normed))     station_x, station_y = station_dict[trace.id[:-4]]     y = np.sqrt((station_x - event[\"x(km)\"]) ** 2 + (station_y - event[\"y(km)\"]) ** 2 + event[\"z(km)\"] ** 2)     ax.plot(trace.times(), 10 * normed + y)      for pick in event_picks:     station_x, station_y = station_dict[pick.trace_id]     y = np.sqrt((station_x - event[\"x(km)\"]) ** 2 + (station_y - event[\"y(km)\"]) ** 2 + event[\"z(km)\"] ** 2)     x = pick.peak_time - trace.stats.starttime     if pick.phase == \"P\":         ls = '-'     else:         ls = '--'     ax.plot([x, x], [y - 10, y + 10], 'k', ls=ls)      ax.set_ylim(0) ax.set_xlim(0, np.max(trace.times())) ax.set_ylabel(\"Hypocentral distance [km]\") ax.set_xlabel(\"Time [s]\")  print(\"Event information\") print(event) <pre>Event information\ntime              2014-05-01T00:12:40.929\nmagnitude                             999\nsigma_time                       0.423287\nsigma_amp                               0\ncov_time_amp                            0\ngamma_score                     22.598363\nnumber_picks                           23\nnumber_p_picks                         14\nnumber_s_picks                          9\nevent_index                            10\nx(km)                           540.28391\ny(km)                         7523.216113\nz(km)                          133.849184\nName: 10, dtype: object\n</pre>"},{"location":"GaMMA/example_seisbench/#creating-a-catalog-from-waveforms-to-events","title":"Creating a catalog - From waveforms to events\u00b6","text":"<p>This model shows how to use SeisBench and the GaMMA associator to create an earthquake catalog from raw waveforms. First, we will download raw waveforms and station metadata through FDSN. Second, we use an EQTransformer model in SeisBench to obtain phase arrival picks. Third, we use the GaMMA associator to associate the picks to events. We visualize the results.</p>"},{"location":"GaMMA/example_seisbench/#configuration","title":"Configuration\u00b6","text":"<p>The following codeblock contains all configurations. The first block configures the local coordinate projection. In this case, we use a transverse mercator projection for Chile, as we will be using data from northern Chile. The second, third and fourth blocks configures the gamma associator. Please see it's documentation for details.</p>"},{"location":"GaMMA/example_seisbench/#obtaining-the-data","title":"Obtaining the data\u00b6","text":"<p>We download waveform data for 12 hours from the CX network in Northern Chile. We use the 1st May 2014, exactly one month after the Mw 8.1 Iquique mainshock on 1st April 2014. Therefore, we expect to still see an increased level of seismicity in the region.</p>"},{"location":"GaMMA/example_seisbench/#picking","title":"Picking\u00b6","text":"<p>For this example, we use EQTransformer trained on the INSTANCE dataset from Italy. However, in principal any picker could be used for obtaining the picks with only minimal changes.</p> <p>Warning: This will take some time and requires sufficient main memory. If you are short on memory, reduce the study in the cell before.</p> <p>Note: We automatically check if CUDA is available and run the model on CUDA in this case. Alternatively, the model runs on CPU.</p>"},{"location":"GaMMA/example_seisbench/#association","title":"Association\u00b6","text":"<p>We now run the phase association. This will take a moment. We convert the output into two dataframes, one for the catalog and one for the assignment of picks to the catalog.</p>"},{"location":"GaMMA/example_seisbench/#visualizing-the-catalog","title":"Visualizing the catalog\u00b6","text":"<p>Let's have a look at the catalog.</p>"},{"location":"GaMMA/example_seisbench/#closing-remarks","title":"Closing remarks\u00b6","text":"<p>In this tutorial, we saw how to quickly generate an event catalog from raw seismic waveforms and their metadata using SeisBench and the GaMMA associator.</p> <ul> <li>We used a local coordinate projection for Chile for this tutorial. Depending on the region, you will need to choose a different projection.</li> <li>Both the picker and the model have several tuning parameters. We tuned these parameters loosely, but we would like to make the reader aware that these parameters can have strong influence on the number of picks and events, the number of false positives, and the runtime performance of the associator.</li> <li>While GaMMA outputs locations, these are only preliminary. It is highly recommended to further improve these using a more advances tool for localization.</li> </ul>"},{"location":"GaMMA/example_synthetic/","title":"Example synthetic","text":"In\u00a0[42]: Copied! <pre># !pip install git+https://github.com/wayneweiqiang/GaMMA.git\n</pre> # !pip install git+https://github.com/wayneweiqiang/GaMMA.git In\u00a0[43]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport os, sys\nimport time\nfrom collections import defaultdict\nfrom gamma import BayesianGaussianMixture, GaussianMixture\n</pre> import numpy as np import matplotlib.pyplot as plt import os, sys import time from collections import defaultdict from gamma import BayesianGaussianMixture, GaussianMixture In\u00a0[44]: Copied! <pre>vp = 6.0\nvs = vp/1.75\nnp.random.seed(1)\n\n## small area\n# use_amplitude = False\n# num_event = 4\n# num_station = 15\n# xlim = [0, 200]\n\n## large area\nnum_event = 6\nnum_station = 40\nxlim = [0, 2000]\nuse_amplitude = True\nphase_time_err = 0.0\nphase_amp_err = 0.0\nphase_fp_rate = 0.0\n\nstation_loc = np.linspace(xlim[0]+xlim[1]/20.0, xlim[1]-xlim[1]/20.0, num_station)[:, np.newaxis] + \\\n              np.random.uniform(low=-10, high=10, size=(num_station,1))\nevent_loc = np.linspace(xlim[0]+xlim[1]/5.0, xlim[1]-xlim[1]/5.0, num_event)[:, np.newaxis] + \\\n            np.random.uniform(low=-0, high=0, size=(num_event,1))\nnp.random.shuffle(event_loc)\nevent_t0 = np.linspace(xlim[1]/vp/10, xlim[1]/vp, num_event)\n# np.random.shuffle(event_t0)\n# event_mag = np.linspace(2, 5, num_event)\nevent_mag = np.logspace(np.log10(2), np.log10(3), num_event)\nnp.random.shuffle(event_mag)\nprint(\"True events:\")\nfor i in range(num_event):\n    if use_amplitude:\n        print(f\"loc={event_loc[i,0]:.3f}\\tt0={event_t0[i]:.3f}\\tmag={event_mag[i]:.3f}\")\n    else:\n        print(f\"loc={event_loc[i,0]:.3f}\\tt0={event_t0[i]:.3f}\")\n\n\nphase_event_idx = []\nphase_time = []\nphase_amp = []\nphase_type = []\nphase_loc = []\nc0, c1, c2, c3 = 1.08, 0.93, -0.015, -1.68\nfor i in range(num_event):\n    loc = event_loc[i]\n    t0 = event_t0[i]\n    mag = event_mag[i]\n    dist = np.abs(loc - station_loc)\n    tp = t0 + dist / vp \n    ts = t0 + dist / vs \n    logA = c0 + c1*(mag-3.5) + c3*np.log10(dist)\n    tp_err = np.random.uniform(low=-phase_time_err, high=phase_time_err, size=num_station)\n    ts_err = np.random.uniform(low=-phase_time_err, high=phase_time_err, size=num_station)\n    logA_p_err = np.random.uniform(low=-phase_amp_err, high=phase_amp_err, size=num_station)\n    logA_s_err = np.random.uniform(low=-phase_amp_err, high=phase_amp_err, size=num_station)\n    for j in range(num_station):\n        if logA[j] &gt; -4:\n            if np.random.rand() &lt; 0.8:\n                phase_time.append(tp[j] + tp_err[j]) #p\n                phase_amp.append(logA[j] + logA_p_err[j])\n                phase_type.append(\"p\")\n                phase_loc.append(station_loc[j])\n                phase_event_idx.append(i)\n            if np.random.rand() &lt; 0.8:\n                phase_time.append(ts[j] + ts_err[j]) #s\n                phase_amp.append(logA[j] + + logA_s_err[j])\n                phase_type.append(\"s\")\n                phase_loc.append(station_loc[j])\n                phase_event_idx.append(i)\n\nnp.random.seed(1)\nmin_phase_time, max_phase_time = min(phase_time), max(phase_time) \nmin_phase_amp, max_phase_amp = min(phase_amp), max(phase_amp) \nfor i in range(int(len(phase_time) * phase_fp_rate)):\n    phase_time.append(np.random.uniform(min_phase_time, max_phase_time))\n    phase_amp.append(np.random.uniform(min_phase_amp, max_phase_amp))\n    phase_type.append(np.random.choice([\"p\", \"s\"]))\n    phase_loc.append(station_loc[np.random.randint(num_station)])\n    phase_event_idx.append(num_event)\n\nphase_time = np.array(phase_time)\nphase_amp = np.array(phase_amp)\nphase_loc = np.array(phase_loc)\nphase_event_idx = np.array(phase_event_idx)\n\n#################### plot data ##############################\nplt.figure()\n# plt.gcf().set_size_inches(plt.gcf().get_size_inches() * np.array([2,1]))\ntext_loc = [0.05, 0.95]\nbox = dict(boxstyle='round', facecolor='white', alpha=1)\nplt.subplot(121)\nif use_amplitude:\n    plt.scatter(phase_time, phase_loc, s=(4**np.array(phase_amp))*1e3, color=\"grey\",  marker=\"o\", alpha=0.8)\nelse:\n    plt.scatter(phase_time, phase_loc, s=12, color=\"grey\",  marker=\"o\", alpha=0.8)\nplt.scatter(-1, -1, s=24, color=\"grey\",  marker=\"o\", alpha=0.8, label=\"Picks\")\nplt.xlim(left=0)\nplt.ylim(xlim)\nylim_ = plt.ylim()\nxlim_ = plt.xlim()\nplt.ylabel(\"Distance (km)\")\nplt.xlabel(\"Time (s)\")\nplt.legend()\nplt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment='top',\n         transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box)\nplt.title(\"P/S phase picks\")\nplt.subplot(122)\nif use_amplitude:\n    plt.scatter(phase_time[phase_event_idx!=num_event], phase_loc[phase_event_idx!=num_event], \\\n                s=(4**np.array(phase_amp[phase_event_idx!=num_event]))*1e3, \\\n                color=[f\"C{x}\"for x in phase_event_idx[phase_event_idx!=num_event]],  marker=\"o\", facecolors='none')\n    plt.scatter(phase_time[phase_event_idx==num_event], phase_loc[phase_event_idx==num_event], \\\n                s=(4**np.array(phase_amp[phase_event_idx==num_event]))*1e3, \\\n                color=\"grey\",  marker=\"o\", alpha=0.8)\nelse:\n    plt.scatter(phase_time[phase_event_idx!=num_event], phase_loc[phase_event_idx!=num_event], \\\n                s=12, \\\n                color=[f\"C{x}\"for x in phase_event_idx[phase_event_idx!=num_event]],  marker=\"o\", facecolors='none')\n    plt.scatter(phase_time[phase_event_idx==num_event], phase_loc[phase_event_idx==num_event], \\\n                s=12, \\\n                color=\"grey\",  marker=\"o\", alpha=0.8)\nplt.scatter(-1, -1, s=24, color=\"C0\",  marker=\"o\", facecolors='none', label=\"Associated picks\")\nfor i in range(len(event_mag)):\n    plt.scatter(event_t0[i], event_loc[i], s=5**np.array(event_mag[i])*3, color=f\"C{i}\", marker=\"P\")\nplt.scatter(-1, -1, s=48, color=\"C0\", marker=\"P\", label=\"Earthquakes\")\nplt.xlim(xlim_)\nplt.ylim(ylim_)\nplt.xlabel(\"Time (s)\")\n# plt.ylabel(\"Distance (km)\")\nplt.gca().yaxis.set_ticklabels([])\nplt.legend()\nplt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment='top',\n         transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box)\nplt.title(\"Ground truth\")\nplt.show();\n</pre> vp = 6.0 vs = vp/1.75 np.random.seed(1)  ## small area # use_amplitude = False # num_event = 4 # num_station = 15 # xlim = [0, 200]  ## large area num_event = 6 num_station = 40 xlim = [0, 2000] use_amplitude = True phase_time_err = 0.0 phase_amp_err = 0.0 phase_fp_rate = 0.0  station_loc = np.linspace(xlim[0]+xlim[1]/20.0, xlim[1]-xlim[1]/20.0, num_station)[:, np.newaxis] + \\               np.random.uniform(low=-10, high=10, size=(num_station,1)) event_loc = np.linspace(xlim[0]+xlim[1]/5.0, xlim[1]-xlim[1]/5.0, num_event)[:, np.newaxis] + \\             np.random.uniform(low=-0, high=0, size=(num_event,1)) np.random.shuffle(event_loc) event_t0 = np.linspace(xlim[1]/vp/10, xlim[1]/vp, num_event) # np.random.shuffle(event_t0) # event_mag = np.linspace(2, 5, num_event) event_mag = np.logspace(np.log10(2), np.log10(3), num_event) np.random.shuffle(event_mag) print(\"True events:\") for i in range(num_event):     if use_amplitude:         print(f\"loc={event_loc[i,0]:.3f}\\tt0={event_t0[i]:.3f}\\tmag={event_mag[i]:.3f}\")     else:         print(f\"loc={event_loc[i,0]:.3f}\\tt0={event_t0[i]:.3f}\")   phase_event_idx = [] phase_time = [] phase_amp = [] phase_type = [] phase_loc = [] c0, c1, c2, c3 = 1.08, 0.93, -0.015, -1.68 for i in range(num_event):     loc = event_loc[i]     t0 = event_t0[i]     mag = event_mag[i]     dist = np.abs(loc - station_loc)     tp = t0 + dist / vp      ts = t0 + dist / vs      logA = c0 + c1*(mag-3.5) + c3*np.log10(dist)     tp_err = np.random.uniform(low=-phase_time_err, high=phase_time_err, size=num_station)     ts_err = np.random.uniform(low=-phase_time_err, high=phase_time_err, size=num_station)     logA_p_err = np.random.uniform(low=-phase_amp_err, high=phase_amp_err, size=num_station)     logA_s_err = np.random.uniform(low=-phase_amp_err, high=phase_amp_err, size=num_station)     for j in range(num_station):         if logA[j] &gt; -4:             if np.random.rand() &lt; 0.8:                 phase_time.append(tp[j] + tp_err[j]) #p                 phase_amp.append(logA[j] + logA_p_err[j])                 phase_type.append(\"p\")                 phase_loc.append(station_loc[j])                 phase_event_idx.append(i)             if np.random.rand() &lt; 0.8:                 phase_time.append(ts[j] + ts_err[j]) #s                 phase_amp.append(logA[j] + + logA_s_err[j])                 phase_type.append(\"s\")                 phase_loc.append(station_loc[j])                 phase_event_idx.append(i)  np.random.seed(1) min_phase_time, max_phase_time = min(phase_time), max(phase_time)  min_phase_amp, max_phase_amp = min(phase_amp), max(phase_amp)  for i in range(int(len(phase_time) * phase_fp_rate)):     phase_time.append(np.random.uniform(min_phase_time, max_phase_time))     phase_amp.append(np.random.uniform(min_phase_amp, max_phase_amp))     phase_type.append(np.random.choice([\"p\", \"s\"]))     phase_loc.append(station_loc[np.random.randint(num_station)])     phase_event_idx.append(num_event)  phase_time = np.array(phase_time) phase_amp = np.array(phase_amp) phase_loc = np.array(phase_loc) phase_event_idx = np.array(phase_event_idx)  #################### plot data ############################## plt.figure() # plt.gcf().set_size_inches(plt.gcf().get_size_inches() * np.array([2,1])) text_loc = [0.05, 0.95] box = dict(boxstyle='round', facecolor='white', alpha=1) plt.subplot(121) if use_amplitude:     plt.scatter(phase_time, phase_loc, s=(4**np.array(phase_amp))*1e3, color=\"grey\",  marker=\"o\", alpha=0.8) else:     plt.scatter(phase_time, phase_loc, s=12, color=\"grey\",  marker=\"o\", alpha=0.8) plt.scatter(-1, -1, s=24, color=\"grey\",  marker=\"o\", alpha=0.8, label=\"Picks\") plt.xlim(left=0) plt.ylim(xlim) ylim_ = plt.ylim() xlim_ = plt.xlim() plt.ylabel(\"Distance (km)\") plt.xlabel(\"Time (s)\") plt.legend() plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment='top',          transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box) plt.title(\"P/S phase picks\") plt.subplot(122) if use_amplitude:     plt.scatter(phase_time[phase_event_idx!=num_event], phase_loc[phase_event_idx!=num_event], \\                 s=(4**np.array(phase_amp[phase_event_idx!=num_event]))*1e3, \\                 color=[f\"C{x}\"for x in phase_event_idx[phase_event_idx!=num_event]],  marker=\"o\", facecolors='none')     plt.scatter(phase_time[phase_event_idx==num_event], phase_loc[phase_event_idx==num_event], \\                 s=(4**np.array(phase_amp[phase_event_idx==num_event]))*1e3, \\                 color=\"grey\",  marker=\"o\", alpha=0.8) else:     plt.scatter(phase_time[phase_event_idx!=num_event], phase_loc[phase_event_idx!=num_event], \\                 s=12, \\                 color=[f\"C{x}\"for x in phase_event_idx[phase_event_idx!=num_event]],  marker=\"o\", facecolors='none')     plt.scatter(phase_time[phase_event_idx==num_event], phase_loc[phase_event_idx==num_event], \\                 s=12, \\                 color=\"grey\",  marker=\"o\", alpha=0.8) plt.scatter(-1, -1, s=24, color=\"C0\",  marker=\"o\", facecolors='none', label=\"Associated picks\") for i in range(len(event_mag)):     plt.scatter(event_t0[i], event_loc[i], s=5**np.array(event_mag[i])*3, color=f\"C{i}\", marker=\"P\") plt.scatter(-1, -1, s=48, color=\"C0\", marker=\"P\", label=\"Earthquakes\") plt.xlim(xlim_) plt.ylim(ylim_) plt.xlabel(\"Time (s)\") # plt.ylabel(\"Distance (km)\") plt.gca().yaxis.set_ticklabels([]) plt.legend() plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment='top',          transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box) plt.title(\"Ground truth\") plt.show(); <pre>True events:\nloc=400.000\tt0=33.333\nloc=1600.000\tt0=93.333\nloc=880.000\tt0=153.333\nloc=1360.000\tt0=213.333\nloc=1120.000\tt0=273.333\nloc=640.000\tt0=333.333\n</pre> In\u00a0[45]: Copied! <pre>data = np.hstack([phase_time, phase_amp])\nphase_time_range = np.max(phase_time) - np.min(phase_time)\nphase_amp_range = np.max(phase_amp) - np.min(phase_amp)\n\n\n# Fit a Gaussian mixture with EM \ndummy_prob = 1/((2*np.pi)**(data.shape[-1]/2) * 2)\n\nt_start = time.time()\n\nnum_event_loc_init = 3\nnum_event_init = min(int(len(data)/min(num_station, 20) * 3), len(data))\n\n\ncenters_init = np.vstack([np.vstack([np.ones(num_event_init//num_event_loc_init) * x, np.linspace(data[:,0].min(), data[:,0].max(), num_event_init//num_event_loc_init)]).T\n                          for x in np.linspace(np.min(station_loc), np.max(station_loc), num_event_loc_init+2)[1:-1]])\nif use_amplitude:\n    centers_init = np.hstack([centers_init, 1.0 * np.ones((len(centers_init), 1))])\n\nif not use_amplitude:\n    covariance_prior = np.array([[1]]) \n    data = data[:,0:1]\nelse:\n    covariance_prior = np.array([[1,0], [0,1]]) \n\ngmm = BayesianGaussianMixture(n_components=num_event_init, \n                                      station_locs=phase_loc, \n                                      phase_type=phase_type,\n                                      weight_concentration_prior = 1/num_event_init,\n                                      covariance_prior = covariance_prior,\n                                      init_params=\"centers\",\n                                      centers_init=centers_init.copy(),\n                                      loss_type=\"l1\",\n                                    #   max_covar=10**2,\n                                      ).fit(data)\n\n# gmm = GaussianMixture(n_components=num_event_init,\n#                               station_locs=phase_loc, \n#                               phase_type=phase_type, \n#                               init_params=\"centers\",\n#                               centers_init=centers_init.copy(), \n#                               loss_type=\"l1\",\n#                               max_covar=10,\n#                             #   dummy_comp=True, \n#                             #   dummy_prob=0.01,\n#                               ).fit(data)\n\nt_end = time.time()\nprint(f\"GMMA time = {t_end - t_start}\")\n\npred = gmm.predict(data) \nprob = gmm.predict_proba(data)\nscore = gmm.score_samples(data)\nprob_eq = prob.mean(axis=0)\nstd_eq = gmm.covariances_\n\nmin_picks = min(num_station, 6)\nfiltered_idx = np.array([True if len(data[pred==i, 0]) &gt;= min_picks else False for i in range(len(prob_eq))]) \n#&amp; (np.max(std_eq,axis=(1,2)) &lt; 30) # &amp; (std_eq &lt; 10) #&amp; (prob_eq &gt; 1/num_event_init)\nfiltered_idx = np.arange(len(prob_eq))[filtered_idx]\n\nmarker_size = 10\n\n#################### plot results ##############################\nplt.figure()\nplt.subplot(121)\nif use_amplitude:\n    plt.scatter(phase_time, phase_loc, s=(4**np.array(phase_amp))*1e3, color=\"grey\",  marker=\"o\", alpha=0.8)\nelse:\n    plt.scatter(phase_time, phase_loc, s=12, color=\"grey\",  marker=\"o\", alpha=0.8)\nplt.scatter(-1, -1, s=24, color=\"grey\",  marker=\"o\", alpha=0.8, label=\"Picks\")\nplt.xlim(left=0)\nplt.ylim(xlim)\nylim_ = plt.ylim()\nxlim_ = plt.xlim()\nplt.ylabel(\"Distance (km)\")\nplt.xlabel(\"Time (s)\")\nplt.legend()\nplt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment='top',\n         transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box)\nplt.title(\"P/S phase picks\")\n\nplt.subplot(122)\ncolors = defaultdict(int)\nidx = np.argsort(gmm.centers_[:, 1])\ndum = 0\nprint(\"Associated events:\")\nfor i in idx:\n    if i in filtered_idx:\n        if not use_amplitude:\n            plt.scatter(data[(pred==i), 0], phase_loc[(pred==i), 0], color=f\"C{dum}\", s=12, marker='o', facecolors='none')\n            plt.plot(gmm.centers_[i, 1], gmm.centers_[i, 0], \"P\", c=f\"C{dum}\", markersize=8)\n            print(f\"loc={gmm.centers_[i, 0]:.1f}\\tt0={gmm.centers_[i, 1]:.1f}\\tsigma11={std_eq[i,0,0]:.3f}\")\n        else:\n            plt.scatter(data[(pred==i), 0], phase_loc[(pred==i), 0], color=f\"C{dum}\", s=4**(data[(pred==i), 1])*1e3, marker='o', facecolors='none')\n            plt.scatter(gmm.centers_[i, 1], gmm.centers_[i, 0], color=f\"C{dum}\", s=4**gmm.centers_[i, 2]*3, marker=\"P\")\n            print(f\"loc={gmm.centers_[i, 0]:.1f}\\tt0={gmm.centers_[i, 1]:.1f}\\tmag={gmm.centers_[i, 2]:.1f}\\tsigma11={std_eq[i,0,0]:.3f}\")\n        dum += 1\nplt.scatter(-1, -1, s=24, color=\"C0\",  marker=\"o\", facecolors='none', label=\"Associated picks\")\nplt.scatter(-1, -1, s=48, color=\"C0\", marker=\"P\", label=\"Earthquakes\")\nplt.xlim(xlim_)\nplt.ylim(ylim_)\nplt.xlabel(\"Time (s)\")\nplt.gca().yaxis.set_ticklabels([])\nplt.legend()\nplt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment='top',\n         transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box)\nplt.title(\"GMMA prediction\")\nplt.show()\n</pre> data = np.hstack([phase_time, phase_amp]) phase_time_range = np.max(phase_time) - np.min(phase_time) phase_amp_range = np.max(phase_amp) - np.min(phase_amp)   # Fit a Gaussian mixture with EM  dummy_prob = 1/((2*np.pi)**(data.shape[-1]/2) * 2)  t_start = time.time()  num_event_loc_init = 3 num_event_init = min(int(len(data)/min(num_station, 20) * 3), len(data))   centers_init = np.vstack([np.vstack([np.ones(num_event_init//num_event_loc_init) * x, np.linspace(data[:,0].min(), data[:,0].max(), num_event_init//num_event_loc_init)]).T                           for x in np.linspace(np.min(station_loc), np.max(station_loc), num_event_loc_init+2)[1:-1]]) if use_amplitude:     centers_init = np.hstack([centers_init, 1.0 * np.ones((len(centers_init), 1))])  if not use_amplitude:     covariance_prior = np.array([[1]])      data = data[:,0:1] else:     covariance_prior = np.array([[1,0], [0,1]])   gmm = BayesianGaussianMixture(n_components=num_event_init,                                        station_locs=phase_loc,                                        phase_type=phase_type,                                       weight_concentration_prior = 1/num_event_init,                                       covariance_prior = covariance_prior,                                       init_params=\"centers\",                                       centers_init=centers_init.copy(),                                       loss_type=\"l1\",                                     #   max_covar=10**2,                                       ).fit(data)  # gmm = GaussianMixture(n_components=num_event_init, #                               station_locs=phase_loc,  #                               phase_type=phase_type,  #                               init_params=\"centers\", #                               centers_init=centers_init.copy(),  #                               loss_type=\"l1\", #                               max_covar=10, #                             #   dummy_comp=True,  #                             #   dummy_prob=0.01, #                               ).fit(data)  t_end = time.time() print(f\"GMMA time = {t_end - t_start}\")  pred = gmm.predict(data)  prob = gmm.predict_proba(data) score = gmm.score_samples(data) prob_eq = prob.mean(axis=0) std_eq = gmm.covariances_  min_picks = min(num_station, 6) filtered_idx = np.array([True if len(data[pred==i, 0]) &gt;= min_picks else False for i in range(len(prob_eq))])  #&amp; (np.max(std_eq,axis=(1,2)) &lt; 30) # &amp; (std_eq &lt; 10) #&amp; (prob_eq &gt; 1/num_event_init) filtered_idx = np.arange(len(prob_eq))[filtered_idx]  marker_size = 10  #################### plot results ############################## plt.figure() plt.subplot(121) if use_amplitude:     plt.scatter(phase_time, phase_loc, s=(4**np.array(phase_amp))*1e3, color=\"grey\",  marker=\"o\", alpha=0.8) else:     plt.scatter(phase_time, phase_loc, s=12, color=\"grey\",  marker=\"o\", alpha=0.8) plt.scatter(-1, -1, s=24, color=\"grey\",  marker=\"o\", alpha=0.8, label=\"Picks\") plt.xlim(left=0) plt.ylim(xlim) ylim_ = plt.ylim() xlim_ = plt.xlim() plt.ylabel(\"Distance (km)\") plt.xlabel(\"Time (s)\") plt.legend() plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment='top',          transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box) plt.title(\"P/S phase picks\")  plt.subplot(122) colors = defaultdict(int) idx = np.argsort(gmm.centers_[:, 1]) dum = 0 print(\"Associated events:\") for i in idx:     if i in filtered_idx:         if not use_amplitude:             plt.scatter(data[(pred==i), 0], phase_loc[(pred==i), 0], color=f\"C{dum}\", s=12, marker='o', facecolors='none')             plt.plot(gmm.centers_[i, 1], gmm.centers_[i, 0], \"P\", c=f\"C{dum}\", markersize=8)             print(f\"loc={gmm.centers_[i, 0]:.1f}\\tt0={gmm.centers_[i, 1]:.1f}\\tsigma11={std_eq[i,0,0]:.3f}\")         else:             plt.scatter(data[(pred==i), 0], phase_loc[(pred==i), 0], color=f\"C{dum}\", s=4**(data[(pred==i), 1])*1e3, marker='o', facecolors='none')             plt.scatter(gmm.centers_[i, 1], gmm.centers_[i, 0], color=f\"C{dum}\", s=4**gmm.centers_[i, 2]*3, marker=\"P\")             print(f\"loc={gmm.centers_[i, 0]:.1f}\\tt0={gmm.centers_[i, 1]:.1f}\\tmag={gmm.centers_[i, 2]:.1f}\\tsigma11={std_eq[i,0,0]:.3f}\")         dum += 1 plt.scatter(-1, -1, s=24, color=\"C0\",  marker=\"o\", facecolors='none', label=\"Associated picks\") plt.scatter(-1, -1, s=48, color=\"C0\", marker=\"P\", label=\"Earthquakes\") plt.xlim(xlim_) plt.ylim(ylim_) plt.xlabel(\"Time (s)\") plt.gca().yaxis.set_ticklabels([]) plt.legend() plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment='top',          transform=plt.gca().transAxes, fontsize=\"medium\", fontweight=\"normal\", bbox=box) plt.title(\"GMMA prediction\") plt.show() <pre>GMMA time = 0.4322082996368408\nAssociated events:\nloc=400.0\tt0=33.3\tsigma11=0.100\nloc=1600.0\tt0=93.3\tsigma11=0.050\nloc=880.0\tt0=153.3\tsigma11=0.077\nloc=1360.0\tt0=213.3\tsigma11=0.041\nloc=1120.0\tt0=273.3\tsigma11=0.029\nloc=640.0\tt0=333.3\tsigma11=0.025\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"GaMMA/example_synthetic/#synthetic-example","title":"Synthetic Example\u00b6","text":""},{"location":"GaMMA/example_synthetic/#1-prepare-synthetic-data","title":"1. Prepare synthetic data\u00b6","text":""},{"location":"GaMMA/example_synthetic/#2-association-with-gamma","title":"2. Association with GaMMA\u00b6","text":""},{"location":"PhaseNet/","title":"PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking Method","text":""},{"location":"PhaseNet/#1-install-miniconda-and-requirements","title":"1.  Install miniconda and requirements","text":"<ul> <li>Download PhaseNet repository</li> </ul> <pre><code>git clone https://github.com/wayneweiqiang/PhaseNet.git\ncd PhaseNet\n</code></pre> <ul> <li>Install to default environment</li> </ul> <pre><code>conda env update -f=env.yaml -n base\n</code></pre> <ul> <li>Install to \"phasenet\" virtual envirionment</li> </ul> <pre><code>conda env create -f env.yaml\nconda activate phasenet\n</code></pre> <ul> <li>For Mac ARM chip, please use the env_mac.yaml file</li> </ul> <pre><code>conda env create -f env_mac.yaml\nconda activate phasenet\n</code></pre>"},{"location":"PhaseNet/#2-pre-trained-model","title":"2. Pre-trained model","text":"<p>Located in directory: model/190703-214543</p>"},{"location":"PhaseNet/#3-related-papers","title":"3. Related papers","text":"<ul> <li>Zhu, Weiqiang, and Gregory C. Beroza. \"PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking Method.\" arXiv preprint arXiv:1803.03211 (2018).</li> <li>Liu, Min, et al. \"Rapid characterization of the July 2019 Ridgecrest, California, earthquake sequence from raw seismic data using machine\u2010learning phase picker.\" Geophysical Research Letters 47.4 (2020): e2019GL086189.</li> <li>Park, Yongsoo, et al. \"Machine\u2010learning\u2010based analysis of the Guy\u2010Greenbrier, Arkansas earthquakes: A tale of two sequences.\" Geophysical Research Letters 47.6 (2020): e2020GL087032.</li> <li>Chai, Chengping, et al. \"Using a deep neural network and transfer learning to bridge scales for seismic phase picking.\" Geophysical Research Letters 47.16 (2020): e2020GL088651.</li> <li>Tan, Yen Joe, et al. \"Machine\u2010Learning\u2010Based High\u2010Resolution Earthquake Catalog Reveals How Complex Fault Structures Were Activated during the 2016\u20132017 Central Italy Sequence.\" The Seismic Record 1.1 (2021): 11-19.</li> </ul>"},{"location":"PhaseNet/#4-batch-prediction","title":"4. Batch prediction","text":"<p>See examples in the notebook: example_batch_prediction.ipynb</p> <p>PhaseNet currently supports four data formats: mseed, sac, hdf5, and numpy. The test data can be downloaded here:</p> <pre><code>wget https://github.com/wayneweiqiang/PhaseNet/releases/download/test_data/test_data.zip\nunzip test_data.zip\n</code></pre> <ul> <li>For mseed format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/mseed.csv --data_dir=test_data/mseed --format=mseed --amplitude --response_xml=test_data/stations.xml --batch_size=1 --sampling_rate=100\n</code></pre> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/mseed2.csv --data_dir=test_data/mseed --format=mseed --amplitude --response_xml=test_data/stations.xml --batch_size=1 --sampling_rate=100\n</code></pre> <ul> <li>For sac format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/sac.csv --data_dir=test_data/sac --format=sac --batch_size=1\n</code></pre> <ul> <li>For numpy format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/npz.csv --data_dir=test_data/npz --format=numpy\n</code></pre> <ul> <li>For hdf5 format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --hdf5_file=test_data/data.h5 --hdf5_group=data --format=hdf5\n</code></pre> <ul> <li>For a seismic array (used by QuakeFlow):</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/mseed_array.csv --data_dir=test_data/mseed_array --stations=test_data/stations.json  --format=mseed_array --amplitude\n</code></pre> <p>Notes: </p> <ol> <li> <p>The reason for using \"--batch_size=1\" is because the mseed or sac files usually are not the same length. If you want to use a larger batch size for a good prediction speed, you need to cut the data to the same length.</p> </li> <li> <p>Remove the \"--plot_figure\" argument for large datasets, because plotting can be very slow.</p> </li> </ol> <p>Optional arguments:</p> <pre><code>usage: predict.py [-h] [--batch_size BATCH_SIZE] [--model_dir MODEL_DIR]\n                  [--data_dir DATA_DIR] [--data_list DATA_LIST]\n                  [--hdf5_file HDF5_FILE] [--hdf5_group HDF5_GROUP]\n                  [--result_dir RESULT_DIR] [--result_fname RESULT_FNAME]\n                  [--min_p_prob MIN_P_PROB] [--min_s_prob MIN_S_PROB]\n                  [--mpd MPD] [--amplitude] [--format FORMAT]\n                  [--s3_url S3_URL] [--stations STATIONS] [--plot_figure]\n                  [--save_prob]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --batch_size BATCH_SIZE\n                        batch size\n  --model_dir MODEL_DIR\n                        Checkpoint directory (default: None)\n  --data_dir DATA_DIR   Input file directory\n  --data_list DATA_LIST\n                        Input csv file\n  --hdf5_file HDF5_FILE\n                        Input hdf5 file\n  --hdf5_group HDF5_GROUP\n                        data group name in hdf5 file\n  --result_dir RESULT_DIR\n                        Output directory\n  --result_fname RESULT_FNAME\n                        Output file\n  --min_p_prob MIN_P_PROB\n                        Probability threshold for P pick\n  --min_s_prob MIN_S_PROB\n                        Probability threshold for S pick\n  --mpd MPD             Minimum peak distance\n  --amplitude           if return amplitude value\n  --format FORMAT       input format\n  --stations STATIONS   seismic station info\n  --plot_figure         If plot figure for test\n  --save_prob           If save result for test\n</code></pre> <ul> <li>The output picks are saved to \"results/picks.csv\" on default</li> </ul> file_name begin_time station_id phase_index phase_time phase_score phase_amp phase_type 2020-10-01T00:00* 2020-10-01T00:00:00.003 CI.BOM..HH 14734 2020-10-01T00:02:27.343 0.708 2.4998866231208325e-14 P 2020-10-01T00:00* 2020-10-01T00:00:00.003 CI.BOM..HH 15487 2020-10-01T00:02:34.873 0.416 2.4998866231208325e-14 S 2020-10-01T00:00* 2020-10-01T00:00:00.003 CI.COA..HH 319 2020-10-01T00:00:03.193 0.762 3.708662269972206e-14 P <p>Notes: 1. The phase_index means which data point is the pick in the original sequence. So phase_time = begin_time + phase_index / sampling rate. The default sampling_rate is 100Hz </p>"},{"location":"PhaseNet/#5-quakeflow-example","title":"5. QuakeFlow example","text":"<p>A complete earthquake detection workflow can be found in the QuakeFlow project.</p>"},{"location":"PhaseNet/#6-interactive-example","title":"6. Interactive example","text":"<p>See details in the notebook: example_interactive.ipynb</p>"},{"location":"PhaseNet/#7-training","title":"7. Training","text":"<ul> <li>Download a small sample dataset:</li> </ul> <pre><code>wget https://github.com/wayneweiqiang/PhaseNet/releases/download/test_data/test_data.zip\nunzip test_data.zip\n</code></pre> <ul> <li>Start training from the pre-trained model</li> </ul> <pre><code>python phasenet/train.py  --model_dir=model/190703-214543/ --train_dir=test_data/npz --train_list=test_data/npz.csv  --plot_figure --epochs=10 --batch_size=10\n</code></pre> <ul> <li>Check results in the log folder</li> </ul>"},{"location":"PhaseNet/example_batch_prediction/","title":"Batch Prediction","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport json\nimport os\nPROJECT_ROOT = os.path.realpath(os.path.join(os.path.abspath(''), \"..\"))\n</pre> import pandas as pd import json import os PROJECT_ROOT = os.path.realpath(os.path.join(os.path.abspath(''), \"..\")) In\u00a0[2]: Copied! <pre>picks_csv = pd.read_csv(os.path.join(PROJECT_ROOT, \"results/picks.csv\"), sep=\"\\t\")\npicks_csv.loc[:, 'p_idx'] = picks_csv[\"p_idx\"].apply(lambda x: x.strip(\"[]\").split(\",\"))\npicks_csv.loc[:, 'p_prob'] = picks_csv[\"p_prob\"].apply(lambda x: x.strip(\"[]\").split(\",\"))\npicks_csv.loc[:, 's_idx'] = picks_csv[\"s_idx\"].apply(lambda x: x.strip(\"[]\").split(\",\"))\npicks_csv.loc[:, 's_prob'] = picks_csv[\"s_prob\"].apply(lambda x: x.strip(\"[]\").split(\",\"))\nprint(picks_csv.iloc[1])\nprint(picks_csv.iloc[0])\n</pre> picks_csv = pd.read_csv(os.path.join(PROJECT_ROOT, \"results/picks.csv\"), sep=\"\\t\") picks_csv.loc[:, 'p_idx'] = picks_csv[\"p_idx\"].apply(lambda x: x.strip(\"[]\").split(\",\")) picks_csv.loc[:, 'p_prob'] = picks_csv[\"p_prob\"].apply(lambda x: x.strip(\"[]\").split(\",\")) picks_csv.loc[:, 's_idx'] = picks_csv[\"s_idx\"].apply(lambda x: x.strip(\"[]\").split(\",\")) picks_csv.loc[:, 's_prob'] = picks_csv[\"s_prob\"].apply(lambda x: x.strip(\"[]\").split(\",\")) print(picks_csv.iloc[1]) print(picks_csv.iloc[0]) <pre>fname      NC.MCV..EH.0361339.npz\nt0        1970-01-01T00:00:00.000\np_idx                [5999, 9015]\np_prob             [0.987, 0.981]\ns_idx                [6181, 9205]\ns_prob             [0.553, 0.873]\nName: 1, dtype: object\nfname      NN.LHV..EH.0384064.npz\nt0        1970-01-01T00:00:00.000\np_idx                          []\np_prob                         []\ns_idx                          []\ns_prob                         []\nName: 0, dtype: object\n</pre> In\u00a0[3]: Copied! <pre>with open(os.path.join(PROJECT_ROOT, \"results/picks.json\")) as fp:\n    picks_json = json.load(fp)  \nprint(picks_json[1])\nprint(picks_json[0])\n</pre> with open(os.path.join(PROJECT_ROOT, \"results/picks.json\")) as fp:     picks_json = json.load(fp)   print(picks_json[1]) print(picks_json[0]) <pre>{'id': 'NC.MCV..EH.0361339.npz', 'timestamp': '1970-01-01T00:01:30.150', 'prob': 0.9811667799949646, 'type': 'p'}\n{'id': 'NC.MCV..EH.0361339.npz', 'timestamp': '1970-01-01T00:00:59.990', 'prob': 0.9872905611991882, 'type': 'p'}\n</pre>"},{"location":"PhaseNet/example_batch_prediction/#batch-prediction","title":"Batch Prediction\u00b6","text":""},{"location":"PhaseNet/example_batch_prediction/#1-download-demo-data","title":"1. Download demo data\u00b6","text":"<pre><code>cd PhaseNet\nwget https://github.com/wayneweiqiang/PhaseNet/releases/download/test_data/test_data.zip\nunzip test_data.zip\n</code></pre>"},{"location":"PhaseNet/example_batch_prediction/#2-run-batch-prediction","title":"2. Run batch prediction\u00b6","text":"<p>PhaseNet currently supports four data formats: mseed, sac, hdf5, and numpy.</p> <ul> <li>For mseed format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/mseed.csv --data_dir=test_data/mseed --format=mseed --plot_figure\n</code></pre> <ul> <li>For sac format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/sac.csv --data_dir=test_data/sac --format=sac --plot_figure\n</code></pre> <ul> <li>For numpy format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/npz.csv --data_dir=test_data/npz --format=numpy --plot_figure\n</code></pre> <ul> <li>For hdf5 format:</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --hdf5_file=test_data/data.h5 --hdf5_group=data --format=hdf5 --plot_figure\n</code></pre> <ul> <li>For a seismic array (used by QuakeFlow):</li> </ul> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/mseed_array.csv --data_dir=test_data/mseed_array --stations=test_data/stations.json  --format=mseed_array --amplitude\n</code></pre> <pre><code>python phasenet/predict.py --model=model/190703-214543 --data_list=test_data/mseed2.csv --data_dir=test_data/mseed --stations=test_data/stations.json  --format=mseed_array --amplitude\n</code></pre> <p>Notes:</p> <ol> <li>Remove the \"--plot_figure\" argument for large datasets, because plotting can be very slow.</li> </ol> <p>Optional arguments:</p> <pre><code>usage: predict.py [-h] [--batch_size BATCH_SIZE] [--model_dir MODEL_DIR]\n                  [--data_dir DATA_DIR] [--data_list DATA_LIST]\n                  [--hdf5_file HDF5_FILE] [--hdf5_group HDF5_GROUP]\n                  [--result_dir RESULT_DIR] [--result_fname RESULT_FNAME]\n                  [--min_p_prob MIN_P_PROB] [--min_s_prob MIN_S_PROB]\n                  [--mpd MPD] [--amplitude] [--format FORMAT]\n                  [--s3_url S3_URL] [--stations STATIONS] [--plot_figure]\n                  [--save_prob]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --batch_size BATCH_SIZE\n                        batch size\n  --model_dir MODEL_DIR\n                        Checkpoint directory (default: None)\n  --data_dir DATA_DIR   Input file directory\n  --data_list DATA_LIST\n                        Input csv file\n  --hdf5_file HDF5_FILE\n                        Input hdf5 file\n  --hdf5_group HDF5_GROUP\n                        data group name in hdf5 file\n  --result_dir RESULT_DIR\n                        Output directory\n  --result_fname RESULT_FNAME\n                        Output file\n  --min_p_prob MIN_P_PROB\n                        Probability threshold for P pick\n  --min_s_prob MIN_S_PROB\n                        Probability threshold for S pick\n  --mpd MPD             Minimum peak distance\n  --amplitude           if return amplitude value\n  --format FORMAT       input format\n  --stations STATIONS   seismic station info\n  --plot_figure         If plot figure for test\n  --save_prob           If save result for test\n</code></pre>"},{"location":"PhaseNet/example_batch_prediction/#3-output-picks","title":"3. Output picks\u00b6","text":"<ul> <li>The output picks are saved to \"results/picks.csv\" on default</li> </ul> file_name begin_time station_id phase_index phase_time phase_score phase_amp phase_type 2020-10-01T00:00* 2020-10-01T00:00:00.003 CI.BOM..HH 14734 2020-10-01T00:02:27.343 0.708 2.4998866231208325e-14 P 2020-10-01T00:00* 2020-10-01T00:00:00.003 CI.BOM..HH 15487 2020-10-01T00:02:34.873 0.416 2.4998866231208325e-14 S 2020-10-01T00:00* 2020-10-01T00:00:00.003 CI.COA..HH 319 2020-10-01T00:00:03.193 0.762 3.708662269972206e-14 P <p>Notes:</p> <ol> <li>The phase_index means which data point is the pick in the original sequence. So phase_time = begin_time + phase_index / sampling rate. The default sampling_rate is 100Hz</li> </ol>"},{"location":"PhaseNet/example_batch_prediction/#3-read-ps-picks","title":"3. Read P/S picks\u00b6","text":"<p>PhaseNet currently outputs two format: CSV and JSON</p>"},{"location":"PhaseNet/example_fastapi/","title":"Interactive Example","text":"In\u00a0[1]: Copied! <pre>import os, sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy\nimport requests\nsys.path.insert(0, os.path.abspath(\"../\"))\n</pre> import os, sys import numpy as np import matplotlib.pyplot as plt import obspy import requests sys.path.insert(0, os.path.abspath(\"../\")) In\u00a0[2]: Copied! <pre># PHASENET_API_URL = \"http://127.0.0.1:8000\"\n# PHASENET_API_URL = \"http://phasenet.quakeflow.com\" ## gcp\n# PHASENET_API_URL = \"http://test.quakeflow.com:8001\" ## local machine\nPHASENET_API_URL = \"https://ai4eps-eqnet.hf.space\"\n# PHASENET_API_URL = \"http://131.215.74.195:8001\" ## local machine\n</pre> # PHASENET_API_URL = \"http://127.0.0.1:8000\" # PHASENET_API_URL = \"http://phasenet.quakeflow.com\" ## gcp # PHASENET_API_URL = \"http://test.quakeflow.com:8001\" ## local machine PHASENET_API_URL = \"https://ai4eps-eqnet.hf.space\" # PHASENET_API_URL = \"http://131.215.74.195:8001\" ## local machine In\u00a0[3]: Copied! <pre>import obspy\nstream = obspy.read()\nstream.plot();\n</pre> import obspy stream = obspy.read() stream.plot(); In\u00a0[4]: Copied! <pre>## Extract 3-component data\nstream = stream.sort()\nassert(len(stream) == 3)\ndata = []\nfor trace in stream:\n    data.append(trace.data)\ndata = np.array(data).T\nassert(data.shape[-1] == 3)\n\ndata_id = stream[0].get_id()[:-1]\ntimestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n</pre> ## Extract 3-component data stream = stream.sort() assert(len(stream) == 3) data = [] for trace in stream:     data.append(trace.data) data = np.array(data).T assert(data.shape[-1] == 3)  data_id = stream[0].get_id()[:-1] timestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] In\u00a0[5]: Copied! <pre>req = {\"id\": [data_id],\n       \"timestamp\": [timestamp],\n       \"vec\": [data.tolist()]}\n\nresp = requests.post(f'{PHASENET_API_URL}/predict', json=req)\nprint(resp)\nprint('Picks', resp.json())\n</pre> req = {\"id\": [data_id],        \"timestamp\": [timestamp],        \"vec\": [data.tolist()]}  resp = requests.post(f'{PHASENET_API_URL}/predict', json=req) print(resp) print('Picks', resp.json()) <pre>&lt;Response [200]&gt;\nPicks [{'station_id': 'B', 'phase_time': '2009-08-24T00:20:07.710', 'phase_score': 0.978, 'phase_type': 'P', 'dt': 0.01}, {'station_id': 'B', 'phase_time': '2009-08-24T00:20:08.680', 'phase_score': 0.924, 'phase_type': 'S', 'dt': 0.01}]\n</pre> In\u00a0[6]: Copied! <pre>resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req)\nprint(resp)\npicks, preds = resp.json() \npreds = np.array(preds)\nprint('Picks', picks)\n\nplt.figure()\nplt.subplot(211)\nplt.plot(data[:,-1], 'k', label=\"Z\")\nplt.subplot(212)\nplt.plot(preds[0, :, 0, 1], label=\"P\")\nplt.plot(preds[0, :, 0, 2], label=\"S\")\nplt.legend()\nplt.show();\n</pre> resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req) print(resp) picks, preds = resp.json()  preds = np.array(preds) print('Picks', picks)  plt.figure() plt.subplot(211) plt.plot(data[:,-1], 'k', label=\"Z\") plt.subplot(212) plt.plot(preds[0, :, 0, 1], label=\"P\") plt.plot(preds[0, :, 0, 2], label=\"S\") plt.legend() plt.show(); <pre>&lt;Response [200]&gt;\nPicks [{'station_id': 'B', 'phase_time': '2009-08-24T00:20:07.710', 'phase_score': 0.978, 'phase_type': 'P', 'dt': 0.01}, {'station_id': 'B', 'phase_time': '2009-08-24T00:20:08.680', 'phase_score': 0.924, 'phase_type': 'S', 'dt': 0.01}]\n</pre>"},{"location":"PhaseNet/example_fastapi/#interactive-example","title":"Interactive Example\u00b6","text":""},{"location":"PhaseNet/example_fastapi/#1-run-phasenet-in-terminal-or-use-quakeflow-api","title":"1. Run PhaseNet in terminal or use QuakeFlow API\u00b6","text":""},{"location":"PhaseNet/example_fastapi/#2-prepare-seismic-waveforms","title":"2. Prepare seismic waveforms\u00b6","text":"<p>Find more details in obspy's tutorials:</p> <p>FDSN web service client for ObsPy</p> <p>Mass Downloader for FDSN Compliant Web Services</p>"},{"location":"PhaseNet/example_fastapi/#3-predict-ps-phase-picks-using-phasenet","title":"3. Predict P/S-phase picks using PhaseNet\u00b6","text":""},{"location":"PhaseNet/example_fastapi/#4-get-both-picks-and-prediction-time-series","title":"4. Get both picks and prediction (time series)\u00b6","text":""},{"location":"PhaseNet/example_gradio/","title":"Example gradio","text":"In\u00a0[1]: Copied! <pre>from gradio_client import Client\nimport obspy\nimport numpy as np\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\n</pre> from gradio_client import Client import obspy import numpy as np import json import pandas as pd import matplotlib.pyplot as plt import json <pre>/Users/weiqiang/.local/miniconda3/envs/gradio/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>import obspy\nstream = obspy.read()\nstream.plot();\n</pre> import obspy stream = obspy.read() stream.plot(); In\u00a0[3]: Copied! <pre>stream.write(\"data.mseed\", format=\"MSEED\")\n</pre> stream.write(\"data.mseed\", format=\"MSEED\") In\u00a0[4]: Copied! <pre>## Extract 3-component data\nstream = stream.sort()\nassert(len(stream) == 3)\ndata = []\nfor trace in stream:\n    data.append(trace.data)\ndata = np.array(data).T\nassert(data.shape[-1] == 3)\n\ndata_id = stream[0].get_id()[:-1]\ntimestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n</pre> ## Extract 3-component data stream = stream.sort() assert(len(stream) == 3) data = [] for trace in stream:     data.append(trace.data) data = np.array(data).T assert(data.shape[-1] == 3)  data_id = stream[0].get_id()[:-1] timestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] In\u00a0[5]: Copied! <pre>model = \"ai4eps/phasenet\"\n# model = \"http://127.0.0.1:7860\"\nclient = Client(model, serialize=True)\n_, picks_csv, picks_json = client.predict([\"data.mseed\"], json.dumps(data.tolist()), json.dumps([data_id]))\n</pre> model = \"ai4eps/phasenet\" # model = \"http://127.0.0.1:7860\" client = Client(model, serialize=True) _, picks_csv, picks_json = client.predict([\"data.mseed\"], json.dumps(data.tolist()), json.dumps([data_id])) <pre>Loaded as API: http://127.0.0.1:7860/ \u2714\n</pre> In\u00a0[7]: Copied! <pre>picks = pd.read_json(picks_json)\n# picks = pd.read_csv(csv, parse_dates=[\"phase_time\"])\n# print(picks)\n</pre> picks = pd.read_json(picks_json) # picks = pd.read_csv(csv, parse_dates=[\"phase_time\"]) # print(picks) In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(len(stream), 1, figsize=(10, 10))\nfor i, tr in enumerate(stream):\n    ax[i].plot(tr.times(), tr.data, label=tr.stats.channel, c=\"k\")\n    for _, pick in picks.iterrows():\n        c = \"blue\" if pick[\"phase_type\"] == \"P\" else \"red\"\n        label = pick[\"phase_type\"] if i == 0 else None\n        ax[i].axvline((pick[\"phase_time\"]-tr.stats.starttime.datetime).total_seconds(), c=c, label=label, alpha=pick[\"phase_score\"])\n    ax[i].legend()\n</pre> fig, ax = plt.subplots(len(stream), 1, figsize=(10, 10)) for i, tr in enumerate(stream):     ax[i].plot(tr.times(), tr.data, label=tr.stats.channel, c=\"k\")     for _, pick in picks.iterrows():         c = \"blue\" if pick[\"phase_type\"] == \"P\" else \"red\"         label = pick[\"phase_type\"] if i == 0 else None         ax[i].axvline((pick[\"phase_time\"]-tr.stats.starttime.datetime).total_seconds(), c=c, label=label, alpha=pick[\"phase_score\"])     ax[i].legend() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"PhaseNet/example_gradio/#interactive-example","title":"Interactive Example\u00b6","text":""},{"location":"PhaseNet/example_gradio/#2-prepare-seismic-waveforms","title":"2. Prepare seismic waveforms\u00b6","text":"<p>Find more details in obspy's tutorials:</p> <p>FDSN web service client for ObsPy</p> <p>Mass Downloader for FDSN Compliant Web Services</p>"},{"location":"PhaseNet/example_gradio/#3-predict-ps-phase-picks-using-phasenet","title":"3. Predict P/S-phase picks using PhaseNet\u00b6","text":""},{"location":"QuakeFlow/","title":"Overview","text":""},{"location":"QuakeFlow/#quakeflow-a-scalable-machine-learning-based-earthquake-monitoring-workflow-with-cloud-computing","title":"QuakeFlow: A Scalable Machine-learning-based Earthquake Monitoring Workflow with Cloud Computing","text":""},{"location":"QuakeFlow/#overview","title":"Overview","text":"<p>QuakeFlow is a scalable deep-learning-based earthquake monitoring system with cloud computing.  It applies the state-of-art deep learning/machine learning models for earthquake detection.  With auto-scaling enabled on Kubernetes, our system can balance computational loads with computational resources. </p>"},{"location":"QuakeFlow/#current-modules","title":"Current Modules","text":""},{"location":"QuakeFlow/#models","title":"Models","text":"<ul> <li>DeepDenoiser: (paper) (example)</li> <li>PhaseNet: (paper) (example)</li> <li>GaMMA: (paper) (example)</li> <li>HypoDD (paper) (example)</li> <li>More models to be added. Contributions are highly welcomed!</li> </ul>"},{"location":"QuakeFlow/#data-stream","title":"Data stream","text":"<ul> <li>Plotly: ui.quakeflow.com</li> <li>Kafka </li> <li>Spark Streaming</li> </ul>"},{"location":"QuakeFlow/#data-process","title":"Data process","text":"<ul> <li>Colab example</li> <li>Kubeflow: (example)</li> </ul>"},{"location":"QuakeFlow/#deployment","title":"Deployment","text":"<p>QuakeFlow can be deployed on any cloud platforms with Kubernetes service.</p> <ul> <li>For google cloud platform (GCP), check out the GCP README.</li> <li>For on-premise servers, check out the Kubernetes README.</li> </ul>"},{"location":"QuakeFlow/data/","title":"Downloading Data using Obspy","text":""},{"location":"QuakeFlow/data_format/","title":"Standard Data Formats of QuakeFlow","text":"<ul> <li>Raw data: <ul> <li>Waveform (MSEED): <ul> <li>Year/Jday/Hour/Network.Station.Location.Channel.mseed</li> </ul> </li> <li>Station (xml):<ul> <li>Network.Station.xml</li> </ul> </li> <li>Events (CSV):<ul> <li>colums: time, latitude, longitude, depth_km, magnitude, event_id</li> </ul> </li> <li>Picks (CSV)<ul> <li>columns: station_id (network.station.location.channel) phase_time, phase_type, phase_score, event_id</li> </ul> </li> </ul> </li> <li>Phase picking:<ul> <li>Picks (CSV):<ul> <li>columns: station_id (network.station.location.channel) phase_time, phase_type, phase_score, phase_polarity</li> </ul> </li> </ul> </li> <li>Phase association:<ul> <li>Events (CSV):<ul> <li>colums: time, latitude, longitude, depth_km, magnitude, event_id</li> </ul> </li> <li>Picks (CSV):<ul> <li>columns: station_id (network.station.location.channel), phase_time, phase_type, phase_score, phase_polarity, event_id</li> </ul> </li> </ul> </li> <li>Earthquake location:<ul> <li>Events (CSV):<ul> <li>colums: time, latitude, longitude, depth_km, magnitude, event_id</li> </ul> </li> </ul> </li> <li>Earthquake relocation:<ul> <li>Events (CSV):<ul> <li>colums: time, latitude, longitude, depth_km, magnitude, event_id</li> </ul> </li> </ul> </li> <li>Focal mechanism:<ul> <li>Focal mechanism (CSV):<ul> <li>columns: strike1, dip1, rake1, strike2, dip2, rake2, event_id</li> </ul> </li> </ul> </li> </ul>"},{"location":"QuakeFlow/fastapi/","title":"Fastapi","text":"In\u00a0[1]: Copied! <pre>import os, sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy\nimport requests\nsys.path.insert(0, os.path.abspath(\"../\"))\nnp.random.seed(1000)\n</pre> import os, sys import numpy as np import matplotlib.pyplot as plt import obspy import requests sys.path.insert(0, os.path.abspath(\"../\")) np.random.seed(1000) In\u00a0[2]: Copied! <pre>import obspy\nstream = obspy.read()\nstream.plot();\n</pre> import obspy stream = obspy.read() stream.plot(); In\u00a0[3]: Copied! <pre>## Extract 3-component data\nstream = stream.sort()\nassert(len(stream) == 3)\ndata = []\nfor trace in stream:\n    data.append(trace.data)\ndata = np.array(data).T\nassert(data.shape[-1] == 3)\ndata_id = stream[0].get_id()[:-1]\ntimestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n\n## Add some noise\nnoisy_data = data + np.random.randn(*data.shape)*np.max(data)/5\n</pre> ## Extract 3-component data stream = stream.sort() assert(len(stream) == 3) data = [] for trace in stream:     data.append(trace.data) data = np.array(data).T assert(data.shape[-1] == 3) data_id = stream[0].get_id()[:-1] timestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  ## Add some noise noisy_data = data + np.random.randn(*data.shape)*np.max(data)/5 In\u00a0[4]: Copied! <pre>req = {\"id\": [data_id],\n       \"timestamp\": [timestamp],\n       \"vec\": [noisy_data.tolist()]}\n\nDEEPDENOISER_API_URL = \"http://127.0.0.1:8003\"\nresp_deepdenoiser = requests.get(f'{DEEPDENOISER_API_URL}/predict', json=req)\n\ndenoised_data = np.array(resp_deepdenoiser.json()[\"vec\"])\n\nplt.figure(figsize=(10,4))\nplt.subplot(331)\nplt.plot(data[:,0], 'k', linewidth=0.5, label=\"E\")\nplt.legend()\nplt.title(\"Raw signal\")\nplt.subplot(332)\nplt.plot(noisy_data[:,0], 'k', linewidth=0.5, label=\"E\")\nplt.title(\"Nosiy signal\")\nplt.subplot(333)\nplt.plot(denoised_data[0, :,0], 'k', linewidth=0.5, label=\"E\")\nplt.title(\"Denoised signal\")\nplt.subplot(334)\nplt.plot(data[:,1], 'k', linewidth=0.5, label=\"N\")\nplt.legend()\nplt.subplot(335)\nplt.plot(noisy_data[:,1], 'k', linewidth=0.5, label=\"N\")\nplt.subplot(336)\nplt.plot(denoised_data[0,:,1], 'k', linewidth=0.5, label=\"N\")\nplt.subplot(337)\nplt.plot(data[:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.legend()\nplt.subplot(338)\nplt.plot(noisy_data[:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.subplot(339)\nplt.plot(denoised_data[0,:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.tight_layout()\nplt.show();\n</pre> req = {\"id\": [data_id],        \"timestamp\": [timestamp],        \"vec\": [noisy_data.tolist()]}  DEEPDENOISER_API_URL = \"http://127.0.0.1:8003\" resp_deepdenoiser = requests.get(f'{DEEPDENOISER_API_URL}/predict', json=req)  denoised_data = np.array(resp_deepdenoiser.json()[\"vec\"])  plt.figure(figsize=(10,4)) plt.subplot(331) plt.plot(data[:,0], 'k', linewidth=0.5, label=\"E\") plt.legend() plt.title(\"Raw signal\") plt.subplot(332) plt.plot(noisy_data[:,0], 'k', linewidth=0.5, label=\"E\") plt.title(\"Nosiy signal\") plt.subplot(333) plt.plot(denoised_data[0, :,0], 'k', linewidth=0.5, label=\"E\") plt.title(\"Denoised signal\") plt.subplot(334) plt.plot(data[:,1], 'k', linewidth=0.5, label=\"N\") plt.legend() plt.subplot(335) plt.plot(noisy_data[:,1], 'k', linewidth=0.5, label=\"N\") plt.subplot(336) plt.plot(denoised_data[0,:,1], 'k', linewidth=0.5, label=\"N\") plt.subplot(337) plt.plot(data[:,2], 'k', linewidth=0.5, label=\"Z\") plt.legend() plt.subplot(338) plt.plot(noisy_data[:,2], 'k', linewidth=0.5, label=\"Z\") plt.subplot(339) plt.plot(denoised_data[0,:,2], 'k', linewidth=0.5, label=\"Z\") plt.tight_layout() plt.show(); In\u00a0[5]: Copied! <pre>PHASENET_API_URL = \"http://127.0.0.1:8001\"\nresp = requests.get(f'{PHASENET_API_URL}/predict', json=req)\nprint('Picks', resp.json())\n</pre> PHASENET_API_URL = \"http://127.0.0.1:8001\" resp = requests.get(f'{PHASENET_API_URL}/predict', json=req) print('Picks', resp.json()) <pre>Picks [{'id': 'BW.RJOB..EH', 'timestamp': '2009-08-24T00:20:08.690', 'prob': 0.41293007135391235, 'amp': 3281.9875522716266, 'type': 's'}]\n</pre> In\u00a0[6]: Copied! <pre>PHASENET_API_URL = \"http://127.0.0.1:8001\"\nresp = requests.get(f'{PHASENET_API_URL}/predict_prob', json=req)\npicks, preds = resp.json() \npreds = np.array(preds)\nprint('Picks', picks)\n\nplt.figure()\nplt.subplot(211)\nplt.plot(noisy_data[:,-1], 'k', label=\"Z\")\nplt.subplot(212)\nplt.plot(preds[0, :, 0, 1], label=\"P\")\nplt.plot(preds[0, :, 0, 2], label=\"S\")\nplt.legend()\nplt.show();\n</pre> PHASENET_API_URL = \"http://127.0.0.1:8001\" resp = requests.get(f'{PHASENET_API_URL}/predict_prob', json=req) picks, preds = resp.json()  preds = np.array(preds) print('Picks', picks)  plt.figure() plt.subplot(211) plt.plot(noisy_data[:,-1], 'k', label=\"Z\") plt.subplot(212) plt.plot(preds[0, :, 0, 1], label=\"P\") plt.plot(preds[0, :, 0, 2], label=\"S\") plt.legend() plt.show(); <pre>Picks [{'id': 'BW.RJOB..EH', 'timestamp': '2009-08-24T00:20:08.690', 'prob': 0.41293007135391235, 'amp': 3281.9875522716266, 'type': 's'}]\n</pre> In\u00a0[7]: Copied! <pre>PHASENET_API_URL = \"http://127.0.0.1:8001\"\nresp = requests.get(f'{PHASENET_API_URL}/predict', json=resp_deepdenoiser.json())\nprint('Picks', resp.json())\n</pre> PHASENET_API_URL = \"http://127.0.0.1:8001\" resp = requests.get(f'{PHASENET_API_URL}/predict', json=resp_deepdenoiser.json()) print('Picks', resp.json()) <pre>Picks [{'id': 'BW.RJOB..EH', 'timestamp': '2009-08-24T00:20:07.030', 'prob': 0.763781726360321, 'amp': 1741.8168170560068, 'type': 'p'}, {'id': 'BW.RJOB..EH', 'timestamp': '2009-08-24T00:20:08.610', 'prob': 0.5108724236488342, 'amp': 1741.8168170560068, 'type': 's'}]\n</pre> In\u00a0[8]: Copied! <pre>PHASENET_API_URL = \"http://127.0.0.1:8001\"\nresp = requests.get(f'{PHASENET_API_URL}/predict_prob', json=resp_deepdenoiser.json())\npicks, preds = resp.json() \npreds = np.array(preds)\nprint('Picks', picks)\n\nplt.figure()\nplt.subplot(211)\nplt.plot(denoised_data[0,:,-1], 'k', label=\"Z\")\nplt.subplot(212)\nplt.plot(preds[0, :, 0, 1], label=\"P\")\nplt.plot(preds[0, :, 0, 2], label=\"S\")\nplt.legend()\nplt.show();\n</pre> PHASENET_API_URL = \"http://127.0.0.1:8001\" resp = requests.get(f'{PHASENET_API_URL}/predict_prob', json=resp_deepdenoiser.json()) picks, preds = resp.json()  preds = np.array(preds) print('Picks', picks)  plt.figure() plt.subplot(211) plt.plot(denoised_data[0,:,-1], 'k', label=\"Z\") plt.subplot(212) plt.plot(preds[0, :, 0, 1], label=\"P\") plt.plot(preds[0, :, 0, 2], label=\"S\") plt.legend() plt.show(); <pre>Picks [{'id': 'BW.RJOB..EH', 'timestamp': '2009-08-24T00:20:07.030', 'prob': 0.763781726360321, 'amp': 1741.8168170560068, 'type': 'p'}, {'id': 'BW.RJOB..EH', 'timestamp': '2009-08-24T00:20:08.610', 'prob': 0.5108724236488342, 'amp': 1741.8168170560068, 'type': 's'}]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"QuakeFlow/fastapi/#test-fastapi","title":"Test FastAPI\u00b6","text":""},{"location":"QuakeFlow/fastapi/#start-fastapi","title":"Start FASTAPI\u00b6","text":"<pre>uvicorn --app-dir=phasenet app:app --reload --port 8001\nuvicorn --app-dir=deepdenoiser app:app --reload --port 8003\n</pre>"},{"location":"QuakeFlow/fastapi/#test-deepdenoiser","title":"Test DeepDenoiser\u00b6","text":""},{"location":"QuakeFlow/fastapi/#test-phasenet","title":"Test PhaseNet\u00b6","text":""},{"location":"QuakeFlow/fastapi/#test-deepdenoiser-phasenet","title":"Test DeepDenoiser + PhaseNet\u00b6","text":""},{"location":"QuakeFlow/gcp_readme/","title":"Quick readme, not detailed","text":"<ol> <li>Create a cluster on GCP with node autoscaling</li> </ol> <pre><code>gcloud container clusters create quakeflow-cluster --zone=\"us-west1-a\" --scopes=\"cloud-platform\" --image-type=\"ubuntu\"  --machine-type=\"n1-standard-2\" --num-nodes=2 --enable-autoscaling --min-nodes 1 --max-nodes 4\n</code></pre> <ol> <li>Switch to the correct context</li> </ol> <pre><code>gcloud container clusters get-credentials quakeflow-cluster\n</code></pre> <ol> <li>Deploy the services on the cluster</li> </ol> <pre><code>kubectl apply -f quakeflow-gcp.yaml \n</code></pre> <ol> <li>Setup the APIs</li> </ol> <p>4.1 Add pods autoscaling</p> <pre><code>kubectl autoscale deployment phasenet-api --cpu-percent=80 --min=1 --max=10\nkubectl autoscale deployment gmma-api --cpu-percent=80 --min=1 --max=10\n</code></pre> <p>4.2 Expose API</p> <pre><code>kubectl expose deployment phasenet-api --type=LoadBalancer --name=phasenet-service\nkubectl expose deployment gmma-api --type=LoadBalancer --name=gmma-service\nkubectl expose deployment quakeflow-ui --type=LoadBalancer --name=quakeflow-ui\n</code></pre> <ol> <li>Install Kafka</li> </ol> <p>5.1 Install</p> <pre><code>helm install quakeflow-kafka bitnami/kafka   \n</code></pre> <p>5.2 Create topics</p> <pre><code>kubectl run --quiet=true -it --rm quakeflow-kafka-client --restart='Never' --image docker.io/bitnami/kafka:2.7.0-debian-10-r68 --restart=Never --command -- bash -c \"kafka-topics.sh --create --topic phasenet_picks --bootstrap-server my-kafka.default.svc.cluster.local:9092 &amp;&amp; kafka-topics.sh --create --topic gmma_events --bootstrap-server my-kafka.default.svc.cluster.local:9092 &amp;&amp; kafka-topics.sh --create --topic waveform_raw --bootstrap-server my-kafka.default.svc.cluster.local:9092\"\n</code></pre> <p>5.3 Check status</p> <pre><code>helm status quakeflow-kafka\n</code></pre> <ol> <li>Rollup restart deployments</li> </ol> <pre><code>kubectl rollout restart deployments   \n</code></pre> <ol> <li>Install Dashboard</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre> <p>Run the following command and visit http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</p> <pre><code>kubectl proxy \n</code></pre> <p>If you are asked to provide a token, get the token with the following command</p> <pre><code>gcloud config config-helper --format=json | jq -r '.credential.access_token'\n</code></pre>"},{"location":"QuakeFlow/k8s_readme/","title":"Quick readme, not detailed","text":""},{"location":"QuakeFlow/k8s_readme/#all-in-one-script","title":"All-in-one script","text":"<p>You need to preinstall helm, kubectl, docker and minikube (or any other local Kubernetes framework)</p> <p>Then deploy everything with the following script!</p> <pre><code>$ git clone --recurse-submodules -j8 git@github.com:wayneweiqiang/QuakeFlow.git\n$ sh deploy_local.sh\n</code></pre>"},{"location":"QuakeFlow/k8s_readme/#prebuilt-kafka","title":"Prebuilt Kafka","text":"<ol> <li>Install</li> </ol> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install quakeflow-kafka bitnami/kafka   \n</code></pre> <ol> <li>Create topics</li> </ol> <pre><code>kubectl run --quiet=true -it --rm quakeflow-kafka-client --restart='Never' --image docker.io/bitnami/kafka:2.7.0-debian-10-r68 --restart=Never --command -- bash -c \"kafka-topics.sh --create --topic phasenet_picks --bootstrap-server quakeflow-kafka.default.svc.cluster.local:9092 &amp;&amp; kafka-topics.sh --create --topic gmma_events --bootstrap-server quakeflow-kafka.default.svc.cluster.local:9092 &amp;&amp; kafka-topics.sh --create --topic waveform_raw --bootstrap-server quakeflow-kafka.default.svc.cluster.local:9092\"\n</code></pre> <ol> <li>Check status</li> </ol> <pre><code>helm status quakeflow-kafka\n</code></pre>"},{"location":"QuakeFlow/k8s_readme/#our-own-containers","title":"Our own containers","text":"<ol> <li>Switch to minikube environment</li> </ol> <pre><code>eval $(minikube docker-env)     \n</code></pre> <p>1.1. Fix metrics-server for auto-scalling (Only for docker) https://stackoverflow.com/questions/54106725/docker-kubernetes-mac-autoscaler-unable-to-find-metrics</p> <pre><code>kubectl apply -f metrics-server.yaml\n</code></pre> <ol> <li>Build the docker images, see the docs for each container</li> </ol> <pre><code>docker build --tag quakeflow-spark:1.0 .\n...\n</code></pre> <ol> <li>Create everything</li> </ol> <pre><code>kubectl apply -f quakeflow-delpoyment.yaml     \n</code></pre> <p>3.1 Add autoscaling</p> <pre><code>kubectl autoscale deployment phasenet-api --cpu-percent=80 --min=1 --max=10\nkubectl autoscale deployment gmma-api --cpu-percent=80 --min=1 --max=10\n</code></pre> <p>3.2 Expose API</p> <pre><code>kubectl expose deployment phasenet-api --type=LoadBalancer --name=phasenet-service\n</code></pre> <ol> <li>Check the pods</li> </ol> <pre><code>kubectl get pods\n</code></pre> <ol> <li>Check the logs (an example)</li> </ol> <pre><code>kubectl logs quakeflow-spark-7699cd45d8-mvv6r\n</code></pre> <ol> <li>Delete a single deployment</li> </ol> <pre><code>kubectl delete deploy quakeflow-spark     \n</code></pre> <ol> <li>Delete everything</li> </ol> <pre><code>kubectl delete -f quakeflow-delpoyment.yaml   \n</code></pre>"},{"location":"deep-earthquake-catalog/","title":"Earthquake Deep Catalogs","text":""},{"location":"deep-earthquake-catalog/#catalog-format","title":"Catalog Format","text":""},{"location":"deep-earthquake-catalog/#catalog-card","title":"Catalog Card","text":""},{"location":"earthquake-catalog-workshop/","title":"Earthquake Catalog Workshop","text":"<p>Welcome to the Earthquake Catalog Workshop! This repository contains materials for a workshop on earthquake catalogs, including data, scripts, and documentation.</p> <p>Contributors: Eric Beauce, Gabrielle Tepp, Clara Yoon, Ellen Yu, Weiqiang Zhu (alphabetical order)</p>"},{"location":"earthquake-catalog-workshop/#introductions","title":"Introductions","text":"<ul> <li>Instructor intros</li> <li>Workshop goals/schedule</li> <li>What is an earthquake catalog? Different end-users and scientific motivations</li> <li>Why earthquake catalogs and how to choose the right one?</li> </ul>"},{"location":"earthquake-catalog-workshop/#regional-seismic-networks-official-catalogs-and-data-access-ellen-gabrielle","title":"Regional Seismic Networks: Official Catalogs and Data Access (Ellen &amp; Gabrielle)","text":"<ul> <li>How regional catalogs are made and what they include</li> <li>SCSN catalog and special datasets</li> <li>ComCat (ANSS)</li> <li>NSF Sage (Earthscope)</li> <li>Accessing waveform data and metadata - AWS Open Dataset vs FDSN Webservices</li> <li>Citing network data &amp; catalogs</li> </ul>"},{"location":"earthquake-catalog-workshop/#break","title":"Break","text":""},{"location":"earthquake-catalog-workshop/#building-custom-catalogs-with-modern-tools","title":"Building Custom Catalogs with Modern Tools","text":"<ul> <li>Catalog Workflow + Machine Learning (Weiqiang &amp; Clara)</li> <li>Template Matching (Eric)</li> </ul>"},{"location":"earthquake-catalog-workshop/#break_1","title":"Break","text":""},{"location":"earthquake-catalog-workshop/#evaluating-catalog-quality","title":"Evaluating Catalog Quality","text":"<ul> <li>Anomaly detection</li> <li>Visualization tools</li> <li>Magnitude of completeness</li> <li>Quality control</li> <li>Tips &amp; tricks</li> </ul>"},{"location":"earthquake-catalog-workshop/#conclusions","title":"Conclusions","text":"<ul> <li>Discuss limitations and reasons for using different types of catalogs</li> <li>When to use STA/LTA vs. deep-learning vs. template-matching vs relocated</li> <li>Combining different methods depending on application</li> <li>How does a user choose among the many options for deep-learning pickers, event associators, event location methods?</li> </ul>"},{"location":"earthquake-catalog-workshop/#discussion-questions-tutorial-help","title":"Discussion, Questions, &amp; Tutorial Help","text":"<p>If you have any questions about the workshop materials or encounter any issues, please open an issue on our GitHub repository.</p> <pre><code>@misc{earthquake_catalog_workshop_2025,\n  author = {Beauce, Eric and Tepp, Gabrielle and Yoon, Clara and Yu, Ellen and Zhu, Weiqiang},\n  title = {Building a High Resolution Earthquake Catalog from Raw Waveforms: A Step-by-Step Guide},\n  year = {2025},\n  url = {https://ai4eps.github.io/Earthquake_Catalog_Workshop/},\n  note = {Seismological Society of America (SSA) Annual Meeting, 2025}\n}\n</code></pre>"},{"location":"earthquake-catalog-workshop/catalog_analysis/","title":"Evaluating Earthquake Catalog","text":"<p>      Fullscreen    </p>"},{"location":"earthquake-catalog-workshop/conclusion/","title":"Conclusions","text":"<p>      Fullscreen    </p>"},{"location":"earthquake-catalog-workshop/introduction/","title":"Introduction","text":"<p>      Fullscreen    </p>"},{"location":"earthquake-catalog-workshop/machine_learning/","title":"Slides","text":"<p>      Fullscreen    </p>"},{"location":"earthquake-catalog-workshop/seismic_network/","title":"Slides","text":"<p>      Fullscreen    </p>"},{"location":"earthquake-catalog-workshop/template_matching/","title":"Slides","text":"Animation         Fullscreen"},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/","title":"Tutorials","text":""},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/#scedc-web-services","title":"SCEDC Web Services","text":"<ul> <li>Retrieve event, station and waveform data for the 2019 Ridgecrest 7.1</li> </ul>"},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/#scedc-pystp","title":"SCEDC PySTP","text":"<ul> <li>Retrieve and plot event triggered data and phase picks using PySTP</li> </ul>"},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/#scedc-public-data-set-pds","title":"SCEDC Public Data Set (PDS)","text":"<ul> <li> <p>Retrieve continuous waveform data from SCEDC PDS </p> <p>This tutorial uses the Clickable Station Map to query and retrieve a list of stations. It then, leverages the SCEDC FDSN availability web service to get time spans for waveform data for these stations. Finally, it retrieves data from PDS for a time span.</p> </li> <li> <p>Create an API to decimate waveform data from SCEDC PDS</p> <p>This tutorial shows how to create an API that runs a Lambda function to decimate a waveform from the SCEDC PDS using ObsPy and store the decimated waveform in another S3 bucket. AWS services used: Lambda, ECR, S3, Cloud9.</p> </li> <li> <p>Create a Lambda function to remove response from waveform data from SCEDC PDS</p> <p>This tutorial shows how to make a Lambda function from a Docker image that uses ObsPy to remove response from waveform data from the PDS. Video Tutorial</p> </li> </ul>"},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/#scedc-aws-ridgecrest-das","title":"SCEDC AWS Ridgecrest DAS","text":"<ul> <li> <p>Retrieve DAS waveform data from AWS and basic processing </p> <p>This tutorial shows how to download distributed acoustic sensing (DAS) data from the Ridgecrest array and how to perform simple processing operations on the data (e.g., plotting earthquake strain, computing cross-correlations).</p> <p>If GitHub cannot render this notebook, one can visualize it at this link: https://nbviewer.org/github/SCEDC/tutorials/blob/main/jupyter-notebooks/DAS_aws_Ridgecrest/access_aws_data.ipynb</p> </li> </ul>"},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/#sample-aws-boto3-script","title":"Sample AWS Boto3 script","text":"<ul> <li>Sample boto3 script to access SCEDC PDS</li> </ul> <p>This script uses boto3 to retrieve a waveform file from the SCEDC PDS and print waveform information using Obspy functions.</p>"},{"location":"earthquake-catalog-workshop/seismic_network/tutorials/#accessing-scedcscsn-data-via-matlab","title":"Accessing SCEDC/SCSN data via MATLAB","text":"<ul> <li>Retrieve event, station, and waveform data from SCEDC using MATLAB: Regular .m Script or Live Script</li> <li> <p>Note that both versions contain the same information and code.</p> <p>This tutorial demonstrates how to retrieve event, station, and waveform data using irisFetch in MATLAB and make basic plots with the data. The tutorial also briefly covers waveform access and plotting with the GISMO toolbox.</p> </li> </ul>"},{"location":"eps207-observational-seismology/","title":"Welcome to EPS207 Observational Seismology","text":"<p>Seismology is the study of earthquakes and seismic waves that traverse the Earth. We can extract rich information from seismic waveform to study earthquake sources, Earth's interior structure, and other processes such as volcanic eruptions, landslides, and glacier movements. Seismology also has many engineering applications such as oil/gas/geothermal energy exploration, natural/induced earthquake monitoring, and earthquake early warning. In this course, we will learn many data processing and machine learning techniques to analyze seismic waveforms and extract useful information.</p> <p>The class will be a combination of lectures and hands-on exercises. We will use primarily Python (obpsy, numpy, scipy, matplotlib, pytorch, etc.) and some Julia as programming languages. The course is designed for graduate students and advanced undergraduate students. No prior knowledge of seismology is required.</p>"},{"location":"eps207-observational-seismology/#syllabus","title":"Syllabus","text":""},{"location":"eps207-observational-seismology/#time-and-location","title":"Time and Location","text":"<ul> <li>Lecture: Monday 9:00 AM - 11:00 AM in McCone 265</li> <li>Office Hour: Monday 3:00 PM - 4:00 PM in McCone 285</li> </ul>"},{"location":"eps207-observational-seismology/#projects","title":"Projects","text":"<ul> <li>Mining the IRIS dataset</li> <li>Working on you own dataset</li> </ul>"},{"location":"eps207-observational-seismology/#schedule","title":"Schedule","text":"Date Topic 08/25 Introduction 08/28 Earthquake Source and Seismic Wave 09/04 Labor Day 09/11 SCEC Meeting (No class) 09/18 Seismic Signal Processing 09/25 Earthquake Detection 10/02 Phase Picking &amp; Association 10/09 Earthquake Location &amp; Relative Location 10/16 Earthquake Statistics 10/23 Focal Mechanism &amp; Moment Tensor 10/30 Focal Mechanism Inversion 11/06 Ambient Noise 11/13 Seismic Tomography 11/20 Full-waveform Inversion 11/27 Other Sensing Technologies 12/04 Final Project Presentations"},{"location":"eps207-observational-seismology/syllabus/","title":"Class Syllabus","text":""},{"location":"eps207-observational-seismology/syllabus/#seismic-waves","title":"Seismic Waves","text":"<ul> <li>Stress and strain constitutive relationship</li> <li>Wave equations</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#seismic-data","title":"Seismic Data","text":"<ul> <li>Obspy, FDSN, Seedlink</li> <li>Matplotlib, PyGMT, Plotly</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#event-detection","title":"Event Detection","text":"<ul> <li>STA/LTA, Template Matching</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#phase-detection-picking","title":"Phase Detection &amp; Picking","text":"<ul> <li>PhaseNet</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#phase-association","title":"Phase Association","text":"<ul> <li>GaMMA</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#earthquake-location","title":"Earthquake Location","text":"<ul> <li>Automatic Differentiation</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#double-difference-relocation","title":"Double Difference Relocation","text":"<ul> <li>HypoDD</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#phase-polarity-and-focal-mechanism","title":"Phase Polarity and Focal Mechanism","text":"<ul> <li>PhaseNet-Polarity, SVM-Polarity</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#earthquake-magnitude","title":"Earthquake Magnitude","text":""},{"location":"eps207-observational-seismology/syllabus/#seismic-tomography","title":"Seismic Tomography","text":"<ul> <li>ADEikonal</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#full-waveform-inversion","title":"Full-waveform Inversion","text":"<ul> <li>ADSeismic</li> </ul>"},{"location":"eps207-observational-seismology/syllabus/#case-studies","title":"Case Studies","text":"<ul> <li>Ridgecrest Earthquake, CA<ul> <li>QuakeFlow</li> <li>EQNet</li> </ul> </li> </ul>"},{"location":"eps207-observational-seismology/earthquake/detection/","title":"Detection","text":"In\u00a0[1]: Copied! <pre>from IPython.display import Image, HTML\n</pre> from IPython.display import Image, HTML <p>We use two methods:</p> <ul> <li>A traditional method STA/LTA (short-time-averaging over long-time-averaging):</li> </ul> <p>image source</p> <p></p> <ul> <li>A machine learning method PhaseNet:</li> </ul> <p></p> In\u00a0[4]: Copied! <pre>import obspy\nfrom obspy import UTCDateTime\nfrom obspy.clients.fdsn import Client\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> import obspy from obspy import UTCDateTime from obspy.clients.fdsn import Client import matplotlib.pyplot as plt import numpy as np <p>Here we look at a few earthquakes occurred at the Mendocino Triple Junction zone:</p> <p></p> <p>We can pick one raspberry shake from the station view website.</p> <p></p> <p>Then we can download the seismic waveforms around the earthquake origin time:</p> In\u00a0[7]: Copied! <pre>client = Client(\"RASPISHAKE\")\n\n# starttime = \"2022-06-18 11:43:09\"\nstarttime = \"2022-07-01 01:40:51\"\n# starttime = \"2022-07-05 07:21:12\"\nstarttime = UTCDateTime(starttime) - 10\nwindow_length = 50\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R5E62\",\n                                 location=\"*\",\n                                 channel=\"EHZ\", \n                                 # starttime=UTCDateTime(\"2022-06-29T14:08:04\"), \n                                 # endtime=UTCDateTime(\"2022-06-29T14:08:04\")+120)  \n                                 starttime=starttime, \n                                 endtime=starttime+window_length)  \nwaveforms = waveforms.detrend()\nwaveforms[0].plot();\n</pre> client = Client(\"RASPISHAKE\")  # starttime = \"2022-06-18 11:43:09\" starttime = \"2022-07-01 01:40:51\" # starttime = \"2022-07-05 07:21:12\" starttime = UTCDateTime(starttime) - 10 window_length = 50 waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R5E62\",                                  location=\"*\",                                  channel=\"EHZ\",                                   # starttime=UTCDateTime(\"2022-06-29T14:08:04\"),                                   # endtime=UTCDateTime(\"2022-06-29T14:08:04\")+120)                                    starttime=starttime,                                   endtime=starttime+window_length)   waveforms = waveforms.detrend() waveforms[0].plot(); In\u00a0[8]: Copied! <pre>from obspy.signal.trigger import classic_sta_lta\nfrom obspy.signal.trigger import plot_trigger, trigger_onset\n</pre> from obspy.signal.trigger import classic_sta_lta from obspy.signal.trigger import plot_trigger, trigger_onset <p>Apply the classic_sta_lta function to the downloaded waveform</p> In\u00a0[9]: Copied! <pre>tr = waveforms[0]\ncft = classic_sta_lta(tr.data, int(5 * tr.stats.sampling_rate), int(10 * tr.stats.sampling_rate))\non_of = trigger_onset(cft, 1.5, 0.5)\n</pre> tr = waveforms[0] cft = classic_sta_lta(tr.data, int(5 * tr.stats.sampling_rate), int(10 * tr.stats.sampling_rate)) on_of = trigger_onset(cft, 1.5, 0.5) <p>Visualize the waveform and corresponding characteristic function of STA/LTA:</p> In\u00a0[10]: Copied! <pre>fig, axes = plt.subplots(2,1, figsize=(8,6))\naxes[0].plot(tr.data, \"k\")\nymin, ymax = axes[0].get_ylim()\nif len(on_of) &gt; 0:\n    axes[0].vlines(on_of[:, 0], ymin, ymax, color='r', linewidth=2)\n    axes[0].vlines(on_of[:, 1], ymin, ymax, color='b', linewidth=2)\naxes[1].plot(cft, 'k')\naxes[1].hlines([1.5, 0.5], 0, len(cft), color=['r', 'b'], linestyle='--')\nplt.show()\n</pre> fig, axes = plt.subplots(2,1, figsize=(8,6)) axes[0].plot(tr.data, \"k\") ymin, ymax = axes[0].get_ylim() if len(on_of) &gt; 0:     axes[0].vlines(on_of[:, 0], ymin, ymax, color='r', linewidth=2)     axes[0].vlines(on_of[:, 1], ymin, ymax, color='b', linewidth=2) axes[1].plot(cft, 'k') axes[1].hlines([1.5, 0.5], 0, len(cft), color=['r', 'b'], linestyle='--') plt.show() <p>We can call the PhaseNet API of the QuakeFlow project for this test.</p> In\u00a0[11]: Copied! <pre>import requests\nPHASENET_API_URL = \"http://phasenet.quakeflow.com\"\n</pre> import requests PHASENET_API_URL = \"http://phasenet.quakeflow.com\" <p>Convert the downloaded waveform into the required data format:</p> In\u00a0[12]: Copied! <pre>data = []\ndata.append(np.zeros_like(waveforms[0].data))\ndata.append(np.zeros_like(waveforms[0].data))\nfor trace in waveforms[0:1]:\n    data.append(trace.data)\ndata = np.array(data).T\n# print(data.shape)\n\ndata_id = waveforms[0].get_id()[:-1]\ntimestamp = waveforms[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n\nreq = {\"id\": [data_id],\n       \"timestamp\": [timestamp],\n       \"vec\": [data.tolist()]}\n</pre> data = [] data.append(np.zeros_like(waveforms[0].data)) data.append(np.zeros_like(waveforms[0].data)) for trace in waveforms[0:1]:     data.append(trace.data) data = np.array(data).T # print(data.shape)  data_id = waveforms[0].get_id()[:-1] timestamp = waveforms[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  req = {\"id\": [data_id],        \"timestamp\": [timestamp],        \"vec\": [data.tolist()]} <p>Get the prediction of PhaseNet and visualize the results:</p> In\u00a0[13]: Copied! <pre>resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req)\nprint(resp)\npicks, preds = resp.json() \npreds = np.array(preds)\nprint('Picks', picks)\n\nfig, axes = plt.subplots(2,1, figsize=(8,6))\naxes[0].plot(data[:,-1], 'k', label=\"Z\")\naxes[1].plot(preds[0, :, 0, 1], label=\"P\")\naxes[1].plot(preds[0, :, 0, 2], label=\"S\")\naxes[1].legend()\nplt.show();\n</pre> resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req) print(resp) picks, preds = resp.json()  preds = np.array(preds) print('Picks', picks)  fig, axes = plt.subplots(2,1, figsize=(8,6)) axes[0].plot(data[:,-1], 'k', label=\"Z\") axes[1].plot(preds[0, :, 0, 1], label=\"P\") axes[1].plot(preds[0, :, 0, 2], label=\"S\") axes[1].legend() plt.show(); <pre>&lt;Response [200]&gt;\nPicks [{'id': 'AM.R5E62.00.EH', 'timestamp': '2022-07-01T01:40:58.434', 'prob': 0.4440344572067261, 'amp': 5381.1312, 'type': 'p'}]\n</pre> In\u00a0[14]: Copied! <pre>client = Client(\"RASPISHAKE\")\n\nstarttime = UTCDateTime(\"2022-07-01 01:00:00\")\nendtime = UTCDateTime(\"2022-07-01 02:00:00\")\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R5E62\",\n                                 location=\"*\",\n                                 channel=\"EHZ\", \n                                 starttime=starttime, \n                                 endtime=endtime)  \nwaveforms = waveforms.detrend()\nwaveforms = waveforms.merge(fill_value=0)\nwaveforms[0].plot();\nwaveforms.write(\"one-hour.mseed\")\n</pre> client = Client(\"RASPISHAKE\")  starttime = UTCDateTime(\"2022-07-01 01:00:00\") endtime = UTCDateTime(\"2022-07-01 02:00:00\") waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R5E62\",                                  location=\"*\",                                  channel=\"EHZ\",                                   starttime=starttime,                                   endtime=endtime)   waveforms = waveforms.detrend() waveforms = waveforms.merge(fill_value=0) waveforms[0].plot(); waveforms.write(\"one-hour.mseed\") <pre>/Users/weiqiang/.local/miniconda3/lib/python3.8/site-packages/obspy/io/mseed/core.py:770: UserWarning: The encoding specified in trace.stats.mseed.encoding does not match the dtype of the data.\nA suitable encoding will be chosen.\n  warnings.warn(msg, UserWarning)\n</pre> <p>We save the mseed data to avoid repeatedly download the continuous waveform.</p> In\u00a0[15]: Copied! <pre>waveforms = obspy.read(\"one-hour.mseed\")\n</pre> waveforms = obspy.read(\"one-hour.mseed\") <ul> <li>First, we apply the STA/LTA method to find earthquakes in the one-hour waveform</li> </ul> In\u00a0[16]: Copied! <pre>tr = waveforms[0]\ncft = classic_sta_lta(tr.data, int(1 * tr.stats.sampling_rate), int(5 * tr.stats.sampling_rate))\non_of = trigger_onset(cft, 3.5, 1.0, max_len=1000)\nprint(f\"Number of detected earthquakes: {len(on_of)}\")\n</pre> tr = waveforms[0] cft = classic_sta_lta(tr.data, int(1 * tr.stats.sampling_rate), int(5 * tr.stats.sampling_rate)) on_of = trigger_onset(cft, 3.5, 1.0, max_len=1000) print(f\"Number of detected earthquakes: {len(on_of)}\") <pre>Number of detected earthquakes: 7\n</pre> <p>We can visualize the waveforms of these detected events.</p> In\u00a0[17]: Copied! <pre>for (t0, tn) in on_of:\n    t0 = t0 - 1000\n    tn = t0 + 7000\n    fig, axes = plt.subplots(2,1, figsize=(8,6), sharex=True)\n    axes[0].plot(tr.data[t0:tn], \"k\")\n    ymin, ymax = axes[0].get_ylim()\n    if len(on_of) &gt; 0:\n        axes[0].vlines(on_of[t0:tn, 0], ymin, ymax, color='r', linewidth=2)\n        axes[0].vlines(on_of[t0:tn, 1], ymin, ymax, color='b', linewidth=2)\n    axes[1].plot(cft[t0:tn], 'k')\n    axes[1].hlines([3.0, 1.0], 0, len(cft[t0:tn]), color=['r', 'b'], linestyle='--')\n    plt.show()\n    # break\n</pre> for (t0, tn) in on_of:     t0 = t0 - 1000     tn = t0 + 7000     fig, axes = plt.subplots(2,1, figsize=(8,6), sharex=True)     axes[0].plot(tr.data[t0:tn], \"k\")     ymin, ymax = axes[0].get_ylim()     if len(on_of) &gt; 0:         axes[0].vlines(on_of[t0:tn, 0], ymin, ymax, color='r', linewidth=2)         axes[0].vlines(on_of[t0:tn, 1], ymin, ymax, color='b', linewidth=2)     axes[1].plot(cft[t0:tn], 'k')     axes[1].hlines([3.0, 1.0], 0, len(cft[t0:tn]), color=['r', 'b'], linestyle='--')     plt.show()     # break <p>Q: Are these detected signals true earthquakes?</p> <ul> <li>Second, we apply PhaseNet to detect earthquakes of the same dataset</li> </ul> In\u00a0[18]: Copied! <pre>data = []\ndata.append(np.zeros_like(waveforms[0].data))\ndata.append(np.zeros_like(waveforms[0].data))\nfor trace in waveforms[0:1]:\n    data.append(trace.data)\ndata = np.array(data).T\ndata = data.astype(\"float32\")\n# print(data.shape)\n\nwindow_length = int(6000*2.5)\nfor i in range(0, len(data) - window_length, window_length):\n    data_id = waveforms[0].get_id()[:-1]\n    timestamp = (starttime + i/100.0).datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n\n    req = {\"id\": [data_id],\n           \"timestamp\": [timestamp],\n           \"vec\": [data[i:i+window_length].tolist()]}\n\n    resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req)\n    # print(resp)\n    picks, preds = resp.json() \n    preds = np.array(preds)\n    if len(picks) &gt; 0:\n        print('Picks', picks)\n        fig, axes = plt.subplots(2,1, figsize=(8,6))\n        axes[0].plot(data[i:i+window_length,-1], 'k', label=\"Z\")\n        axes[1].plot(preds[0, :, 0, 1], label=\"P\")\n        axes[1].plot(preds[0, :, 0, 2], label=\"S\")\n        axes[1].legend()\n        plt.show();\n    else:\n        print(f\"No event between {starttime+i/100.0} and {starttime+i/100.0+window_length/100.0}\")\n</pre> data = [] data.append(np.zeros_like(waveforms[0].data)) data.append(np.zeros_like(waveforms[0].data)) for trace in waveforms[0:1]:     data.append(trace.data) data = np.array(data).T data = data.astype(\"float32\") # print(data.shape)  window_length = int(6000*2.5) for i in range(0, len(data) - window_length, window_length):     data_id = waveforms[0].get_id()[:-1]     timestamp = (starttime + i/100.0).datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]      req = {\"id\": [data_id],            \"timestamp\": [timestamp],            \"vec\": [data[i:i+window_length].tolist()]}      resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req)     # print(resp)     picks, preds = resp.json()      preds = np.array(preds)     if len(picks) &gt; 0:         print('Picks', picks)         fig, axes = plt.subplots(2,1, figsize=(8,6))         axes[0].plot(data[i:i+window_length,-1], 'k', label=\"Z\")         axes[1].plot(preds[0, :, 0, 1], label=\"P\")         axes[1].plot(preds[0, :, 0, 2], label=\"S\")         axes[1].legend()         plt.show();     else:         print(f\"No event between {starttime+i/100.0} and {starttime+i/100.0+window_length/100.0}\") <pre>No event between 2022-07-01T01:00:00.000000Z and 2022-07-01T01:02:30.000000Z\nNo event between 2022-07-01T01:02:30.000000Z and 2022-07-01T01:05:00.000000Z\nNo event between 2022-07-01T01:05:00.000000Z and 2022-07-01T01:07:30.000000Z\nNo event between 2022-07-01T01:07:30.000000Z and 2022-07-01T01:10:00.000000Z\nNo event between 2022-07-01T01:10:00.000000Z and 2022-07-01T01:12:30.000000Z\nNo event between 2022-07-01T01:12:30.000000Z and 2022-07-01T01:15:00.000000Z\nNo event between 2022-07-01T01:15:00.000000Z and 2022-07-01T01:17:30.000000Z\nNo event between 2022-07-01T01:17:30.000000Z and 2022-07-01T01:20:00.000000Z\nNo event between 2022-07-01T01:20:00.000000Z and 2022-07-01T01:22:30.000000Z\nNo event between 2022-07-01T01:22:30.000000Z and 2022-07-01T01:25:00.000000Z\nNo event between 2022-07-01T01:25:00.000000Z and 2022-07-01T01:27:30.000000Z\nNo event between 2022-07-01T01:27:30.000000Z and 2022-07-01T01:30:00.000000Z\nNo event between 2022-07-01T01:30:00.000000Z and 2022-07-01T01:32:30.000000Z\nNo event between 2022-07-01T01:32:30.000000Z and 2022-07-01T01:35:00.000000Z\nNo event between 2022-07-01T01:35:00.000000Z and 2022-07-01T01:37:30.000000Z\nNo event between 2022-07-01T01:37:30.000000Z and 2022-07-01T01:40:00.000000Z\nPicks [{'id': 'AM.R5E62.00.EH', 'timestamp': '2022-07-01T01:40:58.400', 'prob': 0.6126121878623962, 'amp': 5280.05078125, 'type': 'p'}]\n</pre> <pre>No event between 2022-07-01T01:42:30.000000Z and 2022-07-01T01:45:00.000000Z\nNo event between 2022-07-01T01:45:00.000000Z and 2022-07-01T01:47:30.000000Z\nNo event between 2022-07-01T01:47:30.000000Z and 2022-07-01T01:50:00.000000Z\nNo event between 2022-07-01T01:50:00.000000Z and 2022-07-01T01:52:30.000000Z\nNo event between 2022-07-01T01:52:30.000000Z and 2022-07-01T01:55:00.000000Z\nNo event between 2022-07-01T01:55:00.000000Z and 2022-07-01T01:57:30.000000Z\nNo event between 2022-07-01T01:57:30.000000Z and 2022-07-01T02:00:00.000000Z\n</pre> <p>Q1: Is the detected event information same as the first example above?</p> <p>Q2: How do PhaseNet results compared with STA/LTA?</p> In\u00a0[19]: Copied! <pre>starttime = UTCDateTime(\"2022-07-11 01:00:00\")\nendtime = UTCDateTime(\"2022-07-11 05:00:00\")\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R4ABA\",\n                                 location=\"*\",\n                                 channel=\"EHZ\", \n                                 starttime=starttime, \n                                 endtime=endtime)  \nwaveforms = waveforms.detrend()\nwaveforms = waveforms.merge(fill_value=0)\nwaveforms[0].plot();\nwaveforms.write(\"one-hour.mseed\")\n</pre> starttime = UTCDateTime(\"2022-07-11 01:00:00\") endtime = UTCDateTime(\"2022-07-11 05:00:00\") waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R4ABA\",                                  location=\"*\",                                  channel=\"EHZ\",                                   starttime=starttime,                                   endtime=endtime)   waveforms = waveforms.detrend() waveforms = waveforms.merge(fill_value=0) waveforms[0].plot(); waveforms.write(\"one-hour.mseed\") <pre>/Users/weiqiang/.local/miniconda3/lib/python3.8/site-packages/obspy/io/mseed/core.py:770: UserWarning: The encoding specified in trace.stats.mseed.encoding does not match the dtype of the data.\nA suitable encoding will be chosen.\n  warnings.warn(msg, UserWarning)\n</pre> In\u00a0[20]: Copied! <pre>data = []\ndata.append(np.zeros_like(waveforms[0].data))\ndata.append(np.zeros_like(waveforms[0].data))\nfor trace in waveforms[0:1]:\n    data.append(trace.data)\ndata = np.array(data).T\ndata = data.astype(\"float32\")\n# print(data.shape)\n\nwindow_length = int(6000*2.5)\nfor i in range(0, len(data) - window_length, window_length):\n    data_id = waveforms[0].get_id()[:-1]\n    timestamp = (starttime + i/100.0).datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n\n    req = {\"id\": [data_id],\n           \"timestamp\": [timestamp],\n           \"vec\": [data[i:i+window_length].tolist()]}\n\n    resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req)\n    # print(resp)\n    picks, preds = resp.json() \n    preds = np.array(preds)\n    if len(picks) &gt; 0:\n        print('Picks', picks)\n        fig, axes = plt.subplots(2,1, figsize=(8,6))\n        axes[0].plot(data[i:i+window_length,-1], 'k', label=\"Z\")\n        axes[1].plot(preds[0, :, 0, 1], label=\"P\")\n        axes[1].plot(preds[0, :, 0, 2], label=\"S\")\n        axes[1].legend()\n        plt.show();\n    else:\n        print(f\"No event between {starttime+i/100.0} and {starttime+i/100.0+window_length/100.0}\")\n</pre> data = [] data.append(np.zeros_like(waveforms[0].data)) data.append(np.zeros_like(waveforms[0].data)) for trace in waveforms[0:1]:     data.append(trace.data) data = np.array(data).T data = data.astype(\"float32\") # print(data.shape)  window_length = int(6000*2.5) for i in range(0, len(data) - window_length, window_length):     data_id = waveforms[0].get_id()[:-1]     timestamp = (starttime + i/100.0).datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]      req = {\"id\": [data_id],            \"timestamp\": [timestamp],            \"vec\": [data[i:i+window_length].tolist()]}      resp = requests.post(f'{PHASENET_API_URL}/predict_prob', json=req)     # print(resp)     picks, preds = resp.json()      preds = np.array(preds)     if len(picks) &gt; 0:         print('Picks', picks)         fig, axes = plt.subplots(2,1, figsize=(8,6))         axes[0].plot(data[i:i+window_length,-1], 'k', label=\"Z\")         axes[1].plot(preds[0, :, 0, 1], label=\"P\")         axes[1].plot(preds[0, :, 0, 2], label=\"S\")         axes[1].legend()         plt.show();     else:         print(f\"No event between {starttime+i/100.0} and {starttime+i/100.0+window_length/100.0}\") <pre>No event between 2022-07-11T01:00:00.000000Z and 2022-07-11T01:02:30.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T01:03:19.440', 'prob': 0.5183876752853394, 'amp': 520.11572265625, 'type': 'p'}]\n</pre> <pre>No event between 2022-07-11T01:05:00.000000Z and 2022-07-11T01:07:30.000000Z\nNo event between 2022-07-11T01:07:30.000000Z and 2022-07-11T01:10:00.000000Z\nNo event between 2022-07-11T01:10:00.000000Z and 2022-07-11T01:12:30.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T01:14:14.760', 'prob': 0.5133991837501526, 'amp': 2834.1376953125, 'type': 'p'}, {'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T01:14:20.230', 'prob': 0.3006688356399536, 'amp': 2575.741455078125, 'type': 's'}]\n</pre> <pre>No event between 2022-07-11T01:15:00.000000Z and 2022-07-11T01:17:30.000000Z\nNo event between 2022-07-11T01:17:30.000000Z and 2022-07-11T01:20:00.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T01:20:11.600', 'prob': 0.7828038930892944, 'amp': 1446.291748046875, 'type': 'p'}, {'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T01:20:17.280', 'prob': 0.6105919480323792, 'amp': 977.4058837890625, 'type': 's'}]\n</pre> <pre>No event between 2022-07-11T01:22:30.000000Z and 2022-07-11T01:25:00.000000Z\nNo event between 2022-07-11T01:25:00.000000Z and 2022-07-11T01:27:30.000000Z\nNo event between 2022-07-11T01:27:30.000000Z and 2022-07-11T01:30:00.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T01:30:31.600', 'prob': 0.5890001058578491, 'amp': 8428.8212890625, 'type': 'p'}]\n</pre> <pre>No event between 2022-07-11T01:32:30.000000Z and 2022-07-11T01:35:00.000000Z\nNo event between 2022-07-11T01:35:00.000000Z and 2022-07-11T01:37:30.000000Z\nNo event between 2022-07-11T01:37:30.000000Z and 2022-07-11T01:40:00.000000Z\nNo event between 2022-07-11T01:40:00.000000Z and 2022-07-11T01:42:30.000000Z\nNo event between 2022-07-11T01:42:30.000000Z and 2022-07-11T01:45:00.000000Z\nNo event between 2022-07-11T01:45:00.000000Z and 2022-07-11T01:47:30.000000Z\nNo event between 2022-07-11T01:47:30.000000Z and 2022-07-11T01:50:00.000000Z\nNo event between 2022-07-11T01:50:00.000000Z and 2022-07-11T01:52:30.000000Z\nNo event between 2022-07-11T01:52:30.000000Z and 2022-07-11T01:55:00.000000Z\nNo event between 2022-07-11T01:55:00.000000Z and 2022-07-11T01:57:30.000000Z\nNo event between 2022-07-11T01:57:30.000000Z and 2022-07-11T02:00:00.000000Z\nNo event between 2022-07-11T02:00:00.000000Z and 2022-07-11T02:02:30.000000Z\nNo event between 2022-07-11T02:02:30.000000Z and 2022-07-11T02:05:00.000000Z\nNo event between 2022-07-11T02:05:00.000000Z and 2022-07-11T02:07:30.000000Z\nNo event between 2022-07-11T02:07:30.000000Z and 2022-07-11T02:10:00.000000Z\nNo event between 2022-07-11T02:10:00.000000Z and 2022-07-11T02:12:30.000000Z\nNo event between 2022-07-11T02:12:30.000000Z and 2022-07-11T02:15:00.000000Z\nNo event between 2022-07-11T02:15:00.000000Z and 2022-07-11T02:17:30.000000Z\nNo event between 2022-07-11T02:17:30.000000Z and 2022-07-11T02:20:00.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T02:20:36.640', 'prob': 0.5117528438568115, 'amp': 1441.9178466796875, 'type': 'p'}]\n</pre> <pre>No event between 2022-07-11T02:22:30.000000Z and 2022-07-11T02:25:00.000000Z\nNo event between 2022-07-11T02:25:00.000000Z and 2022-07-11T02:27:30.000000Z\nNo event between 2022-07-11T02:27:30.000000Z and 2022-07-11T02:30:00.000000Z\nNo event between 2022-07-11T02:30:00.000000Z and 2022-07-11T02:32:30.000000Z\nNo event between 2022-07-11T02:32:30.000000Z and 2022-07-11T02:35:00.000000Z\nNo event between 2022-07-11T02:35:00.000000Z and 2022-07-11T02:37:30.000000Z\nNo event between 2022-07-11T02:37:30.000000Z and 2022-07-11T02:40:00.000000Z\nNo event between 2022-07-11T02:40:00.000000Z and 2022-07-11T02:42:30.000000Z\nNo event between 2022-07-11T02:42:30.000000Z and 2022-07-11T02:45:00.000000Z\nNo event between 2022-07-11T02:45:00.000000Z and 2022-07-11T02:47:30.000000Z\nNo event between 2022-07-11T02:47:30.000000Z and 2022-07-11T02:50:00.000000Z\nNo event between 2022-07-11T02:50:00.000000Z and 2022-07-11T02:52:30.000000Z\nNo event between 2022-07-11T02:52:30.000000Z and 2022-07-11T02:55:00.000000Z\nNo event between 2022-07-11T02:55:00.000000Z and 2022-07-11T02:57:30.000000Z\nNo event between 2022-07-11T02:57:30.000000Z and 2022-07-11T03:00:00.000000Z\nNo event between 2022-07-11T03:00:00.000000Z and 2022-07-11T03:02:30.000000Z\nNo event between 2022-07-11T03:02:30.000000Z and 2022-07-11T03:05:00.000000Z\nNo event between 2022-07-11T03:05:00.000000Z and 2022-07-11T03:07:30.000000Z\nNo event between 2022-07-11T03:07:30.000000Z and 2022-07-11T03:10:00.000000Z\nNo event between 2022-07-11T03:10:00.000000Z and 2022-07-11T03:12:30.000000Z\nNo event between 2022-07-11T03:12:30.000000Z and 2022-07-11T03:15:00.000000Z\nNo event between 2022-07-11T03:15:00.000000Z and 2022-07-11T03:17:30.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T03:17:57.590', 'prob': 0.3074922263622284, 'amp': 2477.107421875, 'type': 's'}]\n</pre> <pre>No event between 2022-07-11T03:20:00.000000Z and 2022-07-11T03:22:30.000000Z\nNo event between 2022-07-11T03:22:30.000000Z and 2022-07-11T03:25:00.000000Z\nNo event between 2022-07-11T03:25:00.000000Z and 2022-07-11T03:27:30.000000Z\nNo event between 2022-07-11T03:27:30.000000Z and 2022-07-11T03:30:00.000000Z\nNo event between 2022-07-11T03:30:00.000000Z and 2022-07-11T03:32:30.000000Z\nNo event between 2022-07-11T03:32:30.000000Z and 2022-07-11T03:35:00.000000Z\nNo event between 2022-07-11T03:35:00.000000Z and 2022-07-11T03:37:30.000000Z\nNo event between 2022-07-11T03:37:30.000000Z and 2022-07-11T03:40:00.000000Z\nNo event between 2022-07-11T03:40:00.000000Z and 2022-07-11T03:42:30.000000Z\nNo event between 2022-07-11T03:42:30.000000Z and 2022-07-11T03:45:00.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T03:45:49.580', 'prob': 0.305012047290802, 'amp': 2783.478759765625, 'type': 'p'}]\n</pre> <pre>No event between 2022-07-11T03:47:30.000000Z and 2022-07-11T03:50:00.000000Z\nPicks [{'id': 'AM.R4ABA.00.EH', 'timestamp': '2022-07-11T03:51:31.590', 'prob': 0.4973580539226532, 'amp': 1238.3836669921875, 'type': 's'}]\n</pre> <pre>No event between 2022-07-11T03:52:30.000000Z and 2022-07-11T03:55:00.000000Z\nNo event between 2022-07-11T03:55:00.000000Z and 2022-07-11T03:57:30.000000Z\nNo event between 2022-07-11T03:57:30.000000Z and 2022-07-11T04:00:00.000000Z\nNo event between 2022-07-11T04:00:00.000000Z and 2022-07-11T04:02:30.000000Z\nNo event between 2022-07-11T04:02:30.000000Z and 2022-07-11T04:05:00.000000Z\nNo event between 2022-07-11T04:05:00.000000Z and 2022-07-11T04:07:30.000000Z\nNo event between 2022-07-11T04:07:30.000000Z and 2022-07-11T04:10:00.000000Z\nNo event between 2022-07-11T04:10:00.000000Z and 2022-07-11T04:12:30.000000Z\nNo event between 2022-07-11T04:12:30.000000Z and 2022-07-11T04:15:00.000000Z\nNo event between 2022-07-11T04:15:00.000000Z and 2022-07-11T04:17:30.000000Z\nNo event between 2022-07-11T04:17:30.000000Z and 2022-07-11T04:20:00.000000Z\nNo event between 2022-07-11T04:20:00.000000Z and 2022-07-11T04:22:30.000000Z\nNo event between 2022-07-11T04:22:30.000000Z and 2022-07-11T04:25:00.000000Z\nNo event between 2022-07-11T04:25:00.000000Z and 2022-07-11T04:27:30.000000Z\nNo event between 2022-07-11T04:27:30.000000Z and 2022-07-11T04:30:00.000000Z\nNo event between 2022-07-11T04:30:00.000000Z and 2022-07-11T04:32:30.000000Z\nNo event between 2022-07-11T04:32:30.000000Z and 2022-07-11T04:35:00.000000Z\nNo event between 2022-07-11T04:35:00.000000Z and 2022-07-11T04:37:30.000000Z\nNo event between 2022-07-11T04:37:30.000000Z and 2022-07-11T04:40:00.000000Z\nNo event between 2022-07-11T04:40:00.000000Z and 2022-07-11T04:42:30.000000Z\nNo event between 2022-07-11T04:42:30.000000Z and 2022-07-11T04:45:00.000000Z\nNo event between 2022-07-11T04:45:00.000000Z and 2022-07-11T04:47:30.000000Z\nNo event between 2022-07-11T04:47:30.000000Z and 2022-07-11T04:50:00.000000Z\nNo event between 2022-07-11T04:50:00.000000Z and 2022-07-11T04:52:30.000000Z\nNo event between 2022-07-11T04:52:30.000000Z and 2022-07-11T04:55:00.000000Z\nNo event between 2022-07-11T04:55:00.000000Z and 2022-07-11T04:57:30.000000Z\nNo event between 2022-07-11T04:57:30.000000Z and 2022-07-11T05:00:00.000000Z\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/earthquake/detection/#earthquake-detection","title":"Earthquake Detection\u00b6","text":"<p>In the part, we learn how to detect earthquakes from seismic waveforms.</p>"},{"location":"eps207-observational-seismology/earthquake/detection/#download-waveform-using-obpsy","title":"Download waveform using Obpsy\u00b6","text":"<p>First, we download some seismic waveforms of known earthquakes.</p> <p>You can find earthquake information from USGS More information can be reviewed from the Obspy section.</p>"},{"location":"eps207-observational-seismology/earthquake/detection/#detect-earthquakes-using-stalta","title":"Detect earthquakes using STA/LTA\u00b6","text":"<p>Detailed information can be found in the Trigger/Picker Tutorial of Obspy</p>"},{"location":"eps207-observational-seismology/earthquake/detection/#detect-earthquakes-using-phasenet","title":"Detect earthquakes using PhaseNet\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/detection/#detecting-earthquakes-from-continuous-data","title":"Detecting earthquakes from continuous data\u00b6","text":"<p>In the above examples, the earthquakes are detected by USGS, so we can get the a short window of waveforms based on the detected earthquake origin time.</p> <p>In this example, we will detect earthquake signals from raw continuous data and see if we can also find these earthquakes without the USGS catalog.</p> <p>For example, we can download one days' data using Obspy and detect the earthquakes in it.</p>"},{"location":"eps207-observational-seismology/earthquake/detection/#detecting-volcanic-earthquakes","title":"Detecting volcanic earthquakes\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/focal_mechanism/","title":"Focal mechanism","text":"In\u00a0[1]: Copied! <pre>from torch import nn\nimport torch.nn.functional as F\nimport torch\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom dataclasses import dataclass\nimport copy\n</pre> from torch import nn import torch.nn.functional as F import torch import itertools import numpy as np import pandas as pd import matplotlib.pyplot as plt import torch.optim as optim from tqdm import tqdm from dataclasses import dataclass import copy <pre>/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>class FocalMechanism(nn.Module):\n    \"\"\"\n    get radiation pattern given focal mechanism, take-off angle and azimuth angle\n    reference: Aki and Richards-2002-p.108-110\n    Args:\n        fm: focal mechanism, [strike, dip, rake] in degree\n        takeoff: take-off angle in degree, zero points to downward, np.array\n        azimuth: azimuth angle in degree, zero points to north, np.array\n        type: 'P' or 'SH' or 'SV'\n    \"\"\"\n    def __init__(self, strike = [0], dip = [0], rake = [0], scale=False, dtype=torch.float32):\n        super().__init__()\n        self.num_event = len(strike)\n        self.strike = nn.Parameter(torch.tensor(strike, dtype=dtype).unsqueeze(-1))\n        self.dip = nn.Parameter(torch.tensor(dip, dtype=dtype).unsqueeze(-1))\n        self.rake = nn.Parameter(torch.tensor(rake, dtype=dtype).unsqueeze(-1))\n        self.scale = scale\n        if self.scale:\n            self.w = nn.Parameter(torch.zeros(self.num_event, dtype=dtype).unsqueeze(-1))\n            # self.w = nn.Parameter(torch.ones(self.num_event, dtype=dtype).unsqueeze(-1))\n    \n    def forward(self, takeoff, azimuth, phase):\n\n        inc = torch.deg2rad(takeoff)\n        azi = torch.deg2rad(azimuth)\n        strike = torch.deg2rad(self.strike)\n        dip = torch.deg2rad(self.dip)\n        rake = torch.deg2rad(self.rake)\n\n        si = torch.sin(inc)\n        ci = torch.cos(inc)\n        s2i = torch.sin(2 * inc)\n        c2i = torch.cos(2 * inc)\n\n        sd = torch.sin(dip)\n        cd = torch.cos(dip)\n        s2d = torch.sin(2 * dip)\n        c2d = torch.cos(2 * dip)\n        sr = torch.sin(rake)\n        cr = torch.cos(rake)\n        sas = torch.sin(azi - strike)\n        cas = torch.cos(azi - strike)\n\n        s2as = 2 * sas * cas\n        c2as = cas**2 - sas**2\n\n        polarity_p = (\n            -cas * cd * cr * s2i\n            + cr * s2as * sd * si**2\n            + c2d * s2i * sas * sr\n            + s2d * (ci**2 + (-1) * sas**2 * si**2) * sr\n        )\n        polarity_sv = (\n            -c2i * cas * cd * cr\n            + (1 / 2) * cr * s2as * s2i * sd\n            + c2d * c2i * sas * sr\n            + (-1 / 2) * s2d * s2i * (1 + sas**2) * sr\n        )\n        polarity = torch.stack([polarity_p, polarity_sv], dim=-1)\n        # polarity_sh = (\n        #     cd * ci * cr * sas\n        #     + c2as * cr * sd * si\n        #     + c2d * cas * ci * sr\n        #     + (-1 / 2) * s2as * s2d * si * sr\n        # )\n        # polarity = torch.stack([polarity_p, polarity_sv, polarity_sh], dim=-1)\n        polarity = torch.sum(polarity * phase, dim=(-1))\n        if self.scale and self.training:\n            # polarity *= (torch.abs(self.w) + 1.0)\n            # polarity *= (torch.exp(-self.w) + 1.0)\n            # polarity *= (F.softplus(self.w) + 1.0)\n            polarity *= (F.elu(self.w) + 2.0)\n\n        return polarity\n</pre> class FocalMechanism(nn.Module):     \"\"\"     get radiation pattern given focal mechanism, take-off angle and azimuth angle     reference: Aki and Richards-2002-p.108-110     Args:         fm: focal mechanism, [strike, dip, rake] in degree         takeoff: take-off angle in degree, zero points to downward, np.array         azimuth: azimuth angle in degree, zero points to north, np.array         type: 'P' or 'SH' or 'SV'     \"\"\"     def __init__(self, strike = [0], dip = [0], rake = [0], scale=False, dtype=torch.float32):         super().__init__()         self.num_event = len(strike)         self.strike = nn.Parameter(torch.tensor(strike, dtype=dtype).unsqueeze(-1))         self.dip = nn.Parameter(torch.tensor(dip, dtype=dtype).unsqueeze(-1))         self.rake = nn.Parameter(torch.tensor(rake, dtype=dtype).unsqueeze(-1))         self.scale = scale         if self.scale:             self.w = nn.Parameter(torch.zeros(self.num_event, dtype=dtype).unsqueeze(-1))             # self.w = nn.Parameter(torch.ones(self.num_event, dtype=dtype).unsqueeze(-1))          def forward(self, takeoff, azimuth, phase):          inc = torch.deg2rad(takeoff)         azi = torch.deg2rad(azimuth)         strike = torch.deg2rad(self.strike)         dip = torch.deg2rad(self.dip)         rake = torch.deg2rad(self.rake)          si = torch.sin(inc)         ci = torch.cos(inc)         s2i = torch.sin(2 * inc)         c2i = torch.cos(2 * inc)          sd = torch.sin(dip)         cd = torch.cos(dip)         s2d = torch.sin(2 * dip)         c2d = torch.cos(2 * dip)         sr = torch.sin(rake)         cr = torch.cos(rake)         sas = torch.sin(azi - strike)         cas = torch.cos(azi - strike)          s2as = 2 * sas * cas         c2as = cas**2 - sas**2          polarity_p = (             -cas * cd * cr * s2i             + cr * s2as * sd * si**2             + c2d * s2i * sas * sr             + s2d * (ci**2 + (-1) * sas**2 * si**2) * sr         )         polarity_sv = (             -c2i * cas * cd * cr             + (1 / 2) * cr * s2as * s2i * sd             + c2d * c2i * sas * sr             + (-1 / 2) * s2d * s2i * (1 + sas**2) * sr         )         polarity = torch.stack([polarity_p, polarity_sv], dim=-1)         # polarity_sh = (         #     cd * ci * cr * sas         #     + c2as * cr * sd * si         #     + c2d * cas * ci * sr         #     + (-1 / 2) * s2as * s2d * si * sr         # )         # polarity = torch.stack([polarity_p, polarity_sv, polarity_sh], dim=-1)         polarity = torch.sum(polarity * phase, dim=(-1))         if self.scale and self.training:             # polarity *= (torch.abs(self.w) + 1.0)             # polarity *= (torch.exp(-self.w) + 1.0)             # polarity *= (F.softplus(self.w) + 1.0)             polarity *= (F.elu(self.w) + 2.0)          return polarity In\u00a0[3]: Copied! <pre>def generate_data(strike, dip, rake):\n    with torch.no_grad():\n        model = FocalMechanism(strike=strike, dip=dip, rake=rake, scale=False)\n        takeoff_angles = []\n        azimuths = []\n        phase_types = []\n        for event in range(len(strike)):\n            takeoff_angle = torch.from_numpy(np.linspace(0, 180, 50))\n            azimuth = torch.from_numpy(np.linspace(0, 360, 50))\n            takeoff_angle, azimuth = torch.meshgrid(takeoff_angle, azimuth, indexing=\"ij\")\n            takeoff_angle = takeoff_angle.flatten()\n            azimuth = azimuth.flatten()\n            phase_type = torch.zeros([len(takeoff_angle), 2], dtype=torch.float32)\n            phase_type[:, 0] = 1\n            \n            takeoff_angles.append(takeoff_angle)\n            azimuths.append(azimuth)\n            phase_types.append(phase_type)\n\n        takeoff_angle = torch.stack(takeoff_angles, dim=0)\n        azimuth = torch.stack(azimuths, dim=0)\n        phase_type = torch.stack(phase_types, dim=0)\n        polarity = model(takeoff_angle, azimuth, phase=phase_type)\n        polarity = torch.sign(polarity)\n\n    data = {\"phase_polarity\": polarity, \"phase_type\": phase_type, \"takeoff_angle\": takeoff_angle, \"azimuth\": azimuth}\n    return data\n</pre> def generate_data(strike, dip, rake):     with torch.no_grad():         model = FocalMechanism(strike=strike, dip=dip, rake=rake, scale=False)         takeoff_angles = []         azimuths = []         phase_types = []         for event in range(len(strike)):             takeoff_angle = torch.from_numpy(np.linspace(0, 180, 50))             azimuth = torch.from_numpy(np.linspace(0, 360, 50))             takeoff_angle, azimuth = torch.meshgrid(takeoff_angle, azimuth, indexing=\"ij\")             takeoff_angle = takeoff_angle.flatten()             azimuth = azimuth.flatten()             phase_type = torch.zeros([len(takeoff_angle), 2], dtype=torch.float32)             phase_type[:, 0] = 1                          takeoff_angles.append(takeoff_angle)             azimuths.append(azimuth)             phase_types.append(phase_type)          takeoff_angle = torch.stack(takeoff_angles, dim=0)         azimuth = torch.stack(azimuths, dim=0)         phase_type = torch.stack(phase_types, dim=0)         polarity = model(takeoff_angle, azimuth, phase=phase_type)         polarity = torch.sign(polarity)      data = {\"phase_polarity\": polarity, \"phase_type\": phase_type, \"takeoff_angle\": takeoff_angle, \"azimuth\": azimuth}     return data In\u00a0[4]: Copied! <pre>strike = [45, 30]\ndip = [90, 80]\nrake = [0, 60]\ndata = generate_data(strike, dip, rake)\n</pre> strike = [45, 30] dip = [90, 80] rake = [0, 60] data = generate_data(strike, dip, rake) In\u00a0[5]: Copied! <pre>event_index = 0\nx = torch.cos(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index]))\ny = torch.sin(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index]))\nc = data[\"phase_polarity\"][event_index]\nfig, ax = plt.subplots(1, 1, squeeze=False, figsize=(5, 5))    \nax[0, 0].scatter(x, y, s=100, c = c, cmap=\"RdBu\")\nax[0, 0].set_title(\"Ground Truth\")\nplt.savefig(\"data.png\")\n</pre> event_index = 0 x = torch.cos(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index])) y = torch.sin(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index])) c = data[\"phase_polarity\"][event_index] fig, ax = plt.subplots(1, 1, squeeze=False, figsize=(5, 5))     ax[0, 0].scatter(x, y, s=100, c = c, cmap=\"RdBu\") ax[0, 0].set_title(\"Ground Truth\") plt.savefig(\"data.png\") In\u00a0[6]: Copied! <pre>def solve_bfgs(data, model, args=None):\n\n    takeoff_angle = data[\"takeoff_angle\"]\n    azimuth = data[\"azimuth\"]\n    phase_type = data[\"phase_type\"]\n    polarity = data[\"phase_polarity\"]\n    gamma = args.gamma\n\n    optimizer = optim.LBFGS(params=model.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")\n\n    model.train()\n\n    def closure():\n        optimizer.zero_grad()\n        output = model(takeoff_angle, azimuth, phase_type)\n        # loss = F.mse_loss(output, polarity)\n        # loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(torch.abs(model.w))\n        loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(model.w**2)\n        # loss = torch.mean(torch.relu(1.0 - polarity * output)**2) + gamma*torch.mean(model.w**2)\n        loss.backward()\n        return loss\n\n    optimizer.step(closure)\n\n    return model\n</pre> def solve_bfgs(data, model, args=None):      takeoff_angle = data[\"takeoff_angle\"]     azimuth = data[\"azimuth\"]     phase_type = data[\"phase_type\"]     polarity = data[\"phase_polarity\"]     gamma = args.gamma      optimizer = optim.LBFGS(params=model.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")      model.train()      def closure():         optimizer.zero_grad()         output = model(takeoff_angle, azimuth, phase_type)         # loss = F.mse_loss(output, polarity)         # loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(torch.abs(model.w))         loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(model.w**2)         # loss = torch.mean(torch.relu(1.0 - polarity * output)**2) + gamma*torch.mean(model.w**2)         loss.backward()         return loss      optimizer.step(closure)      return model In\u00a0[7]: Copied! <pre>def solve_sgd(data, model, args=None):\n\n    takeoff_angle = data[\"takeoff_angle\"]\n    azimuth = data[\"azimuth\"]\n    phase_type = data[\"phase_type\"]\n    polarity = data[\"phase_polarity\"]\n    gamma = args.gamma\n\n    # optimizer = optim.Adam(model.parameters(), lr=args.lr)\n    # optimizer = optim.Adagrad(model.parameters(), lr=args.lr)\n    optimizer = optim.RMSprop(model.parameters(), lr=args.lr)\n\n    model.train()\n    min_loss = 1e10\n    pbar = tqdm(range(args.epoch), desc=\"Training\")\n    for epoch in pbar:\n        output = model(takeoff_angle, azimuth, phase_type)\n        # loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(torch.abs(model.w))\n        loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(model.w**2)\n        if loss &lt; min_loss:\n            min_loss = loss\n            best_model = copy.deepcopy(model)\n        loss.backward()\n        optimizer.step()\n        pbar.set_postfix(loss=loss.item())\n    \n    return best_model\n    \n</pre> def solve_sgd(data, model, args=None):      takeoff_angle = data[\"takeoff_angle\"]     azimuth = data[\"azimuth\"]     phase_type = data[\"phase_type\"]     polarity = data[\"phase_polarity\"]     gamma = args.gamma      # optimizer = optim.Adam(model.parameters(), lr=args.lr)     # optimizer = optim.Adagrad(model.parameters(), lr=args.lr)     optimizer = optim.RMSprop(model.parameters(), lr=args.lr)      model.train()     min_loss = 1e10     pbar = tqdm(range(args.epoch), desc=\"Training\")     for epoch in pbar:         output = model(takeoff_angle, azimuth, phase_type)         # loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(torch.abs(model.w))         loss = torch.mean(torch.relu(1.0 - polarity * output)) + gamma*torch.mean(model.w**2)         if loss &lt; min_loss:             min_loss = loss             best_model = copy.deepcopy(model)         loss.backward()         optimizer.step()         pbar.set_postfix(loss=loss.item())          return best_model      In\u00a0[8]: Copied! <pre>strike0 = [0.1, 0.1]\ndip0 = [0.1, 0.1]\nrake0 = [0.1, 0.1]\nnum_events = len(strike0)\nmodel = FocalMechanism(strike=strike0, dip=dip0, rake=rake0, scale=True)\n@dataclass\nclass Args:\n    epoch: int = 1000\n    lr: float = 1.0\n    gamma: float = 0.00001\nargs = Args()\nmodel = solve_bfgs(data, model, args)\n# model = solve_sgd(data, model, args)\nfor i in range(num_events):\n    print(f\"strike: {model.strike[i].item():.1f} dip: {model.dip[i].item():.1f}, rake: {model.rake[i].item():.1f}, w: {model.w[i].item():.6f}\")\n</pre> strike0 = [0.1, 0.1] dip0 = [0.1, 0.1] rake0 = [0.1, 0.1] num_events = len(strike0) model = FocalMechanism(strike=strike0, dip=dip0, rake=rake0, scale=True) @dataclass class Args:     epoch: int = 1000     lr: float = 1.0     gamma: float = 0.00001 args = Args() model = solve_bfgs(data, model, args) # model = solve_sgd(data, model, args) for i in range(num_events):     print(f\"strike: {model.strike[i].item():.1f} dip: {model.dip[i].item():.1f}, rake: {model.rake[i].item():.1f}, w: {model.w[i].item():.6f}\") <pre>strike: 45.0 dip: 90.0, rake: 0.0, w: 49.522991\nstrike: 103.3 dip: -31.2, rake: -19.3, w: 32.632763\n</pre> In\u00a0[9]: Copied! <pre>pred = model(data[\"takeoff_angle\"], data[\"azimuth\"], data[\"phase_type\"]).detach().numpy()\nloss = torch.relu(1.0 - data['phase_polarity'] * pred)\nloss = torch.clamp(1 - data['phase_polarity'] * pred, min=0)\nprint(f\"Supports: {torch.count_nonzero(loss, dim=-1)}\")\nprint(f\"Zeros: {torch.count_nonzero(loss == 0, dim=-1)}\")\nprint(f\"Loss: {torch.mean(loss)}\")\n</pre> pred = model(data[\"takeoff_angle\"], data[\"azimuth\"], data[\"phase_type\"]).detach().numpy() loss = torch.relu(1.0 - data['phase_polarity'] * pred) loss = torch.clamp(1 - data['phase_polarity'] * pred, min=0) print(f\"Supports: {torch.count_nonzero(loss, dim=-1)}\") print(f\"Zeros: {torch.count_nonzero(loss == 0, dim=-1)}\") print(f\"Loss: {torch.mean(loss)}\") <pre>Supports: tensor([409, 128])\nZeros: tensor([2091, 2372])\nLoss: 0.07035889619321717\n</pre> In\u00a0[10]: Copied! <pre>for event_index in range(num_events):\n    x = torch.cos(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index]))\n    y = torch.sin(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index]))\n    c = data[\"phase_polarity\"][event_index]\n    fig, ax = plt.subplots(1, 2, squeeze=False, figsize=(10, 5))    \n    ax[0, 0].scatter(x, y, s=100, c = c, cmap=\"RdBu\")\n    ax[0, 0].set_title(\"Ground Truth\")\n    ax[0, 1].scatter(x, y, s=100, c = pred[event_index], cmap=\"RdBu\")\n    ax[0, 1].set_title(\"Prediction\")\n    plt.savefig(\"result.png\")\n</pre> for event_index in range(num_events):     x = torch.cos(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index]))     y = torch.sin(torch.deg2rad(data[\"azimuth\"][event_index]))*torch.sin(torch.deg2rad(data[\"takeoff_angle\"][event_index]))     c = data[\"phase_polarity\"][event_index]     fig, ax = plt.subplots(1, 2, squeeze=False, figsize=(10, 5))         ax[0, 0].scatter(x, y, s=100, c = c, cmap=\"RdBu\")     ax[0, 0].set_title(\"Ground Truth\")     ax[0, 1].scatter(x, y, s=100, c = pred[event_index], cmap=\"RdBu\")     ax[0, 1].set_title(\"Prediction\")     plt.savefig(\"result.png\")"},{"location":"eps207-observational-seismology/earthquake/focal_mechanism/#inverting-earthquake-focal-mechanism-using-automatic-differentiation","title":"Inverting Earthquake Focal Mechanism using Automatic Differentiation\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/focal_mechanism/#build-forward-model-to-calculate-polarity","title":"Build forward model to calculate polarity\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/focal_mechanism/#generate-synthetic-data","title":"Generate synthetic data\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/focal_mechanism/#invert-focal-mechanism","title":"Invert Focal Mechanism\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/location_/","title":"Location","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n##################################################  Data  #############################################################\n</pre> import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm  ##################################################  Data  ############################################################# In\u00a0[\u00a0]: Copied! <pre>np.random.seed(11)\nvp = 6.0\nvs = vp/1.73\nnum_station = 5\nnum_event = 30\nxmax = 100\nstation_loc = np.random.uniform(low=0, high=xmax, size=(num_station,3))\nstation_loc[:, 2] = 0\nstation_dt = (np.random.rand(num_station, 1)-0.5)*2 * 10.0\nevent_loc = np.random.uniform(low=0, high=xmax, size=(num_event,3))\nevent_loc[:, 2] = 0\nevent_time = np.random.uniform(low=0, high=xmax/vp, size=(num_event,1))\n</pre> np.random.seed(11) vp = 6.0 vs = vp/1.73 num_station = 5 num_event = 30 xmax = 100 station_loc = np.random.uniform(low=0, high=xmax, size=(num_station,3)) station_loc[:, 2] = 0 station_dt = (np.random.rand(num_station, 1)-0.5)*2 * 10.0 event_loc = np.random.uniform(low=0, high=xmax, size=(num_event,3)) event_loc[:, 2] = 0 event_time = np.random.uniform(low=0, high=xmax/vp, size=(num_event,1)) In\u00a0[\u00a0]: Copied! <pre>event_index = []\nstation_index = []\nphase_type = []\nphase_time = []\nphase_weight = []\nvelocity = {\"P\": vp, \"S\": vs}\nrandom_time = (np.random.rand(1, num_station, 2) - 0.5)*2.0 * 5.0\nfor evid in range(num_event):\n    for stid in range(num_station):\n        for i, phase in enumerate([\"P\", \"S\"]):\n            event_index.append(evid)\n            station_index.append(stid)\n            phase_type.append(phase)\n            dist = np.linalg.norm(station_loc[stid] - event_loc[evid], axis=-1)\n            t = dist / velocity[phase] + event_time[evid] + random_time[0, stid, i] + station_dt[stid]\n            phase_time.append(t)\n            phase_weight.append(1.0)\n\nevent_index = torch.tensor(np.array(event_index, dtype=np.int64))\nstation_index = torch.tensor(np.array(station_index, dtype=np.int64))\nphase_time = torch.tensor(np.array(phase_time, dtype=np.float32))\nphase_weight = torch.tensor(np.array(phase_weight, dtype=np.float32))\n</pre> event_index = [] station_index = [] phase_type = [] phase_time = [] phase_weight = [] velocity = {\"P\": vp, \"S\": vs} random_time = (np.random.rand(1, num_station, 2) - 0.5)*2.0 * 5.0 for evid in range(num_event):     for stid in range(num_station):         for i, phase in enumerate([\"P\", \"S\"]):             event_index.append(evid)             station_index.append(stid)             phase_type.append(phase)             dist = np.linalg.norm(station_loc[stid] - event_loc[evid], axis=-1)             t = dist / velocity[phase] + event_time[evid] + random_time[0, stid, i] + station_dt[stid]             phase_time.append(t)             phase_weight.append(1.0)  event_index = torch.tensor(np.array(event_index, dtype=np.int64)) station_index = torch.tensor(np.array(station_index, dtype=np.int64)) phase_time = torch.tensor(np.array(phase_time, dtype=np.float32)) phase_weight = torch.tensor(np.array(phase_weight, dtype=np.float32)) In\u00a0[\u00a0]: Copied! <pre># plt.figure()\n# idx_event = 0\n# plt.scatter(station_loc[:,0], station_loc[:,1], c=tp[idx_event,:], marker=\"^\", label=\"stations\")\n# plt.plot(event_loc[:,0], event_loc[:,1], 'x', color='gray', label=\"events\")\n# plt.plot(event_loc[idx_event,0], event_loc[idx_event,1], 'rx', markersize=10)\n# plt.xlim([0, xmax])\n# plt.ylim([0, xmax])\n# plt.legend()\n# plt.colorbar()\n# plt.axis(\"scaled\")\n# plt.show()\n\n##################################################  Model  #############################################################\n</pre> # plt.figure() # idx_event = 0 # plt.scatter(station_loc[:,0], station_loc[:,1], c=tp[idx_event,:], marker=\"^\", label=\"stations\") # plt.plot(event_loc[:,0], event_loc[:,1], 'x', color='gray', label=\"events\") # plt.plot(event_loc[idx_event,0], event_loc[idx_event,1], 'rx', markersize=10) # plt.xlim([0, xmax]) # plt.ylim([0, xmax]) # plt.legend() # plt.colorbar() # plt.axis(\"scaled\") # plt.show()  ##################################################  Model  ############################################################# In\u00a0[\u00a0]: Copied! <pre>class TravelTime(nn.Module):\n\n    def __init__(self, num_event, num_station, station_loc, station_dt=None, event_loc=None, event_time=None, reg=0.001, velocity={\"P\": 6.0, \"S\": 6.0/1.73}, dtype=torch.float32):\n        super().__init__()\n        self.num_event = num_event\n        self.event_loc = nn.Embedding(num_event, 3)\n        self.event_time = nn.Embedding(num_event, 1)\n        self.station_dt = nn.Embedding(num_station, 1)\n        if station_dt is not None:\n            self.station_dt.weight = torch.nn.Parameter(torch.tensor(station_dt, dtype=dtype))\n        else:\n            self.station_dt.weight = torch.nn.Parameter(torch.zeros(num_station, 1, dtype=dtype))\n        self.register_buffer('station_loc', torch.tensor(station_loc, dtype=dtype))\n        self.velocity = velocity\n        self.reg = reg\n        if event_loc is not None:\n            self.event_loc.weight = torch.nn.Parameter(torch.tensor(event_loc, dtype=dtype))\n        if event_time is not None:\n            self.event_time.weight = torch.nn.Parameter(torch.tensor(event_time, dtype=dtype))\n\n    def calc_time(self, event_loc, station_loc, phase_type):\n\n        dist  = torch.linalg.norm(event_loc - station_loc, axis=-1, keepdim=True)\n        vel = self.velocity\n        tt = dist / torch.tensor([vel[p] for p in phase_type]).unsqueeze(-1)\n\n        return tt\n    \n    def forward(self, station_index, event_index=None, phase_type=None, phase_time=None, phase_weight=None, use_pair=False):\n\n        station_loc = self.station_loc[station_index]\n        station_dt = self.station_dt(station_index)\n\n        event_loc = self.event_loc(event_index)\n        event_time = self.event_time(event_index)\n\n        tt = self.calc_time(event_loc, station_loc, phase_type)\n        t = event_time + tt + station_dt\n\n        if use_pair:\n            t = t[0] - t[1]\n\n        if phase_time is None:\n            loss = None\n        else:\n            # loss = torch.mean(phase_weight * (t - phase_time) ** 2)\n            loss = torch.mean(F.huber_loss(t, phase_time, reduction=\"none\") * phase_weight)\n            loss += self.reg * torch.mean(torch.abs(station_dt)) ## prevent the trade-off between station_dt and event_time\n\n        return {\"phase_time\": t, \"loss\": loss}\n\n\n################################################## Absolute location  #############################################################\n</pre> class TravelTime(nn.Module):      def __init__(self, num_event, num_station, station_loc, station_dt=None, event_loc=None, event_time=None, reg=0.001, velocity={\"P\": 6.0, \"S\": 6.0/1.73}, dtype=torch.float32):         super().__init__()         self.num_event = num_event         self.event_loc = nn.Embedding(num_event, 3)         self.event_time = nn.Embedding(num_event, 1)         self.station_dt = nn.Embedding(num_station, 1)         if station_dt is not None:             self.station_dt.weight = torch.nn.Parameter(torch.tensor(station_dt, dtype=dtype))         else:             self.station_dt.weight = torch.nn.Parameter(torch.zeros(num_station, 1, dtype=dtype))         self.register_buffer('station_loc', torch.tensor(station_loc, dtype=dtype))         self.velocity = velocity         self.reg = reg         if event_loc is not None:             self.event_loc.weight = torch.nn.Parameter(torch.tensor(event_loc, dtype=dtype))         if event_time is not None:             self.event_time.weight = torch.nn.Parameter(torch.tensor(event_time, dtype=dtype))      def calc_time(self, event_loc, station_loc, phase_type):          dist  = torch.linalg.norm(event_loc - station_loc, axis=-1, keepdim=True)         vel = self.velocity         tt = dist / torch.tensor([vel[p] for p in phase_type]).unsqueeze(-1)          return tt          def forward(self, station_index, event_index=None, phase_type=None, phase_time=None, phase_weight=None, use_pair=False):          station_loc = self.station_loc[station_index]         station_dt = self.station_dt(station_index)          event_loc = self.event_loc(event_index)         event_time = self.event_time(event_index)          tt = self.calc_time(event_loc, station_loc, phase_type)         t = event_time + tt + station_dt          if use_pair:             t = t[0] - t[1]          if phase_time is None:             loss = None         else:             # loss = torch.mean(phase_weight * (t - phase_time) ** 2)             loss = torch.mean(F.huber_loss(t, phase_time, reduction=\"none\") * phase_weight)             loss += self.reg * torch.mean(torch.abs(station_dt)) ## prevent the trade-off between station_dt and event_time          return {\"phase_time\": t, \"loss\": loss}   ################################################## Absolute location  ############################################################# In\u00a0[\u00a0]: Copied! <pre>travel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs})\ntt = travel_time(station_index, event_index, phase_type)[\"phase_time\"]\nprint(\"True location: \", F.mse_loss(tt, phase_time))\n</pre> travel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs}) tt = travel_time(station_index, event_index, phase_type)[\"phase_time\"] print(\"True location: \", F.mse_loss(tt, phase_time)) In\u00a0[\u00a0]: Copied! <pre>travel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs})\ntt = travel_time(station_index, event_index, phase_type)[\"phase_time\"]\nprint(\"Initial loss\", F.mse_loss(tt, phase_time))\ninit_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninit_event_time = travel_time.event_time.weight.clone().detach().numpy()\n</pre> travel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs}) tt = travel_time(station_index, event_index, phase_type)[\"phase_time\"] print(\"Initial loss\", F.mse_loss(tt, phase_time)) init_event_loc = travel_time.event_loc.weight.clone().detach().numpy() init_event_time = travel_time.event_time.weight.clone().detach().numpy() In\u00a0[\u00a0]: Copied! <pre>optimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    optimizer.zero_grad()\n    loss = travel_time(station_index, event_index, phase_type, phase_time, phase_weight)[\"loss\"]\n    loss.backward()\n    return loss\n\noptimizer.step(closure)\n    \ntt = travel_time(station_index, event_index, phase_type)[\"phase_time\"]\nprint(\"Optimized loss\", F.mse_loss(tt, phase_time))\ninvert_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninvert_event_time = travel_time.event_time.weight.clone().detach().numpy()\ninvert_station_dt = travel_time.station_dt.weight.clone().detach().numpy()\n</pre> optimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")  def closure():     optimizer.zero_grad()     loss = travel_time(station_index, event_index, phase_type, phase_time, phase_weight)[\"loss\"]     loss.backward()     return loss  optimizer.step(closure)      tt = travel_time(station_index, event_index, phase_type)[\"phase_time\"] print(\"Optimized loss\", F.mse_loss(tt, phase_time)) invert_event_loc = travel_time.event_loc.weight.clone().detach().numpy() invert_event_time = travel_time.event_time.weight.clone().detach().numpy() invert_station_dt = travel_time.station_dt.weight.clone().detach().numpy() In\u00a0[\u00a0]: Copied! <pre>plt.figure()\n# plt.scatter(station_loc[:,0], station_loc[:,1], c=tp[idx_event,:])\nplt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\")\nplt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\")\nplt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\")\nplt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6)\nplt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6)\nplt.xlim([0, xmax])\nplt.ylim([0, xmax])\nplt.axis(\"scaled\")\nplt.legend()\nplt.savefig(\"absolute_location.png\", dpi=300)\n# plt.show()\n\n################################################## Relative location  #############################################################\n</pre> plt.figure() # plt.scatter(station_loc[:,0], station_loc[:,1], c=tp[idx_event,:]) plt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\") plt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\") plt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\") plt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6) plt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6) plt.xlim([0, xmax]) plt.ylim([0, xmax]) plt.axis(\"scaled\") plt.legend() plt.savefig(\"absolute_location.png\", dpi=300) # plt.show()  ################################################## Relative location  ############################################################# In\u00a0[\u00a0]: Copied! <pre>pair_event_index = []\npair_station_index = []\npair_type = []\npair_dtime = []\npair_weight = []\nvelocity = {\"P\": vp, \"S\": vs}\nfor evid1 in range(num_event):\n    for evid2 in range(evid1+1, num_event):\n        for stid in range(num_station):\n            for phase in [\"P\", \"S\"]:\n                pair_event_index.append([evid1, evid2])\n                pair_station_index.append(stid)\n                pair_type.append(phase)\n                dist1 = np.linalg.norm(station_loc[stid] - event_loc[evid1], axis=-1)\n                tt1 = dist1 / velocity[phase] \n                t1 = tt1 + event_time[evid1] + random_time[0, stid, i]\n                dist2 = np.linalg.norm(station_loc[stid] - event_loc[evid2], axis=-1)\n                tt2 = dist2 / velocity[phase] \n                t2 = tt2 + event_time[evid2] + random_time[0, stid, i]\n                pair_dtime.append(t1 - t2)\n                pair_weight.append(1.0)\n\npair_event_index = torch.tensor(np.array(pair_event_index, dtype=np.int64).T)\npair_station_index = torch.tensor(np.array(pair_station_index, dtype=np.int64))\npair_dtime = torch.tensor(np.array(pair_dtime, dtype=np.float32))\npair_weight = torch.tensor(np.array(pair_weight, dtype=np.float32))\n</pre> pair_event_index = [] pair_station_index = [] pair_type = [] pair_dtime = [] pair_weight = [] velocity = {\"P\": vp, \"S\": vs} for evid1 in range(num_event):     for evid2 in range(evid1+1, num_event):         for stid in range(num_station):             for phase in [\"P\", \"S\"]:                 pair_event_index.append([evid1, evid2])                 pair_station_index.append(stid)                 pair_type.append(phase)                 dist1 = np.linalg.norm(station_loc[stid] - event_loc[evid1], axis=-1)                 tt1 = dist1 / velocity[phase]                  t1 = tt1 + event_time[evid1] + random_time[0, stid, i]                 dist2 = np.linalg.norm(station_loc[stid] - event_loc[evid2], axis=-1)                 tt2 = dist2 / velocity[phase]                  t2 = tt2 + event_time[evid2] + random_time[0, stid, i]                 pair_dtime.append(t1 - t2)                 pair_weight.append(1.0)  pair_event_index = torch.tensor(np.array(pair_event_index, dtype=np.int64).T) pair_station_index = torch.tensor(np.array(pair_station_index, dtype=np.int64)) pair_dtime = torch.tensor(np.array(pair_dtime, dtype=np.float32)) pair_weight = torch.tensor(np.array(pair_weight, dtype=np.float32)) In\u00a0[\u00a0]: Copied! <pre>travel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs})\ndt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"]\nprint(\"True location: \", F.mse_loss(dt, pair_dtime))\n</pre> travel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs}) dt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"] print(\"True location: \", F.mse_loss(dt, pair_dtime)) In\u00a0[\u00a0]: Copied! <pre>travel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs})\ntt = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"phase_time\"]\nprint(\"Initial loss\", F.mse_loss(tt, pair_dtime))\ninit_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninit_event_time = travel_time.event_time.weight.clone().detach().numpy()\n</pre> travel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs}) tt = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"phase_time\"] print(\"Initial loss\", F.mse_loss(tt, pair_dtime)) init_event_loc = travel_time.event_loc.weight.clone().detach().numpy() init_event_time = travel_time.event_time.weight.clone().detach().numpy() In\u00a0[\u00a0]: Copied! <pre>optimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    optimizer.zero_grad()\n    loss = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"loss\"]\n    loss.backward()\n    return loss\n\noptimizer.step(closure)\n    \ntt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"]\nprint(\"Optimized loss\", F.mse_loss(tt, pair_dtime))\ninvert_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninvert_event_time = travel_time.event_time.weight.clone().detach().numpy()\n</pre> optimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")  def closure():     optimizer.zero_grad()     loss = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"loss\"]     loss.backward()     return loss  optimizer.step(closure)      tt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"] print(\"Optimized loss\", F.mse_loss(tt, pair_dtime)) invert_event_loc = travel_time.event_loc.weight.clone().detach().numpy() invert_event_time = travel_time.event_time.weight.clone().detach().numpy() In\u00a0[\u00a0]: Copied! <pre>plt.figure()\nplt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\")\nplt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\")\nplt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\")\nplt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6)\nplt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6)\nplt.xlim([0, xmax])\nplt.ylim([0, xmax])\nplt.axis(\"scaled\")\nplt.legend()\nplt.savefig(\"relative_location.png\", dpi=300)\n# plt.show()\n</pre> plt.figure() plt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\") plt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\") plt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\") plt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6) plt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6) plt.xlim([0, xmax]) plt.ylim([0, xmax]) plt.axis(\"scaled\") plt.legend() plt.savefig(\"relative_location.png\", dpi=300) # plt.show()"},{"location":"eps207-observational-seismology/earthquake/sorting/","title":"Sorting","text":"In\u00a0[14]: Copied! <pre>import os\n!os.system('curl -O -J -L https://osf.io/945dq/download')\n</pre> import os !os.system('curl -O -J -L https://osf.io/945dq/download') <pre>zsh:1: number expected\n</pre> In\u00a0[15]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> import pandas as pd import matplotlib.pyplot as plt import numpy as np In\u00a0[16]: Copied! <pre>events = pd.read_csv(\"catalog_gamma.csv\", sep=\"\\t\")\n\nX = []\ndeg2km = 111.1949\ncenter = [events[\"longitude\"].mean(), events[\"latitude\"].mean()]\nfor index, event in events.iterrows():\n    X.append([(event[\"longitude\"]-center[0])*deg2km*np.cos(np.deg2rad(event[\"latitude\"])), \n              (event[\"latitude\"]-center[1])*deg2km, \n               event[\"depth(m)\"]/1e3])\n    # X.append([(event[\"longitude\"])*deg2km*np.cos(np.deg2rad(event[\"latitude\"])), \n    #           (event[\"latitude\"])*deg2km, \n    #            event[\"depth(m)\"]/1e3])\nX = np.array(X)\nprint(X.shape)\nX = X[:10000]\n</pre> events = pd.read_csv(\"catalog_gamma.csv\", sep=\"\\t\")  X = [] deg2km = 111.1949 center = [events[\"longitude\"].mean(), events[\"latitude\"].mean()] for index, event in events.iterrows():     X.append([(event[\"longitude\"]-center[0])*deg2km*np.cos(np.deg2rad(event[\"latitude\"])),                (event[\"latitude\"]-center[1])*deg2km,                 event[\"depth(m)\"]/1e3])     # X.append([(event[\"longitude\"])*deg2km*np.cos(np.deg2rad(event[\"latitude\"])),      #           (event[\"latitude\"])*deg2km,      #            event[\"depth(m)\"]/1e3]) X = np.array(X) print(X.shape) X = X[:10000] <pre>(24479, 3)\n</pre> In\u00a0[17]: Copied! <pre>plt.figure()\n# plt.scatter(events[\"longitude\"], events[\"latitude\"], c=events[\"depth(m)\"]/1e3, alpha=0.5, s=1.0)\nplt.scatter(X[:,0], X[:,1], c=X[:,2], alpha=0.5, s=1.0)\nplt.show()\n</pre> plt.figure() # plt.scatter(events[\"longitude\"], events[\"latitude\"], c=events[\"depth(m)\"]/1e3, alpha=0.5, s=1.0) plt.scatter(X[:,0], X[:,1], c=X[:,2], alpha=0.5, s=1.0) plt.show() In\u00a0[18]: Copied! <pre>from scipy.cluster.hierarchy import dendrogram, linkage, leaves_list\n</pre> from scipy.cluster.hierarchy import dendrogram, linkage, leaves_list In\u00a0[19]: Copied! <pre>Z = linkage(X, 'ward', optimal_ordering=False)\nindex = leaves_list(Z)\n</pre> Z = linkage(X, 'ward', optimal_ordering=False) index = leaves_list(Z) In\u00a0[20]: Copied! <pre>X_ = X[index]\nD = np.sqrt(np.sum((X_[:,np.newaxis,:2] - X_[np.newaxis,:,:2])**2, axis=-1))\n</pre> X_ = X[index] D = np.sqrt(np.sum((X_[:,np.newaxis,:2] - X_[np.newaxis,:,:2])**2, axis=-1)) In\u00a0[21]: Copied! <pre>plt.figure()\n# plt.pcolormesh(D)\nplt.imshow(D)\nplt.axis(\"scaled\")\nplt.colorbar()\nplt.show()\n</pre> plt.figure() # plt.pcolormesh(D) plt.imshow(D) plt.axis(\"scaled\") plt.colorbar() plt.show() In\u00a0[22]: Copied! <pre>from scipy.sparse.csgraph import minimum_spanning_tree\nfrom scipy.sparse.csgraph import breadth_first_order\n</pre> from scipy.sparse.csgraph import minimum_spanning_tree from scipy.sparse.csgraph import breadth_first_order In\u00a0[23]: Copied! <pre>D = np.sqrt(((X[:, np.newaxis, :2] -  X[np.newaxis, :, :2])**2).sum(axis=-1))\n</pre> D = np.sqrt(((X[:, np.newaxis, :2] -  X[np.newaxis, :, :2])**2).sum(axis=-1)) In\u00a0[24]: Copied! <pre>Tcsr = minimum_spanning_tree(D)\n</pre> Tcsr = minimum_spanning_tree(D) In\u00a0[25]: Copied! <pre>index = breadth_first_order(Tcsr, i_start=0, directed=False, return_predecessors=False)\n</pre> index = breadth_first_order(Tcsr, i_start=0, directed=False, return_predecessors=False) In\u00a0[26]: Copied! <pre>X_ = X[index]\nD = np.sqrt(np.sum((X_[:,np.newaxis,:2] - X_[np.newaxis,:,:2])**2, axis=-1))\n</pre> X_ = X[index] D = np.sqrt(np.sum((X_[:,np.newaxis,:2] - X_[np.newaxis,:,:2])**2, axis=-1)) In\u00a0[27]: Copied! <pre>plt.figure()\n# plt.pcolormesh(D)\nplt.imshow(D)\nplt.axis(\"scaled\")\nplt.colorbar()\nplt.show()\n</pre> plt.figure() # plt.pcolormesh(D) plt.imshow(D) plt.axis(\"scaled\") plt.colorbar() plt.show()"},{"location":"eps207-observational-seismology/earthquake/sorting/#how-to-properly-sort-earthquakes-based-on-their-locations","title":"How to properly sort earthquakes based on their locations?\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/sorting/#download-test-data","title":"Download test data\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/sorting/#read-catalog","title":"Read catalog\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/sorting/#sorting-using-hierarchy-clustering","title":"Sorting using Hierarchy Clustering\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/sorting/#sorting-using-minimum-spanning-tree","title":"Sorting using Minimum Spanning Tree\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/statistics/","title":"Statistics","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom datetime import datetime\nimport obspy\nfrom obspy import UTCDateTime\nfrom obspy.clients.fdsn import Client\nclient = Client(\"IRIS\")\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from datetime import datetime import obspy from obspy import UTCDateTime from obspy.clients.fdsn import Client client = Client(\"IRIS\") In\u00a0[2]: Copied! <pre>minlatitude=33\nmaxlatitude=35\nminlongitude=-119\nmaxlongitude=-117\nstarttime = UTCDateTime(\"1970-01-01T00:00:00.000\")\nendtime = UTCDateTime(\"2022-07-01T00:00:00.000\")\nevents = client.get_events(starttime=starttime, endtime=endtime,\n                           minmagnitude=2.0,\n                           minlatitude=minlatitude, maxlatitude=maxlatitude,\n                           minlongitude=minlongitude, maxlongitude=maxlongitude)\n# events.plot(projection=\"local\", resolution=\"f\");\n</pre> minlatitude=33 maxlatitude=35 minlongitude=-119 maxlongitude=-117 starttime = UTCDateTime(\"1970-01-01T00:00:00.000\") endtime = UTCDateTime(\"2022-07-01T00:00:00.000\") events = client.get_events(starttime=starttime, endtime=endtime,                            minmagnitude=2.0,                            minlatitude=minlatitude, maxlatitude=maxlatitude,                            minlongitude=minlongitude, maxlongitude=maxlongitude) # events.plot(projection=\"local\", resolution=\"f\"); In\u00a0[3]: Copied! <pre>events_df = []\nfor event in events:\n    origin = event.origins[0]\n    tmp = {\"time\": origin.time.datetime.isoformat(), \n           \"latitude\": origin.latitude, \n           \"longitude\": origin.longitude,\n           \"depth_m\": origin.depth, \n           \"magnitude\": event.magnitudes[0].mag}\n    events_df.append(tmp)\nevents_df = pd.DataFrame(events_df)\n</pre> events_df = [] for event in events:     origin = event.origins[0]     tmp = {\"time\": origin.time.datetime.isoformat(),             \"latitude\": origin.latitude,             \"longitude\": origin.longitude,            \"depth_m\": origin.depth,             \"magnitude\": event.magnitudes[0].mag}     events_df.append(tmp) events_df = pd.DataFrame(events_df) In\u00a0[4]: Copied! <pre>df = events_df\ndf[\"LAT\"] = df[\"latitude\"]\ndf[\"LON\"] = df[\"longitude\"]\ndf[\"DEPTH\"] = df[\"depth_m\"]\ndf[\"MAG\"] = df[\"magnitude\"]\ndf[\"datetime\"] = df[\"time\"]\n</pre> df = events_df df[\"LAT\"] = df[\"latitude\"] df[\"LON\"] = df[\"longitude\"] df[\"DEPTH\"] = df[\"depth_m\"] df[\"MAG\"] = df[\"magnitude\"] df[\"datetime\"] = df[\"time\"] In\u00a0[5]: Copied! <pre>fig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot(projection=ccrs.Mercator())\nax.set_extent([-119, -117, 33, 35], ccrs.PlateCarree())\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.RIVERS)\nax.add_feature(cfeature.BORDERS)\nax.add_feature(cfeature.STATES)\nax.add_feature(cfeature.COASTLINE)\nax.gridlines(draw_labels=True)\nax.scatter(df[\"LON\"].values, df[\"LAT\"].values, c='b', s=3, transform=ccrs.PlateCarree())\n</pre> fig = plt.figure(figsize=(12, 9)) ax = fig.add_subplot(projection=ccrs.Mercator()) ax.set_extent([-119, -117, 33, 35], ccrs.PlateCarree()) ax.add_feature(cfeature.LAND) ax.add_feature(cfeature.RIVERS) ax.add_feature(cfeature.BORDERS) ax.add_feature(cfeature.STATES) ax.add_feature(cfeature.COASTLINE) ax.gridlines(draw_labels=True) ax.scatter(df[\"LON\"].values, df[\"LAT\"].values, c='b', s=3, transform=ccrs.PlateCarree())  Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x11ff74280&gt;</pre> In\u00a0[6]: Copied! <pre>fig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot(projection=ccrs.Mercator())\nax.set_extent([-119, -117, 33, 35], ccrs.PlateCarree())\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.RIVERS)\nax.add_feature(cfeature.BORDERS)\nax.add_feature(cfeature.STATES)\nax.add_feature(cfeature.COASTLINE)\nax.gridlines(draw_labels=True)\nax.scatter(df[\"LON\"].values, df[\"LAT\"].values, c='b', s=np.exp(0.5*df[\"MAG\"].values), transform=ccrs.PlateCarree())\n</pre> fig = plt.figure(figsize=(12, 9)) ax = fig.add_subplot(projection=ccrs.Mercator()) ax.set_extent([-119, -117, 33, 35], ccrs.PlateCarree()) ax.add_feature(cfeature.LAND) ax.add_feature(cfeature.RIVERS) ax.add_feature(cfeature.BORDERS) ax.add_feature(cfeature.STATES) ax.add_feature(cfeature.COASTLINE) ax.gridlines(draw_labels=True) ax.scatter(df[\"LON\"].values, df[\"LAT\"].values, c='b', s=np.exp(0.5*df[\"MAG\"].values), transform=ccrs.PlateCarree())  Out[6]: <pre>&lt;matplotlib.collections.PathCollection at 0x12a1376a0&gt;</pre> In\u00a0[7]: Copied! <pre>data = df.copy()\ndata['datetime'] = pd.to_datetime(data['datetime'])\ndata['year'] = data['datetime'].dt.year\n</pre> data = df.copy() data['datetime'] = pd.to_datetime(data['datetime']) data['year'] = data['datetime'].dt.year In\u00a0[8]: Copied! <pre>ref_date = datetime(data.year.min(), 1, 1, 0, 0, 0, 0)\nprint(ref_date)\n</pre> ref_date = datetime(data.year.min(), 1, 1, 0, 0, 0, 0) print(ref_date) <pre>1970-01-01 00:00:00\n</pre> In\u00a0[24]: Copied! <pre>data['serial_time'] = data['datetime'] - ref_date\ndata['serial_time'] = data['serial_time'].dt.total_seconds()\n# data\n</pre> data['serial_time'] = data['datetime'] - ref_date data['serial_time'] = data['serial_time'].dt.total_seconds() # data In\u00a0[26]: Copied! <pre>catalog_duration = data.year.max() - data.year.min()\nM = data.MAG\nMc = 2.0\n\nmag_range = np.arange(M.min(),7,0.1)\ndef GRcount(M,mag_range,Mc):\n    num_quakes = np.zeros(mag_range.shape)\n    for i in range(mag_range.shape[0]):\n        num_quakes[i] = M[M &gt;= mag_range[i]].shape[0]\n    \n    M_avg = np.mean(M[M &gt; Mc])\n    b = (np.log10(np.exp(1)))/(M_avg-Mc)\n    a = np.log10(num_quakes[mag_range &gt;= Mc][0])\n    N = (10**(a - (b*(mag_range - Mc))))\n\n    return num_quakes/catalog_duration, N/catalog_duration, b\n        \nnum_quakes, N, b = GRcount(M,mag_range,Mc)\n</pre> catalog_duration = data.year.max() - data.year.min() M = data.MAG Mc = 2.0  mag_range = np.arange(M.min(),7,0.1) def GRcount(M,mag_range,Mc):     num_quakes = np.zeros(mag_range.shape)     for i in range(mag_range.shape[0]):         num_quakes[i] = M[M &gt;= mag_range[i]].shape[0]          M_avg = np.mean(M[M &gt; Mc])     b = (np.log10(np.exp(1)))/(M_avg-Mc)     a = np.log10(num_quakes[mag_range &gt;= Mc][0])     N = (10**(a - (b*(mag_range - Mc))))      return num_quakes/catalog_duration, N/catalog_duration, b          num_quakes, N, b = GRcount(M,mag_range,Mc)  In\u00a0[27]: Copied! <pre>fig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot()\n# ax.plot(mag_range,N,color='red')\nax.plot(mag_range,num_quakes,marker='o',color='black',linestyle='None')\nax.set_yscale('log')\nax.set_xlabel('Magnitude')\nax.set_ylabel('Number of Earthquakes per Year')\n</pre> fig = plt.figure(figsize=(12, 9)) ax = fig.add_subplot() # ax.plot(mag_range,N,color='red') ax.plot(mag_range,num_quakes,marker='o',color='black',linestyle='None') ax.set_yscale('log') ax.set_xlabel('Magnitude') ax.set_ylabel('Number of Earthquakes per Year')  Out[27]: <pre>Text(0, 0.5, 'Number of Earthquakes per Year')</pre> In\u00a0[28]: Copied! <pre>eq_times = data.serial_time\ntime_range = np.linspace(eq_times.min(),eq_times.max(),5000)\n\neq_count = np.zeros(time_range.shape)\nfor i in range(time_range.shape[0]):\n    eq_count[i] = eq_times[eq_times &lt;= time_range[i]].shape[0]\n</pre> eq_times = data.serial_time time_range = np.linspace(eq_times.min(),eq_times.max(),5000)  eq_count = np.zeros(time_range.shape) for i in range(time_range.shape[0]):     eq_count[i] = eq_times[eq_times &lt;= time_range[i]].shape[0]  In\u00a0[29]: Copied! <pre>fig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot()\nax.plot(data.year.min()+time_range/(365.25*24*60*60), eq_count, color='black')\nax.set_xlabel('Time (years)')\nax.set_ylabel('Total Number of Earthquakes')\n</pre> fig = plt.figure(figsize=(12, 9)) ax = fig.add_subplot() ax.plot(data.year.min()+time_range/(365.25*24*60*60), eq_count, color='black') ax.set_xlabel('Time (years)') ax.set_ylabel('Total Number of Earthquakes') Out[29]: <pre>Text(0, 0.5, 'Total Number of Earthquakes')</pre> In\u00a0[30]: Copied! <pre>fig = plt.figure(figsize=(15, 9))\nax = fig.add_subplot()\nax.plot(data.year.min()+data.serial_time/(365.25*24*60*60), data.MAG, color='black', marker='o', linestyle='None')\nax.set_xlabel('Time (years)')\nax.set_ylabel('Magnitude')\n</pre> fig = plt.figure(figsize=(15, 9)) ax = fig.add_subplot() ax.plot(data.year.min()+data.serial_time/(365.25*24*60*60), data.MAG, color='black', marker='o', linestyle='None') ax.set_xlabel('Time (years)') ax.set_ylabel('Magnitude') Out[30]: <pre>Text(0, 0.5, 'Magnitude')</pre> In\u00a0[43]: Copied! <pre>data_afsh = data[(data.year &gt;= 1994) &amp; (data.year &lt; 1995)]\nmainshock_time = data_afsh[data_afsh.MAG == data_afsh.MAG.max()]['serial_time'].values #datetime(1994, 1, 17, 12, 30, 55, 390) \ndata_afsh['t_since_eq'] = data_afsh['serial_time'] - mainshock_time\ndata_afsh.drop(data_afsh[data_afsh['t_since_eq'] &lt; 0].index, inplace=True)\n# data_afsh\n</pre> data_afsh = data[(data.year &gt;= 1994) &amp; (data.year &lt; 1995)] mainshock_time = data_afsh[data_afsh.MAG == data_afsh.MAG.max()]['serial_time'].values #datetime(1994, 1, 17, 12, 30, 55, 390)  data_afsh['t_since_eq'] = data_afsh['serial_time'] - mainshock_time data_afsh.drop(data_afsh[data_afsh['t_since_eq'] &lt; 0].index, inplace=True) # data_afsh <pre>/var/folders/gc/lpnp82h92tv35c_7v97w97cm0000gn/T/ipykernel_19327/2954165421.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_afsh['t_since_eq'] = data_afsh['serial_time'] - mainshock_time\n/Users/weiqiang/.local/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  return super().drop(\n</pre> In\u00a0[44]: Copied! <pre>eq_times_afsh = data_afsh.serial_time\ntime_range_afsh = np.linspace(eq_times_afsh.min(),eq_times_afsh.max(),5000)\n\neq_count_afsh = np.zeros(time_range_afsh.shape)\nfor i in range(time_range_afsh.shape[0]):\n    eq_count_afsh[i] = eq_times_afsh[eq_times_afsh &lt;= time_range_afsh[i]].shape[0]\n\neq_count_afsh\n</pre> eq_times_afsh = data_afsh.serial_time time_range_afsh = np.linspace(eq_times_afsh.min(),eq_times_afsh.max(),5000)  eq_count_afsh = np.zeros(time_range_afsh.shape) for i in range(time_range_afsh.shape[0]):     eq_count_afsh[i] = eq_times_afsh[eq_times_afsh &lt;= time_range_afsh[i]].shape[0]  eq_count_afsh Out[44]: <pre>array([  1.,  29.,  58., ..., 792., 792., 793.])</pre> In\u00a0[45]: Copied! <pre>t = data_afsh.t_since_eq.values/(24*60*60)\nk = 1.3\nc = 0.09 #12\np = 1.18 #0.08\nn = k/((c + t)**p)\n\nn_total = np.cumsum(n)\n\n# n_total\n</pre> t = data_afsh.t_since_eq.values/(24*60*60) k = 1.3 c = 0.09 #12 p = 1.18 #0.08 n = k/((c + t)**p)  n_total = np.cumsum(n)  # n_total In\u00a0[46]: Copied! <pre>fig = plt.figure(figsize=(15, 9))\nax = fig.add_subplot()\nax.plot(data.year.min()+data_afsh.serial_time/(365.25*24*60*60), data_afsh.MAG, color='black', marker='o', linestyle='None')\nax.set_xlabel('Time (years)')\nax.set_ylabel('Magnitude')\n</pre> fig = plt.figure(figsize=(15, 9)) ax = fig.add_subplot() ax.plot(data.year.min()+data_afsh.serial_time/(365.25*24*60*60), data_afsh.MAG, color='black', marker='o', linestyle='None') ax.set_xlabel('Time (years)') ax.set_ylabel('Magnitude') Out[46]: <pre>Text(0, 0.5, 'Magnitude')</pre> In\u00a0[47]: Copied! <pre>fig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot()\nax.plot(data.year.min()+time_range_afsh/(365.25*24*60*60), eq_count_afsh, color='black')\nax.plot(data.year.min()+(mainshock_time + data_afsh.t_since_eq)/(365.25*24*60*60), n_total, color='red')\nax.set_xlabel('Time (years)')\nax.set_ylabel('Total Number of Earthquakes')\n</pre> fig = plt.figure(figsize=(12, 9)) ax = fig.add_subplot() ax.plot(data.year.min()+time_range_afsh/(365.25*24*60*60), eq_count_afsh, color='black') ax.plot(data.year.min()+(mainshock_time + data_afsh.t_since_eq)/(365.25*24*60*60), n_total, color='red') ax.set_xlabel('Time (years)') ax.set_ylabel('Total Number of Earthquakes') Out[47]: <pre>Text(0, 0.5, 'Total Number of Earthquakes')</pre>"},{"location":"eps207-observational-seismology/earthquake/statistics/#earthquake-statistics","title":"Earthquake statistics\u00b6","text":"<p>by James Atterholt and Erin Hightower</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#import-your-new-data","title":"Import Your New Data\u00b6","text":"<p>Make sure your script is in the same directory as your data or that you supply the right path name.</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#plot-the-earthquakes-in-map-view","title":"Plot the earthquakes in map view\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/statistics/#extract-and-modify-the-data-for-the-values-you-need","title":"Extract and modify the data for the values you need\u00b6","text":"<p>First we will make a copy of the dataframe, so that we can retain our original data if we need it.</p> <p>We then convert the year/month/day column and the hour/min/sec column to a datetime object that python can recognize as a date and time. Extract the year as its own column, and drop the old time columns from the new dataframe.</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#calculate-serial-time","title":"Calculate serial time\u00b6","text":"<p>Serial time in this case is essentially continuous time give in decimal seconds, rather than year-month-day-hour-min-second format. To calculate it, we need a reference date to use as a start point. We will use the beginning year of our catalog: 1970.</p> <p>Serial time is then determine by subtracting the reference date from the datetime column in the dataframe. The new values is saved as a new variable 'serial_time'. The datetime function of pandas then has the option to convert this value to total_seconds.</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#the-gutenberg-richter-law-earthquake-magnitude-frequency-distribution","title":"The Gutenberg Richter Law: Earthquake Magnitude-Frequency Distribution\u00b6","text":"<p>The Gutenberg-Richter Law is one of the most fundamental empirical statistical observations in seismology. It gives the frequency of earthquake occurrence depending on the magnitude. The key observation is that large earthquakes occur much more infrequently than small earthquakes.</p> <p>Here we will take the magnitudes of the earthquakes in our catalog and count how many of them fall above a certain magnitude for each magnitude in some predefined range. Here, that range is around 2.0 to 10.0. The following cell will calculate the number of quakes in each bin (the actual data we will plot) and the predicted line of the GR law, using a and b values calculated from the data and our given magnitude of completeness.</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#plot-the-gutenberg-richter-law-with-the-data","title":"Plot the Gutenberg-Richter Law with the data.\u00b6","text":"<p>Hint: Make sure to make the y-axis logarithmic!</p> <p>What patterns do you observe in the data?</p> <p>What happens at higher magnitudes? Why do you think that happens?</p> <p>What is the largest magnitude earthquake in this catalog?</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#plotting-earthquakes-with-time","title":"Plotting Earthquakes with Time\u00b6","text":"<p>Looking at the cumulative number of earthquakes with time and their magnitudes can tell us how the seismicity rate (the frequency with which earthquakes occur) is changing with time. Most of these changes happen as a result of aftershocks!</p> <p>Here we do a similar thing as before, and count the cumulative number of earthquakes with time since the start of our catalog.</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#plot-the-cumulative-number-of-earthquakes-with-time","title":"Plot the cumulative number of earthquakes with time\u00b6","text":"<p>What trends do you see in the plot you made?</p> <p>Look at the years where the jumps are. Based on what you have learned about historical earthquakes in this class so far, which events do you think were responsbile for these jumps in seismic activity? Why do you think this happens?</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#plot-the-magnitudes-of-the-earthquakes-with-time","title":"Plot the magnitudes of the earthquakes with time\u00b6","text":"<p>What patterns do you see in the data?</p> <p>Where and when are the largest earthquakes? Do you see any charactertistic patterns after those earthquakes?</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#the-omori-utsu-law-aftershock-decay","title":"The Omori-Utsu Law: Aftershock Decay\u00b6","text":"<p>The Omori Law (Omori, 1894) states that after a strong earthquake, the frequency of aftershocks decays with time on average according to a hyperbolic equation.</p> <p>$ n(t) = \\frac{k}{(c+t)} $</p> <p>This equation was later modified by Utsu (Utsu, 1961) into a power law known as the Omori-Utsu Law. The exponent p is usually around 1.0, reducing the equation to the original Omori law. However, p has been observed to have a rather large amount of variability depending on location.</p> <p>$n(t) = \\frac{k}{(c+t)^p}$</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#zoom-in-to-the-time-period-from-1994-to-1996","title":"Zoom in to the time period from 1994 to 1996\u00b6","text":"<p>Calculate a new serial_time column using the time of the mainshock earthquake as the reference time.</p>"},{"location":"eps207-observational-seismology/earthquake/statistics/#calculate-omoris-law-using-the-new-t_since_eq-as-the-t-variable","title":"Calculate Omori's Law using the new 't_since_eq' as the t variable\u00b6","text":"<p>n_total will give you the total number of earthquakes with time since the mainshock.</p>"},{"location":"eps207-observational-seismology/earthquake/travel_time/","title":"Travel time","text":"In\u00a0[28]: Copied! <pre>import matplotlib.pyplot as plt\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n</pre> import matplotlib.pyplot as plt import itertools import numpy as np import torch import torch.nn.functional as F import torch.optim as optim In\u00a0[29]: Copied! <pre>############################################################################################################\n# |\\nabla u| = f\n\n# ((u - a1)^+)^2 + ((u - a2)^+)^2 + ((u - a3)^+)^2 = f^2 h^2\n\n\ndef calculate_unique_solution(a, b, f, h):\n\n    d = abs(a - b)\n    if d &gt;= f * h:\n        return min(a, b) + f * h\n    else:\n        return (a + b + np.sqrt(2 * f * f * h * h - (a - b) ** 2)) / 2\n\n\ndef sweeping_over_I_J_K(u, I, J, f, h):\n\n    m = len(I)\n    n = len(J)\n    \n    for (i,j) in itertools.product(I, J):\n        if i == 0:\n            uxmin = u[i + 1, j]\n        elif i == m - 1:\n            uxmin = u[i - 1, j]\n        else:\n            uxmin = np.min([u[i - 1, j], u[i + 1, j]])\n\n        if j == 0:\n            uymin = u[i, j + 1]\n        elif j == n - 1:\n            uymin = u[i, j - 1]\n        else:\n            uymin = np.min([u[i, j - 1], u[i, j + 1]])\n\n        u_new = calculate_unique_solution(uxmin, uymin, f[i, j], h)\n\n        u[i, j] = np.min([u_new, u[i, j]])\n\n    return u\n\n\ndef sweeping(u, v, h):\n\n    f = 1.0 / v  ## slowness\n\n    m, n = u.shape\n    I = list(range(m))\n    iI = I[::-1]\n    J = list(range(n))\n    iJ = J[::-1]\n\n    u = sweeping_over_I_J_K(u, I, J, f, h)\n    u = sweeping_over_I_J_K(u, iI, J, f, h)\n    u = sweeping_over_I_J_K(u, iI, iJ, f, h)\n    u = sweeping_over_I_J_K(u, I, iJ, f, h)\n\n    return u\n\n\ndef eikonal_solve(u, f, h):\n\n    for i in range(50):\n        u_old = np.copy(u)\n        u = sweeping(u, f, h)\n\n        err = np.max(np.abs(u - u_old))\n        print(f\"Iteration {i}, Error = {err}\")\n        if err &lt; 1e-6:\n            break\n\n    return u\n\n############################################################################################################\n\ndef normalize(vars_, bounds):\n    mean = torch.tensor(\n        [\n            [\n                (bounds[0][0] + bounds[0][1]) / 2,\n                (bounds[1][0] + bounds[1][1]) / 2,\n                (bounds[2][0] + bounds[2][1]) / 2,\n            ],\n        ],\n        dtype=torch.float32,\n    )\n    std = torch.tensor(\n        [\n            [\n                (bounds[0][1] - bounds[0][0]) / 2,\n                (bounds[1][1] - bounds[1][0]) / 2,\n                (bounds[2][1] - bounds[2][0]) / 2,\n            ]\n        ],\n        dtype=torch.float32,\n    )\n    vars = (vars_ - mean) / std\n    vars = torch.tanh(vars)\n    vars = (vars * std) + mean\n\n    return vars\n</pre> ############################################################################################################ # |\\nabla u| = f  # ((u - a1)^+)^2 + ((u - a2)^+)^2 + ((u - a3)^+)^2 = f^2 h^2   def calculate_unique_solution(a, b, f, h):      d = abs(a - b)     if d &gt;= f * h:         return min(a, b) + f * h     else:         return (a + b + np.sqrt(2 * f * f * h * h - (a - b) ** 2)) / 2   def sweeping_over_I_J_K(u, I, J, f, h):      m = len(I)     n = len(J)          for (i,j) in itertools.product(I, J):         if i == 0:             uxmin = u[i + 1, j]         elif i == m - 1:             uxmin = u[i - 1, j]         else:             uxmin = np.min([u[i - 1, j], u[i + 1, j]])          if j == 0:             uymin = u[i, j + 1]         elif j == n - 1:             uymin = u[i, j - 1]         else:             uymin = np.min([u[i, j - 1], u[i, j + 1]])          u_new = calculate_unique_solution(uxmin, uymin, f[i, j], h)          u[i, j] = np.min([u_new, u[i, j]])      return u   def sweeping(u, v, h):      f = 1.0 / v  ## slowness      m, n = u.shape     I = list(range(m))     iI = I[::-1]     J = list(range(n))     iJ = J[::-1]      u = sweeping_over_I_J_K(u, I, J, f, h)     u = sweeping_over_I_J_K(u, iI, J, f, h)     u = sweeping_over_I_J_K(u, iI, iJ, f, h)     u = sweeping_over_I_J_K(u, I, iJ, f, h)      return u   def eikonal_solve(u, f, h):      for i in range(50):         u_old = np.copy(u)         u = sweeping(u, f, h)          err = np.max(np.abs(u - u_old))         print(f\"Iteration {i}, Error = {err}\")         if err &lt; 1e-6:             break      return u  ############################################################################################################  def normalize(vars_, bounds):     mean = torch.tensor(         [             [                 (bounds[0][0] + bounds[0][1]) / 2,                 (bounds[1][0] + bounds[1][1]) / 2,                 (bounds[2][0] + bounds[2][1]) / 2,             ],         ],         dtype=torch.float32,     )     std = torch.tensor(         [             [                 (bounds[0][1] - bounds[0][0]) / 2,                 (bounds[1][1] - bounds[1][0]) / 2,                 (bounds[2][1] - bounds[2][0]) / 2,             ]         ],         dtype=torch.float32,     )     vars = (vars_ - mean) / std     vars = torch.tanh(vars)     vars = (vars * std) + mean      return vars In\u00a0[30]: Copied! <pre>xlim = [0, 30]\nylim = [0, 30]\nzlim = [0, 20]  ## depth\nh = 0.3\nedge_grids = 3\n\nrlim = [0, ((xlim[1] - xlim[0]) ** 2 + (ylim[1] - ylim[0]) ** 2) ** 0.5]\n\nrx = np.arange(rlim[0]-edge_grids*h, rlim[1], h)\nzx = np.arange(zlim[0]-edge_grids*h, zlim[1], h)\nm = len(rx)\nn = len(zx)\n\nvp = np.ones((m, n)) * 6.0\nvs = np.ones((m, n)) * (6.0 / 1.75)\n\nup = 1000 * np.ones((m, n))\nup[edge_grids, edge_grids] = 0.0\nup = eikonal_solve(up, vp, h)\n\nus = 1000 * np.ones((m, n))\nus[edge_grids, edge_grids] = 0.0\nus = eikonal_solve(us, vs, h)\n\n############################## Check eikonal ##################################\nrgrid, zgrid = np.meshgrid(rx, zx, indexing=\"ij\")\nup_true = np.sqrt((rgrid - 0) ** 2 + (zgrid - 0) ** 2) / np.mean(vp)\nus_true = np.sqrt((rgrid - 0) ** 2 + (zgrid - 0) ** 2) / np.mean(vs)\n\nfig, axes = plt.subplots(2, 1, squeeze=False, figsize=(8, 6 * n / m * 2))\nim0 = axes[0,0].pcolormesh(rx, zx, up.T)\naxes[0,0].axis(\"scaled\")\naxes[0,0].invert_yaxis()\naxes[0,0].set_title(\"P-wave traveltime\")\nfig.colorbar(im0, ax=axes[0,0])\n\nim1 = axes[1,0].pcolormesh(rx, zx, (up - up_true).T)\naxes[1,0].axis(\"scaled\")\naxes[1,0].invert_yaxis()\naxes[1,0].set_title(\"Difference\")\nfig.colorbar(im1, ax=axes[1,0])\nplt.tight_layout()\nfig.savefig(\"test_vp.png\")\n\nfig, axes = plt.subplots(2, 1, squeeze=False, figsize=(8, 6 * n / m * 2))\nim0 = axes[0,0].pcolormesh(rx, zx, us.T)\naxes[0,0].axis(\"scaled\")\naxes[0,0].invert_yaxis()\naxes[0,0].set_title(\"S-wave traveltime\")\nfig.colorbar(im0, ax=axes[0,0])\n\nim1 = axes[1,0].pcolormesh(rx, zx, (us - us_true).T)\naxes[1,0].axis(\"scaled\")\naxes[1,0].invert_yaxis()\naxes[1,0].set_title(\"Difference\")\nfig.colorbar(im1, ax=axes[1,0])\nplt.tight_layout()\nfig.savefig(\"test_vs.png\")\n</pre> xlim = [0, 30] ylim = [0, 30] zlim = [0, 20]  ## depth h = 0.3 edge_grids = 3  rlim = [0, ((xlim[1] - xlim[0]) ** 2 + (ylim[1] - ylim[0]) ** 2) ** 0.5]  rx = np.arange(rlim[0]-edge_grids*h, rlim[1], h) zx = np.arange(zlim[0]-edge_grids*h, zlim[1], h) m = len(rx) n = len(zx)  vp = np.ones((m, n)) * 6.0 vs = np.ones((m, n)) * (6.0 / 1.75)  up = 1000 * np.ones((m, n)) up[edge_grids, edge_grids] = 0.0 up = eikonal_solve(up, vp, h)  us = 1000 * np.ones((m, n)) us[edge_grids, edge_grids] = 0.0 us = eikonal_solve(us, vs, h)  ############################## Check eikonal ################################## rgrid, zgrid = np.meshgrid(rx, zx, indexing=\"ij\") up_true = np.sqrt((rgrid - 0) ** 2 + (zgrid - 0) ** 2) / np.mean(vp) us_true = np.sqrt((rgrid - 0) ** 2 + (zgrid - 0) ** 2) / np.mean(vs)  fig, axes = plt.subplots(2, 1, squeeze=False, figsize=(8, 6 * n / m * 2)) im0 = axes[0,0].pcolormesh(rx, zx, up.T) axes[0,0].axis(\"scaled\") axes[0,0].invert_yaxis() axes[0,0].set_title(\"P-wave traveltime\") fig.colorbar(im0, ax=axes[0,0])  im1 = axes[1,0].pcolormesh(rx, zx, (up - up_true).T) axes[1,0].axis(\"scaled\") axes[1,0].invert_yaxis() axes[1,0].set_title(\"Difference\") fig.colorbar(im1, ax=axes[1,0]) plt.tight_layout() fig.savefig(\"test_vp.png\")  fig, axes = plt.subplots(2, 1, squeeze=False, figsize=(8, 6 * n / m * 2)) im0 = axes[0,0].pcolormesh(rx, zx, us.T) axes[0,0].axis(\"scaled\") axes[0,0].invert_yaxis() axes[0,0].set_title(\"S-wave traveltime\") fig.colorbar(im0, ax=axes[0,0])  im1 = axes[1,0].pcolormesh(rx, zx, (us - us_true).T) axes[1,0].axis(\"scaled\") axes[1,0].invert_yaxis() axes[1,0].set_title(\"Difference\") fig.colorbar(im1, ax=axes[1,0]) plt.tight_layout() fig.savefig(\"test_vs.png\") <pre>Iteration 0, Error = 999.95\nIteration 1, Error = 0.0\nIteration 0, Error = 999.9125\nIteration 1, Error = 0.0\n</pre> In\u00a0[31]: Copied! <pre>def traveltime(event_loc, station_locs, time_table, dx, rgrid, zgrid, sigma=1, bounds=None):\n\n    if bounds is not None:\n        vars = normalize(event_loc, bounds)\n    else:\n        vars = event_loc\n\n    r = torch.sqrt(torch.sum((event_loc[0, :2] - station_locs[:, :2]) ** 2, dim=-1))\n    z = torch.abs(event_loc[0, 2] - station_locs[:, 2])\n\n    r = r.unsqueeze(-1).unsqueeze(-1)\n    z = z.unsqueeze(-1).unsqueeze(-1)\n\n    magn = (\n        1.0\n        / (2.0 * np.pi * sigma)\n        * torch.exp(-(((rgrid - r) / (np.sqrt(2 * sigma) * dx)) ** 2 + ((zgrid - z) / (np.sqrt(2 * sigma) * dx)) ** 2))\n    )\n    sum_magn = torch.sum(magn, dim=(-1, -2))\n    tt = torch.sum(time_table * magn, dim=(-1, -2)) / sum_magn\n\n    return tt\n</pre> def traveltime(event_loc, station_locs, time_table, dx, rgrid, zgrid, sigma=1, bounds=None):      if bounds is not None:         vars = normalize(event_loc, bounds)     else:         vars = event_loc      r = torch.sqrt(torch.sum((event_loc[0, :2] - station_locs[:, :2]) ** 2, dim=-1))     z = torch.abs(event_loc[0, 2] - station_locs[:, 2])      r = r.unsqueeze(-1).unsqueeze(-1)     z = z.unsqueeze(-1).unsqueeze(-1)      magn = (         1.0         / (2.0 * np.pi * sigma)         * torch.exp(-(((rgrid - r) / (np.sqrt(2 * sigma) * dx)) ** 2 + ((zgrid - z) / (np.sqrt(2 * sigma) * dx)) ** 2))     )     sum_magn = torch.sum(magn, dim=(-1, -2))     tt = torch.sum(time_table * magn, dim=(-1, -2)) / sum_magn      return tt In\u00a0[32]: Copied! <pre>rx = torch.from_numpy(rx)\nzx = torch.from_numpy(zx)\nup = torch.from_numpy(up)\nus = torch.from_numpy(us)\n</pre> rx = torch.from_numpy(rx) zx = torch.from_numpy(zx) up = torch.from_numpy(up) us = torch.from_numpy(us) In\u00a0[33]: Copied! <pre>sigma = 0.8\nevent_locs = torch.tensor([[30, 40, 20]], requires_grad=True, dtype=torch.float32)  # (1, 3)\nstation_locs = torch.tensor([[20, 20, 0]])  # (nsta, 3)\nr = torch.sqrt(torch.sum((event_locs[0, :2] - station_locs[:, :2]) ** 2, axis=-1))  # (nsta, 3)\nz = torch.abs(event_locs[0, 2] - station_locs[:, 2])  # (nsta, 1)\n\nr = r.unsqueeze(-1).unsqueeze(-1)\nz = z.unsqueeze(-1).unsqueeze(-1)\nrgrid, zgrid = torch.meshgrid(rx, zx, indexing=\"ij\")\n\ntp = traveltime(event_locs, station_locs, up, h, rgrid, zgrid, sigma=sigma)\nts = traveltime(event_locs, station_locs, us, h, rgrid, zgrid, sigma=sigma)\n\ntp_true = (r[:, 0, 0] ** 2 + z[:, 0, 0] ** 2) ** 0.5 / np.mean(vp)\nts_true = (r[:, 0, 0] ** 2 + z[:, 0, 0] ** 2) ** 0.5 / np.mean(vs)\n\nprint(f\"tp = {tp}, tp_true = {tp_true}\")\nprint(f\"ts = {ts}, ts_true = {ts_true}\")\n</pre> sigma = 0.8 event_locs = torch.tensor([[30, 40, 20]], requires_grad=True, dtype=torch.float32)  # (1, 3) station_locs = torch.tensor([[20, 20, 0]])  # (nsta, 3) r = torch.sqrt(torch.sum((event_locs[0, :2] - station_locs[:, :2]) ** 2, axis=-1))  # (nsta, 3) z = torch.abs(event_locs[0, 2] - station_locs[:, 2])  # (nsta, 1)  r = r.unsqueeze(-1).unsqueeze(-1) z = z.unsqueeze(-1).unsqueeze(-1) rgrid, zgrid = torch.meshgrid(rx, zx, indexing=\"ij\")  tp = traveltime(event_locs, station_locs, up, h, rgrid, zgrid, sigma=sigma) ts = traveltime(event_locs, station_locs, us, h, rgrid, zgrid, sigma=sigma)  tp_true = (r[:, 0, 0] ** 2 + z[:, 0, 0] ** 2) ** 0.5 / np.mean(vp) ts_true = (r[:, 0, 0] ** 2 + z[:, 0, 0] ** 2) ** 0.5 / np.mean(vs)  print(f\"tp = {tp}, tp_true = {tp_true}\") print(f\"ts = {ts}, ts_true = {ts_true}\") <pre>tp = tensor([5.0416], dtype=torch.float64, grad_fn=&lt;DivBackward0&gt;), tp_true = tensor([5.], grad_fn=&lt;DivBackward0&gt;)\nts = tensor([8.8229], dtype=torch.float64, grad_fn=&lt;DivBackward0&gt;), ts_true = tensor([8.7500], grad_fn=&lt;DivBackward0&gt;)\n</pre> In\u00a0[34]: Copied! <pre>external_grad = torch.tensor([1., ])\ntp.backward(gradient=external_grad)\n</pre> external_grad = torch.tensor([1., ]) tp.backward(gradient=external_grad)  In\u00a0[35]: Copied! <pre>event_grad = event_locs.grad\nazimuth = torch.atan2(event_grad[0, 1], event_grad[0, 0])\ntakeoff_angle = torch.atan2(torch.sqrt(event_grad[0, 0] ** 2 + event_grad[0, 1] ** 2), event_grad[0, 2])\n</pre> event_grad = event_locs.grad azimuth = torch.atan2(event_grad[0, 1], event_grad[0, 0]) takeoff_angle = torch.atan2(torch.sqrt(event_grad[0, 0] ** 2 + event_grad[0, 1] ** 2), event_grad[0, 2])  In\u00a0[36]: Copied! <pre>print(f\"azimuth = {torch.rad2deg(azimuth):.2f}, takeoff angle = {torch.rad2deg(takeoff_angle):.2f}\")\n</pre> print(f\"azimuth = {torch.rad2deg(azimuth):.2f}, takeoff angle = {torch.rad2deg(takeoff_angle):.2f}\") <pre>azimuth = 63.43, takeoff angle = 77.90\n</pre>"},{"location":"eps207-observational-seismology/earthquake/travel_time/#calculate-travel-time-azimuth-and-takeoff-angle-based-on-eikonal-equation","title":"Calculate travel time, azimuth, and takeoff angle based on Eikonal equation\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/travel_time/#solve-eikonal-equation","title":"Solve Eikonal equation\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/travel_time/#extract-event-travel-time","title":"Extract event travel time\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/travel_time/#calculate-azimuth-and-takeoff-angle","title":"Calculate azimuth and takeoff angle\u00b6","text":""},{"location":"eps207-observational-seismology/earthquake/ambient_noise/PyCC_step1/","title":"PyCC step1","text":"In\u00a0[\u00a0]: Copied! <pre># This script does preprocessing for ambient noise cross-correlation, including:\n# differentiation, detrend, bandpass filter, decimation,\n# demean (of channels), temporal normalization\n# Yan Yang 2022-07-10\n\nimport glob\nimport os\nfrom time import time\n\nimport h5py\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\n\nfrom func_PyCC import *\n</pre> # This script does preprocessing for ambient noise cross-correlation, including: # differentiation, detrend, bandpass filter, decimation, # demean (of channels), temporal normalization # Yan Yang 2022-07-10  import glob import os from time import time  import h5py import numpy as np from joblib import Parallel, delayed from tqdm import tqdm  from func_PyCC import * In\u00a0[\u00a0]: Copied! <pre>output_preprocessed = \"./preprocessed/\"\nfilelist = glob.glob(\"/kuafu/DASdata/Ridgecrest_ODH3_2_Hourly/*.h5\")\nfilelist.sort()\n# filelist = filelist[:2]\nfilelist = filelist[:1]\nprint(filelist)\n\nfs = 50  # sampling frequency\nf1, f2 = 0.1, 10  # bandpass filter in preprocessing\nDecimation = 2  # if not 1, decimation after filtering\nDiff = True  # whether differentiate strain to strain rate\nram_win = 2  # temporal normalization windowm, usually  1/f1/5 ~ 1/f1/2 #\nmin_length = 60  # length of the segment in preprocessing, in sec, if shorter than this length, skip the file\nmin_npts = int(min_length * fs)\nnjobs = 5  # number f jobs if parallel\n\n\ndef preprocess(x, fs, f1, f2, Decimation, Diff=Diff, ram_win=ram_win):\n    \"\"\"\n    :param x: input data shape (nch, npts)\n    :param fs, f1, f2, Decimation, Diff, ram_win: see above\n    :return:\n    \"\"\"\n    if Diff:\n        x = np.gradient(x, axis=-1) * fs\n    x = detrend(x, axis=-1)\n    x = filter(x, fs, f1, f2)\n    x = x[:, ::Decimation]\n    fs_deci = fs / Decimation\n    x = x - np.median(x, 0)\n    x = temporal_normalization(x, fs_deci, ram_win)\n    x = x.astype(\"float32\")\n    return x\n</pre> output_preprocessed = \"./preprocessed/\" filelist = glob.glob(\"/kuafu/DASdata/Ridgecrest_ODH3_2_Hourly/*.h5\") filelist.sort() # filelist = filelist[:2] filelist = filelist[:1] print(filelist)  fs = 50  # sampling frequency f1, f2 = 0.1, 10  # bandpass filter in preprocessing Decimation = 2  # if not 1, decimation after filtering Diff = True  # whether differentiate strain to strain rate ram_win = 2  # temporal normalization windowm, usually  1/f1/5 ~ 1/f1/2 # min_length = 60  # length of the segment in preprocessing, in sec, if shorter than this length, skip the file min_npts = int(min_length * fs) njobs = 5  # number f jobs if parallel   def preprocess(x, fs, f1, f2, Decimation, Diff=Diff, ram_win=ram_win):     \"\"\"     :param x: input data shape (nch, npts)     :param fs, f1, f2, Decimation, Diff, ram_win: see above     :return:     \"\"\"     if Diff:         x = np.gradient(x, axis=-1) * fs     x = detrend(x, axis=-1)     x = filter(x, fs, f1, f2)     x = x[:, ::Decimation]     fs_deci = fs / Decimation     x = x - np.median(x, 0)     x = temporal_normalization(x, fs_deci, ram_win)     x = x.astype(\"float32\")     return x In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(output_preprocessed):\n    os.mkdir(output_preprocessed)\n\nfor ifile in tqdm(filelist):\n    outputname = os.path.join(\n        output_preprocessed,\n        ifile.split(\"_\")[-1]\n        .replace(\"-\", \"\")\n        .replace(\"T\", \"\")\n        .replace(\":\", \"\")\n        .replace(\"Z\", \"\")\n        .replace(\" \", \"\")\n        .replace(\"T\", \"\")\n        .replace(\":\", \"\")[-17:],\n    )\n    # try not overlap\n    # if os.path.exists(outputname):\n    #     print(outputname, \"exists\")\n    #     continue\n\n    fid = h5py.File(ifile, \"r\")\n    fs_data = fid[\"Data\"].attrs[\"fs\"]\n    nt_data = fid[\"Data\"].attrs[\"nt\"]\n    if fs_data != fs:\n        print(f\"wrong fs: {ifile}\")\n        fid.close()\n        continue\n    if nt_data &lt; min_npts:\n        print(f\"too short file: {ifile}\")\n        fid.close()\n        continue\n\n    # t1 = time()\n\n    data = fid[\"Data\"][:]\n    nch = data.shape[0]\n    npts = data.shape[1]\n    fid.close()\n\n    # print('read', time() - t1)\n    # t1 = time()\n\n    nchunk = int(np.ceil(npts / min_npts))\n    out = Parallel(n_jobs=njobs)(\n        delayed(preprocess)(data[:, int(min_length * fs * i) : int(min_length * fs * (i + 1))], fs, f1, f2, Decimation)\n        for i in range(nchunk)\n    )\n    data_out = np.concatenate(out, axis=-1)\n    # print('parallel', time() - t1)\n\n    # t1 = time()\n    fs_deci = fs / Decimation\n    output_h5 = h5py.File(outputname, \"w\")\n    output_data = output_h5.create_dataset(\"Data\", data=data_out)\n    output_data.attrs[\"fs\"] = fs_deci\n    output_data.attrs[\"dt\"] = 1 / fs_deci\n    output_data.attrs[\"nt\"] = data_out.shape[1]\n    output_data.attrs[\"nCh\"] = data_out.shape[0]\n    output_h5.close()\n    # print(ifile,'save', time() - t1)\n</pre> if not os.path.exists(output_preprocessed):     os.mkdir(output_preprocessed)  for ifile in tqdm(filelist):     outputname = os.path.join(         output_preprocessed,         ifile.split(\"_\")[-1]         .replace(\"-\", \"\")         .replace(\"T\", \"\")         .replace(\":\", \"\")         .replace(\"Z\", \"\")         .replace(\" \", \"\")         .replace(\"T\", \"\")         .replace(\":\", \"\")[-17:],     )     # try not overlap     # if os.path.exists(outputname):     #     print(outputname, \"exists\")     #     continue      fid = h5py.File(ifile, \"r\")     fs_data = fid[\"Data\"].attrs[\"fs\"]     nt_data = fid[\"Data\"].attrs[\"nt\"]     if fs_data != fs:         print(f\"wrong fs: {ifile}\")         fid.close()         continue     if nt_data &lt; min_npts:         print(f\"too short file: {ifile}\")         fid.close()         continue      # t1 = time()      data = fid[\"Data\"][:]     nch = data.shape[0]     npts = data.shape[1]     fid.close()      # print('read', time() - t1)     # t1 = time()      nchunk = int(np.ceil(npts / min_npts))     out = Parallel(n_jobs=njobs)(         delayed(preprocess)(data[:, int(min_length * fs * i) : int(min_length * fs * (i + 1))], fs, f1, f2, Decimation)         for i in range(nchunk)     )     data_out = np.concatenate(out, axis=-1)     # print('parallel', time() - t1)      # t1 = time()     fs_deci = fs / Decimation     output_h5 = h5py.File(outputname, \"w\")     output_data = output_h5.create_dataset(\"Data\", data=data_out)     output_data.attrs[\"fs\"] = fs_deci     output_data.attrs[\"dt\"] = 1 / fs_deci     output_data.attrs[\"nt\"] = data_out.shape[1]     output_data.attrs[\"nCh\"] = data_out.shape[0]     output_h5.close()     # print(ifile,'save', time() - t1)"},{"location":"eps207-observational-seismology/earthquake/ambient_noise/PyCC_step2/","title":"PyCC step2","text":"In\u00a0[\u00a0]: Copied! <pre># This script does temporal normalization, spatial whitening, and cc in frequency domain\nimport glob\nimport os\nimport time\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom func_PyCC import *\n</pre> # This script does temporal normalization, spatial whitening, and cc in frequency domain import glob import os import time  import h5py import numpy as np import pandas as pd from tqdm import tqdm  from func_PyCC import * In\u00a0[\u00a0]: Copied! <pre>path_preprocessed = \"./preprocessed/\"\noutput_CC = \"./CC/\"\nif not os.path.exists(output_CC):\n    os.mkdir(output_CC)\n\n# starting and ending dates to compute daily stacked CC\ndates = [x.strftime(\"%Y%m%d\") for x in pd.date_range(start=\"6/15/2021\", end=\"6/15/2021\", freq=\"1D\")]\n\n# Now give the No. of channel pairs to calculate CC.\n# below is an example of common-shot\nnch = 1250\npair_channel1 = 500 * np.ones(nch, dtype=\"int\")\npair_channel2 = np.arange(nch)\nnpair = len(pair_channel1)\n#############\n\nf1, f2 = 0.1, 10  # frequency band in spectral whitening\nfs = 25  # sampling frequency\nwindow_freq = 0  # 0 means aggresive spectral whitening, otherwise running mean\nmax_lag = 30  # in sec, the time lag of the output CC\nnpts_lag = int(max_lag * fs)\nxcorr_seg = 40  # in sec, the length of the segment to compute CC, slightly larger than max_lag is good\nnpts_seg = int(xcorr_seg * fs)\n\nnpair_chunk = 2000  # depends on # of channels, sampling frequency, and xcorr_seg, needs to be adaptive\nnchunk = int(np.ceil(npair / npair_chunk))\n\ndevice = \"cuda:0\"  # GPU device, needs to be changed to multi\n</pre> path_preprocessed = \"./preprocessed/\" output_CC = \"./CC/\" if not os.path.exists(output_CC):     os.mkdir(output_CC)  # starting and ending dates to compute daily stacked CC dates = [x.strftime(\"%Y%m%d\") for x in pd.date_range(start=\"6/15/2021\", end=\"6/15/2021\", freq=\"1D\")]  # Now give the No. of channel pairs to calculate CC. # below is an example of common-shot nch = 1250 pair_channel1 = 500 * np.ones(nch, dtype=\"int\") pair_channel2 = np.arange(nch) npair = len(pair_channel1) #############  f1, f2 = 0.1, 10  # frequency band in spectral whitening fs = 25  # sampling frequency window_freq = 0  # 0 means aggresive spectral whitening, otherwise running mean max_lag = 30  # in sec, the time lag of the output CC npts_lag = int(max_lag * fs) xcorr_seg = 40  # in sec, the length of the segment to compute CC, slightly larger than max_lag is good npts_seg = int(xcorr_seg * fs)  npair_chunk = 2000  # depends on # of channels, sampling frequency, and xcorr_seg, needs to be adaptive nchunk = int(np.ceil(npair / npair_chunk))  device = \"cuda:0\"  # GPU device, needs to be changed to multi In\u00a0[\u00a0]: Copied! <pre>for idate in tqdm(dates):\n\n    ccall = np.zeros((npair, int(max_lag * fs * 2 + 1)))\n\n    output_file_tmp = f\"{output_CC}/{idate}.npy\"\n    if os.path.exists(output_file_tmp):\n        print(output_file_tmp)\n        continue\n\n    filelist = glob.glob(os.path.join(path_preprocessed, idate + \"*h5\"))\n    filelist.sort()\n    filelist = filelist[:1]\n    if len(filelist) == 0:\n        print(f\"{idate}: no file\")\n        continue\n    flag_mean = 0\n    # t1 = time.time()\n    for ifile in filelist:\n        fid = h5py.File(ifile, \"r\")\n        data = fid[\"Data\"][:]\n        fid.close()\n\n        nch = data.shape[0]\n        npts = data.shape[1]\n\n        npts = npts // npts_seg * npts_seg\n        if npts &lt; npts_seg:  # or nch!=nch_end:\n            continue\n\n        data = data[:, :npts]\n\n        nseg = int(npts / npts_seg)\n        flag_mean += nseg\n        # print(time.time() - t1)\n</pre> for idate in tqdm(dates):      ccall = np.zeros((npair, int(max_lag * fs * 2 + 1)))      output_file_tmp = f\"{output_CC}/{idate}.npy\"     if os.path.exists(output_file_tmp):         print(output_file_tmp)         continue      filelist = glob.glob(os.path.join(path_preprocessed, idate + \"*h5\"))     filelist.sort()     filelist = filelist[:1]     if len(filelist) == 0:         print(f\"{idate}: no file\")         continue     flag_mean = 0     # t1 = time.time()     for ifile in filelist:         fid = h5py.File(ifile, \"r\")         data = fid[\"Data\"][:]         fid.close()          nch = data.shape[0]         npts = data.shape[1]          npts = npts // npts_seg * npts_seg         if npts &lt; npts_seg:  # or nch!=nch_end:             continue          data = data[:, :npts]          nseg = int(npts / npts_seg)         flag_mean += nseg         # print(time.time() - t1) In\u00a0[\u00a0]: Copied! <pre>        for ichunk in range(nchunk):\n\n            ich1 = pair_channel1[npair_chunk * ichunk : npair_chunk * (ichunk + 1)]\n            ich2 = pair_channel2[npair_chunk * ichunk : npair_chunk * (ichunk + 1)]\n            data1 = torch.from_numpy(data[ich1, :].reshape(-1, npts_seg)).to(device)\n            data2 = torch.from_numpy(data[ich2, :].reshape(-1, npts_seg)).to(device)\n\n            whitening_params = [fs, window_freq, f1, f2]\n            # t1 = time.time()\n            cc = (\n                cross_correlation(data1, data2, is_spectral_whitening=True, whitening_params=whitening_params)\n                .cpu()\n                .numpy()\n            )\n            cc = np.sum(cc.reshape(len(ich1), nseg, -1), 1)\n            ccall[npair_chunk * ichunk : npair_chunk * (ichunk + 1), :] += cc[\n                :, npts_seg - npts_lag - 1 : npts_lag - npts_seg + 1\n            ]\n        # print(time.time() - t1)\n        # del data1, data2, cc\n    if flag_mean &gt; 0:\n        ccall /= flag_mean\n        # print('Cross correlation of', idate, time.time() - t1)\n        np.save(output_file_tmp, ccall)\n</pre>         for ichunk in range(nchunk):              ich1 = pair_channel1[npair_chunk * ichunk : npair_chunk * (ichunk + 1)]             ich2 = pair_channel2[npair_chunk * ichunk : npair_chunk * (ichunk + 1)]             data1 = torch.from_numpy(data[ich1, :].reshape(-1, npts_seg)).to(device)             data2 = torch.from_numpy(data[ich2, :].reshape(-1, npts_seg)).to(device)              whitening_params = [fs, window_freq, f1, f2]             # t1 = time.time()             cc = (                 cross_correlation(data1, data2, is_spectral_whitening=True, whitening_params=whitening_params)                 .cpu()                 .numpy()             )             cc = np.sum(cc.reshape(len(ich1), nseg, -1), 1)             ccall[npair_chunk * ichunk : npair_chunk * (ichunk + 1), :] += cc[                 :, npts_seg - npts_lag - 1 : npts_lag - npts_seg + 1             ]         # print(time.time() - t1)         # del data1, data2, cc     if flag_mean &gt; 0:         ccall /= flag_mean         # print('Cross correlation of', idate, time.time() - t1)         np.save(output_file_tmp, ccall) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nccall = np.load(output_file_tmp)\nvmax = np.percentile(np.abs(ccall), 99)\nplt.imshow(\n    # filter(ccall, 25, 1, 10),\n    ccall,\n    aspect=\"auto\",\n    vmax=vmax,\n    vmin=-vmax,\n    # extent=(-max_lag, max_lag, ccall.shape[0], 0),\n    cmap=\"RdBu\",\n)\nplt.colorbar()\nplt.savefig(\"cc.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n</pre>  import matplotlib.pyplot as plt  ccall = np.load(output_file_tmp) vmax = np.percentile(np.abs(ccall), 99) plt.imshow(     # filter(ccall, 25, 1, 10),     ccall,     aspect=\"auto\",     vmax=vmax,     vmin=-vmax,     # extent=(-max_lag, max_lag, ccall.shape[0], 0),     cmap=\"RdBu\", ) plt.colorbar() plt.savefig(\"cc.png\", dpi=300, bbox_inches=\"tight\") plt.show()"},{"location":"eps207-observational-seismology/earthquake/ambient_noise/func_PyCC/","title":"func PyCC","text":"In\u00a0[\u00a0]: Copied! <pre># This script contains function for filter, temporal normalization, spatial whitening,\n# and cross-correlation in frequency domain. Pytorch required.\n# Yan Yang July 2022\n\nimport numpy as np\nimport torch\nimport torch.fft\nfrom scipy.signal import butter, detrend, filtfilt, tukey\n</pre> # This script contains function for filter, temporal normalization, spatial whitening, # and cross-correlation in frequency domain. Pytorch required. # Yan Yang July 2022  import numpy as np import torch import torch.fft from scipy.signal import butter, detrend, filtfilt, tukey In\u00a0[\u00a0]: Copied! <pre>def filter(data, fs, f1, f2, alpha=0.05):\n    \"\"\"\n    :param data: shape: nch * npts\n    :param fs: sampling frequency\n    :param f1: low frequency\n    :param f2: high frequency\n    :param alpha: taper length\n    :return: filtered data\n    \"\"\"\n    window = tukey(data.shape[1], alpha=alpha)\n    passband = [f1 * 2 / fs, f2 * 2 / fs]\n    b, a = butter(2, passband, \"bandpass\")\n\n    dataf = filtfilt(b, a, data * window)\n    return dataf\n\n\ndef running_absolute_mean(trace, nwin):\n    \"\"\"\n    :param trace: 1d array shape: npts\n    :param nwin: # of points in moving window\n    :return: smoothed data\n    \"\"\"\n    npts = len(trace)\n    tmp = np.zeros(npts + 2 * nwin)\n    tmp[nwin:-nwin] = np.abs(trace)\n    tmp[:nwin] = tmp[nwin]\n    tmp[-nwin:] = tmp[-nwin - 1]\n    return trace / np.convolve(tmp, np.ones(nwin) / nwin, mode=\"same\")[nwin:-nwin]\n\n\ndef temporal_normalization(data, fs, window_time):\n    \"\"\"\n    running absolute mean normalization or one-bit, depending on window_time\n    :param data: shape: nch * npts\n    :param fs: sampling frequency\n    :param window_time: running window length, in seconds. recommended: half the longest period\n    :return: normalized data\n    \"\"\"\n    if window_time == 0:  # 1-bit\n        return np.sign(data)\n    else:\n        nwin = int(fs * window_time)\n        nch = data.shape[0]\n        for i in range(nch):\n            data[i, :] = running_absolute_mean(data[i, :], nwin)\n        return data\n\n\ndef spectral_whitening(rfftdata, df, window_freq, f1, f2):\n    \"\"\"\n    phase-only or running absolute mean spectral whitening, depending on window_freq\n    :param rfftdata: shape: nch * npts, !!torch.tensor!!\n    :param df: frequency interval\n    :param window_freq: running window length, in Hz.\n    :return: whitened spectra\n    \"\"\"\n    idxf1 = int(np.floor(f1 / df))\n    idxf2 = int(np.ceil(f2 / df))\n    rfftdata_angle = torch.angle(rfftdata)\n\n    if window_freq == 0:  # phase-only\n        rfftdata = torch.exp(1j * rfftdata_angle)\n\n    else:  # running absolute mean\n        nwin = int(window_freq / df)\n        nch = rfftdata.shape[0]\n        for i in range(nch):\n            rfftdata[i, :] = torch.from_numpy(running_absolute_mean(rfftdata[i, :].cpu().numpy(), nwin))\n\n    rfftdata[:, :idxf1] = (\n        torch.cos(torch.linspace(np.pi / 2, np.pi, idxf1, device=rfftdata.device)) ** 2 * rfftdata[:, :idxf1]\n    )\n    rfftdata[:, idxf2:] = (\n        torch.cos(torch.linspace(np.pi, np.pi / 2, rfftdata.shape[-1] - idxf2, device=rfftdata.device)) ** 2\n        * rfftdata[:, idxf2:]\n    )\n\n    return rfftdata\n\n\ndef nextpow2(i):\n    n = 1\n    while n &lt; i:\n        n *= 2\n    return n\n\n\ndef cross_correlation(signal_1, signal_2, is_spectral_whitening=False, whitening_params=0):\n    \"\"\"\n    :param signal_1: data1: shape: nch * npts\n    :param signal_2: data2: shape: nch * npts\n    :param spectral_whitening: whether to apply spectral whitening\n    :param whitening_params: fs, window_freq, f1, f2\n    :return: CC: nch * (2*npts-1)\n    \"\"\"\n    if len(signal_1.shape) &lt; 2 | len(signal_2.shape) &lt; 2:\n        print(\"input dimension must be ntrace*npts !\")\n        return 0\n    else:\n        signal_length = signal_1.shape[-1]\n        x_cor_sig_length = signal_length * 2 - 1\n        fast_length = nextpow2(x_cor_sig_length)\n\n        fft_1 = torch.fft.rfft(signal_1, fast_length, dim=-1)\n        fft_2 = torch.fft.rfft(signal_2, fast_length, dim=-1)\n\n        if is_spectral_whitening:\n            fs, window_freq, f1, f2 = whitening_params\n            df = fs / fast_length\n            fft_1 = spectral_whitening(fft_1, df, window_freq, f1, f2)\n            fft_2 = spectral_whitening(fft_2, df, window_freq, f1, f2)\n\n        # take the complex conjugate of one of the spectrums. Which one you choose depends on domain specific conventions\n        fft_multiplied = torch.conj(fft_1) * fft_2\n\n        # back to time domain.\n        prelim_correlation = torch.fft.irfft(fft_multiplied, dim=-1)\n\n        # shift the signal to make it look like a proper crosscorrelation,\n        # and transform the output to be purely real\n        final_result = torch.roll(prelim_correlation, fast_length // 2, dims=-1)[\n            :, fast_length // 2 - x_cor_sig_length // 2 : fast_length // 2 - x_cor_sig_length // 2 + x_cor_sig_length\n        ]\n        return final_result\n</pre> def filter(data, fs, f1, f2, alpha=0.05):     \"\"\"     :param data: shape: nch * npts     :param fs: sampling frequency     :param f1: low frequency     :param f2: high frequency     :param alpha: taper length     :return: filtered data     \"\"\"     window = tukey(data.shape[1], alpha=alpha)     passband = [f1 * 2 / fs, f2 * 2 / fs]     b, a = butter(2, passband, \"bandpass\")      dataf = filtfilt(b, a, data * window)     return dataf   def running_absolute_mean(trace, nwin):     \"\"\"     :param trace: 1d array shape: npts     :param nwin: # of points in moving window     :return: smoothed data     \"\"\"     npts = len(trace)     tmp = np.zeros(npts + 2 * nwin)     tmp[nwin:-nwin] = np.abs(trace)     tmp[:nwin] = tmp[nwin]     tmp[-nwin:] = tmp[-nwin - 1]     return trace / np.convolve(tmp, np.ones(nwin) / nwin, mode=\"same\")[nwin:-nwin]   def temporal_normalization(data, fs, window_time):     \"\"\"     running absolute mean normalization or one-bit, depending on window_time     :param data: shape: nch * npts     :param fs: sampling frequency     :param window_time: running window length, in seconds. recommended: half the longest period     :return: normalized data     \"\"\"     if window_time == 0:  # 1-bit         return np.sign(data)     else:         nwin = int(fs * window_time)         nch = data.shape[0]         for i in range(nch):             data[i, :] = running_absolute_mean(data[i, :], nwin)         return data   def spectral_whitening(rfftdata, df, window_freq, f1, f2):     \"\"\"     phase-only or running absolute mean spectral whitening, depending on window_freq     :param rfftdata: shape: nch * npts, !!torch.tensor!!     :param df: frequency interval     :param window_freq: running window length, in Hz.     :return: whitened spectra     \"\"\"     idxf1 = int(np.floor(f1 / df))     idxf2 = int(np.ceil(f2 / df))     rfftdata_angle = torch.angle(rfftdata)      if window_freq == 0:  # phase-only         rfftdata = torch.exp(1j * rfftdata_angle)      else:  # running absolute mean         nwin = int(window_freq / df)         nch = rfftdata.shape[0]         for i in range(nch):             rfftdata[i, :] = torch.from_numpy(running_absolute_mean(rfftdata[i, :].cpu().numpy(), nwin))      rfftdata[:, :idxf1] = (         torch.cos(torch.linspace(np.pi / 2, np.pi, idxf1, device=rfftdata.device)) ** 2 * rfftdata[:, :idxf1]     )     rfftdata[:, idxf2:] = (         torch.cos(torch.linspace(np.pi, np.pi / 2, rfftdata.shape[-1] - idxf2, device=rfftdata.device)) ** 2         * rfftdata[:, idxf2:]     )      return rfftdata   def nextpow2(i):     n = 1     while n &lt; i:         n *= 2     return n   def cross_correlation(signal_1, signal_2, is_spectral_whitening=False, whitening_params=0):     \"\"\"     :param signal_1: data1: shape: nch * npts     :param signal_2: data2: shape: nch * npts     :param spectral_whitening: whether to apply spectral whitening     :param whitening_params: fs, window_freq, f1, f2     :return: CC: nch * (2*npts-1)     \"\"\"     if len(signal_1.shape) &lt; 2 | len(signal_2.shape) &lt; 2:         print(\"input dimension must be ntrace*npts !\")         return 0     else:         signal_length = signal_1.shape[-1]         x_cor_sig_length = signal_length * 2 - 1         fast_length = nextpow2(x_cor_sig_length)          fft_1 = torch.fft.rfft(signal_1, fast_length, dim=-1)         fft_2 = torch.fft.rfft(signal_2, fast_length, dim=-1)          if is_spectral_whitening:             fs, window_freq, f1, f2 = whitening_params             df = fs / fast_length             fft_1 = spectral_whitening(fft_1, df, window_freq, f1, f2)             fft_2 = spectral_whitening(fft_2, df, window_freq, f1, f2)          # take the complex conjugate of one of the spectrums. Which one you choose depends on domain specific conventions         fft_multiplied = torch.conj(fft_1) * fft_2          # back to time domain.         prelim_correlation = torch.fft.irfft(fft_multiplied, dim=-1)          # shift the signal to make it look like a proper crosscorrelation,         # and transform the output to be purely real         final_result = torch.roll(prelim_correlation, fast_length // 2, dims=-1)[             :, fast_length // 2 - x_cor_sig_length // 2 : fast_length // 2 - x_cor_sig_length // 2 + x_cor_sig_length         ]         return final_result"},{"location":"eps207-observational-seismology/lectures/00_introduction/","title":"Observational Seismology","text":""},{"location":"eps207-observational-seismology/lectures/00_introduction/#machine-learning-seismology","title":"(Machine Learning Seismology)","text":""},{"location":"eps207-observational-seismology/lectures/00_introduction/#large-destructive-earthquakes","title":"Large destructive earthquakes","text":"Year Magnitude MMI Deaths Injuries Event 2023 7.8 XII 57,350+ 130,000+ 2023 Turkey\u2013Syria earthquake 2011 9.1 IX 19,747 6,000 2011 T\u014dhoku earthquake and tsunami 2008 7.9 XI 87,587 374,177 2008 Sichuan earthquake"},{"location":"eps207-observational-seismology/lectures/00_introduction/#hayward-fault","title":"Hayward Fault","text":"<p>History</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#the-california-memorial-stadium","title":"The California Memorial Stadium","text":""},{"location":"eps207-observational-seismology/lectures/00_introduction/#an-estimated-magnitude-of-63-or-greater","title":"An estimated magnitude of 6.3 or greater.","text":"<p>Many more small earthquakes - California - Alaska - Hawaii - Oklahoma &amp; Texas</p> <p>Seismic Networks - California - Alaska - Hawaii - Oklahoma &amp; Texas</p> <p>GPS Networks</p>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#large-n-and-large-t-challenge","title":"Large-N and Large-T challenge","text":"<p>IRIS dataset </p>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#mining-the-iris-dataset","title":"Mining the IRIS dataset","text":""},{"location":"eps207-observational-seismology/lectures/00_introduction/#what-information-can-we-get-from-seismic-data","title":"What information can we get from seismic data?","text":"<ul> <li>Take a look at seismic waveforms: ncedc.org/waveformDisplay/</li> </ul> <p>Station Channel Codes </p> <p>Station Channels Codes IRIS</p> <p></p> <ul> <li>Can you find an earthquake?</li> </ul> <p>Raspberry Shake Network </p>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#what-information-can-we-get-from-seismic-data_1","title":"What information can we get from seismic data?","text":"<ul> <li>Take a look at a recent earthquake: M 5.1 - 7 km SE of Ojai, CA </li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#how-are-information-extracteddetermined","title":"How are information extracted/determined?","text":"<ul> <li>Detection of earthquakes</li> <li>Earthquake origin time and location</li> <li>Earthquake magnitude</li> <li>Earthquake focal mechanism/moment tensor</li> <li>Shake map/ground motion prediction</li> <li>Earthquake early warning</li> <li>\"Did you feel it?\"</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#what-additional-information-can-we-get-from-millions-of-earthquakes","title":"What additional information can we get from millions of earthquakes?","text":"<ul> <li>Earthquake catalog</li> <li>Earthquake statistics</li> <li>Earthquake triggering</li> <li>Earthquake forecasting</li> <li>Fault zone structure</li> <li>Seismic tomography</li> <li>Volcano, glacier, and landslide monitoring</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#how-to-use-these-information","title":"How to use these information?","text":"<ul> <li>Monitoring earthquakes and earthquake early warning</li> <li>Understand earthquake source physics</li> <li>Understanding the Earth's structure</li> <li>Applying seismology to environmental science, planetary science, climate science, etc.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#earthquake-monitoring-and-earthquake-rick","title":"Earthquake monitoring and earthquake rick?","text":"<ul> <li>Before an earthquake</li> <li>A few seconds after an earthquake</li> <li>Hours/days after an earthquake</li> <li>Years after an earthquake</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#before-an-earthquake","title":"Before an earthquake","text":"<ul> <li>Eathquake Hazard Map </li> </ul> <ul> <li>Simulating earthquake scenarios Hayward Fault Scenarios </li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#a-few-seconds-after-an-earthquake","title":"A few seconds after an earthquake","text":"<ul> <li> <p>MyShake https://myshake.berkeley.edu/</p> </li> <li> <p>Mobile phones as seismometers Android EEW </p> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#hoursdays-after-an-earthquake","title":"Hours/days after an earthquake","text":"<ul> <li>Emergency response and damage assessment Fault Dimensions</li> </ul> Magnitude Mw Fault Area km\u00b2 Typical rupture dimensions (km x km) 4 1 1 x 1 5 10 3 x 3 6 100 10 x 10 7 1,000 30 x 30 8 10,000 50 x 200 <ul> <li>Aftershock prediction</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#years-after-an-earthquake","title":"Years after an earthquake","text":"<ul> <li>Understand earthquake rupture process</li> <li>Improve ground motion prediction models (GMPE)</li> <li>Improve hazard map and building codes</li> <li>Earthquake forecasting models </li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#how-can-we-better-monitor-earthquakes","title":"How can we better monitor earthquakes?","text":"<p>Instrument side (How to collect more and better data?)</p> <ul> <li>Dense seismic networks</li> <li>New sensors: broadband seismometer, nodal array, and DAS (Distributed Acoustic Sensing)</li> <li>Remote sensing, LiDAR, etc.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#how-can-we-better-monitor-earthquakes_1","title":"How can we better monitor earthquakes?","text":"<p>Algorithm side (New techniques for processing data and extracting information?)</p> <ul> <li> <p>Many signal processing algorithms, such as, STA/LTA, template matching, filtering, etc.</p> </li> <li> <p>Machine learning &amp; deep learning</p> </li> <li> <p>Numerical simulation</p> </li> <li> <p>Inverse theory</p> </li> <li> <p>Statistical analysis</p> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#things-to-learn-in-this-course","title":"Things to learn in this course","text":"<ul> <li>Faimilar with seismic data</li> <li>Learn the state-of-the-art machine learning methods for seismic data processing</li> <li>Process seismic data, build seismic catalogs, and analyzing seismicity</li> <li>Learn basic inverse theory for earthquake location, focal mechanism, seismic tomography, etc.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#the-advantages-of-machine-learning","title":"The advantages of machine learning","text":"<p>Deep Learning (Deep Neural Networks) is a new paradigm of software development</p> <ul> <li> <p>Software 2.0</p> </li> <li> <p>Universal Approximation Theorem</p> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#applications-of-deep-learning-in-seismology","title":"Applications of deep learning in seismology","text":"<ul> <li>Neural Networks</li> <li>Automatic Differentiation</li> <li>Optimization/Inversion</li> </ul> <p>Machine Learning and Inversion</p> 09/18 Seismic Data Processing 09/25 Earthquake Detection 10/02 Phase Picking &amp; Association 10/09 Earthquake Location &amp; Relative Location 10/16 Focal Mechanism &amp; Moment Tensor 10/23 Earthquake Statistics 10/30 Ambient Noise 11/06 Seismic Tomography 11/13 Full-waveform Inversion"},{"location":"eps207-observational-seismology/lectures/00_introduction/#grading","title":"Grading","text":"<ul> <li>Attendance and participation (50%)</li> <li>Final project (50%)<ul> <li>Project proposal (10%)</li> <li>Project presentation (20%)</li> <li>Project report (20%)</li> </ul> </li> <li>Extra credit (up to 10%)</li> </ul>"},{"location":"eps207-observational-seismology/lectures/00_introduction/#questions","title":"Questions?","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/","title":"Earthquake Source and Seismic Wave","text":"<p>Textbooks:  Shearer, P. M. (2019). Introduction to seismology. Cambridge University Press. GEOPHYS 210: Earthquake Seismology by Eric Dunham Segall, P. (2010). Earthquake and volcano deformation. Princeton University Press.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-faults","title":"Earthquake faults","text":"<p>Earthquakes may be idealized as movement across a planar fault of arbitrary orientation</p> <ul> <li>strike: \\(\\phi\\), the azimuth of the fault from north where it intersects a horizontal surface \\(0^\\circ \\leq \\phi \\leq 360^\\circ\\)</li> <li>dip: \\(\\delta\\), the angle from the horizontal \\(0^\\circ \\leq \\delta \\leq 90^\\circ\\)</li> <li>rake: \\(\\lambda\\), the angle between the slip vector and the strike \\(0^\\circ \\leq \\lambda \\leq 360^\\circ\\)</li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-faults_1","title":"Earthquake faults","text":"<p>Thrust faulting: reverse faulting on faults with dip angles less than 45  Overthrust faults: Nearly horizontal thrust faults Strike-slip faulting: horizontal motion between the fault surfaces Dip-slip faulting: vertical motion Right-lateral strike\u2013slip motion: standing on one side of a fault, sees the adjacent block move to the right \\(\\lambda = 0^\\circ\\): left-lateral faulting \\(\\lambda = 180^\\circ\\): right-lateral faulting The San Andreas Fault: Right-lateral fault</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-double-couple","title":"Earthquake double couple","text":"<ul> <li>An earthquake is usually modeled as slip on a fault, a discontinuity in displacement across an internal surface in the elastic media.</li> <li>Internal forces resulting from an explosion or stress release on a fault must act in opposing directions so as to conserve momentum.</li> <li>A force couple is a pair of opposing point forces separated by a small distance </li> <li>A double couple is a pair of complementary couples that produce no net torque</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor","title":"Moment tensor","text":"<p>We define the force couple \\(M_{ij}\\) as a pair of equal and opposite forces pointing in the \\(i\\) direction and separated by a unit distance in the \\(j\\) direction. </p> <p>The magnitude of \\(M_{ij}\\) is the product of the force and the distance \\(f \\times d\\).</p> \\[ M_{ij} = \\begin{bmatrix}     M_{11} &amp; M_{12} &amp; M_{13} \\\\     M_{21} &amp; M_{22} &amp; M_{23} \\\\     M_{31} &amp; M_{32} &amp; M_{33} \\end{bmatrix} \\] <p>The condition that angular momentum be conserved requires that is symmetric (e.g., \\(M_{ij} = M_{ji}\\)).</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor_1","title":"Moment tensor","text":"<p>For example, right-lateral movement on a vertical fault oriented in the \\(x_1\\) direction corresponds to the moment tensor representation</p> \\[ M = \\begin{bmatrix}     M_{11} &amp; M_{12} &amp; M_{13} \\\\     M_{21} &amp; M_{22} &amp; M_{23} \\\\     M_{31} &amp; M_{32} &amp; M_{33} \\end{bmatrix} = \\begin{bmatrix}     0 &amp; M_0 &amp; 0 \\\\     M_0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 \\end{bmatrix} \\] <p>where \\(M_0\\) is the scalar seismic moment: </p> \\[ M_0 = \\mu d A \\] <p>where \\(\\mu\\) is the shear modulus, \\(d\\) is the average fault displacement, and \\(A\\) is the area of the fault.</p> <p>The units for \\(M_0\\) are N\\(\\cdot\\)m (or dyne\\(\\cdot\\)cm), the same as for force couples.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#global-cmt-catalog","title":"Global CMT catalog","text":"<p>Global Centroid Moment Tensor</p> <p></p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#beach-balls","title":"Beach balls","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor_2","title":"Moment tensor","text":"<p>Because \\(M_{ij} = M_{ji}\\), there are two fault planes that correspond to a double-couple model.  In general, there are two fault planes that are consistent with distant seismic observations in the double-couple model. The primary fault plane: The real fault plane The auxiliary fault plane: The other plane</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#eigenvectors","title":"Eigenvectors","text":"<p>The moment tensor \\(M_{ij}\\) is a symmetric tensor, so it has three real eigenvalues and three orthogonal eigenvectors.</p> <p>Tension axis \\(T\\) Pressure axis \\(P\\)</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#non-double-couple-sources","title":"Non-double couple sources","text":"<p>Isotropic part of \\(M\\): \\(M^{o} = \\frac{1}{3} \\text{tr}(M) I\\)</p> <p>$$ M^{o} = \\begin{bmatrix}     M_{11} &amp; 0 &amp; 0 \\     0 &amp; M_{22} &amp; 0 \\     0 &amp; 0 &amp; M_{33} \\end{bmatrix} $$ where \\(M_{11} = M_{22} = M_{33}\\)</p> <p>Decomposing \\(M\\) into isotropic and deviatoric parts: \\(M = M^{o} + M'\\) where \\(\\text{tr}(M') = 0\\), free from isotropic sources by may contain non-double couple sources</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#non-double-couple-sources_1","title":"Non-double couple sources","text":"<p>Diagonalize \\(M'\\) by rotating to coordinates of principal axes:</p> <p>$$ M' = \\begin{bmatrix}     \\sigma_1 &amp; 0 &amp; 0 \\     0 &amp; \\sigma_2 &amp; 0 \\     0 &amp; 0 &amp; \\sigma_3 \\end{bmatrix} $$ where \\(\\sigma_1 \\geq \\sigma_2 \\geq \\sigma_3\\).</p> <p>Because \\(tr(M') = 0\\), \\(\\sigma_1 + \\sigma_2 + \\sigma_3 = 0\\).  For a pure double couple source, \\(\\sigma_2 = 0\\) and \\(\\sigma_1 = -\\sigma_3\\).</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#non-double-couple-sources_2","title":"Non-double couple sources","text":"<p>We can decompose \\(M'\\) into a best-fitting double couple \\(M^{DC}\\) and a non-double couple part \\(M^{CLVD}\\):</p> \\[ \\begin{aligned} M' &amp;= M^{DC} + M^{CLVD} \\\\ &amp;= \\begin{bmatrix}     (\\sigma_1 - \\sigma_3)/2 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; -(\\sigma_1 - \\sigma_3)/2 \\end{bmatrix}  + \\begin{bmatrix}     -\\sigma_2/2 &amp; 0 &amp; 0 \\\\     0 &amp; \\sigma_2 &amp; 0 \\\\     0 &amp; 0 &amp; -\\sigma_2/2 \\end{bmatrix} \\end{aligned} \\] <p>The complete decomposition of the original \\(M\\) is:</p> \\[ M = M^{o} + M^{DC} + M^{CLVD} \\]"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#non-double-couple-sources_3","title":"Non-double couple sources","text":"<p>Example of the decomposition of a moment tensor into isotropic, best-\ufb01tting double couple, and compensated linear vector dipole terms:</p> <p></p> <p>The decomposition of \\(M'\\) into \\(M^{DC}\\) and \\(M^{CLVD}\\) is unique because we have defined \\(M^{DC}\\) as the best-fitting double couple, i.e., minimizing the CLVD part.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#non-double-couple-sources_4","title":"Non-double couple sources","text":"<p>A measure of the mis\ufb01t between \\(M'\\) and a pure double-couple source is provided by the ratio \\(\\sigma_2\\) to the remaining eigenvalue with the largest magnitude to the largest eigenvalue of \\(M'\\)</p> \\[ \\epsilon = \\frac{|\\sigma_2|}{\\max(|\\sigma_1|, |\\sigma_3|)} \\] <p>\\(\\epsilon = 0\\): pure double couple \\(\\epsilon = \\pm 0.5\\): close to pure CLVD</p> <p>Physically, non-double-couple components can arise from simultaneous faulting on faults of different orientations or on a curved fault surface.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#greens-function","title":"Green's function","text":"<p>Review:</p> <p>The momentum equation:</p> \\[ \\rho \\frac{\\partial^2 u}{\\partial t^2} = \\partial_i \\tau_{ij} + f_i \\] <p>The stress-strain relation:</p> \\[ \\tau_{ij} = \\lambda \\delta_{ij} \\partial_k u_k + 2 \\mu \\varepsilon_{ij} \\] <p>The strain:</p> \\[ \\varepsilon_{ij} = \\frac{1}{2} \\left( \\partial_i u_j + \\partial_j u_i \\right) \\]"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#greens-function_1","title":"Green's function","text":"<p>We consider a unit impulse source \\(f_i(x_0, t_0) = \\delta(t - t_0) \\delta(x-x_0)\\) at point \\(x_0\\) at time \\(t_0\\).</p> <p>The unit force function is a useful concept because more realistic sources can be described as a sum of these force vectors.</p> <p>For every \\(f(x_0, t_0)\\), there is a unique \\(u(x, t)\\) that describes the Earth\u2019s response, which could be computed if we knew the Earth\u2019s structure to suf\ufb01cient accuracy.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#greens-function_2","title":"Green's function","text":"<p>We define the notation:</p> <p>$$ u_i(x, t) =  G_{ij}(x, t; x_0, t_0) f_j(x_0, t_0) $$ where \\(G_{ij}\\) is the Green's function, \\(u_i\\) is the displacement, and \\(f_j\\) is the force.</p> <p>Assuming that \\(G_{ij}\\) can be computed, the displacement resulting from any body force distribution can be computed as the sum or superposition of the solutions for the individual point sources.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#greens-function-moment-tensor","title":"Green's function + Moment tensor","text":"<p>The moment tensor provides a general representation of the internally generated forces that can act at a point in an elastic medium. Although it is an idealization, it has proven to be a good approximation for modeling the distant seismic response for sources that are small compared to the observed seismic wavelengths.</p> <p>So the displacement \\(u_i\\) resulting from a force couple at \\(x_0\\) is given by:</p> \\[ \\begin{aligned} u_i(x, t) &amp;= G_{ij}(x, t; x_0, t_0) f_j(x_0, t_0) - G_{ij}(x, t; x_0-\\hat{d}, t_0) f_j(x_0, t_0) \\\\ &amp;= \\frac{\\partial G_{ij}(x, t; x_0, t_0)}{\\partial x_{0k}} \\hat{d}_k f_j(x_0, t_0) \\\\ u_i(x, t) &amp;= \\frac{\\partial G_{ij}(x, t; x_0, t_0)}{\\partial x_{0k}} M_{jk} \\end{aligned} \\]"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-patterns","title":"Radiation patterns","text":"<p>Solving for the Green's function is rather complicated. Here we consider the simple case of a spherical wavefront from an isotropic source. Review: The solution for the P-wave potential: $$ \\phi(r, t) = \\frac{-f(t-r/\\alpha)}{r} $$ where \\(\\alpha\\) is the P-wave velocity, \\(r\\) is the distance from the point source, and \\(4\\pi\\delta(r)f(t)\\) is the source-time function.</p> <p>The displacement field = the gradient of the displacement potential: $$ u(r, t) = \\frac{\\partial \\phi(r, t)}{\\partial r} = \\frac{1}{r^2} f(t-r/\\alpha) - \\frac{1}{r} \\frac{\\partial f(t-r/\\alpha)}{\\partial r} $$</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-patterns_1","title":"Radiation patterns","text":"<p>Define \\(\\tau = t - r/\\alpha\\) as the delay time: $$ \\frac{\\partial f(t-r/\\alpha)}{\\partial r} = \\frac{\\partial f(t-r/\\alpha)}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial r} = -\\frac{1}{\\alpha} \\frac{\\partial f(t - r/\\alpha)}{\\partial \\tau}  $$ So the displacement field is: $$ u(r, t) = \\frac{1}{r^2} f(t-r/\\alpha) + \\frac{1}{r\\alpha} \\frac{\\partial f(t-r/\\alpha)}{\\partial \\tau} $$ </p> <p>The first term decays as \\(1/r^2\\), and is called the near-field term, which represents the permanent static displacement due to the source The second term decays as \\(1/r\\), and is called the far-field term, which represents the dynamic response (the transient seismic waves radiated by the source that cause no permanent displacement)</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-patterns_2","title":"Radiation patterns","text":"\\[ u(r, t) = \\frac{1}{r^2} f(t-r/\\alpha) + \\frac{1}{r\\alpha} \\frac{\\partial f(t-r/\\alpha)}{\\partial \\tau} \\] <p>The first term decays as \\(1/r^2\\), and is called the near-field term, which represents the permanent static displacement due to the source</p> <p>The second term decays as \\(1/r\\), and is called the far-field term, which represents the dynamic response - the transient seismic waves radiated by the source.  These waves cause no permanent displacement, and their displacements are given by the first time derivative of the source-time function.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-patterns_3","title":"Radiation patterns","text":"<p>Most seismic observations are made at suf\ufb01cient distance from faults that only the far-\ufb01eld terms are important. Consider the far-\ufb01eld P-wave displacement for a double-couple source, assuming the fault is in the \\(x_1, x_2\\) plane with motion in the \\(x_1\\) direction:</p> \\[\\mathbf{u}^P=\\frac{1}{4 \\pi \\rho \\alpha^3} \\sin 2 \\theta \\cos \\phi \\frac{1}{r} \\dot{M}_0\\left(t-\\frac{r}{\\alpha}\\right) \\hat{\\mathbf{r}}\\] <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-far-field-radiation-pattern-for-p-waves","title":"The far-\ufb01eld radiation pattern for P-waves","text":"<ul> <li>The fault plane and the auxiliary fault plane form nodal lines of zero motion. </li> <li>The outward pointing vectors in the compressional quadrant. </li> <li>The inward pointing vectors in the dilatational quadrant. </li> <li>The tension (T axis) is in the middle of the compressional quadrant; </li> <li>The pressure (P axis) is in the middle of the dilatational quadrant.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-far-field-radiation-pattern-for-s-waves","title":"The far-\ufb01eld radiation pattern for S-waves","text":"<p>The far-\ufb01eld S displacements: \\(\\(\\mathbf{u}^S(\\mathbf{x}, t)=\\frac{1}{4 \\pi \\rho \\beta^3}(\\cos 2 \\theta \\cos \\phi \\hat{\\boldsymbol{\\theta}}-\\cos \\theta \\sin \\phi \\hat{\\boldsymbol{\\phi}}) \\frac{1}{r} \\dot{M}_0\\left(t-\\frac{r}{\\beta}\\right)\\)\\) where \\(\\beta\\) is the S-wave velocity.</p> <p>There are no nodal planes, but there are nodal points.  S-wave polarities generally point toward the T axis and away from the P axis.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#beach-balls_1","title":"Beach balls","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#plotting-beach-balls","title":"Plotting beach balls","text":"<p>Projection of the focal sphere onto an equal-area lower-hemisphere map. The numbers around the outside circle show the fault strike in degrees. The circles show fault dip angles with 0\u00b0 dip (a horizontal fault) to 90\u00b0 dip (a vertical fault). The curved line ABC shows the intersection of the fault with the focal sphere.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#review-basic-types-of-faulting","title":"Review: Basic types of faulting","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#basic-types-of-faulting","title":"Basic types of faulting","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#body-force","title":"Body force","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#first-motion-polarity","title":"First-motion polarity","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-pattern","title":"Radiation pattern","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-pattern_1","title":"Radiation pattern","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism_1","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism_2","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism_3","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism_4","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism_5","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#pt-axes-vs-compressionaldilatational-quadrants","title":"P/T axes v.s. Compressional/Dilatational quadrants","text":"<p>The Pressure/Tension (P/T) axes are defined inside the beach ball; The Compressional/Dilatational quadrants are defined outside the beach ball;</p> <p>The tension (T axis) is in the middle of the compressional quadrant; The pressure (P axis) is in the middle of the dilatational quadrant.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor-and-beach-ball","title":"Moment tensor and Beach ball","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor-and-radiation-pattern","title":"Moment tensor and Radiation pattern","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor-decomposition","title":"Moment tensor decomposition","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#magnitude-m_0","title":"Magnitude \\(M_0\\)","text":"<p>The magnitude of the equivalent body forces is \\(M_0\\) The scalar seismic moment of the earthquake; units of dyn-cm, or N-m</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#time-dependent-seismic-moment-mt","title":"Time-dependent seismic moment \\(M(t)\\)","text":"<p>The Haskell source model</p> <p></p> <p>The rise time: \\(\\tau_r\\) describes the slip duration at any point on a fault.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#rupture-propagation","title":"Rupture propagation","text":"<p>For a long, narrow fault, we assume that the rupture propagates along the fault of length \\(L\\) from left to right at a rupture velocity \\(v_r\\) The total duration \\(\\tau_d\\) of the rupture is \\(T = L/v_r\\)</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#far-field-the-apparent-rupture-time","title":"Far field the apparent rupture time","text":"<p>The apparent rupture duration time \\(\\tau_{\\alpha}\\) for P-waves: Rupturing directly toward us: \\(\\tau_{\\alpha}(\\text{toward}) = L \\left( \\frac{1}{v_r} - \\frac{1}{\\alpha} \\right)\\) Rupturing directly away from us: \\(\\tau_{\\alpha}(\\text{away}) = L \\left( \\frac{1}{v_r} + \\frac{1}{\\alpha} \\right)\\)</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#doppler-effect","title":"Doppler effect","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#general-apparent-rupture-duration","title":"General apparent rupture duration","text":"<p>The apparent rupture duration for a seismic phase with local horizontal phase velocity \\(c\\) at the observing station as $$ \\tau_c(\\theta) = L \\left( \\frac{1}{v_r} - \\frac{\\cos \\theta}{c} \\right) $$ where \\(\\theta\\) is the station azimuth relative to the rupture direction. </p> <p>The changes in \\(\\tau_c\\) as a function of receiver location are termed directivity effects.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#rupture-length","title":"Rupture Length","text":"<p>Since \\(\\tau_\\text{max} = L \\left( \\frac{1}{v_r} - \\frac{1}{c} \\right)\\) and \\(\\tau_\\text{min} = L \\left( \\frac{1}{v_r} + \\frac{1}{c} \\right)\\) The rupture length \\(L\\) is $$ L = \\frac{1}{2} (\\tau_\\text{max} - \\tau_\\text{min}){c} $$ The true rupture duration is  $$ \\tau_d = L/v_r = \\frac{1}{2} (\\tau_\\text{max} + \\tau_\\text{min}) $$ The average rupture velocity is $$ v_r = L/\\tau_d = \\frac{\\tau_\\text{max} - \\tau_\\text{min}}{\\tau_\\text{max} + \\tau_\\text{min}} c $$</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#example-2004-sumatra-earthquake-directivity","title":"Example: 2004 Sumatra Earthquake Directivity","text":"<p> High-frequency (2\u20134 Hz) envelopes from teleseismic P-wave observations of the 2004 Sumatra earthquake.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#example-2004-sumatra-earthquake-directivity_1","title":"Example: 2004 Sumatra Earthquake Directivity","text":"<p> We have \\(\\tau_{\\text{min}} \\simeq 400\\)s, \\(\\tau_{\\text{max}} \\simeq 600\\); and assume \\(c = 12.6 \\text{ km/s}\\), so: * \\(L = \\frac{1}{2} (\\tau_\\text{max} - \\tau_\\text{min}){c} = 1/2 \\times 200 \\times 12.6 = 1260\\) km * \\(\\tau_d = \\frac{1}{2} (\\tau_\\text{max} + \\tau_\\text{min}) = 500\\) s * \\(v_r = L/\\tau_d = 1260/500 = 2.52\\) km/s</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#rupture-velocity","title":"Rupture velocity","text":"<p>The rupture velocity is generally observed to be somewhat less than the shear-wave velocity for most earthquakes.  Anomalously fast ruptures sometimes exceed the local S-wave velocity and are termed supershear ruptures.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-rupture-simulation","title":"Earthquake rupture simulation","text":"<p>Eric Dunham</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-faults_2","title":"Earthquake faults","text":"<p>Earthquakes may be idealized as movement across a planar fault of arbitrary orientation</p> <ul> <li>strike: \\(\\phi\\), the azimuth of the fault from north where it intersects a horizontal surface \\(0^\\circ \\leq \\phi \\leq 360^\\circ\\)</li> <li>dip: \\(\\delta\\), the angle from the horizontal \\(0^\\circ \\leq \\delta \\leq 90^\\circ\\)</li> <li>rake: \\(\\lambda\\), the angle between the slip vector and the strike \\(0^\\circ \\leq \\lambda \\leq 360^\\circ\\)</li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-focal-mechanism_6","title":"Earthquake Focal Mechanism","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-tensor-decomposition_1","title":"Moment tensor decomposition","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-rupture","title":"Earthquake rupture","text":"<p>The Haskell source model</p> <p>\\(\\tau_r\\): the rise time, describes the slip duration at any point on a fault. \\(\\tau_d\\): the duration of the rupture (\\(L/v_r\\))</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-magnitude","title":"Earthquake Magnitude","text":"<p>How to quantify the size of an earthquake?</p> <ul> <li>For historical reasons the most well-known measure of earthquake size is the  earthquake magnitude.</li> <li>Derived from the largest amplitude that is recorded on  seismograms.</li> <li>There are now many different types of magnitude  scales, but all are connected in some way to the earliest definitions of  magnitude.</li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#richter-magnitude-local-magnitude-m_l","title":"Richter Magnitude (Local magnitude \\(M_L\\))","text":"<p>The original magnitude scale is based on the maximum amplitude recorded on a standard Wood-Anderson torsion seismograph.</p> <p>$$ M_L = \\log_{10} A(X) - \\log_{10} A_0(X) $$ \\(A_0\\): the amplitude of the reference event \\(X\\): the epicentral distance</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#richter-magnitude-local-magnitude-m_l_1","title":"Richter Magnitude (Local magnitude \\(M_L\\))","text":"<p>An approximate empirical formula has been derived for \\(\\log_{10} A_0(X)\\) at different ranges.  The local magnitude can be calculated by $$ M_L = \\log_{10} A(X) + 2.56 \\log_{10} X - 1.67 $$ where \\(A(X)\\) is the displacement amplitude in microns (10\\(^{-6}\\) m) and X is in  kilometers.</p> <ul> <li>Events below about \\(M_L 3\\) are generally not felt</li> <li>Significant damage to structures in California begins to  occur at about \\(M_L 5.5\\)</li> <li>A \\(M_L 6.0\\) earthquake implies amplitude 100 times greater than a \\(M_L 4.0\\) event.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#global-earthquakes-body-wave-magnitude-m_b","title":"Global earthquakes: body wave magnitude \\(m_b\\)","text":"\\[ m_b = \\log_{10} (A/T) + Q(h, \\Delta) \\] <p>where A is the ground displacement in microns, T is the dominant period of  the measured waves, \\(\\Delta\\) is the epicentral distance in degrees, and Q is an  empirical function of range and event depth h.</p> <ul> <li>Why \\(A/T\\)?</li> <li>h?</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#global-earthquakes-surface-wave-magnitude-m_s","title":"Global earthquakes: surface wave magnitude \\(M_s\\)","text":"<p>For Rayleigh waves on vertical instruments: $$ M_s = \\log_{10} (A/T) + 1.66 \\log_{10} \\Delta + 3.30 $$</p> <p>Since the strongest Rayleigh wave arrivals are generally at a period of 20 s, this expression is  often written as $$ M_s = \\log_{10} A_{20} + 2.46 \\log_{10} \\Delta + 2.0 $$</p> <ul> <li>Note that this equation is applicable only to shallow events</li> <li>surface wave amplitudes are greatly reduced for deep events.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#magnitude-saturation","title":"Magnitude saturation","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-haskell-fault-model","title":"The Haskell fault model","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#source-spectra","title":"Source spectra","text":"<p>A boxcar pulse in the time domain produces a sinc function in  the frequency domain: \\(\\text{sinc}(x) = \\frac{\\sin(x)}{x}\\)</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#source-spectra_1","title":"Source spectra","text":"<p>The far-field amplitude spectrum for the Haskell fault model  may be expressed as</p> \\[ |A(\\omega)|=g M_0\\left|\\operatorname{sinc}\\left(\\omega \\tau_r / 2\\right)\\right|\\left|\\operatorname{sinc}\\left(\\omega \\tau_d / 2\\right)\\right|, \\] <p>where \\(g\\) is a scaling term that includes geometrical spreading, etc</p> \\[ \\log |A(\\omega)|=G+\\log \\left(M_0\\right)+\\log \\left|\\operatorname{sinc}\\left(\\omega \\tau_r / 2\\right)\\right|+\\log \\left|\\operatorname{sinc}\\left(\\omega \\tau_d / 2\\right)\\right| \\] <p>where \\(G=\\log g\\). </p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#source-spectra_2","title":"Source spectra","text":"<p>We can approximate \\(\\left|\\operatorname{sinc} x\\right| \\simeq 1\\) for \\(x&lt;1\\) and \\(1/x\\) for \\(x&gt;1\\)</p> \\[ \\log |A(\\omega)|-G = \\log \\left(M_0\\right)+\\log \\left|\\operatorname{sinc}\\left(\\omega \\tau_r / 2\\right)\\right|+\\log \\left|\\operatorname{sinc}\\left(\\omega \\tau_d / 2\\right)\\right| \\] \\[ \\begin{aligned} \\log |A(\\omega)|-G &amp; =\\log M_0, &amp; &amp; \\omega&lt;2 / \\tau_d \\\\ &amp; =\\log M_0-\\log \\frac{\\tau_d}{2}-\\log \\omega, &amp; &amp; 2 / \\tau_d&lt;\\omega&lt;2 / \\tau_r \\\\ &amp; =\\log M_0-\\log \\frac{\\tau_d \\tau_r}{4}-2 \\log \\omega, &amp; &amp; 2 / \\tau_r&lt;\\omega\\end{aligned} \\]"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-amplitude-spectrum-for-the-haskell-fault-model","title":"The amplitude spectrum for the Haskell fault model.","text":"\\[ \\begin{aligned} \\log |A(\\omega)|-G &amp; =\\log M_0, &amp; &amp; \\omega&lt;2 / \\tau_d \\\\ &amp; =\\log M_0-\\log \\frac{\\tau_d}{2}-\\log \\omega, &amp; &amp; 2 / \\tau_d&lt;\\omega&lt;2 / \\tau_r \\\\ &amp; =\\log M_0-\\log \\frac{\\tau_d \\tau_r}{4}-2 \\log \\omega, &amp; &amp; 2 / \\tau_r&lt;\\omega\\end{aligned} \\]"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#magnitude-saturation_1","title":"Magnitude saturation","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#moment-magnitude-m_w","title":"Moment magnitude \\(M_w\\)","text":"<p>The saturation of the and scales for large events helped motivate  development of the moment magnitude \\(M_w\\)</p> <p>$$ M_w = \\frac{2}{3} (\\log_{10} M_0 - 9.1) $$ where is the moment measured in N-m.</p> <ul> <li>The advantage of the scale is that it is clearly related to a physical property of the source and it does not saturate for even the largest  earthquakes.</li> <li>One unit increase in \\(M_w\\) corresponds to a \\(10^{3/2} \\approx 32\\) times increase in the moment.</li> <li>A \\(M_w 7\\) earthquake releases about 1000 times more energy than a \\(M_w 5\\) event.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#magnitude-as-a-function-of-moment","title":"Magnitude as a function of moment","text":"<p>USGS Magnitude Types; Latest earthquake</p> <p></p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-intensity-scale","title":"The intensity scale","text":"<p>The local strength of ground shaking as determined by damage to  structures and the perceptions of people who experienced the earthquake.</p> <ul> <li>One earthquake can have different intensities at different locations.</li> </ul> <p>USGS Latest Earthquakes</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-mercalli-intensity-scale-mmi","title":"The Mercalli intensity scale (MMI)","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#jma-intensity-scale","title":"JMA intensity scale","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#the-earthquake-cycle","title":"The Earthquake Cycle","text":"<ul> <li>Elastic rebound</li> </ul>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#spring-block-model-notebook","title":"Spring-block model (notebook)","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#spring-block-model","title":"Spring-block model","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-recurrence-model","title":"Earthquake recurrence model","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#stress-drop","title":"Stress drop","text":"<p>Moment \\(M_0 = \\mu A D\\)</p> <p>Stress drop \\(\\Delta \\sigma = \\sigma_\\text{final} - \\sigma_\\text{initial}\\)</p> <p>large D \\(\\times\\) small A v.s. small D \\(\\times\\) large A? </p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#circular-crack-model","title":"Circular crack model","text":"<p>Moment </p> \\[M_0 = \\mu A D = \\mu \\pi r^2 D\\] <p>Stress drop </p> \\[\\Delta \\sigma = \\frac{7 \\pi \\mu D}{16 r} = \\frac{7 M_0}{16 r^3}\\]"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#far-field-seismic-pulse","title":"Far-field seismic pulse","text":"<p>Area under displacement pulse \\(f(h\\tau)\\) is related to seismic moment \\(M_0\\)</p> <p>Radiation patterns, e.g.: \\(\\(\\mathbf{u}^P(t)=\\frac{1}{4 \\pi \\rho \\alpha^3} \\sin 2 \\theta \\cos \\phi \\frac{1}{r} \\dot{M}_0\\left(t-\\frac{r}{\\alpha}\\right) \\hat{\\mathbf{r}}\\)\\)</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#far-field-seismic-pulse-shape","title":"Far-field seismic pulse shape","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#self-similar-earthquake-scaling","title":"Self-Similar Earthquake Scaling","text":"<p>Assuming dimensions are scaled proportionally, displacement D will increase by b</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#self-similar-earthquake-scaling_1","title":"Self-Similar Earthquake Scaling","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#self-similar-earthquake-scaling_2","title":"Self-Similar Earthquake Scaling","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#self-similar-earthquake-scaling_3","title":"Self-similar Earthquake Scaling","text":"<p>Are different sized events self-similar?</p> <ul> <li> <p>Self-similarity predicts smaller events are \u201cshifted\u201d versions of larger events.</p> </li> <li> <p>Following \\(\\omega^{-3}\\) for self-similarity</p> </li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#self-similar-earthquake-scaling_4","title":"Self-similar Earthquake Scaling","text":"<ul> <li> <p>Stacking spectra following \\(\\omega^{-3}\\), spectral shapes are similar within uncertainties.</p> </li> <li> <p>Self-similarity implies that apparent stress is size independent</p> </li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#brune-model-1970","title":"Brune model (1970)","text":"\\[u(f) = \\frac{M_0}{1 + (f/f_c)^2}\\] <p>From model fit obtained:</p> <ul> <li>Corner frequency (\\(f_c\\))</li> <li>Radiated energy (\\(E_s\\))</li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#corner-frequency","title":"Corner frequency","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#far-field-seismic-pulse-shape_1","title":"Far-field seismic pulse shape","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-energy-partitioning","title":"Earthquake Energy Partitioning","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-energy-partitioning_1","title":"Earthquake Energy Partitioning","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#earthquake-scaling-static-and-dynamic","title":"Earthquake Scaling: static and dynamic","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#stess-and-radiated-energy","title":"Stess and Radiated Energy","text":"<p>Is the stress drop close to the absolute stress level (weak faults) or just a small portion (strong faults)?</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#radiation-efficiency","title":"Radiation efficiency","text":"\\[ \\eta_R = \\frac{E_R}{E_R + E_G} \\] <p>For simple slip-weakening friction</p> \\[ \\eta_R = \\frac{E_R}{\\frac{1}{2} \\Delta \\sigma D A} = \\frac{2 \\mu}{\\Delta \\sigma}  \\frac{E_R}{M_0} = 2\\mu \\frac{\\tilde{e}}{\\Delta \\sigma} \\] <p>where \\(\\tilde{e} = \\frac{E_R}{M_0}\\) is called the scaled energy and is dimensionless.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#seismic-efficiency","title":"Seismic efficiency","text":"<p>The radiation efficiency should not be confused with the seismic  efficiency</p> \\[ \\eta_s = \\frac{E_R}{R} = \\frac{E_R}{\\bar{\\sigma} \\bar{D} A} = \\frac{\\mu E_R}{\\bar{\\sigma} M_0} = \\frac{\\mu \\tilde{e}}{\\bar{\\sigma}} \\] <p>The seismic efficiency is more difficult to estimate than the radiation  efficiency because it depends upon the poorly constrained absolute stress  level on the fault.</p>"},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#apparent-stress-abercrombie-1995","title":"Apparent Stress (Abercrombie, 1995)","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#apparent-stress-ide-and-beroza-2001","title":"Apparent Stress (Ide and Beroza, 2001)","text":""},{"location":"eps207-observational-seismology/lectures/01_source_and_wave/#seismic-wave-propagation-notebook","title":"Seismic wave propagation (notebook)","text":"P-wave S-wave"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/","title":"Seismic Signal Processing","text":"<p>Notebooks:  Signal Processing Obspy Deep Denoiser</p>"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#signal-processing-101","title":"Signal Processing 101","text":"<ul> <li>Fourier Transform (FFT)</li> <li>Filtering</li> <li>Spectrogram</li> <li>Convolution and Cross-correlation</li> <li>Short-time Fourier Transform (STFT)</li> <li>Wavelet Transform</li> <li>Hilbert Transform</li> <li>...</li> </ul>"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#fourier-transform","title":"Fourier Transform","text":"<p>Fourier Transform (FT) is a mathematical operation that decomposes a function into its constituent frequencies.</p> <p>The Fourier Transform of a function \\(f(t)\\) is given by:</p> \\[F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-i\\omega t} dt\\] <p>The inverse Fourier Transform is given by:</p> \\[f(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) e^{i\\omega t} d\\omega\\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#example-fourier-transform-of-a-sine-function","title":"Example: Fourier Transform of a Sine Function","text":"\\[ \\begin{align} f(t) &amp;= \\sin(\\omega_0 t) \\\\ F(\\omega) &amp;= \\int_{-\\infty}^{\\infty} \\sin(\\omega_0 t) e^{-i\\omega t} dt \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\frac{e^{i\\omega_0 t} - e^{-i\\omega_0 t}}{2i} e^{-i\\omega t} dt \\\\ &amp;= \\frac{1}{2i} \\int_{-\\infty}^{\\infty} e^{i(\\omega_0 - \\omega) t} dt - \\frac{1}{2i} \\int_{-\\infty}^{\\infty} e^{i(\\omega_0 + \\omega) t} dt \\\\ &amp;= \\frac{1}{2i} \\delta(\\omega_0 - \\omega) - \\frac{1}{2i} \\delta(\\omega_0 + \\omega) \\\\ \\end{align} \\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#example-fourier-transform-of-a-box-function","title":"Example: Fourier Transform of a Box Function","text":"\\[ \\begin{align} f(t) &amp;= \\begin{cases} 1 &amp; \\text{if } -\\frac{1}{2} \\leq t \\leq \\frac{1}{2} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\\\ \\end{align} \\] <p>Wiki</p> \\[ \\begin{align} F(\\omega) &amp;= \\int_{-\\infty}^{\\infty} f(t) e^{-i\\omega t} dt \\\\ &amp;= \\int_{-\\frac{1}{2}}^{\\frac{1}{2}} e^{-i\\omega t} dt \\\\ &amp;= \\frac{1}{-i\\omega} \\left[ e^{-i\\omega t} \\right]_{-\\frac{1}{2}}^{\\frac{1}{2}} \\\\ &amp;= \\frac{1}{-i\\omega} \\left[ \\cos\\left(\\frac{\\omega}{2}\\right) - i\\sin\\left(\\frac{\\omega}{2}\\right) - \\cos\\left(\\frac{\\omega}{2}\\right) - i\\sin\\left(\\frac{\\omega}{2}\\right) \\right] \\\\ &amp;= \\frac{1}{-i\\omega} \\left[ -2i\\sin\\left(\\frac{\\omega}{2}\\right) \\right] \\\\ &amp;= \\frac{\\sin\\left(\\frac{\\omega}{2}\\right)}{\\frac{\\omega}{2}} \\end{align} \\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#fourier-transform_1","title":"Fourier Transform","text":""},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#fourier-analysis","title":"Fourier Analysis","text":""},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#filtering","title":"Filtering","text":"<p>Filtering is a process of removing unwanted components or features from a signal.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#example-butterworth-low-pass-filter-hs-frac11-leftfracsomega_cright2n","title":"Example: Butterworth Low-pass Filter \\(H(s) = \\frac{1}{1 + \\left(\\frac{s}{\\omega_c}\\right)^{2n}}\\)","text":""},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#example-chebyshev-filter","title":"Example: Chebyshev Filter","text":""},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#convolution","title":"Convolution","text":"<p>Convolution is a mathematical operation on two functions \\(f\\) and \\(g\\) that produces a third function that expresses how the shape of one is modified by the other.</p> \\[ \\begin{align} (f * g)(t) &amp;= \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) d\\tau \\\\ &amp;= \\int_{-\\infty}^{\\infty} f(t - \\tau) g(\\tau) d\\tau \\\\ \\end{align} \\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#convolution-in-frequency-domain","title":"Convolution in Frequency Domain","text":"\\[ \\begin{align} &amp; \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) d\\tau e^{-i\\omega t}  dt \\\\ &amp; = \\int_{-\\infty}^{\\infty} f(\\tau) \\int_{-\\infty}^{\\infty} g(t - \\tau) e^{-i\\omega t} dt d\\tau \\\\ &amp; = \\int_{-\\infty}^{\\infty} f(\\tau) \\int_{-\\infty}^{\\infty} g(\\tau') e^{-i\\omega (\\tau' + \\tau)} d\\tau' d\\tau, \\tau'=t-\\tau \\\\ &amp; = \\int_{-\\infty}^{\\infty} f(\\tau) e^{-i\\omega \\tau} d\\tau \\int_{-\\infty}^{\\infty} g(\\tau') e^{-i\\omega \\tau'} d\\tau'  \\\\ &amp; = F(\\omega) G(\\omega) \\end{align} \\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#cross-correlation","title":"Cross-correlation","text":"<p>Cross-correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.</p> \\[ \\begin{align} (f \\star g)(t) &amp;= \\int_{-\\infty}^{\\infty} f(\\tau) g(t + \\tau) d\\tau \\\\ &amp;= \\int_{-\\infty}^{\\infty} f(t + \\tau) g(\\tau) d\\tau \\\\ \\end{align} \\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#cross-correlation-in-frequency-domain","title":"Cross-correlation in Frequency Domain","text":"\\[ \\begin{align} &amp; \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(\\tau) g(t + \\tau) d\\tau e^{-i\\omega t}  dt \\\\ &amp; = \\int_{-\\infty}^{\\infty} f(\\tau) \\int_{-\\infty}^{\\infty} g(t + \\tau) e^{-i\\omega t} dt d\\tau \\\\ &amp; = \\int_{-\\infty}^{\\infty} f(\\tau) \\int_{-\\infty}^{\\infty} g(\\tau') e^{-i\\omega (\\tau' - \\tau)} d\\tau' d\\tau, \\tau'=t+\\tau \\\\ &amp; = \\int_{-\\infty}^{\\infty} f(\\tau) e^{-i (-\\omega) \\tau} d\\tau \\int_{-\\infty}^{\\infty} g(\\tau') e^{-i\\omega \\tau'} d\\tau'  \\\\ &amp; = F(-\\omega) G(\\omega) \\end{align} \\]"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#seismic-signal-processing-using-obpsy","title":"Seismic Signal Processing using Obpsy","text":""},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#deep-denoiser","title":"Deep Denoiser","text":"<ul> <li>Short-time Fourier Transform (STFT) + Wiener Filter + Neural Network</li> </ul> <p>Short-time Fourier Transform (STFT)</p> <p>Time-frequency representation of a signal. </p> <p>Wiener Filter</p> <p>The Wiener filter minimizes the mean square error between the estimated signal and the desired one. When the signal and noise are stationary and have known power spectral densities, the Wiener filter can be expressed in the frequency domain as:</p> \\[ H(f) = \\frac{S_x(f)}{S_x(f) + S_n(f)} \\] <ul> <li>\\(H(f)\\) is the frequency response of the Wiener filter.</li> <li>\\(S_x(f)\\) is the power spectral density of the desired signal.</li> <li>\\(S_n(f)\\) is the power spectral density of the noise.</li> </ul> <p>Neural Network</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/02_signal_processing/#deep-denoiser-for-seismic-data","title":"Deep Denoiser for Seismic Data","text":"<p>Paper</p>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/","title":"Earthquake Detection","text":"<p>Notebooks: codes/earthquake_detection.ipynb</p>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#how-to-detect-earthquakes","title":"How to detect earthquakes?","text":"<ul> <li>Amplitude threshold</li> <li>STA/LTA</li> <li>Template matching / Matched filter</li> <li>Deep learning</li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#amplitude-threshold","title":"Amplitude threshold","text":"<ul> <li>PGA (Peak Ground Acceleration)</li> <li>PGV (Peak Ground Velocity)</li> <li>Displacement</li> </ul> <p>Recent M4.5 earthquake</p>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#the-modified-mercalli-intensity-scale-mmi","title":"The Modified Mercalli Intensity Scale (MMI)","text":"Instrumental Intensity Acceleration (g) Velocity (cm/s) Perceived shaking Potential damage I &lt; 0.000464 &lt; 0.0215 Not felt None II\u2013III 0.000464 \u2013 0.00297 0.135 \u2013 1.41 Weak None IV 0.00297 \u2013 0.0276 1.41 \u2013 4.65 Light None V 0.0276 \u2013 0.115 4.65 \u2013 9.64 Moderate Very light Instrumental Intensity Acceleration (g) Velocity (cm/s) Perceived shaking Potential damage VI 0.115 \u2013 0.215 9.64 \u2013 20 Strong Light VII 0.215 \u2013 0.401 20 \u2013 41.4 Very strong Moderate VIII 0.401 \u2013 0.747 41.4 \u2013 85.8 Severe Moderate to heavy IX 0.747 \u2013 1.39 85.8 \u2013 178 Violent Heavy X+ &gt; 1.39 &gt; 178 Extreme Very heavy PGA Magnitude Depth Fatalities Earthquake 3.23g 7.8 15 km 2 2016 Kaikoura earthquake 2.7g 9.1 30 km 19,759 2011 T\u014dhoku earthquake 1.92g 7.7 8 km 2,415 1999 Jiji earthquake 1.82g 6.7 18 km 57 1994 Northridge earthquake 1.62g 7.8 10 km 57,658 2023 Turkey\u2013Syria earthquake 0.65 6.9 19 km 63 1989 Loma Prieta earthquake"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#amplitude-threshold_1","title":"Amplitude threshold","text":"<ul> <li>Pros:<ul> <li>Simple and fast</li> <li>Physical parameter</li> <li>Directly related to shaking/damage</li> </ul> </li> <li>Cons:<ul> <li>Limit to large earthquakes</li> <li>Need backgroud noise level for small earthquakes</li> </ul> </li> <li>Improvments:<ul> <li>How to make the threshold adaptive to the background noise level?</li> </ul> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#stalta","title":"STA/LTA","text":"<ul> <li>STA/LTA = Short-Term Average / Long-Term Average </li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#stalta_1","title":"STA/LTA","text":""},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#stalta_2","title":"STA/LTA","text":"<ul> <li> <p>Pros:</p> <ul> <li>Simple and fast</li> <li>More sensitive than amplitude threshold</li> <li>More robust for noisy data</li> </ul> </li> <li> <p>Cons:</p> <ul> <li>More parameters for tuning</li> <li>Prone to false detections</li> </ul> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#template-matching-matched-filter","title":"Template matching / Matched filter","text":"<p>Review of convolution and cross-correlation in last lecture: cross-correlation</p> <p>Notebook: cross-correlation</p>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#qtm-quake-template-matching","title":"(QTM) Quake Template Matching","text":""},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#template-matching-matched-filter_1","title":"Template matching / Matched filter","text":"<ul> <li>Pros:<ul> <li>Robust to noise</li> <li>More sensitive to small earthquakes</li> </ul> </li> <li>Cons:<ul> <li>High computational cost</li> <li>Need existing catalog to build templates</li> <li>Limited to waveform similarity with templates</li> </ul> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#similarity-search","title":"Similarity search","text":""},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#fast-fingerprint-and-similarity-thresholding","title":"FAST (Fingerprint And Similarity Thresholding)","text":""},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#siminlarity-search","title":"Siminlarity search","text":"<ul> <li>Pros:<ul> <li>Sensitive to small earthquakes</li> <li>Computational efficient</li> </ul> </li> <li>Cons:<ul> <li>Detect all repeating signals</li> <li>Complex to implement</li> </ul> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#deep-learning","title":"Deep learning","text":"<ul> <li>Generalized similarity search</li> </ul>"},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#convolutional-neural-network-for-earthquake-detection-and-location","title":"Convolutional Neural Network for Earthquake detection and location","text":""},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#residual-network-of-convolutional-and-recurrent-units-for-earthquake-signal-detection","title":"Residual Network of Convolutional and Recurrent Units for Earthquake Signal Detection","text":""},{"location":"eps207-observational-seismology/lectures/03_earthquake_detection/#deep-learning_1","title":"Deep learning","text":"<ul> <li>Pros:<ul> <li>Robust to noise</li> <li>Sensitive to small earthquakes</li> <li>Fast prediction</li> </ul> </li> <li>Cons:<ul> <li>Need large amount of labeled data</li> <li>Black box</li> <li>Generalization ability</li> </ul> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/04_phase_picking/","title":"Phase Picking","text":"<p>Notebooks: codes/quakeflow_demo.ipynb</p>"},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#seismic-waves","title":"Seismic waves","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#seismic-phases","title":"Seismic phases","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#information-from-seismic-phases","title":"Information from seismic phases","text":"<ul> <li>Earthquake source</li> <li>Earth's (Planetary) interior structure</li> <li>Subsurface exploration (reservior, geothermal, etc.)</li> <li>...</li> </ul>"},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#picking-p-and-s-waves","title":"Picking P and S waves","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#background-semantic-segmentation-vs-classification","title":"Background: Semantic Segmentation vs. Classification","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#demo-segment-anything-model-sam","title":"Demo: Segment Anything Model (SAM)","text":"<p>Try the SAM model: link</p>"},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#how-to-apply-deep-learning-to-seismic-phase-picking","title":"How to apply deep learning to seismic phase picking?","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#generalized-seismic-phase-detection-with-deep-learning","title":"Generalized seismic phase detection with deep learning","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#phasenet","title":"PhaseNet","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#eqtransformer-for-simultaneous-earthquake-detection-and-phase-picking","title":"EQTransformer for simultaneous earthquake detection and phase picking","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#next-generation-seismic-monitoring-with-neural-operators-phaseno","title":"Next-Generation Seismic Monitoring with Neural Operators (PhaseNO)","text":""},{"location":"eps207-observational-seismology/lectures/04_phase_picking/#large-training-dataset-clear-objective-function","title":"Large training dataset + Clear objective function","text":"<p>How to design the next model for detecting earthquake / picking seismic phases? What other seismic tasks can be solved with deep learning? What are other potential applications in Earth and Planetary Science?</p>"},{"location":"eps207-observational-seismology/lectures/05_phase_association/","title":"Phase Association","text":"<p>Notebooks: codes/quakeflow_demo.ipynb</p>"},{"location":"eps207-observational-seismology/lectures/05_phase_association/#what-is-phase-association","title":"What is phase association?","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#grid-search-back-projection-eg-real","title":"Grid-search / Back-projection, e.g. REAL","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#graph-neural-network-based-eg-genie","title":"Graph-Neural-Network-based, e.g. GENIE","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#clustering-based-unsupervised-eg-gamma","title":"Clustering-based (Unsupervised), e.g. GaMMA","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#clustering","title":"Clustering","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#k-means","title":"K-means","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#gaussian-mixture-model-gmm","title":"Gaussian Mixture Model (GMM)","text":""},{"location":"eps207-observational-seismology/lectures/05_phase_association/#gaussian-mixture-model-association-gamma","title":"Gaussian Mixture Model Association (GaMMA)","text":"<p>How to apply other clustering methods to phase association? How to solve phase association at the global scale?</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/","title":"Earthquake Location and Relocation","text":"<p>Notebooks: codes/earthquake_location.ipynb</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#how-to-locate-an-earthquake","title":"How to locate an earthquake?","text":""},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#how-to-locate-an-earthquake_1","title":"How to locate an earthquake?","text":""},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#optimization-inverse-problem","title":"Optimization (Inverse) problem","text":"<ul> <li>Minimize the difference between observed and predicted values</li> </ul>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#how-to-solve-an-optimizationinversion-problem","title":"How to solve an optimization/inversion problem?","text":"<ul> <li>Forward function</li> <li>Objective/Loss function</li> <li>Gradient</li> <li>Optimizer</li> </ul>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#locating-earthquake-using-absolute-arrival-times","title":"Locating earthquake using absolute arrival times","text":"<p>notebook</p> <p>Earthquake location problem:</p> <ul> <li>Given:</li> <li>Observed arrival times at multiple stations</li> <li>Velocity model</li> <li>Goal:</li> <li>Locate the hypocenter and origin time of the earthquake</li> </ul>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#forward-function","title":"Forward function:","text":"\\[\\hat{t}^i=f^i(\\mathbf{m})\\] <p>where \\(\\hat{t}^i\\) is the predicted arrival time at station \\(i\\), \\(f^i\\) is the forward non-linear function (e.g., ray tracing or eikonal equation), and \\(\\mathbf{m}\\) is the model parameter (e.g., source location, origin time, and velocity).</p> <p>For a uniform velocity: $$ \\hat{t}^i=f^i(\\mathbf{m})=\\frac{\\sqrt{(x^i-x_0)^2+(y^i-y_0)^2+(z^i-z_0)^2}}{v} + t^i_0 $$ where \\((x^i,y^i,z^i)\\) is the location of the \\(i\\)-th station, \\((x_0,y_0,z_0)\\) is the location of the source, \\(t_0\\) is the origin time, and \\(v\\) is the uniform velocity.</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#objectiveloss-function","title":"Objective/Loss function:","text":"<p>The difference between the observed and predicted times is: $$ r^i=t^i-\\hat{t}^i=t^i-f^i(\\mathbf{m}) $$</p> <p>Loss functions:</p> <ul> <li>Mean squared error (MSE): \\(\\mathcal{L}2= \\sum_{i=1}^n\\left\\|r^i\\right\\|_2\\)</li> <li>Absolute error: \\(\\mathcal{L}1= \\sum_{i=1}^n\\left|r^i\\right|\\)</li> <li>Huber loss: \\(\\mathcal{L}_{\\text {huber }}=\\sum_{i=1}^n \\begin{cases}\\left\\|r^i\\right\\|_2^2 &amp; \\text { if } \\left\\|r^i\\right\\|_2 \\leq \\delta \\\\ 2 \\delta\\left(\\left\\|r^i\\right\\|_2-\\frac{\\delta}{2}\\right) &amp; \\text { if } \\left\\|r^i\\right\\|_2&gt;\\delta\\end{cases}\\)</li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#iterative-location-methods","title":"Iterative location methods","text":"<p>$$ \\begin{aligned} \\hat{t}^i(m) &amp;= \\hat{t}^i(m_0) + \\frac{\\partial \\hat{t}^i}{\\partial m_j}\\Delta m_j \\end{aligned} $$ where \\(m_0\\) is the initial model, \\(\\Delta m_j\\) is the perturbation of \\((x, y, z, t0, v)\\).</p> \\[ \\begin{aligned} r^{m} &amp;= t^i - \\hat{t}^i(m) \\\\ &amp;= t^i - \\hat{t}^i(m_0) - \\frac{\\partial \\hat{t}^i}{\\partial m_j}\\Delta m_j \\\\ &amp;= r^i(m_0) - \\frac{\\partial \\hat{t}^i}{\\partial m_j}\\Delta m_j \\end{aligned} \\]"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#iterative-location-methods_1","title":"Iterative location methods","text":"<p>We seek to find the \\(\\Delta m\\) that  $$ \\begin{aligned} r^i(m_0) &amp;= \\frac{\\partial \\hat{t}^i}{\\partial m_j}\\Delta m_j \\ r^i(m_0) &amp;= G \\Delta m \\end{aligned} $$</p> <p>\\(\\Delta m\\) can be obtained using standard least squares. Next, we set \\(m_0\\) to \\(m_0 + \\Delta m\\) and repeat the process until the locatin converges.</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#how-to-evaluate-the-results-of-earthquake-location","title":"How to evaluate the results of earthquake location?","text":"<p>How do we define the \"best\" location?</p> <p>The average least square residual: $$ \\epsilon = \\frac{1}{n_df}\\sum_{i=1}^n\\left|t^i-\\hat{t}^i\\right|_2 $$ is called the variance of the residuals, where \\(n_{df}\\) is the number of degrees of freedom.</p> <p>A common term is variance reduction (VR), which is defined as: $$ \\text{VR} = \\frac{\\epsilon_{\\text{old}}-\\epsilon_{\\text{new}}}{\\epsilon_{\\text{old}}} \\times 100\\% $$</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#how-to-define-the-uncertainty-in-the-location","title":"How to define the uncertainty in the location?","text":"<p>Based on least squares and L2 norm, we define:</p> \\[ \\chi^2 = \\sum_{i=1}^n\\left(\\frac{t^i-\\hat{t}^i}{\\sigma^i}\\right)^2 \\] <p>where \\(\\sigma^i\\) is the uncertainty of the \\(i\\)-th residual.</p> <p>The \\(\\chi^2\\) distribution approximate the degree of freedom of the residuals \\(n_{df}\\).</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#the-chi2-distribution","title":"The \\(\\chi^2\\) distribution","text":"<p>The \\(\\chi^2\\) distribution is a probability distribution that describes the sum of the squares of independent standard normal random variables.</p> <p>The probability density function of the \\(\\chi^2\\) distribution is: $$ f(x;k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2} $$</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#90-confidence-interval-of-chi2","title":"90% confidence interval of \\(\\chi^2\\)","text":"<p>The 90% confidence interval of the \\(\\chi^2\\) distribution is bounded by: $$ \\chi^2_{0.05;n_{df}} \\leq \\chi^2 \\leq \\chi^2_{0.95;n_{df}} $$</p> <p>Table for \\(n_{df}=5, 10, 20, 50, 100\\):</p> ndf \\(\\chi^2_{0.05}\\) \\(\\chi^2_{0.50}\\) \\(\\chi^2_{0.95}\\) 5 0.412 4.35 11.1 10 3.94 9.34 18.3 20 10.9 19.3 31.4 50 34.8 49.3 71.4 100 77.9 99.3 129.6"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#how-to-apply-to-real-data","title":"How to apply to real data?","text":"<p>Note that the \\(\\sigma^i\\) are critical in the analysis, which is based on the assumption that the data misfit are random, uncorrelated, and have a Gaussian distribution.</p> <p>The estimated data uncertainty \\(\\sigma^i\\) is often estimated from the residual of the best location:</p> <p>$$ \\sigma^i(m^*) = \\frac{1}{n_{df}}\\sum_{i=1}^n\\left|t^i-\\hat{t}^i\\right|_2 $$ where \\(m^*\\) is the best-fitting location. </p> <p>Then we can use the estimated \\(\\sigma^i\\) to calculate the \\(\\chi^2\\) value; then obtain an estimate of the 95% confidence ellipse for the solution.</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#challenges-unmodeled-velocity-heterogeneity","title":"Challenges: unmodeled velocity heterogeneity","text":"<p>Case: Earthquakes located along a fault will often be mislocated if the seismic velocity changes across the fault.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#challenges-trade-off-between-event-dpeth-and-origin-time","title":"Challenges: trade-off between event dpeth and origin time","text":"<p>Case: Earthquake locations for events outside of a network are often not well constrained.</p> <p>Mitigations:</p> <ul> <li>\\(S-P\\) time can be used to estimate the source-receiver range at each station</li> <li>Adding depth phase \\(pP\\) (using the differential time \\(pP - P\\)) can help constrain the depth</li> </ul>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#locating-earthquake-using-relative-arrival-times","title":"Locating earthquake using relative arrival times","text":"<p>notebook</p> <p>In the common situation where the location error is dominated by the biasing effects of unmodeled 3-D velocity structure, the relative location among events within a localized region can be determined with much greater accuracy than the absolute location of any of the events.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#hypodd-double-difference-earthquake-location","title":"HypoDD: Double-difference earthquake location","text":"<p>$$ \\Delta r_k^{i j}=\\left(t_k^i-t_k^j\\right)-\\left(\\hat{t}_k^i-\\hat{t}_k^j\\right) $$ where \\(t_k^i\\) and \\(\\hat{t}_k^i\\) are the observed and predicted arrival times at the \\(k\\)-th station for the \\(i\\)-th earthquake, respectively.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#hypodd-double-difference-earthquake-location_1","title":"HypoDD: Double-difference earthquake location","text":""},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#growclust-a-hierarchical-clustering-algorithm-for-relative-earthquake-relocation","title":"GrowClust: A Hierarchical Clustering Algorithm for Relative Earthquake Relocation","text":"<p>Review: clusering</p>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#more-on-uncertainty","title":"More on: Uncertainty","text":"<ul> <li>Aleatoric uncertainty</li> <li>The irreducible part of the uncertainty</li> <li>Uncertainty due to inherent randomness, e.g., the outcome of flipping a coin</li> <li>Epistemic uncertainty</li> <li>The reducible part of the uncertainty</li> <li>Uncertainty due to lack of knowledge, e.g., lack of data</li> </ul>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#uncertainty-quantification","title":"Uncertainty Quantification","text":"<ul> <li>Standard deviation of slope and intercept of linear regression</li> <li>Bootstrapping</li> <li>Markov Chain Monte Carlo (MCMC)</li> <li>Hamiltonian Monte Carlo (HMC)</li> <li>Stein Variational Gradient Descent (SVGD)</li> <li>Dropout as a Bayesian Approximation</li> </ul>"},{"location":"eps207-observational-seismology/lectures/06_location_and_relocation/#hyposvi-hypocentre-inversion-with-stein-variational-inference","title":"HypoSVI: Hypocentre inversion with Stein variational inference","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/","title":"Earthquake Statistics","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-earthquake-cycle","title":"The Earthquake Cycle","text":"<ul> <li>Elastic rebound</li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#spring-block-model","title":"Spring-block model","text":"<p>When the force exerted by the spring  exceeds the static friction \\(\\mu_s\\), the block will slide until the dynamic friction \\(\\mu_d\\) balances the reduced level of stress. If \\(\\mu_s\\), \\(\\mu_d\\), and \\(v\\) are all constant, then the \u201cearthquakes\u201d will repeat at regular recurrence intervals.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#earthquake-recurrence-model","title":"Earthquake recurrence model","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#parkfield-earthquake","title":"Parkfield earthquake","text":"<p>Significant earthquakes at Parkfield, California, have repeated at  fairly regular intervals since 1850, leading to predictions of another event  before 1993. However the earthquake did not occur until 2004.</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-block-slider-model","title":"The block-slider model","text":"<p>A problem with the characteristic earthquake hypothesis is that it ignores the interactions with adjacent segments on the same fault, as well as interactions with other faults</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#self-similar-and-fractial-scaling-relationship","title":"Self-similar and fractial scaling relationship","text":"<ul> <li>Power-law  distribution of seismicity rates (the b-value relationship)</li> <li>Nearly constant value of stress drop over a wide range of earthquake sizes</li> <li>Fractal dimension D approximately twice the b-value (Turcotte, 1997)</li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#aftershocks","title":"Aftershocks","text":"<p>Earthquakes are thought to trigger aftershocks either from the dynamic effects of their radiated seismic waves or the resulting permanent static  stress changes</p> <ul> <li> <p>The seismicity rate decays with time, following a power law  relationship, called Omori\u2019s law after Omori (1894)</p> </li> <li> <p>Coulomb failure function (CFF) \\(\\(CFF = |\\tau_s| + \\mu (\\tau_n + P)\\)\\)</p> </li> </ul> <p>where \\(\\tau_s\\) is the shear traction on the fault, \\(\\tau_n\\) is the normal traction (positive for tension), \\(P\\) is the pore fluid pressure, and \\(\\mu\\) is the coefficient of static friction.</p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#earthquake-source-parameters","title":"Earthquake Source Parameters","text":"<ul> <li>Magnitude</li> <li>Origin time</li> <li>Location</li> <li>Focal mechanism</li> <li>Stress drop</li> <li>Energy</li> <li>Frequency</li> <li>...</li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#statistical-relationship-between-source-parameters","title":"Statistical relationship between source parameters","text":"<p>wiki - Gutenberg-Richter Law (1944) - Omori Law (1894) - B\u00e5th's Law (1965) - The Epidemic Type Aftershock Sequence (ETAS) model (1988) - ...</p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-gutenberg-richter-law","title":"The Gutenberg-Richter Law","text":"<p>$$ N=10^{a-b M} $$ Where: - \\(N\\) is the number of events greater or equal to \\(M\\) - \\(M\\) is magnitude - \\(a\\) and \\(b\\) are constants</p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-gutenberg-richter-law_1","title":"The Gutenberg-Richter Law","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-gutenberg-richter-law_2","title":"The Gutenberg-Richter Law","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#what-controls-the-slop-b","title":"What controls the slop \\(b\\)?","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#temporal-variation-of-b","title":"Temporal variation of \\(b\\)","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#temporal-variation-of-b_1","title":"Temporal variation of \\(b\\)","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-magnitude-completeness-m_c","title":"The magnitude completeness (\\(M_c\\))","text":"<p>What affects the magnitude completeness?</p> <ul> <li>Station coverage</li> <li>Background noise</li> <li>Detection algorithms</li> <li>...</li> </ul> <p></p> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#omori-law","title":"Omori Law","text":"<p>$$ n(t) = \\frac{K}{c+t} $$ The number of events \\(n(t)\\) in time \\(t\\) after the mainshock</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#a-modified-omori-law","title":"A modified Omori Law","text":"<p>$$ n(t) = \\frac{K}{(c+t)^p} $$ \ud835\udc3e: productivity of aftershocks \ud835\udc5d: decay rate c: delay time</p> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-decay-rate-p","title":"The decay rate \\(p\\)","text":"<ul> <li>\\(p \\sim 1.1\\)</li> <li>valid for a long time range</li> <li>independent of magnitude</li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-aftershock-productivity-k","title":"The aftershock productivity \\(K\\)","text":"<ul> <li>Combined with the Gutenberg-Richter law</li> </ul> \\[ K = K_0 10^{b (M_{main} - M)} \\] \\[ n(t, M) = \\frac{10^{a + b (M_{main} - M)}}{(c+t)^p} \\]"},{"location":"eps207-observational-seismology/lectures/07_statistics/#how-about-for-foreshocks","title":"How about for foreshocks?","text":"<ul> <li> <p>Inverse Omori law \\(n(t) \\propto t^p\\)</p> </li> <li> <p>but individual sequences rarely display this behavior</p> </li> </ul> <p></p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-epidemic-type-aftershock-sequence-etas-model","title":"The Epidemic Type Aftershock Sequence (ETAS) model","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-epidemic-type-aftershock-sequence-etas-model_1","title":"The Epidemic Type Aftershock Sequence (ETAS) model","text":"<p>$$ % g\\left(t-t_i, M ; \\theta\\right)=\\frac{K \\cdot \\exp \\left(\\beta\\left(M-M_c\\right)\\right)}{\\left(t-t_i+c\\right)^p} \\lambda(t)=\\mu+\\sum_{t_i&lt;t} K \\cdot \\exp \\left(\\beta\\left(M_i-M_c\\right)\\right) \\cdot\\left(t-t_i+c\\right)^{-p} $$ - \\(\\mu\\) is the background rate - \\(K\\) is the productivity - \\(M_c\\) is the magnitude completeness - \\(p\\) is the decay rate - \\(c\\) is the delay time - \\(\\beta\\) is the magnitude scaling - \\(t_i\\) is the occurrence times of previous earthquakes.</p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#the-etas-model","title":"The ETAS model","text":"<ul> <li>Modeling earthquake activity of a Poissonian background and a cluster process</li> <li>Analyzing \u201cbackground\u201d or \u201cclustered\u201d events</li> <li>Most widely used model for earthquake forecasting</li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#incorporate-spatial-triggering-into-etas","title":"Incorporate spatial triggering into ETAS","text":"\\[ \\lambda(x, y, t \\mid \\mathcal{H})=\\mu+\\sum_{t_i&lt;t} \\frac{K \\cdot \\exp \\left(\\beta\\left(M_i-M_c\\right)\\right)}{\\left(t-t_i+c\\right)^p} \\cdot \\frac{1}{\\left(x^2+y^2+d\\right)^q} \\] <ul> <li>\\(q\\): the spatial decay rate of intensity following an event</li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#physical-models-on-aftershocks-spatial-distribution","title":"Physical models on aftershocks spatial distribution","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#coulomb-failure-stress-cfs-static-triggering","title":"Coulomb failure stress (CFS) (Static triggering)","text":"<p>$$ \\Delta \\sigma_f=\\Delta \\tau+\\mu\\left(\\Delta \\sigma_n+\\Delta p\\right) $$ \\(\\Delta \\tau\\) : change in shear stress \\(\\Delta \\sigma_n\\) : change in normal stress (positive for tension) \\(\\Delta p\\) : change in pore pressure \\(\\mu\\) : friction coefficient</p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#coulomb-failure-stress-cfs","title":"Coulomb failure stress (CFS)","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#dynamic-triggering","title":"Dynamic triggering","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#dynamic-triggering_1","title":"Dynamic triggering","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#earthquake-swarms","title":"Earthquake swarms","text":"<p>\u201c[a sequence] where the number and the magnitude of earthquakes gradually increase with time, and then decreases after a certain period. There is no single predominant principal earthquake\u201d - Mogi (1963)</p> <p></p> <p>2012 Brawley,CA swarm </p> <p></p> <p>2016 Cahuilla,CA swarm</p> <p></p> <p>2018 Pahala,Hawaii swarm</p> <p></p> <p>2018 Pahala,Hawaii swarm</p> <p> </p>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#spatial-temporal-evolution-patterns-of-swarms","title":"Spatial-temporal evolution patterns of swarms","text":"<ul> <li> <p>Migration distance vs. time</p> <ul> <li>\\(R \\propto t^{\\alpha}\\), \\(\\alpha \\sim 0.5, 1\\)</li> <li>\\(R \\propto \\log(t)\\)</li> </ul> </li> <li> <p>Migration speeds</p> <ul> <li>m/day to km/hour</li> </ul> </li> <li> <p>Similarity to induced seismicity</p> </li> </ul>"},{"location":"eps207-observational-seismology/lectures/07_statistics/#deep-learning-for-earthquake-statistics","title":"Deep learning for earthquake statistics","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#deep-learning-for-earthquake-statistics_1","title":"Deep learning for earthquake statistics","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#deep-learning-for-earthquake-statistics_2","title":"Deep learning for earthquake statistics","text":""},{"location":"eps207-observational-seismology/lectures/07_statistics/#deep-learning-for-earthquake-statistics_3","title":"Deep learning for earthquake statistics","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/","title":"Focal Mechanism and Moment Tensor","text":"<p>Author: Yifang Cheng Link</p> <p></p> <p></p> <p>Momentum equation for an elastic continuum</p> <p>\\begin{equation} \\rho \\frac{\\partial^2 u_i}{\\partial t^2} = {\\partial_j \\tau_{ij}} + f_i \\end{equation}</p> <p>\ud835\udf0c: density</p> <p>\ud835\udc62: displacement</p> <p>\ud835\udf0f: stress tensor</p> <p>\ud835\udc53: body force term</p> <p>If we only concentrate on the force term, we can solve this linear, time-invariant differential equation using convolution:</p> <p>\\begin{equation} u_i(x, t) = G_{ij}(x, t; x_0, t_0)f_j(x_0, t_0) \\end{equation}</p> <p>\ud835\udc3a: elastodynamic Green's function. \ud835\udc62: displacement at location x and time t \ud835\udc53: force at location \ud835\udc650 and \ud835\udc610</p> <p>Because it is linear, the displacement from any body-force distribution can be computed as the sum of individual point sources.</p> <p>Force couple and double couple</p> <p>Since an earthquake is usually modeled as slip on a fault, which has the slip distribution equivalent to that generated by two force vectors of magnitude f, pointing to opposite directions, separated by a distance d (force couple).</p> <p>To balance the angular momentum, a complementary couple is used. (double couple)</p> <p></p> <p>Stress tensor versus moment tensor</p> <ul> <li><p>stress tensor describes the state of stress at a particular point.</p> </li> <li><p>moment tensor describes the force (caused by deformation) at the source location that generates seismic waves.</p> </li> </ul> <p></p> <p>\\begin{equation} u_i(x, t) = \\frac{\\partial G_{ij}(x, t; x_0, t_0)} {\\partial {(x_0)}_k}M_{jk}(x_0, t_0) \\end{equation}</p> <p></p> <p>Different type of moment tensors</p> <p></p> <p></p> <p>From seismology to rock mechanics and structural geology</p> <p>example from https://www.globalcmt.org/</p> <p></p> <p>examples from Scholz, C. H. (2019). The mechanics of earthquakes and faulting. Cambridge university press.</p> <p>Large earthquake focal mechanisms --&gt; Main fault and large-scale tectonic motions</p> <p></p> <p>Small earthquake focal mechanisms --&gt; local fault structures</p> <p></p> <p>For large fault with varying creep rate</p> <p>using known fault orientation and focal mechanism to solve slip direction</p> <p></p> <p>For small single earthquakes</p> <p>Combine rupture directivity to find the fault orientation and slip direction</p> <p></p> <p></p> <ul> <li><p>stress + fault orientation --&gt; slip direction</p> </li> <li><p>Focal mechanism = fault orientation + slip direction --&gt; stress inversion!</p> </li> </ul> <p>Assumption: stress in the same bin is uniform and all focal mechanisms are optimally oriented to fail under the stress</p> <p></p> In\u00a0[20]: Copied! <pre># install cps for synthesizing green's function\n# more documents: https://www.eas.slu.edu/eqc/eqc_cps/getzip.html\n%%capture\nimport os\nif not os.path.exists(\"PROGRAMS.330\"):\n    !wget \"http://www.eas.slu.edu/eqc/eqc_cps/Download/NP330.Oct-26-2023.tgz\"\n    !tar -xzf NP330.Oct-26-2023.tgz\n    %cd PROGRAMS.330\n    !./Setup LINUX\n    !./C\n    %cd ..\n</pre> # install cps for synthesizing green's function # more documents: https://www.eas.slu.edu/eqc/eqc_cps/getzip.html %%capture import os if not os.path.exists(\"PROGRAMS.330\"):     !wget \"http://www.eas.slu.edu/eqc/eqc_cps/Download/NP330.Oct-26-2023.tgz\"     !tar -xzf NP330.Oct-26-2023.tgz     %cd PROGRAMS.330     !./Setup LINUX     !./C     %cd .. <p></p> <p>\\begin{equation} u_z = M_{xx}[\\,\\frac{ZSS}{2}cos(2\\alpha z)-\\frac{ZDD}{6}+\\frac{ZEP}{3}]\\, + M_{yy}[\\,-\\frac{ZSS}{2}cos(2\\alpha z)-\\frac{ZDD}{6}+\\frac{ZEP}{3}]\\, + M_{zz}[\\,\\frac{ZDD}{3}+\\frac{ZEP}{3}]\\, + M_{xy}[\\,{ZSS}sin(2\\alpha z)]\\,+ M_{xz}[\\,{ZDS}cos(\\alpha z)]\\,+ M_{yz}[\\,{ZDS}sin(\\alpha z)]\\, \\end{equation} .</p> <p>.</p> <p>\\begin{equation} u_r = M_{xx}[\\,\\frac{RSS}{2}cos(2\\alpha z)-\\frac{RDD}{6}+\\frac{REP}{3}]\\, + M_{yy}[\\,-\\frac{RSS}{2}cos(2\\alpha z)-\\frac{RDD}{6}+\\frac{REP}{3}]\\, + M_{zz}[\\,\\frac{RDD}{3}+\\frac{REP}{3}]\\, + M_{xy}[\\,{RSS}sin(2\\alpha z)]\\,+ M_{xz}[\\,{RDS}cos(\\alpha z)]\\,+ M_{yz}[\\,{RDS}sin(\\alpha z)]\\, \\end{equation} .</p> <p>.</p> <p>\\begin{equation} u_t = M_{xx}[\\,\\frac{TSS}{2}sin(2\\alpha z)]\\, + M_{yy}[\\,-\\frac{TSS}{2}sin(2\\alpha z)]\\, + M_{xy}[\\,-{TSS}cos(2\\alpha z)]\\,+ M_{xz}[\\,{TDS}sin(\\alpha z)]\\,+ M_{yz}[\\,-{TDS}cos(\\alpha z)]\\, \\end{equation} .</p> <p>.</p> <p>\ud835\udc62: Surface displacement</p> <p>\ud835\udc46\ud835\udc46: vertical strike-slip Green's function</p> <p>\ud835\udc37\ud835\udc46: vertical dip-slip Green's function</p> <p>\ud835\udc37\ud835\udc37: 45 degrees dip-slip Green's function</p> <p>\ud835\udc38\ud835\udc43: explosion Green's function</p> <p>Equation (6-8) in Minson, S. E., &amp; Dreger, D. S. (2008). Stable inversions for complete moment tensors. Geophysical Journal International, 174(2), 585-592.</p> In\u00a0[3]: Copied! <pre>import os\nfrom obspy import read\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npath = \"./results\"\npath_bin = 'PROGRAMS.330/bin'\n</pre> import os from obspy import read import matplotlib.pyplot as plt import numpy as np  path = \"./results\" path_bin = 'PROGRAMS.330/bin' In\u00a0[4]: Copied! <pre>velocity_model = \"\"\"MODEL.01\nGIL7\nISOTROPIC\nKGS\nFLAT EARTH\n1-D\nCONSTANT VELOCITY\nLINE08\nLINE09\nLINE10\nLINE11\n H(KM)   VP(KM/S)   VS(KM/S) RHO(GM/CC)         QP         QS   ETAP       ETAS      FREFP      FREFS\n1.0000       3.20       1.50       2.28     600.00     300.00   0.00       0.00       1.00       1.00\n2.0000       4.50       2.40       2.28     600.00     300.00   0.00       0.00       1.00       1.00\n1.0000       4.80       2.78       2.58     600.00     300.00   0.00       0.00       1.00       1.00\n1.0000       5.51       3.18       2.58     600.00     300.00   0.00       0.00       1.00       1.00\n12.000       6.21       3.40       2.68     600.00     300.00   0.00       0.00       1.00       1.00\n8.0000       6.89       3.98       3.00     600.00     300.00   0.00       0.00       1.00       1.00\n0.0000       7.83       4.52       3.26     600.00     300.00   0.00       0.00       1.00       1.00\n\"\"\"\nwith open('gil7.d','w') as f:\n    f.write(velocity_model)\n\ndfile = \"\"\"50 1.00 256 0 0.0\n100 1.00 256 0 0.0\n150 1.00 256 0 0.0\n\"\"\"\nwith open(\"dfile\", 'w') as f:\n    f.write(dfile)\n</pre> velocity_model = \"\"\"MODEL.01 GIL7 ISOTROPIC KGS FLAT EARTH 1-D CONSTANT VELOCITY LINE08 LINE09 LINE10 LINE11  H(KM)   VP(KM/S)   VS(KM/S) RHO(GM/CC)         QP         QS   ETAP       ETAS      FREFP      FREFS 1.0000       3.20       1.50       2.28     600.00     300.00   0.00       0.00       1.00       1.00 2.0000       4.50       2.40       2.28     600.00     300.00   0.00       0.00       1.00       1.00 1.0000       4.80       2.78       2.58     600.00     300.00   0.00       0.00       1.00       1.00 1.0000       5.51       3.18       2.58     600.00     300.00   0.00       0.00       1.00       1.00 12.000       6.21       3.40       2.68     600.00     300.00   0.00       0.00       1.00       1.00 8.0000       6.89       3.98       3.00     600.00     300.00   0.00       0.00       1.00       1.00 0.0000       7.83       4.52       3.26     600.00     300.00   0.00       0.00       1.00       1.00 \"\"\" with open('gil7.d','w') as f:     f.write(velocity_model)  dfile = \"\"\"50 1.00 256 0 0.0 100 1.00 256 0 0.0 150 1.00 256 0 0.0 \"\"\" with open(\"dfile\", 'w') as f:     f.write(dfile) In\u00a0[5]: Copied! <pre>model = \"gil7\"\ndepths = sorted([5,10,15,20]) # compute GF at 10, 20 km\ndistances = sorted([50,100,150])\nnpts = int(256) # number of points in the time series, must be a power of 2\nvred = 0 # Reduction velocity in km/sec, 0 sets the reference time to origin time\nt0 = int(0) # used to define the first sample point, t0 + distance_in_km/vred\n\n# Filter\nfreqmin = 0.02\nfreqmax = 0.05\ncorners = 3\n\n# Desired sampling interval\ndt = 1.0\n\n# time before and after reference time, data will be cut before and after the reference time\ntime_before = 30\ntime_after = 200\n\n# green_dir = path + \"%s/%s\"%('test',model)\ngreen_dir = os.path.join(path, model)\nif not os.path.isdir(green_dir):\n    os.makedirs(green_dir, exist_ok=True )\n</pre> model = \"gil7\" depths = sorted([5,10,15,20]) # compute GF at 10, 20 km distances = sorted([50,100,150]) npts = int(256) # number of points in the time series, must be a power of 2 vred = 0 # Reduction velocity in km/sec, 0 sets the reference time to origin time t0 = int(0) # used to define the first sample point, t0 + distance_in_km/vred  # Filter freqmin = 0.02 freqmax = 0.05 corners = 3  # Desired sampling interval dt = 1.0  # time before and after reference time, data will be cut before and after the reference time time_before = 30 time_after = 200  # green_dir = path + \"%s/%s\"%('test',model) green_dir = os.path.join(path, model) if not os.path.isdir(green_dir):     os.makedirs(green_dir, exist_ok=True ) In\u00a0[6]: Copied! <pre>!PROGRAMS.330/bin/hprep96 -M gil7.d -d dfile -HS 5.0000 -HR 0 -EQEX\n!PROGRAMS.330/bin/hspec96\n!PROGRAMS.330/bin/hpulse96 -D -i &gt; file96\n!PROGRAMS.330/bin/f96tosac -B file96\n</pre> !PROGRAMS.330/bin/hprep96 -M gil7.d -d dfile -HS 5.0000 -HR 0 -EQEX !PROGRAMS.330/bin/hspec96 !PROGRAMS.330/bin/hpulse96 -D -i &gt; file96 !PROGRAMS.330/bin/f96tosac -B file96 <pre>      0.0976562     10.0000000\n        256         1       129\n ieqex=            0  EARTHQUAKE + EXPLOSION\n   10\n  TOP  OF MODEL IS FREE SURFACE  \n  BASE OF MODEL IS HALFSPACE WITH PROPERTIES OF BOTTOM LAYER\n gil7.d\n LAYER             H      P-VEL     S-VEL   DENSITY  \n     1       1.000      3.200      1.500      2.280    \n     2       2.000      4.500      2.400      2.280    \n     3       1.000      4.800      2.780      2.580    \n     4       1.000      5.510      3.180      2.580    \n     5       12.00      6.210      3.400      2.680    \n     6       8.000      6.890      3.980      3.000    \n     7       0.000      7.830      4.520      3.260    \n Working model\n        d         a         b         rho      1/qa      1/qb       bsh    1/qbsh\n      1.000     3.200     1.500     2.280  0.001667  0.003333     1.500     2.280\n      2.000     4.500     2.400     2.280  0.001667  0.003333     2.400     2.280\n      1.000     4.800     2.780     2.580  0.001667  0.003333     2.780     2.580\n      1.000     5.510     3.180     2.580  0.001667  0.003333     3.180     2.580\n     12.000     6.210     3.400     2.680  0.001667  0.003333     3.400     2.680\n      8.000     6.890     3.980     3.000  0.001667  0.003333     3.980     3.000\n                7.830     4.520     3.260  0.001667  0.003333     4.520     3.260\n \nXLENG=  0.2496000E+04 XFAC=  0.4000000E+01\nWAVENUMBER FILTERING bounded in phase velocites\n[cmax,c1,c2,cmin]=[    -1.000,    -1.000,    -1.000,    -1.000]\n( -1.0 means 0.0 for cmin and infinity for cmax)\n  LAYER INSERTION DONE\n mmax=           7\n 0.1000E+01 0.3200E+01 0.1500E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.2000E+01 0.4500E+01 0.2400E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.4800E+01 0.2780E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.5510E+01 0.3180E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1200E+02 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.8000E+01 0.6890E+01 0.3980E+01 0.3000E+01 0.1667E-02 0.3333E-02\n 0.0000E+00 0.7830E+01 0.4520E+01 0.3260E+01 0.1667E-02 0.3333E-02\nfl =  0.0000         fu = 0.50000         df = 3.90625E-03\n    n1 =   1     n2 = 129      n =  256\nvmin  =  1.5000     vamin =  3.2000     vamax =  7.8300    \n                  vbmin =  1.5000     vbmax =  4.5200    \n SOURCE DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepths =         5.00000     (         5.00000    )\n RECEIVER DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepthr =         0.00000     (         0.00000    )\n RECEIVER DISTANCES (           3 )\n     r =         50.0000     tshift=         0.00000     vred=         0.00000    \n     r =         100.000     tshift=         0.00000     vred=         0.00000    \n     r =         150.000     tshift=         0.00000     vred=         0.00000    \nalpha =  9.7656250E-03     dt =       1.00    \nfrequencies for which response computed     \n 3.90625E-05 3.90625E-03 7.81250E-03 1.17188E-02 1.56250E-02 1.95312E-02 2.34375E-02 2.73438E-02\n 3.12500E-02 3.51562E-02 3.90625E-02 4.29688E-02 4.68750E-02 5.07812E-02 5.46875E-02 5.85938E-02\n 6.25000E-02 6.64062E-02 7.03125E-02 7.42188E-02 7.81250E-02 8.20312E-02 8.59375E-02 8.98438E-02\n 9.37500E-02 9.76562E-02 0.10156     0.10547     0.10938     0.11328     0.11719     0.12109    \n 0.12500     0.12891     0.13281     0.13672     0.14062     0.14453     0.14844     0.15234    \n 0.15625     0.16016     0.16406     0.16797     0.17188     0.17578     0.17969     0.18359    \n 0.18750     0.19141     0.19531     0.19922     0.20312     0.20703     0.21094     0.21484    \n 0.21875     0.22266     0.22656     0.23047     0.23438     0.23828     0.24219     0.24609    \n 0.25000     0.25391     0.25781     0.26172     0.26562     0.26953     0.27344     0.27734    \n 0.28125     0.28516     0.28906     0.29297     0.29688     0.30078     0.30469     0.30859    \n 0.31250     0.31641     0.32031     0.32422     0.32812     0.33203     0.33594     0.33984    \n 0.34375     0.34766     0.35156     0.35547     0.35938     0.36328     0.36719     0.37109    \n 0.37500     0.37891     0.38281     0.38672     0.39062     0.39453     0.39844     0.40234    \n 0.40625     0.41016     0.41406     0.41797     0.42188     0.42578     0.42969     0.43359    \n 0.43750     0.44141     0.44531     0.44922     0.45312     0.45703     0.46094     0.46484    \n 0.46875     0.47266     0.47656     0.48047     0.48438     0.48828     0.49219     0.49609    \n 0.50000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000    \n Transition to non-asymptotic at   1.56250000E-02\n     r(km)   t0(sec)    hs(km)    hr(km)    Tp(sec)   Tsv(sec)   Tsh(sec)\n   50.0       0.00       5.00       0.00       8.77       16.1       16.1    \n   100.       0.00       5.00       0.00       16.7       30.5       30.5    \n   150.       0.00       5.00       0.00       23.6       41.6       41.6    \n</pre> In\u00a0[7]: Copied! <pre>dfile = (\"{dist:.0f} {dt:.2f} {npts:d} {t0:d} {vred:.1f}\\n\")\ndfile_out = os.path.join(path,\"dfile\")\n\nfor depth in depths:\n    with open(dfile_out,\"w\") as f:\n        for distance in distances:\n            f.write(dfile.format(dist=distance,dt=dt,npts=npts,t0=t0,vred=vred))\n    # Generate the synthetics\n    os.system(\"cd %s\" % (path))\n    os.system(\"PROGRAMS.330/bin/hprep96 -M %s.d -d %s -HS %.4f -HR 0 -EQEX\"%(model,dfile_out,depth))\n    os.system(\"PROGRAMS.330/bin/hspec96\")\n    os.system(\"PROGRAMS.330/bin/hpulse96 -D -i &gt; file96\")\n    os.system(\"PROGRAMS.330/bin/f96tosac -B file96\")\n    # Filter and save the synthetic Green's functions\n    greens = (\"ZDD\",\"RDD\",\"ZDS\",\"RDS\",\"TDS\",\"ZSS\",\"RSS\",\"TSS\",\"ZEX\",\"REX\")\n\n    for index,distance in enumerate(distances):\n        for j,grn in enumerate(greens):\n            sacin = \"B%03d%02d%s.sac\"%(index+1,j+1,grn)\n            sacout = \"%s/%.0f.%.4f\"%(green_dir,distance,depth)\n            tmp = read(sacin,format='SAC')\n            tmp.filter('bandpass',freqmin=freqmin,freqmax=freqmax,corners=corners,zerophase=True)\n            tmp.write(\"%s.%s\"%(sacout,grn),format=\"SAC\") # overwrite\n\n# Uncomment to remove unfiltered synthetic SAC files\nos.system(\"rm B*.sac\") # remove the unfiltered SAC files\n</pre> dfile = (\"{dist:.0f} {dt:.2f} {npts:d} {t0:d} {vred:.1f}\\n\") dfile_out = os.path.join(path,\"dfile\")  for depth in depths:     with open(dfile_out,\"w\") as f:         for distance in distances:             f.write(dfile.format(dist=distance,dt=dt,npts=npts,t0=t0,vred=vred))     # Generate the synthetics     os.system(\"cd %s\" % (path))     os.system(\"PROGRAMS.330/bin/hprep96 -M %s.d -d %s -HS %.4f -HR 0 -EQEX\"%(model,dfile_out,depth))     os.system(\"PROGRAMS.330/bin/hspec96\")     os.system(\"PROGRAMS.330/bin/hpulse96 -D -i &gt; file96\")     os.system(\"PROGRAMS.330/bin/f96tosac -B file96\")     # Filter and save the synthetic Green's functions     greens = (\"ZDD\",\"RDD\",\"ZDS\",\"RDS\",\"TDS\",\"ZSS\",\"RSS\",\"TSS\",\"ZEX\",\"REX\")      for index,distance in enumerate(distances):         for j,grn in enumerate(greens):             sacin = \"B%03d%02d%s.sac\"%(index+1,j+1,grn)             sacout = \"%s/%.0f.%.4f\"%(green_dir,distance,depth)             tmp = read(sacin,format='SAC')             tmp.filter('bandpass',freqmin=freqmin,freqmax=freqmax,corners=corners,zerophase=True)             tmp.write(\"%s.%s\"%(sacout,grn),format=\"SAC\") # overwrite  # Uncomment to remove unfiltered synthetic SAC files os.system(\"rm B*.sac\") # remove the unfiltered SAC files <pre>      0.0976562     10.0000000\n        256         1       129\n ieqex=            0  EARTHQUAKE + EXPLOSION\n   10\n  TOP  OF MODEL IS FREE SURFACE  \n  BASE OF MODEL IS HALFSPACE WITH PROPERTIES OF BOTTOM LAYER\n gil7.d\n LAYER             H      P-VEL     S-VEL   DENSITY  \n     1       1.000      3.200      1.500      2.280    \n     2       2.000      4.500      2.400      2.280    \n     3       1.000      4.800      2.780      2.580    \n     4       1.000      5.510      3.180      2.580    \n     5       12.00      6.210      3.400      2.680    \n     6       8.000      6.890      3.980      3.000    \n     7       0.000      7.830      4.520      3.260    \n Working model\n        d         a         b         rho      1/qa      1/qb       bsh    1/qbsh\n      1.000     3.200     1.500     2.280  0.001667  0.003333     1.500     2.280\n      2.000     4.500     2.400     2.280  0.001667  0.003333     2.400     2.280\n      1.000     4.800     2.780     2.580  0.001667  0.003333     2.780     2.580\n      1.000     5.510     3.180     2.580  0.001667  0.003333     3.180     2.580\n     12.000     6.210     3.400     2.680  0.001667  0.003333     3.400     2.680\n      8.000     6.890     3.980     3.000  0.001667  0.003333     3.980     3.000\n                7.830     4.520     3.260  0.001667  0.003333     4.520     3.260\n \nXLENG=  0.2496000E+04 XFAC=  0.4000000E+01\nWAVENUMBER FILTERING bounded in phase velocites\n[cmax,c1,c2,cmin]=[    -1.000,    -1.000,    -1.000,    -1.000]\n( -1.0 means 0.0 for cmin and infinity for cmax)\n  LAYER INSERTION DONE\n mmax=           7\n 0.1000E+01 0.3200E+01 0.1500E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.2000E+01 0.4500E+01 0.2400E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.4800E+01 0.2780E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.5510E+01 0.3180E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1200E+02 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.8000E+01 0.6890E+01 0.3980E+01 0.3000E+01 0.1667E-02 0.3333E-02\n 0.0000E+00 0.7830E+01 0.4520E+01 0.3260E+01 0.1667E-02 0.3333E-02\nfl =  0.0000         fu = 0.50000         df = 3.90625E-03\n    n1 =   1     n2 = 129      n =  256\nvmin  =  1.5000     vamin =  3.2000     vamax =  7.8300    \n                  vbmin =  1.5000     vbmax =  4.5200    \n SOURCE DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepths =         5.00000     (         5.00000    )\n RECEIVER DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepthr =         0.00000     (         0.00000    )\n RECEIVER DISTANCES (           3 )\n     r =         50.0000     tshift=         0.00000     vred=         0.00000    \n     r =         100.000     tshift=         0.00000     vred=         0.00000    \n     r =         150.000     tshift=         0.00000     vred=         0.00000    \nalpha =  9.7656250E-03     dt =       1.00    \nfrequencies for which response computed     \n 3.90625E-05 3.90625E-03 7.81250E-03 1.17188E-02 1.56250E-02 1.95312E-02 2.34375E-02 2.73438E-02\n 3.12500E-02 3.51562E-02 3.90625E-02 4.29688E-02 4.68750E-02 5.07812E-02 5.46875E-02 5.85938E-02\n 6.25000E-02 6.64062E-02 7.03125E-02 7.42188E-02 7.81250E-02 8.20312E-02 8.59375E-02 8.98438E-02\n 9.37500E-02 9.76562E-02 0.10156     0.10547     0.10938     0.11328     0.11719     0.12109    \n 0.12500     0.12891     0.13281     0.13672     0.14062     0.14453     0.14844     0.15234    \n 0.15625     0.16016     0.16406     0.16797     0.17188     0.17578     0.17969     0.18359    \n 0.18750     0.19141     0.19531     0.19922     0.20312     0.20703     0.21094     0.21484    \n 0.21875     0.22266     0.22656     0.23047     0.23438     0.23828     0.24219     0.24609    \n 0.25000     0.25391     0.25781     0.26172     0.26562     0.26953     0.27344     0.27734    \n 0.28125     0.28516     0.28906     0.29297     0.29688     0.30078     0.30469     0.30859    \n 0.31250     0.31641     0.32031     0.32422     0.32812     0.33203     0.33594     0.33984    \n 0.34375     0.34766     0.35156     0.35547     0.35938     0.36328     0.36719     0.37109    \n 0.37500     0.37891     0.38281     0.38672     0.39062     0.39453     0.39844     0.40234    \n 0.40625     0.41016     0.41406     0.41797     0.42188     0.42578     0.42969     0.43359    \n 0.43750     0.44141     0.44531     0.44922     0.45312     0.45703     0.46094     0.46484    \n 0.46875     0.47266     0.47656     0.48047     0.48438     0.48828     0.49219     0.49609    \n 0.50000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000    \n Transition to non-asymptotic at   1.56250000E-02\n     r(km)   t0(sec)    hs(km)    hr(km)    Tp(sec)   Tsv(sec)   Tsh(sec)\n   50.0       0.00       5.00       0.00       8.77       16.1       16.1    \n   100.       0.00       5.00       0.00       16.7       30.5       30.5    \n   150.       0.00       5.00       0.00       23.6       41.6       41.6    \n      0.0976562     10.0000000\n        256         1       129\n ieqex=            0  EARTHQUAKE + EXPLOSION\n   10\n  TOP  OF MODEL IS FREE SURFACE  \n  BASE OF MODEL IS HALFSPACE WITH PROPERTIES OF BOTTOM LAYER\n gil7.d\n LAYER             H      P-VEL     S-VEL   DENSITY  \n     1       1.000      3.200      1.500      2.280    \n     2       2.000      4.500      2.400      2.280    \n     3       1.000      4.800      2.780      2.580    \n     4       1.000      5.510      3.180      2.580    \n     5       12.00      6.210      3.400      2.680    \n     6       8.000      6.890      3.980      3.000    \n     7       0.000      7.830      4.520      3.260    \n Working model\n        d         a         b         rho      1/qa      1/qb       bsh    1/qbsh\n      1.000     3.200     1.500     2.280  0.001667  0.003333     1.500     2.280\n      2.000     4.500     2.400     2.280  0.001667  0.003333     2.400     2.280\n      1.000     4.800     2.780     2.580  0.001667  0.003333     2.780     2.580\n      1.000     5.510     3.180     2.580  0.001667  0.003333     3.180     2.580\n     12.000     6.210     3.400     2.680  0.001667  0.003333     3.400     2.680\n      8.000     6.890     3.980     3.000  0.001667  0.003333     3.980     3.000\n                7.830     4.520     3.260  0.001667  0.003333     4.520     3.260\n \nXLENG=  0.2496000E+04 XFAC=  0.4000000E+01\nWAVENUMBER FILTERING bounded in phase velocites\n[cmax,c1,c2,cmin]=[    -1.000,    -1.000,    -1.000,    -1.000]\n( -1.0 means 0.0 for cmin and infinity for cmax)\n  LAYER INSERTION DONE\n mmax=           8\n 0.1000E+01 0.3200E+01 0.1500E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.2000E+01 0.4500E+01 0.2400E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.4800E+01 0.2780E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.5510E+01 0.3180E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.5000E+01 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.7000E+01 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.8000E+01 0.6890E+01 0.3980E+01 0.3000E+01 0.1667E-02 0.3333E-02\n 0.0000E+00 0.7830E+01 0.4520E+01 0.3260E+01 0.1667E-02 0.3333E-02\nfl =  0.0000         fu = 0.50000         df = 3.90625E-03\n    n1 =   1     n2 = 129      n =  256\nvmin  =  1.5000     vamin =  3.2000     vamax =  7.8300    \n                  vbmin =  1.5000     vbmax =  4.5200    \n SOURCE DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepths =         10.0000     (         10.0000    )\n RECEIVER DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepthr =         0.00000     (         0.00000    )\n RECEIVER DISTANCES (           3 )\n     r =         50.0000     tshift=         0.00000     vred=         0.00000    \n     r =         100.000     tshift=         0.00000     vred=         0.00000    \n     r =         150.000     tshift=         0.00000     vred=         0.00000    \nalpha =  9.7656250E-03     dt =       1.00    \nfrequencies for which response computed     \n 3.90625E-05 3.90625E-03 7.81250E-03 1.17188E-02 1.56250E-02 1.95312E-02 2.34375E-02 2.73438E-02\n 3.12500E-02 3.51562E-02 3.90625E-02 4.29688E-02 4.68750E-02 5.07812E-02 5.46875E-02 5.85938E-02\n 6.25000E-02 6.64062E-02 7.03125E-02 7.42188E-02 7.81250E-02 8.20312E-02 8.59375E-02 8.98438E-02\n 9.37500E-02 9.76562E-02 0.10156     0.10547     0.10938     0.11328     0.11719     0.12109    \n 0.12500     0.12891     0.13281     0.13672     0.14062     0.14453     0.14844     0.15234    \n 0.15625     0.16016     0.16406     0.16797     0.17188     0.17578     0.17969     0.18359    \n 0.18750     0.19141     0.19531     0.19922     0.20312     0.20703     0.21094     0.21484    \n 0.21875     0.22266     0.22656     0.23047     0.23438     0.23828     0.24219     0.24609    \n 0.25000     0.25391     0.25781     0.26172     0.26562     0.26953     0.27344     0.27734    \n 0.28125     0.28516     0.28906     0.29297     0.29688     0.30078     0.30469     0.30859    \n 0.31250     0.31641     0.32031     0.32422     0.32812     0.33203     0.33594     0.33984    \n 0.34375     0.34766     0.35156     0.35547     0.35938     0.36328     0.36719     0.37109    \n 0.37500     0.37891     0.38281     0.38672     0.39062     0.39453     0.39844     0.40234    \n 0.40625     0.41016     0.41406     0.41797     0.42188     0.42578     0.42969     0.43359    \n 0.43750     0.44141     0.44531     0.44922     0.45312     0.45703     0.46094     0.46484    \n 0.46875     0.47266     0.47656     0.48047     0.48438     0.48828     0.49219     0.49609    \n 0.50000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000    \n Transition to non-asymptotic at   7.81250000E-03\n     r(km)   t0(sec)    hs(km)    hr(km)    Tp(sec)   Tsv(sec)   Tsh(sec)\n   50.0       0.00       10.0       0.00       8.89       16.3       16.3    \n   100.       0.00       10.0       0.00       16.7       29.6       29.6    \n   150.       0.00       10.0       0.00       23.1       40.6       40.6    \n      0.0976562     10.0000000\n        256         1       129\n ieqex=            0  EARTHQUAKE + EXPLOSION\n   10\n  TOP  OF MODEL IS FREE SURFACE  \n  BASE OF MODEL IS HALFSPACE WITH PROPERTIES OF BOTTOM LAYER\n gil7.d\n LAYER             H      P-VEL     S-VEL   DENSITY  \n     1       1.000      3.200      1.500      2.280    \n     2       2.000      4.500      2.400      2.280    \n     3       1.000      4.800      2.780      2.580    \n     4       1.000      5.510      3.180      2.580    \n     5       12.00      6.210      3.400      2.680    \n     6       8.000      6.890      3.980      3.000    \n     7       0.000      7.830      4.520      3.260    \n Working model\n        d         a         b         rho      1/qa      1/qb       bsh    1/qbsh\n      1.000     3.200     1.500     2.280  0.001667  0.003333     1.500     2.280\n      2.000     4.500     2.400     2.280  0.001667  0.003333     2.400     2.280\n      1.000     4.800     2.780     2.580  0.001667  0.003333     2.780     2.580\n      1.000     5.510     3.180     2.580  0.001667  0.003333     3.180     2.580\n     12.000     6.210     3.400     2.680  0.001667  0.003333     3.400     2.680\n      8.000     6.890     3.980     3.000  0.001667  0.003333     3.980     3.000\n                7.830     4.520     3.260  0.001667  0.003333     4.520     3.260\n \nXLENG=  0.2496000E+04 XFAC=  0.4000000E+01\nWAVENUMBER FILTERING bounded in phase velocites\n[cmax,c1,c2,cmin]=[    -1.000,    -1.000,    -1.000,    -1.000]\n( -1.0 means 0.0 for cmin and infinity for cmax)\n  LAYER INSERTION DONE\n mmax=           8\n 0.1000E+01 0.3200E+01 0.1500E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.2000E+01 0.4500E+01 0.2400E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.4800E+01 0.2780E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.5510E+01 0.3180E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1000E+02 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.2000E+01 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.8000E+01 0.6890E+01 0.3980E+01 0.3000E+01 0.1667E-02 0.3333E-02\n 0.0000E+00 0.7830E+01 0.4520E+01 0.3260E+01 0.1667E-02 0.3333E-02\nfl =  0.0000         fu = 0.50000         df = 3.90625E-03\n    n1 =   1     n2 = 129      n =  256\nvmin  =  1.5000     vamin =  3.2000     vamax =  7.8300    \n                  vbmin =  1.5000     vbmax =  4.5200    \n SOURCE DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepths =         15.0000     (         15.0000    )\n RECEIVER DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepthr =         0.00000     (         0.00000    )\n RECEIVER DISTANCES (           3 )\n     r =         50.0000     tshift=         0.00000     vred=         0.00000    \n     r =         100.000     tshift=         0.00000     vred=         0.00000    \n     r =         150.000     tshift=         0.00000     vred=         0.00000    \nalpha =  9.7656250E-03     dt =       1.00    \nfrequencies for which response computed     \n 3.90625E-05 3.90625E-03 7.81250E-03 1.17188E-02 1.56250E-02 1.95312E-02 2.34375E-02 2.73438E-02\n 3.12500E-02 3.51562E-02 3.90625E-02 4.29688E-02 4.68750E-02 5.07812E-02 5.46875E-02 5.85938E-02\n 6.25000E-02 6.64062E-02 7.03125E-02 7.42188E-02 7.81250E-02 8.20312E-02 8.59375E-02 8.98438E-02\n 9.37500E-02 9.76562E-02 0.10156     0.10547     0.10938     0.11328     0.11719     0.12109    \n 0.12500     0.12891     0.13281     0.13672     0.14062     0.14453     0.14844     0.15234    \n 0.15625     0.16016     0.16406     0.16797     0.17188     0.17578     0.17969     0.18359    \n 0.18750     0.19141     0.19531     0.19922     0.20312     0.20703     0.21094     0.21484    \n 0.21875     0.22266     0.22656     0.23047     0.23438     0.23828     0.24219     0.24609    \n 0.25000     0.25391     0.25781     0.26172     0.26562     0.26953     0.27344     0.27734    \n 0.28125     0.28516     0.28906     0.29297     0.29688     0.30078     0.30469     0.30859    \n 0.31250     0.31641     0.32031     0.32422     0.32812     0.33203     0.33594     0.33984    \n 0.34375     0.34766     0.35156     0.35547     0.35938     0.36328     0.36719     0.37109    \n 0.37500     0.37891     0.38281     0.38672     0.39062     0.39453     0.39844     0.40234    \n 0.40625     0.41016     0.41406     0.41797     0.42188     0.42578     0.42969     0.43359    \n 0.43750     0.44141     0.44531     0.44922     0.45312     0.45703     0.46094     0.46484    \n 0.46875     0.47266     0.47656     0.48047     0.48438     0.48828     0.49219     0.49609    \n 0.50000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000    \n Transition to non-asymptotic at   7.81250000E-03\n     r(km)   t0(sec)    hs(km)    hr(km)    Tp(sec)   Tsv(sec)   Tsh(sec)\n   50.0       0.00       15.0       0.00       9.02       16.4       16.4    \n   100.       0.00       15.0       0.00       16.2       28.6       28.6    \n   150.       0.00       15.0       0.00       22.6       39.6       39.6    \n      0.0976562     10.0000000\n        256         1       129\n ieqex=            0  EARTHQUAKE + EXPLOSION\n   10\n  TOP  OF MODEL IS FREE SURFACE  \n  BASE OF MODEL IS HALFSPACE WITH PROPERTIES OF BOTTOM LAYER\n gil7.d\n LAYER             H      P-VEL     S-VEL   DENSITY  \n     1       1.000      3.200      1.500      2.280    \n     2       2.000      4.500      2.400      2.280    \n     3       1.000      4.800      2.780      2.580    \n     4       1.000      5.510      3.180      2.580    \n     5       12.00      6.210      3.400      2.680    \n     6       8.000      6.890      3.980      3.000    \n     7       0.000      7.830      4.520      3.260    \n Working model\n        d         a         b         rho      1/qa      1/qb       bsh    1/qbsh\n      1.000     3.200     1.500     2.280  0.001667  0.003333     1.500     2.280\n      2.000     4.500     2.400     2.280  0.001667  0.003333     2.400     2.280\n      1.000     4.800     2.780     2.580  0.001667  0.003333     2.780     2.580\n      1.000     5.510     3.180     2.580  0.001667  0.003333     3.180     2.580\n     12.000     6.210     3.400     2.680  0.001667  0.003333     3.400     2.680\n      8.000     6.890     3.980     3.000  0.001667  0.003333     3.980     3.000\n                7.830     4.520     3.260  0.001667  0.003333     4.520     3.260\n \nXLENG=  0.2496000E+04 XFAC=  0.4000000E+01\nWAVENUMBER FILTERING bounded in phase velocites\n[cmax,c1,c2,cmin]=[    -1.000,    -1.000,    -1.000,    -1.000]\n( -1.0 means 0.0 for cmin and infinity for cmax)\n  LAYER INSERTION DONE\n mmax=           8\n 0.1000E+01 0.3200E+01 0.1500E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.2000E+01 0.4500E+01 0.2400E+01 0.2280E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.4800E+01 0.2780E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1000E+01 0.5510E+01 0.3180E+01 0.2580E+01 0.1667E-02 0.3333E-02\n 0.1200E+02 0.6210E+01 0.3400E+01 0.2680E+01 0.1667E-02 0.3333E-02\n 0.3000E+01 0.6890E+01 0.3980E+01 0.3000E+01 0.1667E-02 0.3333E-02\n 0.5000E+01 0.6890E+01 0.3980E+01 0.3000E+01 0.1667E-02 0.3333E-02\n 0.0000E+00 0.7830E+01 0.4520E+01 0.3260E+01 0.1667E-02 0.3333E-02\nfl =  0.0000         fu = 0.50000         df = 3.90625E-03\n    n1 =   1     n2 = 129      n =  256\nvmin  =  1.5000     vamin =  3.2000     vamax =  7.8300    \n                  vbmin =  1.5000     vbmax =  4.5200    \n SOURCE DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepths =         20.0000     (         20.0000    )\n RECEIVER DEPTH IN WORKING AND ORIGINAL MODEL (           1 )\ndepthr =         0.00000     (         0.00000    )\n RECEIVER DISTANCES (           3 )\n     r =         50.0000     tshift=         0.00000     vred=         0.00000    \n     r =         100.000     tshift=         0.00000     vred=         0.00000    \n     r =         150.000     tshift=         0.00000     vred=         0.00000    \nalpha =  9.7656250E-03     dt =       1.00    \nfrequencies for which response computed     \n 3.90625E-05 3.90625E-03 7.81250E-03 1.17188E-02 1.56250E-02 1.95312E-02 2.34375E-02 2.73438E-02\n 3.12500E-02 3.51562E-02 3.90625E-02 4.29688E-02 4.68750E-02 5.07812E-02 5.46875E-02 5.85938E-02\n 6.25000E-02 6.64062E-02 7.03125E-02 7.42188E-02 7.81250E-02 8.20312E-02 8.59375E-02 8.98438E-02\n 9.37500E-02 9.76562E-02 0.10156     0.10547     0.10938     0.11328     0.11719     0.12109    \n 0.12500     0.12891     0.13281     0.13672     0.14062     0.14453     0.14844     0.15234    \n 0.15625     0.16016     0.16406     0.16797     0.17188     0.17578     0.17969     0.18359    \n 0.18750     0.19141     0.19531     0.19922     0.20312     0.20703     0.21094     0.21484    \n 0.21875     0.22266     0.22656     0.23047     0.23438     0.23828     0.24219     0.24609    \n 0.25000     0.25391     0.25781     0.26172     0.26562     0.26953     0.27344     0.27734    \n 0.28125     0.28516     0.28906     0.29297     0.29688     0.30078     0.30469     0.30859    \n 0.31250     0.31641     0.32031     0.32422     0.32812     0.33203     0.33594     0.33984    \n 0.34375     0.34766     0.35156     0.35547     0.35938     0.36328     0.36719     0.37109    \n 0.37500     0.37891     0.38281     0.38672     0.39062     0.39453     0.39844     0.40234    \n 0.40625     0.41016     0.41406     0.41797     0.42188     0.42578     0.42969     0.43359    \n 0.43750     0.44141     0.44531     0.44922     0.45312     0.45703     0.46094     0.46484    \n 0.46875     0.47266     0.47656     0.48047     0.48438     0.48828     0.49219     0.49609    \n 0.50000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000     -1.0000    \n Transition to non-asymptotic at   3.90625000E-03\n     r(km)   t0(sec)    hs(km)    hr(km)    Tp(sec)   Tsv(sec)   Tsh(sec)\n   50.0       0.00       20.0       0.00       9.00       16.2       16.2    \n   100.       0.00       20.0       0.00       15.8       27.8       27.8    \n   150.       0.00       20.0       0.00       22.2       38.9       38.9    \n</pre> Out[7]: <pre>0</pre> In\u00a0[8]: Copied! <pre>depth = 10\ndistance = 50\n\n\nZDD = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZDD'))[0].data\nRDD = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'RDD'))[0].data\nZDS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZDS'))[0].data\nRDS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'RDS'))[0].data\nTDS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'TDS'))[0].data\nZSS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZSS'))[0].data\nRSS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'RSS'))[0].data\nTSS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'TSS'))[0].data\nZEX = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZEX'))[0].data\nREX = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'REX'))[0].data\n\nfig = plt.figure()\nplt.subplot2grid((5,2), (0,0))\nplt.plot(ZDD)\nplt.title('ZDD')\nplt.subplot2grid((5,2), (0,1))\nplt.plot(RDD)\nplt.title('RDD')\nplt.subplot2grid((5,2), (1,0))\nplt.plot(ZDS)\nplt.title('ZDS')\nplt.subplot2grid((5,2), (1,1))\nplt.plot(RDS)\nplt.title('RDS')\nplt.subplot2grid((5,2), (2,0))\nplt.plot(TDS)\nplt.title('TDS')\nplt.subplot2grid((5,2), (2,1))\nplt.plot(ZSS)\nplt.title('ZSS')\nplt.subplot2grid((5,2), (3,0))\nplt.plot(RSS)\nplt.title('RSS')\nplt.subplot2grid((5,2), (3,1))\nplt.plot(TSS)\nplt.title('TSS')\nplt.subplot2grid((5,2), (4,0))\nplt.plot(ZEX)\nplt.title('ZEX')\nplt.subplot2grid((5,2), (4,1))\nplt.plot(REX)\nplt.title('REX')\nfig.tight_layout()\n</pre> depth = 10 distance = 50   ZDD = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZDD'))[0].data RDD = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'RDD'))[0].data ZDS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZDS'))[0].data RDS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'RDS'))[0].data TDS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'TDS'))[0].data ZSS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZSS'))[0].data RSS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'RSS'))[0].data TSS = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'TSS'))[0].data ZEX = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'ZEX'))[0].data REX = read(\"%s/%.0f.%.4f.%s\"%(green_dir,distance,depth,'REX'))[0].data  fig = plt.figure() plt.subplot2grid((5,2), (0,0)) plt.plot(ZDD) plt.title('ZDD') plt.subplot2grid((5,2), (0,1)) plt.plot(RDD) plt.title('RDD') plt.subplot2grid((5,2), (1,0)) plt.plot(ZDS) plt.title('ZDS') plt.subplot2grid((5,2), (1,1)) plt.plot(RDS) plt.title('RDS') plt.subplot2grid((5,2), (2,0)) plt.plot(TDS) plt.title('TDS') plt.subplot2grid((5,2), (2,1)) plt.plot(ZSS) plt.title('ZSS') plt.subplot2grid((5,2), (3,0)) plt.plot(RSS) plt.title('RSS') plt.subplot2grid((5,2), (3,1)) plt.plot(TSS) plt.title('TSS') plt.subplot2grid((5,2), (4,0)) plt.plot(ZEX) plt.title('ZEX') plt.subplot2grid((5,2), (4,1)) plt.plot(REX) plt.title('REX') fig.tight_layout() In\u00a0[9]: Copied! <pre>def syn_wf(ZDD,RDD,ZDS,RDS,TDS,ZSS,RSS,TSS,ZEX,REX,Mxx,Myy,Mzz,Mxy,Mxz,Myz,az):\n    uz = Mxx*(ZSS/2*np.cos(2*az)-ZDD/6+ZEX/3)+ \\\n          Myy*(-ZSS/2*np.cos(2*az)-ZDD/6+ZEX/3)+ \\\n          Mzz*(ZDD/3+ZEX/3)+Mxy*ZSS*np.sin(2*az)+ \\\n          Mxz*ZDS*np.cos(az)+Myz*ZDS*np.sin(az)\n\n    ur = Mxx*(RSS/2*np.cos(2*az)-RDD/6+REX/3)+ \\\n          Myy*(-RSS/2*np.cos(2*az)-RDD/6+REX/3)+ \\\n          Mzz*(RDD/3+REX/3)+Mxy*RSS*np.sin(2*az)+ \\\n          Mxz*(RDS*np.cos(az))+Myz*(RDS*np.sin(az))\n\n    ut = Mxx*(TSS/2*np.sin(2*az))+Myy*(-TSS/2*np.sin(2*az))+ \\\n          Mxy*(-TSS*np.cos(2*az))+Mxz*(TDS*np.sin(az))+Myz*(-TDS*np.cos(az))\n    return uz,ur,ut\n</pre> def syn_wf(ZDD,RDD,ZDS,RDS,TDS,ZSS,RSS,TSS,ZEX,REX,Mxx,Myy,Mzz,Mxy,Mxz,Myz,az):     uz = Mxx*(ZSS/2*np.cos(2*az)-ZDD/6+ZEX/3)+ \\           Myy*(-ZSS/2*np.cos(2*az)-ZDD/6+ZEX/3)+ \\           Mzz*(ZDD/3+ZEX/3)+Mxy*ZSS*np.sin(2*az)+ \\           Mxz*ZDS*np.cos(az)+Myz*ZDS*np.sin(az)      ur = Mxx*(RSS/2*np.cos(2*az)-RDD/6+REX/3)+ \\           Myy*(-RSS/2*np.cos(2*az)-RDD/6+REX/3)+ \\           Mzz*(RDD/3+REX/3)+Mxy*RSS*np.sin(2*az)+ \\           Mxz*(RDS*np.cos(az))+Myz*(RDS*np.sin(az))      ut = Mxx*(TSS/2*np.sin(2*az))+Myy*(-TSS/2*np.sin(2*az))+ \\           Mxy*(-TSS*np.cos(2*az))+Mxz*(TDS*np.sin(az))+Myz*(-TDS*np.cos(az))     return uz,ur,ut <p></p> In\u00a0[10]: Copied! <pre>az_all = np.arange(0,360,10)/180*np.pi\n\nuz_all = np.zeros((len(az_all),len(ZDD)))\nur_all = np.zeros((len(az_all),len(ZDD)))\nut_all = np.zeros((len(az_all),len(ZDD)))\n\n#Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([1,1,1,0,0,0])/np.sqrt(3) # isotropic explosion\n#Mxx,Myy,Mzz,Mxy,Mxz,Myz = -np.asarray([0,0,0,1,0,0])/np.sqrt(2) # double couple, strike slip\n#Mxx,Myy,Mzz,Mxy,Mxz,Myz = -np.asarray([0,0,0,0,1,0])/np.sqrt(2) # double couple, vertical dip slip\n#Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([-1,0,1,0,0,0])/np.sqrt(2) # double couple, 45 degree dip slip\n#Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([1,-2,1,0,0,0])/np.sqrt(6) # CLVD 1\nMxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([1,1,-2,0,0,0])/np.sqrt(6) # CLVD 2\n\nfor i in np.arange(len(az_all)):\n    uz_all[i,:],ur_all[i,:],ut_all[i,:] = syn_wf(ZDD,RDD,ZDS,RDS,TDS,ZSS,RSS,TSS,ZEX,REX,Mxx,Myy,Mzz,Mxy,Mxz,Myz,az_all[i])\n\nplt.figure(figsize=(10,4))\nplt.subplot(131)\nplt.pcolormesh(np.arange(len(ZDD)),az_all/np.pi*180,uz_all,cmap='gray')\nplt.xlabel('Time (s)')\nplt.ylabel('Azimuth (degree)')\nplt.title('uz')\nplt.subplot(132)\nplt.pcolormesh(np.arange(len(ZDD)),az_all/np.pi*180,ur_all,cmap='gray')\nplt.xlabel('Time (s)')\nplt.title('ur')\nplt.subplot(133)\nplt.pcolormesh(np.arange(len(ZDD)),az_all/np.pi*180,ut_all,cmap='gray')\nplt.xlabel('Time (s)')\nplt.title('ut')\n</pre> az_all = np.arange(0,360,10)/180*np.pi  uz_all = np.zeros((len(az_all),len(ZDD))) ur_all = np.zeros((len(az_all),len(ZDD))) ut_all = np.zeros((len(az_all),len(ZDD)))  #Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([1,1,1,0,0,0])/np.sqrt(3) # isotropic explosion #Mxx,Myy,Mzz,Mxy,Mxz,Myz = -np.asarray([0,0,0,1,0,0])/np.sqrt(2) # double couple, strike slip #Mxx,Myy,Mzz,Mxy,Mxz,Myz = -np.asarray([0,0,0,0,1,0])/np.sqrt(2) # double couple, vertical dip slip #Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([-1,0,1,0,0,0])/np.sqrt(2) # double couple, 45 degree dip slip #Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([1,-2,1,0,0,0])/np.sqrt(6) # CLVD 1 Mxx,Myy,Mzz,Mxy,Mxz,Myz = np.asarray([1,1,-2,0,0,0])/np.sqrt(6) # CLVD 2  for i in np.arange(len(az_all)):     uz_all[i,:],ur_all[i,:],ut_all[i,:] = syn_wf(ZDD,RDD,ZDS,RDS,TDS,ZSS,RSS,TSS,ZEX,REX,Mxx,Myy,Mzz,Mxy,Mxz,Myz,az_all[i])  plt.figure(figsize=(10,4)) plt.subplot(131) plt.pcolormesh(np.arange(len(ZDD)),az_all/np.pi*180,uz_all,cmap='gray') plt.xlabel('Time (s)') plt.ylabel('Azimuth (degree)') plt.title('uz') plt.subplot(132) plt.pcolormesh(np.arange(len(ZDD)),az_all/np.pi*180,ur_all,cmap='gray') plt.xlabel('Time (s)') plt.title('ur') plt.subplot(133) plt.pcolormesh(np.arange(len(ZDD)),az_all/np.pi*180,ut_all,cmap='gray') plt.xlabel('Time (s)') plt.title('ut') Out[10]: <pre>Text(0.5, 1.0, 'ut')</pre> <p>For inversion:</p> <ol> <li>preprocess earthquake waveforms</li> <li>generate green's functions</li> <li>Loop over all possible moment tensors and synthesize waveforms, compare the differences between observed and synthetic waveforms</li> <li>find the best-fitting moment tensor that can explain the observation</li> </ol> <p></p> <p>Solving the elastodynamic Green's function G is rather complicated. If we consider the simple case of a spherical wavefront from an isotropic source, some insight can also be obtained!</p> <p></p> In\u00a0[11]: Copied! <pre>def P_S_pol_amp(theta,phi):\n    # P wave amplitude\n    P_amp = abs(np.sin(2*theta)*np.cos(phi))\n    P_pol = np.sign(np.sin(2*theta)*np.cos(phi))\n    # S wave amplitude\n    s1 = np.cos(2*theta)*np.cos(phi)\n    s2 = -np.cos(theta)*np.sin(phi)\n    S_amp = np.sqrt(s1*s1+s2*s2)\n    return P_pol, P_amp, S_amp\n\n\ntheta_all = np.arange(0,360,5)/180*np.pi\nphi_all = np.arange(0,180,5)/180*np.pi\n\nP_amp_all = np.zeros((len(theta_all),len(phi_all)))\nS_amp_all = np.zeros((len(theta_all),len(phi_all)))\nP_pol_all = np.zeros((len(theta_all),len(phi_all)))\n\nfor itheta in np.arange(len(theta_all)):\n    for iphi in np.arange(len(phi_all)):\n        P_pol_all[itheta,iphi],P_amp_all[itheta,iphi],S_amp_all[itheta,iphi] =P_S_pol_amp(theta_all[itheta],phi_all[iphi])\n</pre> def P_S_pol_amp(theta,phi):     # P wave amplitude     P_amp = abs(np.sin(2*theta)*np.cos(phi))     P_pol = np.sign(np.sin(2*theta)*np.cos(phi))     # S wave amplitude     s1 = np.cos(2*theta)*np.cos(phi)     s2 = -np.cos(theta)*np.sin(phi)     S_amp = np.sqrt(s1*s1+s2*s2)     return P_pol, P_amp, S_amp   theta_all = np.arange(0,360,5)/180*np.pi phi_all = np.arange(0,180,5)/180*np.pi  P_amp_all = np.zeros((len(theta_all),len(phi_all))) S_amp_all = np.zeros((len(theta_all),len(phi_all))) P_pol_all = np.zeros((len(theta_all),len(phi_all)))  for itheta in np.arange(len(theta_all)):     for iphi in np.arange(len(phi_all)):         P_pol_all[itheta,iphi],P_amp_all[itheta,iphi],S_amp_all[itheta,iphi] =P_S_pol_amp(theta_all[itheta],phi_all[iphi]) In\u00a0[12]: Copied! <pre>fig = plt.figure()\nplt.subplot(221)\nplt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,P_pol_all.T,cmap='bwr')\nplt.colorbar()\nplt.ylabel('Dip phi (degree)')\nplt.title('P wave polarity')\nplt.subplot(222)\nplt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,P_amp_all.T,cmap='jet')\nplt.colorbar()\nplt.title('P wave amplitude')\nplt.subplot(223)\nplt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,S_amp_all.T,cmap='jet')\nplt.colorbar()\nplt.ylabel('Dip phi (degree)')\nplt.xlabel('Azi theta (degree)')\nplt.title('S wave amplitude')\nplt.subplot(224)\nplt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,np.log10(P_amp_all/S_amp_all).T,cmap='jet',vmin=-2,vmax=2)\nplt.colorbar()\nplt.xlabel('Azi theta (degree)')\nplt.title('Log10(P/S wave ratio)')\nfig.tight_layout()\n</pre> fig = plt.figure() plt.subplot(221) plt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,P_pol_all.T,cmap='bwr') plt.colorbar() plt.ylabel('Dip phi (degree)') plt.title('P wave polarity') plt.subplot(222) plt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,P_amp_all.T,cmap='jet') plt.colorbar() plt.title('P wave amplitude') plt.subplot(223) plt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,S_amp_all.T,cmap='jet') plt.colorbar() plt.ylabel('Dip phi (degree)') plt.xlabel('Azi theta (degree)') plt.title('S wave amplitude') plt.subplot(224) plt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,np.log10(P_amp_all/S_amp_all).T,cmap='jet',vmin=-2,vmax=2) plt.colorbar() plt.xlabel('Azi theta (degree)') plt.title('Log10(P/S wave ratio)') fig.tight_layout() <pre>/var/folders/gc/lpnp82h92tv35c_7v97w97cm0000gn/T/ipykernel_5711/3513014322.py:18: RuntimeWarning: divide by zero encountered in log10\n  plt.pcolormesh(theta_all/np.pi*180,phi_all/np.pi*180,np.log10(P_amp_all/S_amp_all).T,cmap='jet',vmin=-2,vmax=2)\n</pre> <p>For inversion:</p> <ol> <li>measure polarity and amplitude ratio from earthquake waveforms</li> <li>synthesize polarity and amplitude ratio for a double-couple source</li> <li>rotate the synthesized waveform features to all possible strike, dip and rake</li> <li>find the best-fitting focal mechanism that can best explain all observations</li> </ol> <p></p> In\u00a0[13]: Copied! <pre>from obspy.clients.fdsn import Client\nfrom obspy import read_events, UTCDateTime\nfrom obspy.clients.fdsn.mass_downloader import CircularDomain, Restrictions, MassDownloader\nimport os\n</pre> from obspy.clients.fdsn import Client from obspy import read_events, UTCDateTime from obspy.clients.fdsn.mass_downloader import CircularDomain, Restrictions, MassDownloader import os  In\u00a0[14]: Copied! <pre>event_bool = True\n\nif event_bool:\n    dataCenter=\"IRIS\"\n    client = Client(dataCenter)\n    starttime = UTCDateTime(\"2019-07-16T00:00:00\")\n    endtime = UTCDateTime(\"2019-07-16T23:59:59\")\n    catalog = client.get_events(starttime=starttime, endtime=endtime,\n                        minmagnitude=4,maxmagnitude=5,\n                        minlatitude=36, maxlatitude=38,\n                        minlongitude=-122, maxlongitude=-120)\n</pre> event_bool = True  if event_bool:     dataCenter=\"IRIS\"     client = Client(dataCenter)     starttime = UTCDateTime(\"2019-07-16T00:00:00\")     endtime = UTCDateTime(\"2019-07-16T23:59:59\")     catalog = client.get_events(starttime=starttime, endtime=endtime,                         minmagnitude=4,maxmagnitude=5,                         minlatitude=36, maxlatitude=38,                         minlongitude=-122, maxlongitude=-120) In\u00a0[15]: Copied! <pre>dataCenter=\"NCEDC\"\n\n# Time before and after event origin for waveform segments\ntime_before = 60\ntime_after = 300\ndownload_bool = True\n\n#obtain event information\nevent = catalog[0]\nevid = str(event.origins[0].resource_id).split(\"=\")[-1]\nevorigin = event.preferred_origin().time\nevlo = event.preferred_origin().longitude\nevla = event.preferred_origin().latitude\nevdep = event.preferred_origin().depth # in meters\nevmag = event.preferred_magnitude().mag\nauth = event.preferred_origin().extra.catalog.value.replace(\" \",\"\")\n# setup waveform download criteria\nstarttime = evorigin - time_before\nendtime = evorigin + time_after\n\ndomain = CircularDomain(latitude=evla, longitude=evlo, minradius=0.7, maxradius=1.3)\nrestrictions = Restrictions(starttime=starttime, endtime=endtime,\n        reject_channels_with_gaps=True,\n        minimum_length=0.95,\n        network=\"BK\",\n        channel_priorities=[\"BH[ZNE12]\", \"HH[ZNE12]\"],\n        sanitize=True)\n\n# Dowanload waveforms and metadata\nevent_dir = path+evid\nif not os.path.isdir(event_dir):\n    os.mkdir(event_dir)\nif download_bool:\n    mseed_storage = \"%s/waveforms\"%event_dir\n    stationxml_storage = \"%s/stations\"%event_dir\n    mdl = MassDownloader(providers=[dataCenter])\n    mdl_helper = mdl.download(domain, restrictions,\n        mseed_storage=mseed_storage,stationxml_storage=stationxml_storage)\n    print(\"%s download completed\"%event_dir)\n</pre> dataCenter=\"NCEDC\"  # Time before and after event origin for waveform segments time_before = 60 time_after = 300 download_bool = True  #obtain event information event = catalog[0] evid = str(event.origins[0].resource_id).split(\"=\")[-1] evorigin = event.preferred_origin().time evlo = event.preferred_origin().longitude evla = event.preferred_origin().latitude evdep = event.preferred_origin().depth # in meters evmag = event.preferred_magnitude().mag auth = event.preferred_origin().extra.catalog.value.replace(\" \",\"\") # setup waveform download criteria starttime = evorigin - time_before endtime = evorigin + time_after  domain = CircularDomain(latitude=evla, longitude=evlo, minradius=0.7, maxradius=1.3) restrictions = Restrictions(starttime=starttime, endtime=endtime,         reject_channels_with_gaps=True,         minimum_length=0.95,         network=\"BK\",         channel_priorities=[\"BH[ZNE12]\", \"HH[ZNE12]\"],         sanitize=True)  # Dowanload waveforms and metadata event_dir = path+evid if not os.path.isdir(event_dir):     os.mkdir(event_dir) if download_bool:     mseed_storage = \"%s/waveforms\"%event_dir     stationxml_storage = \"%s/stations\"%event_dir     mdl = MassDownloader(providers=[dataCenter])     mdl_helper = mdl.download(domain, restrictions,         mseed_storage=mseed_storage,stationxml_storage=stationxml_storage)     print(\"%s download completed\"%event_dir)  <pre>[2023-10-28 11:01:42,699] - obspy.clients.fdsn.mass_downloader - INFO: Initializing FDSN client(s) for NCEDC.\n[2023-10-28 11:01:42,771] - obspy.clients.fdsn.mass_downloader - INFO: Successfully initialized 1 client(s): NCEDC.\n[2023-10-28 11:01:42,773] - obspy.clients.fdsn.mass_downloader - INFO: Total acquired or preexisting stations: 0\n[2023-10-28 11:01:42,773] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Requesting unreliable availability.\n[2023-10-28 11:01:44,703] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully requested availability (1.93 seconds)\n[2023-10-28 11:01:44,727] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Found 13 stations (39 channels).\n[2023-10-28 11:01:44,727] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Will attempt to download data from 13 stations.\n[2023-10-28 11:01:44,729] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Status for 39 time intervals/channels before downloading: NEEDS_DOWNLOADING\n[2023-10-28 11:01:46,690] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded 36 channels (of 39)\n[2023-10-28 11:01:46,691] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Launching basic QC checks...\n[2023-10-28 11:01:46,712] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Downloaded 3.7 MB [1939.79 KB/sec] of data, 0.0 MB of which were discarded afterwards.\n[2023-10-28 11:01:46,712] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Status for 3 time intervals/channels after downloading: DOWNLOAD_FAILED\n[2023-10-28 11:01:46,712] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Status for 36 time intervals/channels after downloading: DOWNLOADED\n[2023-10-28 11:01:49,592] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.CMB.xml'.\n[2023-10-28 11:01:49,593] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.CVS.xml'.\n[2023-10-28 11:01:49,728] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.BUCR.xml'.\n[2023-10-28 11:01:52,462] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.FARB.xml'.\n[2023-10-28 11:01:52,462] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.MCCM.xml'.\n[2023-10-28 11:01:52,573] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.MNRC.xml'.\n[2023-10-28 11:01:55,552] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.OAKV.xml'.\n[2023-10-28 11:01:55,556] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.QRDG.xml'.\n[2023-10-28 11:01:55,580] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.RUSS.xml'.\n[2023-10-28 11:01:58,501] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.SCZ.xml'.\n[2023-10-28 11:01:58,503] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.SAO.xml'.\n[2023-10-28 11:01:58,599] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Successfully downloaded './results40191336/stations/BK.WELL.xml'.\n[2023-10-28 11:01:58,613] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Downloaded 12 station files [1.1 MB] in 11.9 seconds [93.82 KB/sec].\n[2023-10-28 11:01:58,617] - obspy.clients.fdsn.mass_downloader - INFO: ============================== Final report\n[2023-10-28 11:01:58,617] - obspy.clients.fdsn.mass_downloader - INFO: 0 MiniSEED files [0.0 MB] already existed.\n[2023-10-28 11:01:58,617] - obspy.clients.fdsn.mass_downloader - INFO: 0 StationXML files [0.0 MB] already existed.\n[2023-10-28 11:01:58,618] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Acquired 36 MiniSEED files [3.7 MB].\n[2023-10-28 11:01:58,618] - obspy.clients.fdsn.mass_downloader - INFO: Client 'NCEDC' - Acquired 12 StationXML files [1.1 MB].\n[2023-10-28 11:01:58,619] - obspy.clients.fdsn.mass_downloader - INFO: Downloaded 4.8 MB in total.\n</pre> <pre>./results40191336 download completed\n</pre> In\u00a0[16]: Copied! <pre>from obspy import read, read_inventory, UTCDateTime\nfrom obspy.geodetics.base import gps2dist_azimuth\nfrom obspy.core.util.attribdict import AttribDict\n\npre_filt = (0.004,0.007,10,20)\nsac_dir = \"%s/sac\"%event_dir # Output directory\nif not os.path.isdir(sac_dir):\n    os.mkdir(sac_dir)\n# Read response files\ninv = read_inventory(\"%s/stations/*\"%event_dir,format='STATIONXML')\n# Read data\nst = read(\"%s/waveforms/*\"%event_dir,format='MSEED')\n# Detrend and remove instrument response\nst.detrend(type=\"linear\") # equivalent to rtr in SAC\nst.remove_response(inventory=inv, pre_filt=pre_filt, output=\"DISP\", zero_mean=True) # correct to displacement\nst.detrend(type=\"linear\")\nst.detrend(type=\"demean\") # remove mean\n# Define SAC headers and calculate back-azimtuh for rotation\norigin_time = UTCDateTime(evorigin)\ndepth = evdep*1000\nfor tr in st:\n    meta = inv.get_channel_metadata(tr.id)\n    dist, az, baz = gps2dist_azimuth(evla,evlo,meta['latitude'],meta['longitude'])\n    omarker = origin_time - tr.stats.starttime\n    # Obspy trace headers\n    tr.stats.distance = dist\n    tr.stats.back_azimuth = baz\n    # SAC headers\n    sacd = AttribDict()\n    sacd.stla = meta['latitude']\n    sacd.stlo = meta['longitude']\n    sacd.stel = meta['elevation']\n    sacd.evla = evla\n    sacd.evlo = evlo\n    sacd.evdp = evdep # in meters\n    sacd.az = az\n    sacd.baz = baz\n    sacd.dist = dist/1000 # convert to kilometers\n    sacd.o = 0\n    sacd.b = -1*omarker\n    tr.stats.sac = sacd\n# Rotate to ZNE\nst._rotate_to_zne(inv,components=(\"ZNE\",\"Z12\"))\n# Get station names\nnetstaloccha = sorted(set(\n            [(tr.stats.network, tr.stats.station, tr.stats.location, tr.stats.channel[:-1]) for tr in st]\n            ))\n# Keep only three-component seismograms, then rotate horizontals to RT\nfor net, sta, loc, cha in netstaloccha:\n    traces = st.select(network=net,station=sta,location=loc,channel=\"%s[ZNE]\"%cha)\n    if len(traces) != 3:\n        for tr in traces:\n            st.remove(tr)\n    else:\n        traces.rotate(method=\"NE-&gt;RT\")\n\n# Update station names\nnetstaloccha = set(\n        [(tr.stats.network, tr.stats.station, tr.stats.location, tr.stats.channel[:-1], \"ZRT\",\n          tr.stats.sac.dist, tr.stats.sac.az, tr.stats.sac.stlo, tr.stats.sac.stla) for tr in st.select(component=\"Z\")]\n    )\n# save the data in SAC format\nprint(\"Saving instrument corrected data to %s.\"%sac_dir)\nfor tr in st:\n    tr.write(\"%s/%s\"%(sac_dir,tr.id),format=\"SAC\")\n</pre> from obspy import read, read_inventory, UTCDateTime from obspy.geodetics.base import gps2dist_azimuth from obspy.core.util.attribdict import AttribDict  pre_filt = (0.004,0.007,10,20) sac_dir = \"%s/sac\"%event_dir # Output directory if not os.path.isdir(sac_dir):     os.mkdir(sac_dir) # Read response files inv = read_inventory(\"%s/stations/*\"%event_dir,format='STATIONXML') # Read data st = read(\"%s/waveforms/*\"%event_dir,format='MSEED') # Detrend and remove instrument response st.detrend(type=\"linear\") # equivalent to rtr in SAC st.remove_response(inventory=inv, pre_filt=pre_filt, output=\"DISP\", zero_mean=True) # correct to displacement st.detrend(type=\"linear\") st.detrend(type=\"demean\") # remove mean # Define SAC headers and calculate back-azimtuh for rotation origin_time = UTCDateTime(evorigin) depth = evdep*1000 for tr in st:     meta = inv.get_channel_metadata(tr.id)     dist, az, baz = gps2dist_azimuth(evla,evlo,meta['latitude'],meta['longitude'])     omarker = origin_time - tr.stats.starttime     # Obspy trace headers     tr.stats.distance = dist     tr.stats.back_azimuth = baz     # SAC headers     sacd = AttribDict()     sacd.stla = meta['latitude']     sacd.stlo = meta['longitude']     sacd.stel = meta['elevation']     sacd.evla = evla     sacd.evlo = evlo     sacd.evdp = evdep # in meters     sacd.az = az     sacd.baz = baz     sacd.dist = dist/1000 # convert to kilometers     sacd.o = 0     sacd.b = -1*omarker     tr.stats.sac = sacd # Rotate to ZNE st._rotate_to_zne(inv,components=(\"ZNE\",\"Z12\")) # Get station names netstaloccha = sorted(set(             [(tr.stats.network, tr.stats.station, tr.stats.location, tr.stats.channel[:-1]) for tr in st]             )) # Keep only three-component seismograms, then rotate horizontals to RT for net, sta, loc, cha in netstaloccha:     traces = st.select(network=net,station=sta,location=loc,channel=\"%s[ZNE]\"%cha)     if len(traces) != 3:         for tr in traces:             st.remove(tr)     else:         traces.rotate(method=\"NE-&gt;RT\")  # Update station names netstaloccha = set(         [(tr.stats.network, tr.stats.station, tr.stats.location, tr.stats.channel[:-1], \"ZRT\",           tr.stats.sac.dist, tr.stats.sac.az, tr.stats.sac.stlo, tr.stats.sac.stla) for tr in st.select(component=\"Z\")]     ) # save the data in SAC format print(\"Saving instrument corrected data to %s.\"%sac_dir) for tr in st:     tr.write(\"%s/%s\"%(sac_dir,tr.id),format=\"SAC\") <pre>Saving instrument corrected data to ./results40191336/sac.\n</pre> In\u00a0[17]: Copied! <pre>import matplotlib.pyplot as plt\n\nfreqmin = 0.02\nfreqmax = 0.05\ncorners = 2\nst_filt = st.copy()\n\n# Apply filter and taper the edges\nst_filt.filter(\"bandpass\",freqmin=freqmin,freqmax=freqmax,corners=corners,zerophase=True)\nst_filt.taper(max_percentage=0.05)\n\n# Each seismogram is normalized against each trace\nxmin = 0\nxmax = 150\nymin = 75\nymax = 145\nscale = 2 # scale the traces\nfig, axes = plt.subplots(1,3,figsize=(15,10))\nfor component, ax in zip((\"T\",\"R\",\"Z\"),axes):\n    for tr in st_filt.select(component=component):\n        times = tr.times() - (origin_time - tr.stats.starttime)\n        tr.data /= max(abs(tr.data))\n        tr.data *= scale\n        ax.plot(times,tr.data+tr.stats.sac.dist,color=\"black\",linewidth=0.8)\n        ax.text(xmax,tr.stats.sac.dist,\"%s.%s\"%(tr.stats.network,tr.stats.station),va=\"bottom\",ha=\"right\")\n    ax.set_xlim(xmin,xmax)\n    ax.set_ylim(ymin,ymax)\n    ax.set_xlabel(\"Times [s]\")\n    ax.set_ylabel(\"Distance [km]\")\n    ax.set_title(\"%s: bp %.0f-%.0f seconds\"%(component,1/freqmax,1/freqmin))\n</pre> import matplotlib.pyplot as plt  freqmin = 0.02 freqmax = 0.05 corners = 2 st_filt = st.copy()  # Apply filter and taper the edges st_filt.filter(\"bandpass\",freqmin=freqmin,freqmax=freqmax,corners=corners,zerophase=True) st_filt.taper(max_percentage=0.05)  # Each seismogram is normalized against each trace xmin = 0 xmax = 150 ymin = 75 ymax = 145 scale = 2 # scale the traces fig, axes = plt.subplots(1,3,figsize=(15,10)) for component, ax in zip((\"T\",\"R\",\"Z\"),axes):     for tr in st_filt.select(component=component):         times = tr.times() - (origin_time - tr.stats.starttime)         tr.data /= max(abs(tr.data))         tr.data *= scale         ax.plot(times,tr.data+tr.stats.sac.dist,color=\"black\",linewidth=0.8)         ax.text(xmax,tr.stats.sac.dist,\"%s.%s\"%(tr.stats.network,tr.stats.station),va=\"bottom\",ha=\"right\")     ax.set_xlim(xmin,xmax)     ax.set_ylim(ymin,ymax)     ax.set_xlabel(\"Times [s]\")     ax.set_ylabel(\"Distance [km]\")     ax.set_title(\"%s: bp %.0f-%.0f seconds\"%(component,1/freqmax,1/freqmin))"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#1-introduction","title":"1. Introduction\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#11-historical-development-of-earthquake-mechanics","title":"1.1 Historical development of earthquake mechanics\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#12-what-is-moment-tensor","title":"1.2 What is moment tensor?\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#13-what-is-focal-mechanism","title":"1.3 What is focal mechanism?\u00b6","text":"<p>Focal mechanism: Fault orientation and slip direction on which the earthquake occurred</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#14-why-moment-tensor-and-focal-mechanism-is-important","title":"1.4 Why moment tensor and focal mechanism is important?\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#131-fault-orientation","title":"1.3.1 Fault orientation\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#132-slip-direction","title":"1.3.2 Slip direction\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#133-stress-orientation","title":"1.3.3 Stress orientation\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#134-hydrothermal-nuclear-and-other-processes","title":"1.3.4 hydrothermal, nuclear, and other processes\u00b6","text":"<p>From Miller, A. D., Foulger, G. R., &amp; Julian, B. R. (1998). Non\u2010double\u2010couple earthquakes 2. Observations. Reviews of Geophysics, 36(4), 551-568.</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#2-moment-tensor-calculation","title":"2. Moment tensor calculation\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#21-install-packages","title":"2.1 Install packages\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#22-obtain-greens-functions","title":"2.2 Obtain green's functions\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#23-generate-synthetic-waveforms","title":"2.3 Generate synthetic waveforms\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#3-focal-mechanism","title":"3. Focal mechanism\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#31-suppose-the-source-time-function-is","title":"3.1 Suppose the source-time function is\u00b6","text":"<p>\\begin{equation} 4 \\pi \\delta (r)f(t) \\end{equation}</p> <p>The displacement field can be expressed as</p> <p>\\begin{equation} u(r,t)=(\\frac{1}{r^2})f(t-r/ \\alpha)+(\\frac{1}{r\\alpha})(\\frac{\\partial f(t-r/ \\alpha)}{\\partial \\tau}) \\end{equation}</p> <p>The first term decays as 1/\ud835\udc5f2 and is called the near-field term since it is important only relatively close to the source. The second term decays as 1/r and is called the far-field term because it will dominant at large distances from the source.</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#32-more-complicated-point-force-and-double-couple-sources","title":"3.2 More complicated point force and double couple sources\u00b6","text":"<p>\\begin{equation} \\boldsymbol{u_i}^p(\\boldsymbol{x},t) = \\frac{1}{4 \\pi \\rho \\alpha^3}\\frac{x_i x_j x_k}{r^3} \\frac{1}{r} \\dot{M_{jk}}(t-\\frac{r}{\\alpha}) \\end{equation}</p> <p>Where r^2 = \ud835\udc651^2 + \ud835\udc652^2 + \ud835\udc653^2 is the squared distance to the receiver</p> <p>\\dot{M_0} is the time derivative of the moment tensor.</p> <p>This is a general expression of the far-field P displacements for any moment tensor representation of the source</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#33-more-specific-example-of-a-fault-described-by-a-double-couple-source","title":"3.3 More specific example of a fault described by a double-couple source\u00b6","text":"<p>fault is in the (x1,x2) plane with motion in the x1 direction, we then have M13 = M31 = M0 and</p> <p>\\begin{equation} \\boldsymbol{u_i}^p(\\boldsymbol{x},t) = \\frac{1}{2 \\pi \\rho \\alpha^3}\\frac{x_i x_1 x_3}{r^3} \\frac{1}{r} \\dot{M_{0}}(t-\\frac{r}{\\alpha}) \\end{equation}</p> <p>Substituting x using phi and theta based on \\begin{equation} x_3/r = cos \\theta \\end{equation}</p> <p>\\begin{equation} x_1/r = sin \\theta cos \\phi \\end{equation}</p> <p>\\begin{equation} x_i/r = \\hat{r_i} \\end{equation}</p> <p>We have M13 = M31 = M0</p> <p>\\begin{equation} \\boldsymbol{u}^p = \\frac{1}{4 \\pi \\rho \\alpha^3}sin2\\theta cos\\phi \\frac{1}{r}\\dot{M_0}(t-\\frac{r}{\\alpha})\\hat{\\boldsymbol{r}} \\end{equation}</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#34-s-wave-expression","title":"3.4 S wave expression\u00b6","text":"<p>General expression for far-field S displacements:</p> <p>\\begin{equation} \\boldsymbol{u_i}^S(\\boldsymbol{x},t) = \\frac{(\\delta_{ij}-\\gamma_i \\gamma_j)\\gamma_k}{4 \\pi \\rho \\beta^3}\\frac{1}{r} \\dot{M_{jk}}(t-\\frac{r}{\\beta}) \\end{equation}</p> <p>beta is the shear veloity and the direction cosines are \ud835\udefei = xi/r</p> <p>For a double-couple source with geometry shown in the figure above, it can be written as</p> <p>\\begin{equation} \\boldsymbol{u_i}^S(\\boldsymbol{x},t) = \\frac{1}{4 \\pi \\rho \\beta^3}(cos 2\\theta cos \\phi \\boldsymbol{\\hat{\\theta}}-cos\\theta sin\\phi \\boldsymbol{\\hat{\\phi}})\\frac{1}{r} \\dot{M_{0}}(t-\\frac{r}{\\beta}) \\end{equation}</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#35-visualization-of-waveform-features","title":"3.5 visualization of waveform features\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#4-download-and-visualize-earthquake-information","title":"4. Download and visualize earthquake information\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#41-obtain-earthquake-basic-information","title":"4.1 Obtain earthquake basic information\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#42-obtain-earthquake-phase-information","title":"4.2 Obtain earthquake phase information\u00b6","text":"<ul> <li>Southern California: use STP</li> </ul> <pre><code>PHASE \u2013f 10167485.phase -e 10167485\n</code></pre> <ul> <li>Northern California: use FTP</li> </ul> <pre><code>https://ncedc.org/ftp/pub/catalogs/ncss/hypoinverse/phase2k/\n</code></pre>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#43-download-waveform","title":"4.3 Download waveform\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#44-waveform-preprocessing","title":"4.4 waveform preprocessing\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#45-waveform-visualization","title":"4.5 Waveform visualization\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#5-related-packages","title":"5. Related packages\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#package-for-moment-tensor-inversion","title":"Package for moment tensor inversion\u00b6","text":"<p>https://github.com/LLNL/mttime/tree/master</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#package-for-focal-mechanism-calculation","title":"Package for focal mechanism calculation\u00b6","text":"<p>https://github.com/cecece08/REFOC-and-HASH/tree/main</p>"},{"location":"eps207-observational-seismology/lectures/08_focal_mechanism_and_momemt_tensor/#package-for-stress-inversion","title":"Package for stress inversion\u00b6","text":"<p>https://www.ig.cas.cz/en/stress-inverse/</p>"},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/","title":"Focal Mechanism Inversion","text":"<p>Notebooks: codes/focal_mechanism.ipynb</p>"},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/#fault-plane","title":"Fault plane","text":""},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/#focal-mechanism-beachball","title":"Focal Mechanism Beachball","text":""},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/#radiation-pattern","title":"Radiation pattern","text":""},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/#how-to-determine-focal-mechanism","title":"How to determine focal mechanism?","text":"<p>Review: Inverse Problems in Geophysics</p> <ul> <li>Forward function: last lecture</li> <li>Objective/Loss function</li> <li>Gradient</li> <li>Optimizer</li> </ul>"},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/#focal-mechanism-from-first-motion-polarity","title":"Focal mechanism from first motion polarity","text":"<ul> <li>FPFIT</li> </ul> <p>Objective/Loss function:  $$ F^{i, j}=\\frac{\\sum_k \\left{| p_0^{j, k}-p_t^{i, k} \\mid \\cdot w_0^{j, k} \\cdot w_t^{i, k}\\right}}{\\sum_k\\left{w_0^{j, k} \\cdot w_t^{i, k}\\right}} $$</p> <p>\\(P_0^{j, k} are P_t^{i, k}\\) are the observed and theoretical first-motion polarity (0.5 for compression, -0.5 for dilatation). \\(w_t^{i, k}=[A(i, k)]^{1 / 2}\\) is the square root of the normalized theoretical P-wave radiation amplitude \\(A(i, k)\\) of earthquake \\(E^j\\) recorded at the \\(k^{\\text {th }}\\) station for source model \\(M^i\\).</p>"},{"location":"eps207-observational-seismology/lectures/09_focal_mechanism/#focal-mechanism-from-first-motion-polarity_1","title":"Focal mechanism from first motion polarity","text":"<ul> <li>HASH</li> </ul>"},{"location":"eps207-observational-seismology/lectures/codes/denoising/","title":"Denoising","text":"<p>Zhu, W., Mousavi, S. M., &amp; Beroza, G. C. (2019). Seismic signal denoising and decomposition using deep neural networks. IEEE Transactions on Geoscience and Remote Sensing, 57(11), 9476-9488.</p> <p>Paper: https://ieeexplore.ieee.org/abstract/document/8802278</p> <p>Code: https://github.com/AI4EPS/DeepDenoiser</p> <p></p> In\u00a0[1]: Copied! <pre>import os, sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy\nimport requests\nsys.path.insert(0, os.path.abspath(\"../\"))\n</pre> import os, sys import numpy as np import matplotlib.pyplot as plt import obspy import requests sys.path.insert(0, os.path.abspath(\"../\")) In\u00a0[2]: Copied! <pre># DEEPDENOISER_API_URL = \"http://127.0.0.1:8002\"\nDEEPDENOISER_API_URL = \"https://ai4eps-deepdenoiser.hf.space\" # gcp\n# DEEPDENOISER_API_URL = \"http://test.quakeflow.com:8003\" # gcp\n# DEEPDENOISER_API_URL = \"http://131.215.74.195:8003\" # local machine\n</pre> # DEEPDENOISER_API_URL = \"http://127.0.0.1:8002\" DEEPDENOISER_API_URL = \"https://ai4eps-deepdenoiser.hf.space\" # gcp # DEEPDENOISER_API_URL = \"http://test.quakeflow.com:8003\" # gcp # DEEPDENOISER_API_URL = \"http://131.215.74.195:8003\" # local machine In\u00a0[3]: Copied! <pre>import obspy\nstream = obspy.read()\nstream.plot();\n</pre> import obspy stream = obspy.read() stream.plot(); In\u00a0[4]: Copied! <pre>## Extract 3-component data\nstream = stream.sort()\nassert(len(stream) == 3)\ndata = []\nfor trace in stream:\n    data.append(trace.data)\ndata = np.array(data).T\nassert(data.shape[-1] == 3)\ndata_id = stream[0].get_id()[:-1]\ntimestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n\n## Add some noise\nnoisy_data = data + np.random.randn(*data.shape)*np.max(data)/20\n</pre> ## Extract 3-component data stream = stream.sort() assert(len(stream) == 3) data = [] for trace in stream:     data.append(trace.data) data = np.array(data).T assert(data.shape[-1] == 3) data_id = stream[0].get_id()[:-1] timestamp = stream[0].stats.starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  ## Add some noise noisy_data = data + np.random.randn(*data.shape)*np.max(data)/20 In\u00a0[5]: Copied! <pre>req = {\"id\": [data_id],\n       \"timestamp\": [timestamp],\n       \"vec\": [noisy_data.tolist()]}\n\nresp = requests.post(f'{DEEPDENOISER_API_URL}/predict', json=req)\nprint(resp)\n\ndenoised_data = np.array(resp.json()[\"vec\"])\n\nplt.figure(figsize=(10,4))\nplt.subplot(331)\nplt.plot(data[:,0], 'k', linewidth=0.5, label=\"E\")\nplt.legend()\nplt.title(\"Raw signal\")\nplt.subplot(332)\nplt.plot(noisy_data[:,0], 'k', linewidth=0.5, label=\"E\")\nplt.title(\"Nosiy signal\")\nplt.subplot(333)\nplt.plot(denoised_data[0, :,0], 'k', linewidth=0.5, label=\"E\")\nplt.title(\"Denoised signal\")\nplt.subplot(334)\nplt.plot(data[:,1], 'k', linewidth=0.5, label=\"N\")\nplt.legend()\nplt.subplot(335)\nplt.plot(noisy_data[:,1], 'k', linewidth=0.5, label=\"N\")\nplt.subplot(336)\nplt.plot(denoised_data[0,:,1], 'k', linewidth=0.5, label=\"N\")\nplt.subplot(337)\nplt.plot(data[:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.legend()\nplt.subplot(338)\nplt.plot(noisy_data[:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.subplot(339)\nplt.plot(denoised_data[0,:,2], 'k', linewidth=0.5, label=\"Z\")\nplt.tight_layout()\nplt.show();\n</pre> req = {\"id\": [data_id],        \"timestamp\": [timestamp],        \"vec\": [noisy_data.tolist()]}  resp = requests.post(f'{DEEPDENOISER_API_URL}/predict', json=req) print(resp)  denoised_data = np.array(resp.json()[\"vec\"])  plt.figure(figsize=(10,4)) plt.subplot(331) plt.plot(data[:,0], 'k', linewidth=0.5, label=\"E\") plt.legend() plt.title(\"Raw signal\") plt.subplot(332) plt.plot(noisy_data[:,0], 'k', linewidth=0.5, label=\"E\") plt.title(\"Nosiy signal\") plt.subplot(333) plt.plot(denoised_data[0, :,0], 'k', linewidth=0.5, label=\"E\") plt.title(\"Denoised signal\") plt.subplot(334) plt.plot(data[:,1], 'k', linewidth=0.5, label=\"N\") plt.legend() plt.subplot(335) plt.plot(noisy_data[:,1], 'k', linewidth=0.5, label=\"N\") plt.subplot(336) plt.plot(denoised_data[0,:,1], 'k', linewidth=0.5, label=\"N\") plt.subplot(337) plt.plot(data[:,2], 'k', linewidth=0.5, label=\"Z\") plt.legend() plt.subplot(338) plt.plot(noisy_data[:,2], 'k', linewidth=0.5, label=\"Z\") plt.subplot(339) plt.plot(denoised_data[0,:,2], 'k', linewidth=0.5, label=\"Z\") plt.tight_layout() plt.show(); <pre>&lt;Response [200]&gt;\n</pre> <ul> <li>TODO: Try to use the pretrained model to denoise the waveforms</li> </ul>"},{"location":"eps207-observational-seismology/lectures/codes/denoising/#seismic-data-processing-using-deep-learning","title":"Seismic Data Processing Using Deep Learning\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/denoising/#prepare-seismic-waveforms","title":"Prepare seismic waveforms\u00b6","text":"<p>Find more details in obspy's tutorials:</p> <p>FDSN web service client for ObsPy</p> <p>Mass Downloader for FDSN Compliant Web Services</p>"},{"location":"eps207-observational-seismology/lectures/codes/denoising/#get-denoised-waveforms-using-deepdenoiser","title":"Get denoised waveforms using DeepDenoiser\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_detection/","title":"Earthquake Detection","text":"In\u00a0[1]: Copied! <pre>import obspy\nfrom obspy.signal.trigger import plot_trigger\nfrom obspy.signal.trigger import classic_sta_lta\nimport matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(1)\n</pre> import obspy from obspy.signal.trigger import plot_trigger from obspy.signal.trigger import classic_sta_lta import matplotlib.pyplot as plt import numpy as np np.random.seed(1) In\u00a0[2]: Copied! <pre>trace = obspy.read(\"https://examples.obspy.org/ev0_6.a01.gse2\")[0]\ntrace.data = np.float32(trace.data) / np.std(np.float32(trace.data))\n# trace.data[len(trace.data) // 3 - 1:len(trace.data) // 3] = 10\ndf = trace.stats.sampling_rate\n</pre> trace = obspy.read(\"https://examples.obspy.org/ev0_6.a01.gse2\")[0] trace.data = np.float32(trace.data) / np.std(np.float32(trace.data)) # trace.data[len(trace.data) // 3 - 1:len(trace.data) // 3] = 10 df = trace.stats.sampling_rate In\u00a0[3]: Copied! <pre>short_window = 5 #seconds\nlong_window = 10 #seconds\ncft = classic_sta_lta(trace.data, int(short_window * df), int(long_window * df))\ntrigger_on = 1.5\ntrigger_off = 0.5\nplot_trigger(trace, cft, trigger_on, trigger_off)\n</pre> short_window = 5 #seconds long_window = 10 #seconds cft = classic_sta_lta(trace.data, int(short_window * df), int(long_window * df)) trigger_on = 1.5 trigger_off = 0.5 plot_trigger(trace, cft, trigger_on, trigger_off) In\u00a0[4]: Copied! <pre>noise = np.random.randn(len(trace.data))\nnoise_level = 1.0\nnoisy_trace = trace.copy()\nnoisy_trace.data += noise * noise_level / np.max(noise) * np.max(trace.data)\n</pre> noise = np.random.randn(len(trace.data)) noise_level = 1.0 noisy_trace = trace.copy() noisy_trace.data += noise * noise_level / np.max(noise) * np.max(trace.data) In\u00a0[5]: Copied! <pre>short_window = 5 #seconds\nlong_window = 10 #seconds\ncft = classic_sta_lta(noisy_trace.data, int(short_window * df), int(long_window * df))\ntrigger_on = 1.5\ntrigger_off = 0.5\nplot_trigger(noisy_trace, cft, trigger_on, trigger_off)\n</pre> short_window = 5 #seconds long_window = 10 #seconds cft = classic_sta_lta(noisy_trace.data, int(short_window * df), int(long_window * df)) trigger_on = 1.5 trigger_off = 0.5 plot_trigger(noisy_trace, cft, trigger_on, trigger_off) <ul> <li>TODO: Change the noise level to see how the STA/LTA characteristic function changes</li> </ul> <ul> <li>TODO: Find the STA/LTA window length that can detect the earthquake under noise level = 1.0</li> <li>TODO: Find the trigger threshold that can detect the earthquake under noise level = 1.0</li> </ul> <p>Cut template</p> In\u00a0[6]: Copied! <pre>trace_cut = trace.copy()\ntrace_cut.trim(trace.stats.starttime + 32, trace.stats.starttime + 40)\ntrace_cut.plot();\n</pre> trace_cut = trace.copy() trace_cut.trim(trace.stats.starttime + 32, trace.stats.starttime + 40) trace_cut.plot(); <p>Run template matching</p> In\u00a0[7]: Copied! <pre>def template_matching(data, template):\n    data_len = len(data)\n    template_len = len(template)\n    scores = np.empty(data_len - template_len + 1)\n\n    for i in range(data_len - template_len + 1):\n        scores[i] = np.correlate(data[i:i+template_len], template)[0]\n\n    best_match_idx = np.argmax(scores)\n    return best_match_idx, scores\n</pre> def template_matching(data, template):     data_len = len(data)     template_len = len(template)     scores = np.empty(data_len - template_len + 1)      for i in range(data_len - template_len + 1):         scores[i] = np.correlate(data[i:i+template_len], template)[0]      best_match_idx = np.argmax(scores)     return best_match_idx, scores  In\u00a0[9]: Copied! <pre>data = trace.data\ntemplate = trace_cut.data\nnoise = np.random.randn(len(data))\n\nnoise_level = 1.0\ndata = data + noise * noise_level\n\nbest_match_idx, scores = template_matching(data, template)\nprint(\"Best match :\", best_match_idx / df, \"seconds\")\n\n# Visualization\nplt.figure(figsize=(12, 6))\nplt.subplot(3, 1, 1)\nplt.plot(data, label=\"Data\")\nplt.axvspan(best_match_idx, best_match_idx + len(template), color='red', alpha=0.3, label=\"Best Matched Segment\")\nplt.xlim(0, len(data))\nplt.legend()\nplt.subplot(3, 1, 2)\nplt.plot(range(best_match_idx, best_match_idx + len(template)), template, label=\"Template\", color=\"green\")\nplt.xlim(0, len(data))\nplt.legend()\nplt.subplot(3, 1, 3)\nplt.plot(scores, label=\"Matching Scores\")\nplt.axvline(best_match_idx, color='red', linestyle='--', label=\"Best Match Position\")\nplt.xlim(0, len(data))\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> data = trace.data template = trace_cut.data noise = np.random.randn(len(data))  noise_level = 1.0 data = data + noise * noise_level  best_match_idx, scores = template_matching(data, template) print(\"Best match :\", best_match_idx / df, \"seconds\")  # Visualization plt.figure(figsize=(12, 6)) plt.subplot(3, 1, 1) plt.plot(data, label=\"Data\") plt.axvspan(best_match_idx, best_match_idx + len(template), color='red', alpha=0.3, label=\"Best Matched Segment\") plt.xlim(0, len(data)) plt.legend() plt.subplot(3, 1, 2) plt.plot(range(best_match_idx, best_match_idx + len(template)), template, label=\"Template\", color=\"green\") plt.xlim(0, len(data)) plt.legend() plt.subplot(3, 1, 3) plt.plot(scores, label=\"Matching Scores\") plt.axvline(best_match_idx, color='red', linestyle='--', label=\"Best Match Position\") plt.xlim(0, len(data)) plt.legend()  plt.tight_layout() plt.show()  <pre>Best match : 32.0 seconds\n</pre> <ul> <li>TODO: Change the noise level to find the smallest detectable signal using template matching method?</li> <li>TODO: Compare the detection capability of template matching with STA/LTA</li> </ul> <ul> <li>TODO: Think of ways to improve the detection capability of template matching?</li> </ul>"},{"location":"eps207-observational-seismology/lectures/codes/earthquake_detection/#earthquake-detection","title":"Earthquake Detection\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_detection/#read-waveform","title":"Read Waveform\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_detection/#run-stalta","title":"Run STA/LTA\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_detection/#stalta-for-noisy-waveform","title":"STA/LTA for Noisy Waveform\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_detection/#template-matching","title":"Template matching\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_location/","title":"Earthquake Location","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n</pre> import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn from torch.nn import init import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm In\u00a0[2]: Copied! <pre>np.random.seed(11)\nvp = 6.0\nvs = vp/1.73\nnum_station = 5\nnum_event = 30\nxmax = 100\nstation_loc = np.random.uniform(low=0, high=xmax, size=(num_station,3))\nstation_loc[:, 2] = 0\nstation_dt = (np.random.rand(num_station, 1)-0.5)*2 * 1.0\nevent_loc = np.random.uniform(low=0, high=xmax, size=(num_event,3))\nevent_loc[:, 2] = 0\nevent_time = np.random.uniform(low=0, high=xmax/vp, size=(num_event,1))\n\n# %%\nevent_index = []\nstation_index = []\nphase_type = []\nphase_time = []\nphase_weight = []\nvelocity = {\"P\": vp, \"S\": vs}\nrandom_time = (np.random.rand(1, num_station, 2) - 0.5)*2.0 * 3.0\nfor evid in range(num_event):\n    for stid in range(num_station):\n        for i, phase in enumerate([\"P\", \"S\"]):\n            event_index.append(evid)\n            station_index.append(stid)\n            phase_type.append(phase)\n            dist = np.linalg.norm(station_loc[stid] - event_loc[evid], axis=-1)\n            t = dist / velocity[phase] + event_time[evid] + random_time[0, stid, i] + station_dt[stid]\n            phase_time.append(t)\n            phase_weight.append(1.0)\n\nevent_index = torch.tensor(np.array(event_index, dtype=np.int64))\nstation_index = torch.tensor(np.array(station_index, dtype=np.int64))\nphase_time = torch.tensor(np.array(phase_time, dtype=np.float32))\nphase_weight = torch.tensor(np.array(phase_weight, dtype=np.float32))\n</pre> np.random.seed(11) vp = 6.0 vs = vp/1.73 num_station = 5 num_event = 30 xmax = 100 station_loc = np.random.uniform(low=0, high=xmax, size=(num_station,3)) station_loc[:, 2] = 0 station_dt = (np.random.rand(num_station, 1)-0.5)*2 * 1.0 event_loc = np.random.uniform(low=0, high=xmax, size=(num_event,3)) event_loc[:, 2] = 0 event_time = np.random.uniform(low=0, high=xmax/vp, size=(num_event,1))  # %% event_index = [] station_index = [] phase_type = [] phase_time = [] phase_weight = [] velocity = {\"P\": vp, \"S\": vs} random_time = (np.random.rand(1, num_station, 2) - 0.5)*2.0 * 3.0 for evid in range(num_event):     for stid in range(num_station):         for i, phase in enumerate([\"P\", \"S\"]):             event_index.append(evid)             station_index.append(stid)             phase_type.append(phase)             dist = np.linalg.norm(station_loc[stid] - event_loc[evid], axis=-1)             t = dist / velocity[phase] + event_time[evid] + random_time[0, stid, i] + station_dt[stid]             phase_time.append(t)             phase_weight.append(1.0)  event_index = torch.tensor(np.array(event_index, dtype=np.int64)) station_index = torch.tensor(np.array(station_index, dtype=np.int64)) phase_time = torch.tensor(np.array(phase_time, dtype=np.float32)) phase_weight = torch.tensor(np.array(phase_weight, dtype=np.float32)) <p>Visualize travel-time of the first event</p> In\u00a0[3]: Copied! <pre>plt.figure()\nidx = np.arange(len(event_index))[event_index == event_index[10]]\nprint(idx)\nplt.scatter(station_loc[station_index[idx], 0], station_loc[station_index[idx], 1], c=phase_time[idx], marker=\"^\", label=\"stations\")\nplt.scatter(event_loc[event_index[idx], 0], event_loc[event_index[idx], 1], c=phase_time[idx], marker=\"o\", label=\"events\")\nplt.xlim([0, xmax])\nplt.ylim([0, xmax])\nplt.legend()\nplt.colorbar()\nplt.axis(\"scaled\")\nplt.show()\n</pre> plt.figure() idx = np.arange(len(event_index))[event_index == event_index[10]] print(idx) plt.scatter(station_loc[station_index[idx], 0], station_loc[station_index[idx], 1], c=phase_time[idx], marker=\"^\", label=\"stations\") plt.scatter(event_loc[event_index[idx], 0], event_loc[event_index[idx], 1], c=phase_time[idx], marker=\"o\", label=\"events\") plt.xlim([0, xmax]) plt.ylim([0, xmax]) plt.legend() plt.colorbar() plt.axis(\"scaled\") plt.show() <pre>[10 11 12 13 14 15 16 17 18 19]\n</pre> In\u00a0[4]: Copied! <pre>class TravelTime(nn.Module):\n\n    def __init__(self, num_event, num_station, station_loc, station_dt=None, event_loc=None, event_time=None, reg=0.001, velocity={\"P\": 6.0, \"S\": 6.0/1.73}, dtype=torch.float32):\n        super().__init__()\n        self.num_event = num_event\n        self.event_loc = nn.Embedding(num_event, 3)\n        self.event_time = nn.Embedding(num_event, 1)\n        self.station_dt = nn.Embedding(num_station, 1)\n        if station_dt is None:\n            init.constant_(self.station_dt.weight.data, val=0.0)\n        else:\n            self.station_dt.from_pretrained(torch.tensor(station_dt, dtype=dtype))\n        self.register_buffer('station_loc', torch.tensor(station_loc, dtype=dtype))\n        self.velocity = velocity\n        self.reg = reg\n        if event_loc is not None:\n            self.event_loc.from_pretrained(torch.tensor(event_loc, dtype=dtype))\n        if event_time is not None:\n            self.event_time.from_pretrained(torch.tensor(event_time, dtype=dtype))\n\n    def calc_time(self, event_loc, station_loc, phase_type):\n\n        dist  = torch.linalg.norm(event_loc - station_loc, axis=-1, keepdim=True)\n        vel = self.velocity\n        tt = dist / torch.tensor([vel[p] for p in phase_type]).unsqueeze(-1)\n\n        return tt\n    \n    def forward(self, station_index, event_index=None, phase_type=None, phase_time=None, phase_weight=None, use_pair=False):\n\n        station_loc = self.station_loc[station_index]\n        station_dt = self.station_dt(station_index)\n\n        event_loc = self.event_loc(event_index)\n        event_time = self.event_time(event_index)\n\n        tt = self.calc_time(event_loc, station_loc, phase_type)\n        t = event_time + tt + station_dt\n\n        if use_pair:\n            t = t[0] - t[1]\n\n        if phase_time is None:\n            loss = None\n        else:\n            # loss = torch.mean(phase_weight * (t - phase_time) ** 2)\n            loss = torch.mean(F.huber_loss(t, phase_time, reduction=\"none\") * phase_weight)\n            loss += self.reg * torch.mean(torch.abs(station_dt)) ## prevent the trade-off between station_dt and event_time\n\n        return {\"phase_time\": t, \"loss\": loss}\n</pre> class TravelTime(nn.Module):      def __init__(self, num_event, num_station, station_loc, station_dt=None, event_loc=None, event_time=None, reg=0.001, velocity={\"P\": 6.0, \"S\": 6.0/1.73}, dtype=torch.float32):         super().__init__()         self.num_event = num_event         self.event_loc = nn.Embedding(num_event, 3)         self.event_time = nn.Embedding(num_event, 1)         self.station_dt = nn.Embedding(num_station, 1)         if station_dt is None:             init.constant_(self.station_dt.weight.data, val=0.0)         else:             self.station_dt.from_pretrained(torch.tensor(station_dt, dtype=dtype))         self.register_buffer('station_loc', torch.tensor(station_loc, dtype=dtype))         self.velocity = velocity         self.reg = reg         if event_loc is not None:             self.event_loc.from_pretrained(torch.tensor(event_loc, dtype=dtype))         if event_time is not None:             self.event_time.from_pretrained(torch.tensor(event_time, dtype=dtype))      def calc_time(self, event_loc, station_loc, phase_type):          dist  = torch.linalg.norm(event_loc - station_loc, axis=-1, keepdim=True)         vel = self.velocity         tt = dist / torch.tensor([vel[p] for p in phase_type]).unsqueeze(-1)          return tt          def forward(self, station_index, event_index=None, phase_type=None, phase_time=None, phase_weight=None, use_pair=False):          station_loc = self.station_loc[station_index]         station_dt = self.station_dt(station_index)          event_loc = self.event_loc(event_index)         event_time = self.event_time(event_index)          tt = self.calc_time(event_loc, station_loc, phase_type)         t = event_time + tt + station_dt          if use_pair:             t = t[0] - t[1]          if phase_time is None:             loss = None         else:             # loss = torch.mean(phase_weight * (t - phase_time) ** 2)             loss = torch.mean(F.huber_loss(t, phase_time, reduction=\"none\") * phase_weight)             loss += self.reg * torch.mean(torch.abs(station_dt)) ## prevent the trade-off between station_dt and event_time          return {\"phase_time\": t, \"loss\": loss} In\u00a0[5]: Copied! <pre># %%\ntravel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs})\ntt = travel_time(station_index, event_index, phase_type)[\"phase_time\"]\nprint(\"True location: \", F.mse_loss(tt, phase_time))\n\n# %%\ntravel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs})\ntt = travel_time(station_index, event_index, phase_type)[\"phase_time\"]\nprint(\"Initial loss\", F.mse_loss(tt, phase_time))\ninit_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninit_event_time = travel_time.event_time.weight.clone().detach().numpy()\n\n# %%\noptimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    optimizer.zero_grad()\n    loss = travel_time(station_index, event_index, phase_type, phase_time, phase_weight)[\"loss\"]\n    loss.backward()\n    return loss\n\noptimizer.step(closure)\n    \ntt = travel_time(station_index, event_index, phase_type)[\"phase_time\"]\nprint(\"Optimized loss\", F.mse_loss(tt, phase_time))\ninvert_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninvert_event_time = travel_time.event_time.weight.clone().detach().numpy()\ninvert_station_dt = travel_time.station_dt.weight.clone().detach().numpy()\n</pre> # %% travel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs}) tt = travel_time(station_index, event_index, phase_type)[\"phase_time\"] print(\"True location: \", F.mse_loss(tt, phase_time))  # %% travel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs}) tt = travel_time(station_index, event_index, phase_type)[\"phase_time\"] print(\"Initial loss\", F.mse_loss(tt, phase_time)) init_event_loc = travel_time.event_loc.weight.clone().detach().numpy() init_event_time = travel_time.event_time.weight.clone().detach().numpy()  # %% optimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")  def closure():     optimizer.zero_grad()     loss = travel_time(station_index, event_index, phase_type, phase_time, phase_weight)[\"loss\"]     loss.backward()     return loss  optimizer.step(closure)      tt = travel_time(station_index, event_index, phase_type)[\"phase_time\"] print(\"Optimized loss\", F.mse_loss(tt, phase_time)) invert_event_loc = travel_time.event_loc.weight.clone().detach().numpy() invert_event_time = travel_time.event_time.weight.clone().detach().numpy() invert_station_dt = travel_time.station_dt.weight.clone().detach().numpy()  <pre>True location:  tensor(230.5095, grad_fn=&lt;MseLossBackward0&gt;)\nInitial loss tensor(225.0009, grad_fn=&lt;MseLossBackward0&gt;)\n</pre> <pre>Optimized loss tensor(2.1647, grad_fn=&lt;MseLossBackward0&gt;)\n</pre> In\u00a0[6]: Copied! <pre># %%\nplt.figure()\n# plt.scatter(station_loc[:,0], station_loc[:,1], c=tp[idx_event,:])\nplt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\")\nplt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\")\nplt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\")\nplt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6)\nplt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6)\nplt.xlim([0, xmax])\nplt.ylim([0, xmax])\nplt.axis(\"scaled\")\nplt.legend()\n# plt.savefig(\"absolute_location.png\", dpi=300)\nplt.show()\n</pre> # %% plt.figure() # plt.scatter(station_loc[:,0], station_loc[:,1], c=tp[idx_event,:]) plt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\") plt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\") plt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\") plt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6) plt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6) plt.xlim([0, xmax]) plt.ylim([0, xmax]) plt.axis(\"scaled\") plt.legend() # plt.savefig(\"absolute_location.png\", dpi=300) plt.show() In\u00a0[7]: Copied! <pre># %%\npair_event_index = []\npair_station_index = []\npair_type = []\npair_dtime = []\npair_weight = []\nvelocity = {\"P\": vp, \"S\": vs}\nfor evid1 in range(num_event):\n    for evid2 in range(evid1+1, num_event):\n        for stid in range(num_station):\n            for phase in [\"P\", \"S\"]:\n                pair_event_index.append([evid1, evid2])\n                pair_station_index.append(stid)\n                pair_type.append(phase)\n                dist1 = np.linalg.norm(station_loc[stid] - event_loc[evid1], axis=-1)\n                tt1 = dist1 / velocity[phase] \n                t1 = tt1 + event_time[evid1] + random_time[0, stid, i]\n                dist2 = np.linalg.norm(station_loc[stid] - event_loc[evid2], axis=-1)\n                tt2 = dist2 / velocity[phase] \n                t2 = tt2 + event_time[evid2] + random_time[0, stid, i]\n                pair_dtime.append(t1 - t2)\n                pair_weight.append(1.0)\n\npair_event_index = torch.tensor(np.array(pair_event_index, dtype=np.int64).T)\npair_station_index = torch.tensor(np.array(pair_station_index, dtype=np.int64))\npair_dtime = torch.tensor(np.array(pair_dtime, dtype=np.float32))\npair_weight = torch.tensor(np.array(pair_weight, dtype=np.float32))\n\n\n# %%\ntravel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs})\ndt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"]\nprint(\"True location: \", F.mse_loss(dt, pair_dtime))\n\n# %%\ntravel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs})\ntt = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"phase_time\"]\nprint(\"Initial loss\", F.mse_loss(tt, pair_dtime))\ninit_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninit_event_time = travel_time.event_time.weight.clone().detach().numpy()\n\n# %%\noptimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    optimizer.zero_grad()\n    loss = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"loss\"]\n    loss.backward()\n    return loss\n\noptimizer.step(closure)\n    \ntt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"]\nprint(\"Optimized loss\", F.mse_loss(tt, pair_dtime))\ninvert_event_loc = travel_time.event_loc.weight.clone().detach().numpy()\ninvert_event_time = travel_time.event_time.weight.clone().detach().numpy()\n</pre> # %% pair_event_index = [] pair_station_index = [] pair_type = [] pair_dtime = [] pair_weight = [] velocity = {\"P\": vp, \"S\": vs} for evid1 in range(num_event):     for evid2 in range(evid1+1, num_event):         for stid in range(num_station):             for phase in [\"P\", \"S\"]:                 pair_event_index.append([evid1, evid2])                 pair_station_index.append(stid)                 pair_type.append(phase)                 dist1 = np.linalg.norm(station_loc[stid] - event_loc[evid1], axis=-1)                 tt1 = dist1 / velocity[phase]                  t1 = tt1 + event_time[evid1] + random_time[0, stid, i]                 dist2 = np.linalg.norm(station_loc[stid] - event_loc[evid2], axis=-1)                 tt2 = dist2 / velocity[phase]                  t2 = tt2 + event_time[evid2] + random_time[0, stid, i]                 pair_dtime.append(t1 - t2)                 pair_weight.append(1.0)  pair_event_index = torch.tensor(np.array(pair_event_index, dtype=np.int64).T) pair_station_index = torch.tensor(np.array(pair_station_index, dtype=np.int64)) pair_dtime = torch.tensor(np.array(pair_dtime, dtype=np.float32)) pair_weight = torch.tensor(np.array(pair_weight, dtype=np.float32))   # %% travel_time = TravelTime(num_event, num_station, station_loc, station_dt=station_dt, event_loc=event_loc, event_time=event_time, reg=0, velocity={\"P\": vp, \"S\": vs}) dt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"] print(\"True location: \", F.mse_loss(dt, pair_dtime))  # %% travel_time = TravelTime(num_event, num_station, station_loc, velocity={\"P\": vp, \"S\": vs}) tt = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"phase_time\"] print(\"Initial loss\", F.mse_loss(tt, pair_dtime)) init_event_loc = travel_time.event_loc.weight.clone().detach().numpy() init_event_time = travel_time.event_time.weight.clone().detach().numpy()  # %% optimizer = optim.LBFGS(params=travel_time.parameters(), max_iter=1000, line_search_fn=\"strong_wolfe\")  def closure():     optimizer.zero_grad()     loss = travel_time(pair_station_index, pair_event_index, pair_type, pair_dtime, pair_weight, use_pair=True)[\"loss\"]     loss.backward()     return loss  optimizer.step(closure)      tt = travel_time(pair_station_index, pair_event_index, pair_type, use_pair=True)[\"phase_time\"] print(\"Optimized loss\", F.mse_loss(tt, pair_dtime)) invert_event_loc = travel_time.event_loc.weight.clone().detach().numpy() invert_event_time = travel_time.event_time.weight.clone().detach().numpy()  <pre>True location:  tensor(117.7815, grad_fn=&lt;MseLossBackward0&gt;)\nInitial loss tensor(124.0186, grad_fn=&lt;MseLossBackward0&gt;)\nOptimized loss tensor(9.2956e-07, grad_fn=&lt;MseLossBackward0&gt;)\n</pre> In\u00a0[8]: Copied! <pre># %%\nplt.figure()\nplt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\")\nplt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\")\nplt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\")\nplt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6)\nplt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6)\nplt.xlim([0, xmax])\nplt.ylim([0, xmax])\nplt.axis(\"scaled\")\nplt.legend()\n# plt.savefig(\"relative_location.png\", dpi=300)\nplt.show()\n</pre> # %% plt.figure() plt.plot(event_loc[:,0], event_loc[:,1], 'x', markersize=12, color='blue', label=\"True locations\") plt.plot(init_event_loc[:,0], init_event_loc[:,1], 'x', markersize=8, color='green', label=\"Initial locations\") plt.plot(invert_event_loc[:,0], invert_event_loc[:,1], 'x', markersize=8, color='red', label=\"Inverted locations\") plt.scatter(station_loc[:,0], station_loc[:,1], c=station_dt, marker=\"o\", alpha=0.6) plt.scatter(station_loc[:,0]+1, station_loc[:,1]+1, c=invert_station_dt, marker=\"o\",  alpha=0.6) plt.xlim([0, xmax]) plt.ylim([0, xmax]) plt.axis(\"scaled\") plt.legend() # plt.savefig(\"relative_location.png\", dpi=300) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/lectures/codes/earthquake_location/#earthquake-location-and-re-location-using-automatic-differentiation","title":"Earthquake Location and Re-location using Automatic Differentiation\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_location/#generate-synthetic-travel-time-measurements","title":"Generate synthetic travel-time measurements\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_location/#build-forward-model-for-calculating-travel-times","title":"Build forward model for calculating travel-times\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_location/#locating-earthquakes-using-absolute-travel-times","title":"Locating earthquakes using absolute travel times\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/earthquake_location/#locating-earthquakes-using-both-absolute-travel-times-and-relative-travel-time-differences","title":"Locating earthquakes using both absolute travel-times and relative travel-time differences\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/","title":"Focal Mechanism","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport json\nfrom glob import glob\nfrom tqdm.auto import tqdm\nimport obspy\nimport obspy.taup\nimport warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport multiprocessing as mp\nfrom sklearn.neighbors import NearestNeighbors\nimport random\nwarnings.filterwarnings(\"ignore\")\n</pre> import os import pandas as pd import json from glob import glob from tqdm.auto import tqdm import obspy import obspy.taup import warnings import numpy as np import matplotlib.pyplot as plt import multiprocessing as mp from sklearn.neighbors import NearestNeighbors import random warnings.filterwarnings(\"ignore\") <pre>/home/zhuwq/.local/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>root_path = \"./QuakeFlow/slurm/local\"\nregion = \"demo\"\nresult_path = f\"{root_path}/{region}/hash\"\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nwith open(f\"{root_path}/{region}/config.json\", \"r\") as f:\n    config = json.load(f)\nwith open(f\"{root_path}/{region}/obspy/stations.json\", \"r\") as f:\n    stations = json.load(f)\n\nstations = pd.DataFrame.from_dict(stations, orient='index')\nstations.index.name = \"station_id\"\nevents = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_events.csv\", parse_dates=['time'])\nevents['time'] = events['time'].dt.tz_localize(None)\nif \"event_id\" in events.columns:\n    events.rename(columns={\"event_id\": \"event_index\"}, inplace=True)\nevents.set_index(\"event_index\", inplace=True, drop=True)\n\npicks = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_picks.csv\", parse_dates=['phase_time'])\n</pre> root_path = \"./QuakeFlow/slurm/local\" region = \"demo\" result_path = f\"{root_path}/{region}/hash\" if not os.path.exists(result_path):     os.makedirs(result_path) with open(f\"{root_path}/{region}/config.json\", \"r\") as f:     config = json.load(f) with open(f\"{root_path}/{region}/obspy/stations.json\", \"r\") as f:     stations = json.load(f)  stations = pd.DataFrame.from_dict(stations, orient='index') stations.index.name = \"station_id\" events = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_events.csv\", parse_dates=['time']) events['time'] = events['time'].dt.tz_localize(None) if \"event_id\" in events.columns:     events.rename(columns={\"event_id\": \"event_index\"}, inplace=True) events.set_index(\"event_index\", inplace=True, drop=True)  picks = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_picks.csv\", parse_dates=['phase_time']) In\u00a0[3]: Copied! <pre>stations[:3]\n</pre> stations[:3] Out[3]: network station location instrument component sensitivity latitude longitude elevation_m depth_km x_km y_km z_km provider station_id CI.CCC..BH CI CCC BH ENZ [627368000.0, 627368000.0, 627368000.0] 35.52495 -117.36453 670.0 -0.67 12.65 -19.968 -0.67 SCEDC CI.CCC..HH CI CCC HH ENZ [627368000.0, 627368000.0, 627368000.0] 35.52495 -117.36453 670.0 -0.67 12.65 -19.968 -0.67 SCEDC CI.CCC..HN CI CCC HN ENZ [213979.0, 214322.0, 213808.0] 35.52495 -117.36453 670.0 -0.67 12.65 -19.968 -0.67 SCEDC In\u00a0[4]: Copied! <pre>events[:3]\n</pre> events[:3] Out[4]: time magnitude sigma_time sigma_amp cov_time_amp gamma_score num_picks num_p_picks num_s_picks x(km) y(km) z(km) longitude latitude depth_km event_index 142 2019-07-04 17:02:55.043 3.959 0.324 0.488 0.016 128.000 78 35 43 0.401 0.854 16.255 -117.500 35.713 16.255 223 2019-07-04 17:04:02.558 1.992 0.210 0.328 0.037 52.807 40 20 20 -1.312 0.010 13.710 -117.518 35.705 13.710 224 2019-07-04 17:04:11.454 1.207 0.042 0.301 0.001 9.159 6 3 3 -16.960 29.341 6.733 -117.692 35.969 6.733 In\u00a0[5]: Copied! <pre>picks[:3]\n</pre> picks[:3] Out[5]: station_id phase_index phase_time phase_score phase_type phase_polarity phase_amplitude id timestamp amp type prob event_index gamma_score 0 CI.WCS2..HN 519 2019-07-04 17:00:05.188 0.467 P -0.066 1.670000e-06 CI.WCS2..HN 2019-07-04T17:00:05.188000 1.670000e-06 P 0.467 -1 -1.0 1 CI.WRV2..EH 789 2019-07-04 17:00:07.890 0.695 P -0.160 2.042000e-07 CI.WRV2..EH 2019-07-04T17:00:07.890000 2.042000e-07 P 0.695 -1 -1.0 2 CI.WRV2..EH 846 2019-07-04 17:00:08.460 0.473 S 0.000 1.342000e-07 CI.WRV2..EH 2019-07-04T17:00:08.460000 1.342000e-07 S 0.473 -1 -1.0 In\u00a0[7]: Copied! <pre># %%\nstations = stations[stations.index.isin(picks[\"station_id\"].unique())]\n\n# %%\npicks = picks.merge(events, on=\"event_index\", suffixes=(\"_pick\", \"_event\"))\npicks = picks.merge(stations, on=\"station_id\", suffixes=(\"_pick\", \"_station\"))\n\n\nneigh = NearestNeighbors(n_neighbors=min(len(stations), 10))\nneigh.fit(stations[[\"longitude\", \"latitude\"]].values)\n\n# %%\nneigh_ratios = []\nselected_index = []\nfor event_index, event in tqdm(events.iterrows(), total=len(events)):\n    sid = neigh.kneighbors([event[[\"longitude\", \"latitude\"]].values])[1][0]\n    neigh_stations = stations.iloc[sid].index\n    neigh_picks = picks[(picks[\"station_id\"].isin(neigh_stations)) &amp; (picks[\"event_index\"] == event_index)]\n    neigh_stations_with_picks = neigh_picks[\"station_id\"].unique()\n    neigh_ratios.append(len(neigh_stations_with_picks) / len(neigh_stations))\n    if len(neigh_stations_with_picks) / len(neigh_stations) &gt; 0.3:\n        selected_index.append(event_index)\nevents = events.loc[selected_index]\n\n# %%\npicks = picks[picks[\"event_index\"].isin(events.index)]\n</pre> # %% stations = stations[stations.index.isin(picks[\"station_id\"].unique())]  # %% picks = picks.merge(events, on=\"event_index\", suffixes=(\"_pick\", \"_event\")) picks = picks.merge(stations, on=\"station_id\", suffixes=(\"_pick\", \"_station\"))   neigh = NearestNeighbors(n_neighbors=min(len(stations), 10)) neigh.fit(stations[[\"longitude\", \"latitude\"]].values)  # %% neigh_ratios = [] selected_index = [] for event_index, event in tqdm(events.iterrows(), total=len(events)):     sid = neigh.kneighbors([event[[\"longitude\", \"latitude\"]].values])[1][0]     neigh_stations = stations.iloc[sid].index     neigh_picks = picks[(picks[\"station_id\"].isin(neigh_stations)) &amp; (picks[\"event_index\"] == event_index)]     neigh_stations_with_picks = neigh_picks[\"station_id\"].unique()     neigh_ratios.append(len(neigh_stations_with_picks) / len(neigh_stations))     if len(neigh_stations_with_picks) / len(neigh_stations) &gt; 0.3:         selected_index.append(event_index) events = events.loc[selected_index]  # %% picks = picks[picks[\"event_index\"].isin(events.index)] <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2324/2324 [00:06&lt;00:00, 366.54it/s]\n</pre> In\u00a0[8]: Copied! <pre>plt.figure(figsize=(10, 10))\nplt.scatter(stations['longitude'], stations['latitude'], color='k', marker='^')\nplt.scatter(events['longitude'], events['latitude'], s=1, color='r', marker='o')\nplt.gca().set_aspect(1.0/np.cos(np.pi/180*stations['latitude'].mean()))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n</pre> plt.figure(figsize=(10, 10)) plt.scatter(stations['longitude'], stations['latitude'], color='k', marker='^') plt.scatter(events['longitude'], events['latitude'], s=1, color='r', marker='o') plt.gca().set_aspect(1.0/np.cos(np.pi/180*stations['latitude'].mean())) plt.xlabel('Longitude') plt.ylabel('Latitude') plt.show() In\u00a0[9]: Copied! <pre># \"LOCSRCE station LATLON latitude longitdue depth elevation\"\nstation_fmt = \"LOCSRCE {} LATLON {:.4f} {:.4f} 0 {}\\n\"\n# the file containing station information\nstafile = f'./{result_path}/stations.nll'\nwith open(stafile,'w') as fp:\n    for station_id, station in stations.iterrows():\n        fp.writelines(station_fmt.format(station_id, station['latitude'], station['longitude'], station['elevation_m']/1000))\n</pre> # \"LOCSRCE station LATLON latitude longitdue depth elevation\" station_fmt = \"LOCSRCE {} LATLON {:.4f} {:.4f} 0 {}\\n\" # the file containing station information stafile = f'./{result_path}/stations.nll' with open(stafile,'w') as fp:     for station_id, station in stations.iterrows():         fp.writelines(station_fmt.format(station_id, station['latitude'], station['longitude'], station['elevation_m']/1000)) In\u00a0[10]: Copied! <pre># year month day hour minute seconds evla evlo evdp errh errz evid\neventlinefmt = \"{:04d} {:02d} {:02d} {:02d} {:02d} {:05.2f} {:010.6f} {:010.6f} {:09.6f} {:05.2f} {:05.2f} {:}\\n\"\n# station polarity \"qp\" S/P_ratio\nstationlinefmt = \"{} {:1s} 0 {:08.3f}\\n\"\n\npicks_by_event = picks.groupby(\"event_index\")\nfor event_index, picks_ in tqdm(picks_by_event):\n    # the file being created which should contain event information, polarity and S/P data\n    polfile = f'{result_path}/{event_index}.pol.hash'\n\n    event_info = events.loc[event_index]\n    with open(polfile, 'w') as pf:\n        # write event information into the file\n        pf.writelines(eventlinefmt.format(event_info['time'].year, event_info['time'].month, event_info['time'].day, event_info['time'].hour, event_info['time'].minute, event_info['time'].second+1e-6*event_info['time'].microsecond, event_info['latitude'], event_info['longitude'], event_info['depth_km'], 0.0, 0.0, event_index))\n\n        # loop for all station that have picks\n        # if no P pick, continue the next loop\n        # if P pick exists and its polarity prob exceeds a certain value\n        # assign 'U','u','+' or 'D','d','-', respectively, otherwise 'x'\n        # if both P and S picks exist, divide S by P amplitude as S/P ratio\n        # if no S pick, set the ratio to zero\n        picks_by_station = picks_.groupby(\"station_id\")\n        for station_id, picks__ in picks_by_station:\n            if 'P' not in picks__['phase_type'].values:\n                continue\n            P_polarity = picks__[picks__['phase_type'] == 'P']['phase_polarity'].values[0]\n            if P_polarity &lt; -0.3:\n                polarity = '-'\n            elif P_polarity &gt; 0.3:\n                polarity = '+'\n            else:\n                polarity = 'x'\n            if 'S' in picks__['phase_type'].values:\n                S_polarity = picks__[picks__['phase_type'] == 'S']['phase_polarity'].values[0]\n                S_amplitdue = picks__[picks__['phase_type'] == 'S']['phase_amplitude'].values[0]\n                P_amplitdue = picks__[picks__['phase_type'] == 'P']['phase_amplitude'].values[0]\n                SP_ratio = S_amplitdue/P_amplitdue\n            else:\n                SP_ratio = 0\n            # write polarity and S/P ratio data at each station into the file\n            pf.writelines(stationlinefmt.format(station_id, polarity, SP_ratio))\n</pre> # year month day hour minute seconds evla evlo evdp errh errz evid eventlinefmt = \"{:04d} {:02d} {:02d} {:02d} {:02d} {:05.2f} {:010.6f} {:010.6f} {:09.6f} {:05.2f} {:05.2f} {:}\\n\" # station polarity \"qp\" S/P_ratio stationlinefmt = \"{} {:1s} 0 {:08.3f}\\n\"  picks_by_event = picks.groupby(\"event_index\") for event_index, picks_ in tqdm(picks_by_event):     # the file being created which should contain event information, polarity and S/P data     polfile = f'{result_path}/{event_index}.pol.hash'      event_info = events.loc[event_index]     with open(polfile, 'w') as pf:         # write event information into the file         pf.writelines(eventlinefmt.format(event_info['time'].year, event_info['time'].month, event_info['time'].day, event_info['time'].hour, event_info['time'].minute, event_info['time'].second+1e-6*event_info['time'].microsecond, event_info['latitude'], event_info['longitude'], event_info['depth_km'], 0.0, 0.0, event_index))          # loop for all station that have picks         # if no P pick, continue the next loop         # if P pick exists and its polarity prob exceeds a certain value         # assign 'U','u','+' or 'D','d','-', respectively, otherwise 'x'         # if both P and S picks exist, divide S by P amplitude as S/P ratio         # if no S pick, set the ratio to zero         picks_by_station = picks_.groupby(\"station_id\")         for station_id, picks__ in picks_by_station:             if 'P' not in picks__['phase_type'].values:                 continue             P_polarity = picks__[picks__['phase_type'] == 'P']['phase_polarity'].values[0]             if P_polarity &lt; -0.3:                 polarity = '-'             elif P_polarity &gt; 0.3:                 polarity = '+'             else:                 polarity = 'x'             if 'S' in picks__['phase_type'].values:                 S_polarity = picks__[picks__['phase_type'] == 'S']['phase_polarity'].values[0]                 S_amplitdue = picks__[picks__['phase_type'] == 'S']['phase_amplitude'].values[0]                 P_amplitdue = picks__[picks__['phase_type'] == 'P']['phase_amplitude'].values[0]                 SP_ratio = S_amplitdue/P_amplitdue             else:                 SP_ratio = 0             # write polarity and S/P ratio data at each station into the file             pf.writelines(stationlinefmt.format(station_id, polarity, SP_ratio)) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1651/1651 [00:24&lt;00:00, 66.05it/s]\n</pre> In\u00a0[11]: Copied! <pre>velocity_model = \"\"\"0.00  4.74000\n1.00  5.01000\n2.00  5.35000\n3.00  5.71000\n4.00  6.07000\n5.00  6.17000\n6.00  6.27000\n7.00  6.34000\n8.00  6.39000\n30.00 7.80000\n\"\"\"\nwith open(f'{result_path}/ca.vel', 'w') as fp:\n    fp.writelines(velocity_model)\n\n# loop for all polarity data files\npolfiles = glob(f'{result_path}/*.pol.hash')\n# for polfile in tqdm(list(polfiles)):\ndef run(polfile):\n    event_id = os.path.basename(polfile).split('.')[0]\n    # the inputfile which should contain input and output files and parameters\n    iptfile = f'{result_path}/{event_id}.inp.hash'\n    # the file containing the best solutions\n    optfile1 = f'{result_path}/{event_id}.best.fps'\n    # the file containing all solutions\n    optfile2 = f'{result_path}/{event_id}.all.fps'\n    # the file containing ray parameters\n    optfile3 = f'{result_path}/{event_id}.rays'\n    # the file containing velocity profile\n    velfile = f'{result_path}/ca.vel'\n    # Angle increment for grid search\n    dang = 1\n    # Number of perutbations of take-off angles for different source depths and velocity models\n    nmc = 50\n    # Maximum number focal mechanisms that match misfit critria to return\n    maxout = 20\n    # maximum distance to consider (km)\n    maxdist = 100\n    # number of polarities assumed bad\n    nbadpol = 4\n    # log10 of uncertainty factor for s/p ratios.\n    qbadfac = 0.2\n    # Angular distance between different families of focal mechanisms.\n    cangle = 45\n    # Fraction of focal mechanisms that need to be within cangle to make up a new famlily of focal mechanisms.\n    prob_max = 0.2\n    # number of velocity models\n    nvelmod = 1\n    # write these information sequencially into the inputfile\n    with open(iptfile, 'w') as ipt:\n        ipt.writelines('{}\\n{}\\n{}\\n{}\\n{}\\n'.format(polfile,stafile,optfile1,optfile2,optfile3))\n        ipt.writelines('{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n'.format(dang,nmc,maxout,maxdist,nbadpol,qbadfac,cangle,prob_max))\n        ipt.writelines('{}\\n{}\\n'.format(nvelmod, velfile))\n    # run the command\n    os.system(f'QuakeFlow/hashpy2/hash/hash_hashpy1D &lt; {iptfile} &gt; hash.log 2&gt;&amp;1')\n</pre> velocity_model = \"\"\"0.00  4.74000 1.00  5.01000 2.00  5.35000 3.00  5.71000 4.00  6.07000 5.00  6.17000 6.00  6.27000 7.00  6.34000 8.00  6.39000 30.00 7.80000 \"\"\" with open(f'{result_path}/ca.vel', 'w') as fp:     fp.writelines(velocity_model)  # loop for all polarity data files polfiles = glob(f'{result_path}/*.pol.hash') # for polfile in tqdm(list(polfiles)): def run(polfile):     event_id = os.path.basename(polfile).split('.')[0]     # the inputfile which should contain input and output files and parameters     iptfile = f'{result_path}/{event_id}.inp.hash'     # the file containing the best solutions     optfile1 = f'{result_path}/{event_id}.best.fps'     # the file containing all solutions     optfile2 = f'{result_path}/{event_id}.all.fps'     # the file containing ray parameters     optfile3 = f'{result_path}/{event_id}.rays'     # the file containing velocity profile     velfile = f'{result_path}/ca.vel'     # Angle increment for grid search     dang = 1     # Number of perutbations of take-off angles for different source depths and velocity models     nmc = 50     # Maximum number focal mechanisms that match misfit critria to return     maxout = 20     # maximum distance to consider (km)     maxdist = 100     # number of polarities assumed bad     nbadpol = 4     # log10 of uncertainty factor for s/p ratios.     qbadfac = 0.2     # Angular distance between different families of focal mechanisms.     cangle = 45     # Fraction of focal mechanisms that need to be within cangle to make up a new famlily of focal mechanisms.     prob_max = 0.2     # number of velocity models     nvelmod = 1     # write these information sequencially into the inputfile     with open(iptfile, 'w') as ipt:         ipt.writelines('{}\\n{}\\n{}\\n{}\\n{}\\n'.format(polfile,stafile,optfile1,optfile2,optfile3))         ipt.writelines('{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n'.format(dang,nmc,maxout,maxdist,nbadpol,qbadfac,cangle,prob_max))         ipt.writelines('{}\\n{}\\n'.format(nvelmod, velfile))     # run the command     os.system(f'QuakeFlow/hashpy2/hash/hash_hashpy1D &lt; {iptfile} &gt; hash.log 2&gt;&amp;1') In\u00a0[12]: Copied! <pre>ncpu = mp.cpu_count()\nbpar = tqdm(total=len(polfiles))\nctx = mp.get_context('fork')\nwith ctx.Pool(ncpu) as p:\n    for file in polfiles:\n        p.apply_async(run, args=(file,), callback=lambda _: bpar.update())\n    p.close()\n    p.join()\n</pre> ncpu = mp.cpu_count() bpar = tqdm(total=len(polfiles)) ctx = mp.get_context('fork') with ctx.Pool(ncpu) as p:     for file in polfiles:         p.apply_async(run, args=(file,), callback=lambda _: bpar.update())     p.close()     p.join() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1648/1651 [00:22&lt;00:00, 42.02it/s] </pre> In\u00a0[13]: Copied! <pre>filename = list(glob(f'{result_path}/*.best.fps'))\nfilename = random.choice(filename)\nevent_id = os.path.basename(filename).split('.')[0]\n</pre> filename = list(glob(f'{result_path}/*.best.fps')) filename = random.choice(filename) event_id = os.path.basename(filename).split('.')[0] In\u00a0[14]: Copied! <pre>allmecafile = f'{result_path}/{event_id}.all.fps'\ndf_allmeca = pd.read_csv(allmecafile, header=None, delim_whitespace=True)\n</pre> allmecafile = f'{result_path}/{event_id}.all.fps' df_allmeca = pd.read_csv(allmecafile, header=None, delim_whitespace=True) In\u00a0[15]: Copied! <pre>bestmecafile= f'{result_path}/{event_id}.best.fps'\ndf_bestmeca= pd.read_csv(bestmecafile, header=None, delim_whitespace=True)\ndf_bestmeca\n</pre> bestmecafile= f'{result_path}/{event_id}.best.fps' df_bestmeca= pd.read_csv(bestmecafile, header=None, delim_whitespace=True) df_bestmeca Out[15]: 0 1 2 3 4 0 119 86 156 17 A In\u00a0[16]: Copied! <pre>from obspy.imaging.beachball import beach\nfig, ax = plt.subplots()\n# plot the best one first\nstrike, dip, rake = df_bestmeca.loc[0].to_numpy()[:3]\nbball = beach(fm=[strike, dip, rake],facecolor='k', xy=(0,0), width=100)\nax.add_collection(bball)\n# plot all solutions that meet the criterion\nfor event_index in df_allmeca.index:\n    strike, dip, rake = df_allmeca.loc[event_index].to_numpy()[:3]\n    bball = beach(fm=[strike, dip, rake], linewidth=1, nofill=True, xy=(0,0), width=100, edgecolor='gray', alpha=0.5)\n    ax.add_collection(bball)\nplt.xlim(-50, 50)\nplt.ylim(-50, 50)\nax.set_aspect('equal')\nfig.tight_layout()\n</pre> from obspy.imaging.beachball import beach fig, ax = plt.subplots() # plot the best one first strike, dip, rake = df_bestmeca.loc[0].to_numpy()[:3] bball = beach(fm=[strike, dip, rake],facecolor='k', xy=(0,0), width=100) ax.add_collection(bball) # plot all solutions that meet the criterion for event_index in df_allmeca.index:     strike, dip, rake = df_allmeca.loc[event_index].to_numpy()[:3]     bball = beach(fm=[strike, dip, rake], linewidth=1, nofill=True, xy=(0,0), width=100, edgecolor='gray', alpha=0.5)     ax.add_collection(bball) plt.xlim(-50, 50) plt.ylim(-50, 50) ax.set_aspect('equal') fig.tight_layout() In\u00a0[22]: Copied! <pre>bestmecafiles = list(glob(f'{result_path}/*.best.fps'))\nfig, ax = plt.subplots(figsize=(10, 10))\nmin_mag = events['magnitude'].min()\nbballs = []\nfor bestmecafile in bestmecafiles:\n    # get the event information\n    event_id = os.path.basename(bestmecafile).split('.')[0]\n    evot, mag, evla, evlo, evdp = events.loc[int(event_id)][['time', 'magnitude', 'latitude', 'longitude', 'depth_km']]\n    # get strike, dip and rake angles\n    df_bestmeca= pd.read_csv(bestmecafile, header=None, delim_whitespace=True)\n    strike, dip, rake, RMS, Q = df_bestmeca.loc[0].to_numpy()\n    if Q != \"A\":\n        continue\n\n    # # plot and change the width/size according to the earthquake magnitude\n    # bball = beach(fm=[strike, dip, rake], linewidth=0.5, facecolor='k', xy=(evlo,evla), width=(mag-min_mag)*2e-3, alpha=alpha)\n    # ax.add_collection(bball)\n\n    bballs.append({\n        \"fm\": [strike, dip, rake],\n        \"linewidth\": 0.5,\n        \"facecolor\": 'k',\n        \"xy\": (evlo,evla),\n        \"width\": (mag-min_mag)*2e-3,\n        \"alpha\": 1.0,\n        \"mag\": mag\n    })\n\n# sort bbals by magnitude\nbballs = sorted(bballs, key=lambda x: x[\"mag\"])\nfor bball in bballs:\n    bball = beach(fm=bball[\"fm\"], linewidth=bball[\"linewidth\"], facecolor=bball[\"facecolor\"], xy=bball[\"xy\"], width=bball[\"width\"], alpha=bball[\"alpha\"])\n    ax.add_collection(bball)\n\nx0 = events['longitude'].mean()\ndx = events['longitude'].std()\ny0 = events['latitude'].mean()\ndy = events['latitude'].std()\nax.set_xlim(x0-dx*2, x0+dx*2)\nax.set_ylim(y0-dy*2, y0+dy*2)\nax.set_aspect(1.0/np.cos(np.pi/180*stations['latitude'].mean()))\nfig.tight_layout()\n</pre> bestmecafiles = list(glob(f'{result_path}/*.best.fps')) fig, ax = plt.subplots(figsize=(10, 10)) min_mag = events['magnitude'].min() bballs = [] for bestmecafile in bestmecafiles:     # get the event information     event_id = os.path.basename(bestmecafile).split('.')[0]     evot, mag, evla, evlo, evdp = events.loc[int(event_id)][['time', 'magnitude', 'latitude', 'longitude', 'depth_km']]     # get strike, dip and rake angles     df_bestmeca= pd.read_csv(bestmecafile, header=None, delim_whitespace=True)     strike, dip, rake, RMS, Q = df_bestmeca.loc[0].to_numpy()     if Q != \"A\":         continue      # # plot and change the width/size according to the earthquake magnitude     # bball = beach(fm=[strike, dip, rake], linewidth=0.5, facecolor='k', xy=(evlo,evla), width=(mag-min_mag)*2e-3, alpha=alpha)     # ax.add_collection(bball)      bballs.append({         \"fm\": [strike, dip, rake],         \"linewidth\": 0.5,         \"facecolor\": 'k',         \"xy\": (evlo,evla),         \"width\": (mag-min_mag)*2e-3,         \"alpha\": 1.0,         \"mag\": mag     })  # sort bbals by magnitude bballs = sorted(bballs, key=lambda x: x[\"mag\"]) for bball in bballs:     bball = beach(fm=bball[\"fm\"], linewidth=bball[\"linewidth\"], facecolor=bball[\"facecolor\"], xy=bball[\"xy\"], width=bball[\"width\"], alpha=bball[\"alpha\"])     ax.add_collection(bball)  x0 = events['longitude'].mean() dx = events['longitude'].std() y0 = events['latitude'].mean() dy = events['latitude'].std() ax.set_xlim(x0-dx*2, x0+dx*2) ax.set_ylim(y0-dy*2, y0+dy*2) ax.set_aspect(1.0/np.cos(np.pi/180*stations['latitude'].mean())) fig.tight_layout() In\u00a0[18]: Copied! <pre>!wget https://github.com/AI4EPS/INVerse/releases/download/inverse/MT-Exercises.tar &amp;&amp; tar -xf MT-Exercises.tar\n</pre> !wget https://github.com/AI4EPS/INVerse/releases/download/inverse/MT-Exercises.tar &amp;&amp; tar -xf MT-Exercises.tar <pre>--2023-11-30 17:49:35--  https://github.com/AI4EPS/INVerse/releases/download/inverse/MT-Exercises.tar\nResolving github.com (github.com)... 140.82.114.3\nConnecting to github.com (github.com)|140.82.114.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/659493626/37e78cb0-9264-48c5-bfd2-64f92b705645?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231201%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20231201T014935Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=27ac6035d2d335ef76eaf2469cf96591388159a52d7ed72fceb25e1b6d8a0efb&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=659493626&amp;response-content-disposition=attachment%3B%20filename%3DMT-Exercises.tar&amp;response-content-type=application%2Foctet-stream [following]\n--2023-11-30 17:49:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/659493626/37e78cb0-9264-48c5-bfd2-64f92b705645?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231201%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20231201T014935Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=27ac6035d2d335ef76eaf2469cf96591388159a52d7ed72fceb25e1b6d8a0efb&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=659493626&amp;response-content-disposition=attachment%3B%20filename%3DMT-Exercises.tar&amp;response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 81459200 (78M) [application/octet-stream]\nSaving to: \u2018MT-Exercises.tar.4\u2019\n\nMT-Exercises.tar.4  100%[===================&gt;]  77.69M  85.2MB/s    in 0.9s    \n\n2023-11-30 17:49:36 (85.2 MB/s) - \u2018MT-Exercises.tar.4\u2019 saved [81459200/81459200]\n\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1651/1651 [00:38&lt;00:00, 42.02it/s]</pre>"},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#run-quakeflow_demoipynb-first-to-generate-the-input-files","title":"Run QuakeFlow_demo.ipynb first to generate the input files\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#stations-and-events","title":"Stations and events\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#convert-the-station-list-format","title":"Convert the station list format\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#convert-polarity-and-sp-picks","title":"Convert polarity and S/P picks\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#prepare-input-files-for-hash","title":"Prepare input files for HASH\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#check-result-of-a-single-event","title":"Check result of a single event\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#plot-focal-mechanisms-of-all-earthquakes","title":"Plot focal mechanisms of all earthquakes\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/focal_mechanism/#moment-tensor-inversion","title":"Moment tensor inversion\u00b6","text":"<p>TODO: Week2a_Exercise and Week2b_Exercise</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/","title":"Download Data","text":"In\u00a0[1]: Copied! <pre>import obspy\nimport cartopy\nfrom datetime import datetime\nimport numpy as np\nfrom obspy import UTCDateTime\nfrom obspy.clients.fdsn import Client\nclient = Client(\"IRIS\")\n</pre> import obspy import cartopy from datetime import datetime import numpy as np from obspy import UTCDateTime from obspy.clients.fdsn import Client client = Client(\"IRIS\") In\u00a0[2]: Copied! <pre>minlatitude=33.48\nmaxlatitude=34.84\nminlongitude=-118.94\nmaxlongitude=-117.19\nstations = client.get_stations(network=\"CI\", station=\"*\", location=\"*\", channel=\"HH*\",\n                               minlatitude=minlatitude, maxlatitude=maxlatitude,\n                               minlongitude=minlongitude, maxlongitude=maxlongitude)\nstations.plot(projection=\"local\", resolution=\"f\");\n</pre> minlatitude=33.48 maxlatitude=34.84 minlongitude=-118.94 maxlongitude=-117.19 stations = client.get_stations(network=\"CI\", station=\"*\", location=\"*\", channel=\"HH*\",                                minlatitude=minlatitude, maxlatitude=maxlatitude,                                minlongitude=minlongitude, maxlongitude=maxlongitude) stations.plot(projection=\"local\", resolution=\"f\"); In\u00a0[3]: Copied! <pre>now = UTCDateTime(datetime.now())\n# starttime = now - 3600.0*24*30#s\n# endtime = now\nstarttime = UTCDateTime(\"2022-05-26T00:00:00.000\")\nendtime = UTCDateTime(\"2022-06-26T00:00:00.000\")\nevents = client.get_events(starttime=starttime, endtime=endtime,\n                           minmagnitude=2.5,\n                           minlatitude=minlatitude, maxlatitude=maxlatitude,\n                           minlongitude=minlongitude, maxlongitude=maxlongitude)\nevents.plot(projection=\"local\", resolution=\"f\");\n</pre> now = UTCDateTime(datetime.now()) # starttime = now - 3600.0*24*30#s # endtime = now starttime = UTCDateTime(\"2022-05-26T00:00:00.000\") endtime = UTCDateTime(\"2022-06-26T00:00:00.000\") events = client.get_events(starttime=starttime, endtime=endtime,                            minmagnitude=2.5,                            minlatitude=minlatitude, maxlatitude=maxlatitude,                            minlongitude=minlongitude, maxlongitude=maxlongitude) events.plot(projection=\"local\", resolution=\"f\"); In\u00a0[4]: Copied! <pre>print(events)\n</pre> print(events) <pre>1 Event(s) in Catalog:\n2022-06-03T12:05:21.050000Z | +33.901, -118.414 | 2.51 Ml\n</pre> In\u00a0[5]: Copied! <pre># We select the station FMP\nstation = stations.select(station=\"FMP\")\nprint(station[0][0])\n</pre> # We select the station FMP station = stations.select(station=\"FMP\") print(station[0][0]) <pre>Station FMP (Fort Macarthur Park)\n\tStation Code: FMP\n\tChannel Count: 0/175 (Selected/Total)\n\t2000-08-30T00:00:00.000000Z - \n\tAccess: open \n\tLatitude: 33.71, Longitude: -118.29, Elevation: 89.0 m\n\tAvailable Channels:\n\n</pre> In\u00a0[6]: Copied! <pre># The event information:\nprint(events[0].origins[0])\n</pre> # The event information: print(events[0].origins[0]) <pre>Origin\n\t   resource_id: ResourceIdentifier(id=\"smi:service.iris.edu/fdsnws/event/1/query?originid=46955690\")\n\t          time: UTCDateTime(2022, 6, 3, 12, 5, 21, 50000)\n\t     longitude: -118.413667\n\t      latitude: 33.9005\n\t         depth: 11270.0\n\t creation_info: CreationInfo(author='ci,us')\n</pre> In\u00a0[7]: Copied! <pre># download and plot the waveforms\nwaveforms = client.get_waveforms(network=station[0].code,\n                                 station=station[0][0].code,\n                                 location=\"*\",\n                                 channel=\"HH*\", \n                                 starttime=events[0].origins[0].time, \n                                 endtime=events[0].origins[0].time+60)  \nwaveforms.plot();\n</pre> # download and plot the waveforms waveforms = client.get_waveforms(network=station[0].code,                                  station=station[0][0].code,                                  location=\"*\",                                  channel=\"HH*\",                                   starttime=events[0].origins[0].time,                                   endtime=events[0].origins[0].time+60)   waveforms.plot(); <p>Save waveforms for subsequent processing:</p> In\u00a0[8]: Copied! <pre># download and plot the waveforms from the seismic network\nimport obspy\nfrom obspy.clients.fdsn import Client\nclient = Client(\"NCEDC\") ## northern california\n# client = Client(\"SCEDC\") ## sourthern california\n# client = Client(\"IRIS\") ## global\nstarttime = UTCDateTime(\"2022-06-29T14:08:04\")\nwaveforms = client.get_waveforms(network=\"NC\",\n                                 station=\"KPR\",\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=starttime, \n                                 endtime=starttime+120) #s\nwaveforms[0].plot();\n</pre> # download and plot the waveforms from the seismic network import obspy from obspy.clients.fdsn import Client client = Client(\"NCEDC\") ## northern california # client = Client(\"SCEDC\") ## sourthern california # client = Client(\"IRIS\") ## global starttime = UTCDateTime(\"2022-06-29T14:08:04\") waveforms = client.get_waveforms(network=\"NC\",                                  station=\"KPR\",                                  location=\"*\",                                  channel=\"*\",                                   starttime=starttime,                                   endtime=starttime+120) #s waveforms[0].plot(); In\u00a0[9]: Copied! <pre># download and plot the waveforms from the raspberry shake network\nclient = Client(\"RASPISHAKE\")\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R5E62\", ## change this!\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=UTCDateTime(\"2022-06-29T14:08:04\"), \n                                 endtime=UTCDateTime(\"2022-06-29T14:08:04\")+120)  \nwaveforms[0].plot();\n</pre> # download and plot the waveforms from the raspberry shake network client = Client(\"RASPISHAKE\") waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R5E62\", ## change this!                                  location=\"*\",                                  channel=\"*\",                                   starttime=UTCDateTime(\"2022-06-29T14:08:04\"),                                   endtime=UTCDateTime(\"2022-06-29T14:08:04\")+120)   waveforms[0].plot(); In\u00a0[10]: Copied! <pre>waveforms.write(\"waveforms.mseed\")\n</pre> waveforms.write(\"waveforms.mseed\") In\u00a0[11]: Copied! <pre>from obspy.clients.fdsn import RoutingClient\nclient = RoutingClient(\"iris-federator\")\n</pre> from obspy.clients.fdsn import RoutingClient client = RoutingClient(\"iris-federator\") In\u00a0[12]: Copied! <pre>stations = client.get_stations(network=\"CI\",\n                               channel=\"HHZ\", \n                               starttime=events[0].origins[0].time, \n                               endtime=events[0].origins[0].time+60,\n                               latitude=events[0].origins[0].latitude,\n                               longitude=events[0].origins[0].longitude,\n                               maxradius=0.5, level=\"channel\", )  \n# stations.plot(projection=\"local\", resolution=\"f\");\n</pre> stations = client.get_stations(network=\"CI\",                                channel=\"HHZ\",                                 starttime=events[0].origins[0].time,                                 endtime=events[0].origins[0].time+60,                                latitude=events[0].origins[0].latitude,                                longitude=events[0].origins[0].longitude,                                maxradius=0.5, level=\"channel\", )   # stations.plot(projection=\"local\", resolution=\"f\"); In\u00a0[13]: Copied! <pre>waveforms = client.get_waveforms(network=\"CI\",\n                                 channel=\"HHZ\", \n                                 starttime=events[0].origins[0].time, \n                                 endtime=events[0].origins[0].time+60,\n                                 latitude=events[0].origins[0].latitude,\n                                 longitude=events[0].origins[0].longitude,\n                                 maxradius=0.5, level=\"channel\", )  \n# waveforms.plot();\n</pre> waveforms = client.get_waveforms(network=\"CI\",                                  channel=\"HHZ\",                                   starttime=events[0].origins[0].time,                                   endtime=events[0].origins[0].time+60,                                  latitude=events[0].origins[0].latitude,                                  longitude=events[0].origins[0].longitude,                                  maxradius=0.5, level=\"channel\", )   # waveforms.plot(); In\u00a0[14]: Copied! <pre>degree2meter = 111.32 * 1e3 #m\nfor waveform in waveforms:\n    station = stations.select(station=waveform.stats.station)[0][0]\n    waveform.stats.distance = np.sqrt((station.longitude-events[0].origins[0].longitude)**2 + (station.latitude-events[0].origins[0].latitude)**2) * degree2meter\nwaveforms.plot(type='section');\n</pre> degree2meter = 111.32 * 1e3 #m for waveform in waveforms:     station = stations.select(station=waveform.stats.station)[0][0]     waveform.stats.distance = np.sqrt((station.longitude-events[0].origins[0].longitude)**2 + (station.latitude-events[0].origins[0].latitude)**2) * degree2meter waveforms.plot(type='section'); <ul> <li><p>Explore station of the Raspberry Shake network using stationview</p> </li> <li><p>Or download station information using Obspy</p> </li> </ul> In\u00a0[15]: Copied! <pre>client = Client(\"RASPISHAKE\")\nstations = client.get_stations(network=\"AM\", station=\"*\", location=\"*\", channel=\"*\",\n                               minlatitude=minlatitude, maxlatitude=maxlatitude,\n                               minlongitude=minlongitude, maxlongitude=maxlongitude)\nstations.plot(projection=\"local\", resolution=\"f\");\n</pre> client = Client(\"RASPISHAKE\") stations = client.get_stations(network=\"AM\", station=\"*\", location=\"*\", channel=\"*\",                                minlatitude=minlatitude, maxlatitude=maxlatitude,                                minlongitude=minlongitude, maxlongitude=maxlongitude) stations.plot(projection=\"local\", resolution=\"f\"); <ul> <li>Downlaod waveform of the same event using the station R2285 that is close to the event:</li> </ul> In\u00a0[16]: Copied! <pre>client = Client(\"RASPISHAKE\")\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R2285\",\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=events[0].origins[0].time, \n                                 endtime=events[0].origins[0].time+60)  \nwaveforms.plot();\n</pre> client = Client(\"RASPISHAKE\") waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R2285\",                                  location=\"*\",                                  channel=\"*\",                                   starttime=events[0].origins[0].time,                                   endtime=events[0].origins[0].time+60)   waveforms.plot(); In\u00a0[17]: Copied! <pre># download and plot the waveforms from the seismic network\nimport obspy\nfrom obspy.clients.fdsn import Client\nclient = Client(\"NCEDC\") ## northern california\n# client = Client(\"SCEDC\") ## sourthern california\n# client = Client(\"IRIS\") ## global\nstarttime = UTCDateTime(\"2022-06-29T14:08:04\")\nwaveforms = client.get_waveforms(network=\"NC\",\n                                 station=\"KPR\",\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=starttime, \n                                 endtime=starttime+120) #s\nwaveforms[0].plot();\n</pre> # download and plot the waveforms from the seismic network import obspy from obspy.clients.fdsn import Client client = Client(\"NCEDC\") ## northern california # client = Client(\"SCEDC\") ## sourthern california # client = Client(\"IRIS\") ## global starttime = UTCDateTime(\"2022-06-29T14:08:04\") waveforms = client.get_waveforms(network=\"NC\",                                  station=\"KPR\",                                  location=\"*\",                                  channel=\"*\",                                   starttime=starttime,                                   endtime=starttime+120) #s waveforms[0].plot(); In\u00a0[18]: Copied! <pre># download and plot the waveforms from the raspberry shake network\nclient = Client(\"RASPISHAKE\")\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R5E62\", ## change this!\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=UTCDateTime(\"2022-06-29T14:08:04\"), \n                                 endtime=UTCDateTime(\"2022-06-29T14:08:04\")+120)  \nwaveforms[0].plot();\n</pre> # download and plot the waveforms from the raspberry shake network client = Client(\"RASPISHAKE\") waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R5E62\", ## change this!                                  location=\"*\",                                  channel=\"*\",                                   starttime=UTCDateTime(\"2022-06-29T14:08:04\"),                                   endtime=UTCDateTime(\"2022-06-29T14:08:04\")+120)   waveforms[0].plot(); In\u00a0[19]: Copied! <pre>client = Client(\"RASPISHAKE\")\nwaveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R8E9F,RF5B0,R42DE,R6D1A,R891A,RE924,R98B8,RAA32,R06D3,RB07E,RE569\",\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=UTCDateTime(\"2022-07-08 13:10:43\"), \n                                 endtime=UTCDateTime(\"2022-07-08 13:10:43\")+20)  \nwaveforms.plot();\n</pre> client = Client(\"RASPISHAKE\") waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R8E9F,RF5B0,R42DE,R6D1A,R891A,RE924,R98B8,RAA32,R06D3,RB07E,RE569\",                                  location=\"*\",                                  channel=\"*\",                                   starttime=UTCDateTime(\"2022-07-08 13:10:43\"),                                   endtime=UTCDateTime(\"2022-07-08 13:10:43\")+20)   waveforms.plot(); In\u00a0[22]: Copied! <pre>waveforms = client.get_waveforms(network=\"AM\",\n                                 station=\"R8E9F,RF5B0,R42DE,R6D1A,R891A,RE924,R98B8,RAA32,R06D3,RB07E,RE569\",\n                                 location=\"*\",\n                                 channel=\"*\", \n                                 starttime=UTCDateTime(\"2022-07-15 01:19:07\"), \n                                 endtime=UTCDateTime(\"2022-07-15 01:19:07\")+120)  \nwaveforms.normalize()\nwaveforms.plot();\n</pre> waveforms = client.get_waveforms(network=\"AM\",                                  station=\"R8E9F,RF5B0,R42DE,R6D1A,R891A,RE924,R98B8,RAA32,R06D3,RB07E,RE569\",                                  location=\"*\",                                  channel=\"*\",                                   starttime=UTCDateTime(\"2022-07-15 01:19:07\"),                                   endtime=UTCDateTime(\"2022-07-15 01:19:07\")+120)   waveforms.normalize() waveforms.plot(); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#download-seismic-waveforms-using-obspy","title":"Download seismic waveforms using Obspy\u00b6","text":"<p>Reference: https://docs.obspy.org/packages/obspy.clients.fdsn.html</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#explore-seismic-networks","title":"Explore Seismic Networks\u00b6","text":"<p>As an example, we can find all CI stations of Southern California Seismic Network within a region of 33.48 N - 33.84 N, 117.19 - 118.94 W from IRIS</p> <p>We then can find station information using Obspy:</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#explore-earthquakes","title":"Explore Earthquakes\u00b6","text":"<p>We can also use Obspy to find out the events occured in the past month:</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#download-waveforms","title":"Download Waveforms\u00b6","text":"<p>Based on the stations and events information, we now can download the waveforms for analysis.</p> <p>We can first select one event and one station to look at the waveform:</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#compare-waveforms-from-seismic-network-and-raspberry-shake-network","title":"Compare waveforms from seismic network and raspberry shake network\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#download-waveforms-of-multiple-stations","title":"Download waveforms of multiple stations\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#download-waveforms-from-the-raspberry-shake-network","title":"Download Waveforms from the Raspberry Shake network\u00b6","text":"<p>We can also using the Raspberry Shake network to conduct a similar experiment:</p> <p>Now we need to use the client of \"RASPISHAKE\" instead of \"IRIS\"</p> <p>And the network code of Raspberry Shake is AM</p> <p>You can find more details from their manual.</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#compare-waveforms-from-seismic-network-and-raspberry-shake-network","title":"Compare waveforms from seismic network and raspberry shake network\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_download_data/#use-multiple-stations-to-detect-earthquakes","title":"Use multiple stations to detect earthquakes\u00b6","text":"<p>For example, we can look at the waveforms of an earthquake occurred on 2022-07-08:</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/","title":"Signal Processing","text":"In\u00a0[1]: Copied! <pre>import obspy\nfrom obspy import read\nfrom obspy.io.xseed import Parser\nfrom obspy.signal import PPSD\nfrom obspy import read_inventory\nfrom obspy.imaging.cm import pqlx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import cwt, morlet, hilbert, stft\n</pre> import obspy from obspy import read from obspy.io.xseed import Parser from obspy.signal import PPSD from obspy import read_inventory from obspy.imaging.cm import pqlx import numpy as np import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt from scipy.signal import cwt, morlet, hilbert, stft  In\u00a0[2]: Copied! <pre>st = obspy.read()\nst.remove_sensitivity()\ntr = st[0]\ntr.plot();\n</pre> st = obspy.read() st.remove_sensitivity() tr = st[0] tr.plot(); In\u00a0[3]: Copied! <pre>p, f = tr.data, tr.stats.sampling_rate\npsd = np.abs(np.fft.fft(p))**2\nfreq = np.fft.fftfreq(p.size, d=1./f)\n\n# Only consider positive frequencies for plotting\nmask = freq &gt; 0\npsd, freq = psd[mask], freq[mask]\npsd = 10. * np.log10(psd) # Convert to dB\n\nplt.figure(figsize=(8, 4))\nplt.semilogx(freq, psd)\nplt.xlabel(\"Frequency (Hz)\")\nplt.ylabel(\"Power\")\nplt.tight_layout()\nplt.show()\n</pre> p, f = tr.data, tr.stats.sampling_rate psd = np.abs(np.fft.fft(p))**2 freq = np.fft.fftfreq(p.size, d=1./f)  # Only consider positive frequencies for plotting mask = freq &gt; 0 psd, freq = psd[mask], freq[mask] psd = 10. * np.log10(psd) # Convert to dB  plt.figure(figsize=(8, 4)) plt.semilogx(freq, psd) plt.xlabel(\"Frequency (Hz)\") plt.ylabel(\"Power\") plt.tight_layout() plt.show() In\u00a0[4]: Copied! <pre>starttime = tr.stats.starttime\nend_time = tr.stats.endtime\nmid_time = starttime + (end_time - starttime)/2\ntr_eq = tr.slice(starttime=starttime, endtime=mid_time)\ntr_ns = tr.slice(starttime=mid_time, endtime=end_time)\ntr_eq.plot();\ntr_ns.plot();\n</pre> starttime = tr.stats.starttime end_time = tr.stats.endtime mid_time = starttime + (end_time - starttime)/2 tr_eq = tr.slice(starttime=starttime, endtime=mid_time) tr_ns = tr.slice(starttime=mid_time, endtime=end_time) tr_eq.plot(); tr_ns.plot(); In\u00a0[5]: Copied! <pre>p_eq, f_eq = tr_eq.data, tr_eq.stats.sampling_rate\npsd_eq = np.abs(np.fft.fft(p_eq))**2\nfreq_eq = np.fft.fftfreq(p_eq.size, d=1./f_eq)\n\np_ns, f_ns = tr_ns.data, tr_ns.stats.sampling_rate\npsd_ns = np.abs(np.fft.fft(p_ns))**2\nfreq_ns = np.fft.fftfreq(p_ns.size, d=1./f_ns)\n\nmask_eq = freq_eq &gt; 0\npsd_eq, freq_eq = psd_eq[mask_eq], freq_eq[mask_eq]\nmask_ns = freq_ns &gt; 0\npsd_ns, freq_ns = psd_ns[mask_ns], freq_ns[mask_ns]\n\npsd_eq = 10. * np.log10(psd_eq) \npsd_ns = 10. * np.log10(psd_ns)\n\nplt.figure(figsize=(8, 4))\nplt.semilogx(freq_eq, psd_eq, label='Earthquake')\nplt.semilogx(freq_ns, psd_ns, label='Noise')\nplt.xlabel(\"Frequency (Hz)\")\n# plt.xlabel(\"Period (s)\")\nplt.ylabel(\"Power\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> p_eq, f_eq = tr_eq.data, tr_eq.stats.sampling_rate psd_eq = np.abs(np.fft.fft(p_eq))**2 freq_eq = np.fft.fftfreq(p_eq.size, d=1./f_eq)  p_ns, f_ns = tr_ns.data, tr_ns.stats.sampling_rate psd_ns = np.abs(np.fft.fft(p_ns))**2 freq_ns = np.fft.fftfreq(p_ns.size, d=1./f_ns)  mask_eq = freq_eq &gt; 0 psd_eq, freq_eq = psd_eq[mask_eq], freq_eq[mask_eq] mask_ns = freq_ns &gt; 0 psd_ns, freq_ns = psd_ns[mask_ns], freq_ns[mask_ns]  psd_eq = 10. * np.log10(psd_eq)  psd_ns = 10. * np.log10(psd_ns)  plt.figure(figsize=(8, 4)) plt.semilogx(freq_eq, psd_eq, label='Earthquake') plt.semilogx(freq_ns, psd_ns, label='Noise') plt.xlabel(\"Frequency (Hz)\") # plt.xlabel(\"Period (s)\") plt.ylabel(\"Power\") plt.legend() plt.tight_layout() plt.show()  In\u00a0[6]: Copied! <pre>st = read(\"https://examples.obspy.org/BW.KW1..EHZ.D.2011.037\")\ntr = st.select(id=\"BW.KW1..EHZ\")[0]\ninv = read_inventory(\"https://examples.obspy.org/BW_KW1.xml\")\n</pre> st = read(\"https://examples.obspy.org/BW.KW1..EHZ.D.2011.037\") tr = st.select(id=\"BW.KW1..EHZ\")[0] inv = read_inventory(\"https://examples.obspy.org/BW_KW1.xml\") In\u00a0[7]: Copied! <pre>tr.plot();\n</pre> tr.plot(); In\u00a0[8]: Copied! <pre>ppsd = PPSD(tr.stats, metadata=inv)\nppsd.add(st)\n</pre> ppsd = PPSD(tr.stats, metadata=inv) ppsd.add(st) Out[8]: <pre>True</pre> In\u00a0[9]: Copied! <pre>ppsd.plot(cmap=pqlx);\n</pre> ppsd.plot(cmap=pqlx); In\u00a0[10]: Copied! <pre>st = obspy.read()\ntr = st[0]\ntr.plot(title=\"Original Data\");\n</pre> st = obspy.read() tr = st[0] tr.plot(title=\"Original Data\"); <ul> <li>High pass filter</li> </ul> In\u00a0[11]: Copied! <pre>tr_highpass = tr.copy()\ntr_highpass.filter('highpass', freq=1.0, corners=2, zerophase=True)\ntr_highpass.plot(title=\"Highpass Filtered Data\");\n</pre> tr_highpass = tr.copy() tr_highpass.filter('highpass', freq=1.0, corners=2, zerophase=True) tr_highpass.plot(title=\"Highpass Filtered Data\"); <ul> <li>TODO: Low pass filter</li> </ul> In\u00a0[12]: Copied! <pre>tr_lowpass = tr.copy()\n</pre> tr_lowpass = tr.copy() <ul> <li>TODO: Band pass filter</li> </ul> In\u00a0[13]: Copied! <pre>tr_bandpass = tr.copy()\n</pre> tr_bandpass = tr.copy() In\u00a0[14]: Copied! <pre>st = obspy.read()\ntr = st[0]\ntr.spectrogram(log=True, title=\"Spectrogram\", dbscale=True, show=True, cmap='viridis');\n</pre> st = obspy.read() tr = st[0] tr.spectrogram(log=True, title=\"Spectrogram\", dbscale=True, show=True, cmap='viridis'); In\u00a0[15]: Copied! <pre>tr.spectrogram(log=False, title=\"Spectrogram\", dbscale=True, show=True, cmap='viridis');\n</pre> tr.spectrogram(log=False, title=\"Spectrogram\", dbscale=True, show=True, cmap='viridis'); In\u00a0[16]: Copied! <pre>st = obspy.read()\ntr = st[0]\n\n# Compute the STFT\nfrequencies, times, Zxx = stft(tr.data, fs=tr.stats.sampling_rate, nperseg=128, noverlap=127)\n\n# Convert to dB\nZxx_dB = 10. * np.log10(np.abs(Zxx))\n\nplt.figure()\nplt.pcolormesh(times, frequencies, Zxx_dB, shading='auto', cmap='viridis')\nplt.title('STFT Magnitude in dB')\nplt.ylabel('Frequency [Hz]')\nplt.xlabel('Time [s]')\nplt.colorbar(label='dB')\nplt.ylim([0, 50])  # Limit frequency range for display\nplt.show()\n</pre> st = obspy.read() tr = st[0]  # Compute the STFT frequencies, times, Zxx = stft(tr.data, fs=tr.stats.sampling_rate, nperseg=128, noverlap=127)  # Convert to dB Zxx_dB = 10. * np.log10(np.abs(Zxx))  plt.figure() plt.pcolormesh(times, frequencies, Zxx_dB, shading='auto', cmap='viridis') plt.title('STFT Magnitude in dB') plt.ylabel('Frequency [Hz]') plt.xlabel('Time [s]') plt.colorbar(label='dB') plt.ylim([0, 50])  # Limit frequency range for display plt.show() <ul> <li>TODO: Change the values of nperseg and noverlap to see the effect.</li> </ul> <ul> <li>TODO: Visualize the spectrogram after filtering</li> </ul> In\u00a0[17]: Copied! <pre># Define a range of desired center frequencies (in Hz)\ndesired_frequencies = np.linspace(1, 50, 100)  # Example: 100 frequencies between 1 Hz and 50 Hz\n\n# Compute the associated wavelet widths for these frequencies\ndt = tr.stats.delta\nwidths = np.round(2 / (desired_frequencies * dt)).astype(int)\n\n# Compute the CWT using the Morlet wavelet\ncwt_matrix = cwt(tr.data, morlet, widths)\n\n# Convert to magnitude for visualization\ncwt_magnitude = np.abs(cwt_matrix)\n\nplt.imshow(cwt_magnitude, extent=[0, tr.stats.endtime - tr.stats.starttime, desired_frequencies[0], desired_frequencies[-1]], aspect='auto', origin='lower', cmap='jet')\nplt.colorbar(label='Magnitude')\nplt.title('Continuous Wavelet Transform (CWT)')\nplt.xlabel('Time [s]')\nplt.ylabel('Frequency [Hz]')\nplt.yscale('log')  # Logarithmic frequency scale can often be more informative\nplt.show()\n</pre> # Define a range of desired center frequencies (in Hz) desired_frequencies = np.linspace(1, 50, 100)  # Example: 100 frequencies between 1 Hz and 50 Hz  # Compute the associated wavelet widths for these frequencies dt = tr.stats.delta widths = np.round(2 / (desired_frequencies * dt)).astype(int)  # Compute the CWT using the Morlet wavelet cwt_matrix = cwt(tr.data, morlet, widths)  # Convert to magnitude for visualization cwt_magnitude = np.abs(cwt_matrix)  plt.imshow(cwt_magnitude, extent=[0, tr.stats.endtime - tr.stats.starttime, desired_frequencies[0], desired_frequencies[-1]], aspect='auto', origin='lower', cmap='jet') plt.colorbar(label='Magnitude') plt.title('Continuous Wavelet Transform (CWT)') plt.xlabel('Time [s]') plt.ylabel('Frequency [Hz]') plt.yscale('log')  # Logarithmic frequency scale can often be more informative plt.show() In\u00a0[18]: Copied! <pre>st = obspy.read()\ntr = st[0]\n\ntr_filt = tr.copy()\ntr_filt.filter('bandpass', freqmin=1, freqmax=3, corners=2, zerophase=True)\n\ndata_envelope = obspy.signal.filter.envelope(tr_filt.data)\n\nt = np.arange(0, tr.stats.npts / tr.stats.sampling_rate, 1 / tr.stats.sampling_rate)\nplt.plot(t, tr_filt.data, 'k')\nplt.plot(t, data_envelope, 'k:')\nplt.ylabel('Filtered Data w/ Envelope')\nplt.xlabel('Time [s]')\nplt.xlim(4, 10)\nplt.show()\n</pre> st = obspy.read() tr = st[0]  tr_filt = tr.copy() tr_filt.filter('bandpass', freqmin=1, freqmax=3, corners=2, zerophase=True)  data_envelope = obspy.signal.filter.envelope(tr_filt.data)  t = np.arange(0, tr.stats.npts / tr.stats.sampling_rate, 1 / tr.stats.sampling_rate) plt.plot(t, tr_filt.data, 'k') plt.plot(t, data_envelope, 'k:') plt.ylabel('Filtered Data w/ Envelope') plt.xlabel('Time [s]') plt.xlim(4, 10) plt.show() In\u00a0[19]: Copied! <pre>analytic_signal = hilbert(tr_filt.data)\nenvelope = np.abs(analytic_signal)\n\nplt.figure()\nplt.plot(t, tr_filt.data, label=\"Original Seismogram\", color='blue')\nplt.plot(t, envelope, label=\"Envelope\", color='red', linewidth=2)\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Seismogram and its Envelope\")\nplt.legend()\nplt.xlim(4, 10)\nplt.show()\n</pre> analytic_signal = hilbert(tr_filt.data) envelope = np.abs(analytic_signal)  plt.figure() plt.plot(t, tr_filt.data, label=\"Original Seismogram\", color='blue') plt.plot(t, envelope, label=\"Envelope\", color='red', linewidth=2) plt.xlabel(\"Time [s]\") plt.ylabel(\"Amplitude\") plt.title(\"Seismogram and its Envelope\") plt.legend() plt.xlim(4, 10) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#seismic-signal-processing-with-obspy","title":"Seismic Signal Processing with ObsPy\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#fourier-transform-and-power-spectral-densities","title":"Fourier Transform and Power Spectral Densities\u00b6","text":"<p>Probabilistic Power Spectral Densities</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#frequency-of-earthquakes","title":"Frequency of earthquakes\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#psd-of-ambient-noise","title":"PSD of ambient noise\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#reading-microseism","title":"Reading: Microseism\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#filtering","title":"Filtering\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#spectrogram","title":"Spectrogram\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#using-stft-short-time-fourier-transform","title":"Using STFT (Short Time Fourier Transform)\u00b6","text":"<p>Note:</p> <p>nperseg is the length of each segment.</p> <p>noverlap is the number of points to overlap between segments.</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#further-reading-using-wavelet-transform-for-time-frequency-analysis","title":"Further Reading: Using Wavelet Transform for time-frequency analysis\u00b6","text":"<p>Wavelet Transform</p>"},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#seismogram-envelopes","title":"Seismogram Envelopes\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/obspy_process_data/#using-hilbert-transform","title":"Using Hilbert Transform\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/phasenet_das/","title":"Phase Picking on DAS","text":"In\u00a0[1]: Copied! <pre>import os\nimport torch\nimport numpy as np\nimport h5py\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import os import torch import numpy as np import h5py from glob import glob import matplotlib.pyplot as plt import pandas as pd In\u00a0[2]: Copied! <pre>if not os.path.exists(\"data\"):\n    os.mkdir(\"data\")\n\nevent_id = \"ci38595978\"\nif not os.path.exists(f'data/{event_id}.h5'):\n    os.system(f\"wget https://huggingface.co/datasets/AI4EPS/quakeflow_das/resolve/main/data/ridgecrest_north/{event_id}.h5 -P data &gt; log.txt 2&gt;&amp;1\")\n</pre> if not os.path.exists(\"data\"):     os.mkdir(\"data\")  event_id = \"ci38595978\" if not os.path.exists(f'data/{event_id}.h5'):     os.system(f\"wget https://huggingface.co/datasets/AI4EPS/quakeflow_das/resolve/main/data/ridgecrest_north/{event_id}.h5 -P data &gt; log.txt 2&gt;&amp;1\")  In\u00a0[3]: Copied! <pre>normalize = lambda x: (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)\n# h5_files = glob(\"data/*.h5\")\n# for file in h5_files:\nfile = f\"data/{event_id}.h5\"\nwith h5py.File(file, \"r\") as fp:\n    ds = fp[\"data\"]\n    data = ds[...]\n    dt = ds.attrs[\"dt_s\"]\n    dx = ds.attrs[\"dx_m\"]\n    nx, nt = data.shape\n    x = np.arange(nx) * dx\n    t = np.arange(nt) * dt\n        \nplt.figure()\nplt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\")\nplt.xlabel(\"Distance (m)\")\nplt.ylabel(\"Time (s)\")\n</pre> normalize = lambda x: (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True) # h5_files = glob(\"data/*.h5\") # for file in h5_files: file = f\"data/{event_id}.h5\" with h5py.File(file, \"r\") as fp:     ds = fp[\"data\"]     data = ds[...]     dt = ds.attrs[\"dt_s\"]     dx = ds.attrs[\"dx_m\"]     nx, nt = data.shape     x = np.arange(nx) * dx     t = np.arange(nt) * dt          plt.figure() plt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\") plt.xlabel(\"Distance (m)\") plt.ylabel(\"Time (s)\")  Out[3]: <pre>Text(0, 0.5, 'Time (s)')</pre> In\u00a0[4]: Copied! <pre>ngpu = torch.cuda.device_count()\nbase_cmd = \"QuakeFlow/EQNet/predict.py --model phasenet_das --data_list=files.txt --result_path ./results --format=h5  --batch_size 1 --workers 0 --folder_depth 1\"\n</pre> ngpu = torch.cuda.device_count() base_cmd = \"QuakeFlow/EQNet/predict.py --model phasenet_das --data_list=files.txt --result_path ./results --format=h5  --batch_size 1 --workers 0 --folder_depth 1\" In\u00a0[5]: Copied! <pre>with open(\"files.txt\", \"w\") as f:\n    f.write(f\"data/{event_id}.h5\")\n\n\nif ngpu == 0:\n    cmd = f\"python {base_cmd} --device cpu\"\nelif ngpu == 1:\n    cmd = f\"python {base_cmd}\"\nelse:\n    cmd = f\"torchrun --nproc_per_node {ngpu} {base_cmd}\"\n\nprint(cmd)\nos.system(cmd);\n</pre> with open(\"files.txt\", \"w\") as f:     f.write(f\"data/{event_id}.h5\")   if ngpu == 0:     cmd = f\"python {base_cmd} --device cpu\" elif ngpu == 1:     cmd = f\"python {base_cmd}\" else:     cmd = f\"torchrun --nproc_per_node {ngpu} {base_cmd}\"  print(cmd) os.system(cmd); <pre>python QuakeFlow/EQNet/predict.py --model phasenet_das --data_list=files.txt --result_path ./results --format=h5  --batch_size 1 --workers 0 --folder_depth 1 --device cpu\nNot using distributed mode\nNamespace(model='phasenet_das', resume='', backbone='unet', phases=['P', 'S'], device='cpu', workers=0, batch_size=1, use_deterministic_algorithms=False, amp=False, world_size=1, dist_url='env://', data_path='./', data_list='files.txt', hdf5_file=None, prefix='', format='h5', dataset='das', result_path='./results', plot_figure=False, min_prob=0.3, add_polarity=False, add_event=False, highpass_filter=0.0, response_xml=None, folder_depth=1, cut_patch=False, nt=20480, nx=5120, resample_time=False, resample_space=False, system=None, location=None, skip_existing=False, distributed=False)\nTotal samples: 1 files\n</pre> <pre>Predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10&lt;00:00, 10.86s/it]\n</pre> In\u00a0[6]: Copied! <pre>picks = pd.read_csv(f\"results/picks_phasenet_das/{event_id}.csv\")\n</pre> picks = pd.read_csv(f\"results/picks_phasenet_das/{event_id}.csv\") In\u00a0[7]: Copied! <pre>plt.figure()\nplt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\")\ncolor = picks[\"phase_type\"].map({\"P\": \"C0\", \"S\": \"C1\"})\nplt.scatter(picks[\"channel_index\"].values * dx, picks[\"phase_index\"].values * dt, c=color, s=1)\nplt.scatter([], [], c=\"r\", label=\"P\")\nplt.scatter([], [], c=\"b\", label=\"S\")\nplt.legend()\nplt.xlabel(\"Distance (m)\")\nplt.ylabel(\"Time (s)\")\n</pre> plt.figure() plt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\") color = picks[\"phase_type\"].map({\"P\": \"C0\", \"S\": \"C1\"}) plt.scatter(picks[\"channel_index\"].values * dx, picks[\"phase_index\"].values * dt, c=color, s=1) plt.scatter([], [], c=\"r\", label=\"P\") plt.scatter([], [], c=\"b\", label=\"S\") plt.legend() plt.xlabel(\"Distance (m)\") plt.ylabel(\"Time (s)\") Out[7]: <pre>Text(0, 0.5, 'Time (s)')</pre> In\u00a0[8]: Copied! <pre>filename = \"2022-06-18T11:22:49.660Z/Eureka-DT1087-2m-P5kHz-fs250Hz_2022-06-18T112221Z.h5\"\nif not os.path.exists(f'data/Eureka-DT1087-2m-P5kHz-fs250Hz_2022-06-18T112221Z.h5'):\n    os.system(f\"wget https://huggingface.co/datasets/AI4EPS/quakeflow_das/resolve/main/data/eureka/2022-06-18T11:22:49.660Z/Eureka-DT1087-2m-P5kHz-fs250Hz_2022-06-18T112221Z.h5 -P data &gt; log.txt 2&gt;&amp;1\")\n</pre> filename = \"2022-06-18T11:22:49.660Z/Eureka-DT1087-2m-P5kHz-fs250Hz_2022-06-18T112221Z.h5\" if not os.path.exists(f'data/Eureka-DT1087-2m-P5kHz-fs250Hz_2022-06-18T112221Z.h5'):     os.system(f\"wget https://huggingface.co/datasets/AI4EPS/quakeflow_das/resolve/main/data/eureka/2022-06-18T11:22:49.660Z/Eureka-DT1087-2m-P5kHz-fs250Hz_2022-06-18T112221Z.h5 -P data &gt; log.txt 2&gt;&amp;1\") In\u00a0[9]: Copied! <pre>normalize = lambda x: (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)\nwith h5py.File(f\"data/{filename.split('/')[-1]}\", \"r\") as fp:\n\n    dx = fp[\"Acquisition\"].attrs[\"SpatialSamplingInterval\"]\n    fs = fp['Acquisition/Raw[0]'].attrs[\"OutputDataRate\"]\n    dt = 1.0 / fs\n    data = fp[\"Acquisition/Raw[0]/RawData\"][...]\n    data = np.gradient(data, axis=-1) / dt\n\n    nx, nt = data.shape\n    x = np.arange(nx) * dx\n    t = np.arange(nt) * dt\n        \nplt.figure()\nplt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\")\nplt.xlabel(\"Distance (m)\")\nplt.ylabel(\"Time (s)\")\n</pre> normalize = lambda x: (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True) with h5py.File(f\"data/{filename.split('/')[-1]}\", \"r\") as fp:      dx = fp[\"Acquisition\"].attrs[\"SpatialSamplingInterval\"]     fs = fp['Acquisition/Raw[0]'].attrs[\"OutputDataRate\"]     dt = 1.0 / fs     data = fp[\"Acquisition/Raw[0]/RawData\"][...]     data = np.gradient(data, axis=-1) / dt      nx, nt = data.shape     x = np.arange(nx) * dx     t = np.arange(nt) * dt          plt.figure() plt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\") plt.xlabel(\"Distance (m)\") plt.ylabel(\"Time (s)\") Out[9]: <pre>Text(0, 0.5, 'Time (s)')</pre> In\u00a0[10]: Copied! <pre>with open(\"files.txt\", \"w\") as f:\n    f.write(f\"data/{filename.split('/')[-1]}\")\n\nif ngpu == 0:\n    cmd = f\"python {base_cmd} --device cpu --system optasense --cut_patch --nx 2048 --nt 20480\"\nelif ngpu == 1:\n    cmd = f\"python {base_cmd} --system optasense\"\nelse:\n    cmd = f\"torchrun --nproc_per_node {ngpu} {base_cmd} --system optasense\"\n\nprint(cmd)\nos.system(cmd);\n</pre> with open(\"files.txt\", \"w\") as f:     f.write(f\"data/{filename.split('/')[-1]}\")  if ngpu == 0:     cmd = f\"python {base_cmd} --device cpu --system optasense --cut_patch --nx 2048 --nt 20480\" elif ngpu == 1:     cmd = f\"python {base_cmd} --system optasense\" else:     cmd = f\"torchrun --nproc_per_node {ngpu} {base_cmd} --system optasense\"  print(cmd) os.system(cmd); <pre>python QuakeFlow/EQNet/predict.py --model phasenet_das --data_list=files.txt --result_path ./results --format=h5  --batch_size 1 --workers 0 --folder_depth 1 --device cpu --system optasense --cut_patch --nx 2048 --nt 20480\nNot using distributed mode\nNamespace(model='phasenet_das', resume='', backbone='unet', phases=['P', 'S'], device='cpu', workers=0, batch_size=1, use_deterministic_algorithms=False, amp=False, world_size=1, dist_url='env://', data_path='./', data_list='files.txt', hdf5_file=None, prefix='', format='h5', dataset='das', result_path='./results', plot_figure=False, min_prob=0.3, add_polarity=False, add_event=False, highpass_filter=0.0, response_xml=None, folder_depth=1, cut_patch=True, nt=20480, nx=2048, resample_time=False, resample_space=False, system='optasense', location=None, skip_existing=False, distributed=False)\nTotal samples: 1 files\n</pre> <pre>Predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [01:08&lt;00:00, 17.05s/it]\nMerging results/picks_phasenet_das: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 231.36it/s]\n</pre> <pre>Number of detections: 16518\n</pre> In\u00a0[11]: Copied! <pre>picks = pd.read_csv(f\"results/picks_phasenet_das/{filename.split('/')[-1].split('.')[0]}.csv\")\n</pre> picks = pd.read_csv(f\"results/picks_phasenet_das/{filename.split('/')[-1].split('.')[0]}.csv\") In\u00a0[12]: Copied! <pre>plt.figure()\nplt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\")\ncolor = picks[\"phase_type\"].map({\"P\": \"C0\", \"S\": \"C1\"})\nplt.scatter(picks[\"channel_index\"].values * dx, picks[\"phase_index\"].values * dt, c=color, s=1)\nplt.scatter([], [], c=\"r\", label=\"P\")\nplt.scatter([], [], c=\"b\", label=\"S\")\nplt.legend()\nplt.xlabel(\"Distance (m)\")\nplt.ylabel(\"Time (s)\")\n</pre> plt.figure() plt.imshow(normalize(data).T, cmap=\"seismic\", vmin=-1, vmax=1, aspect=\"auto\", extent=[x[0], x[-1], t[-1], t[0]], interpolation=\"none\") color = picks[\"phase_type\"].map({\"P\": \"C0\", \"S\": \"C1\"}) plt.scatter(picks[\"channel_index\"].values * dx, picks[\"phase_index\"].values * dt, c=color, s=1) plt.scatter([], [], c=\"r\", label=\"P\") plt.scatter([], [], c=\"b\", label=\"S\") plt.legend() plt.xlabel(\"Distance (m)\") plt.ylabel(\"Time (s)\") Out[12]: <pre>Text(0, 0.5, 'Time (s)')</pre> <p>Notes:</p> <ul> <li>The model was trained on data with a channel spacing of 8-10 m, and a sampling rate of 100 Hz. The model will have a reduced performance on data with different channel spacing and sampling rate.</li> </ul>"},{"location":"eps207-observational-seismology/lectures/codes/phasenet_das/#download-event-data","title":"Download event data\u00b6","text":"<p>More test examples: https://huggingface.co/datasets/AI4EPS/quakeflow_das</p>"},{"location":"eps207-observational-seismology/lectures/codes/phasenet_das/#run-phasenet-das","title":"Run PhaseNet-DAS\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/phasenet_das/#plot-results","title":"Plot results\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/phasenet_das/#support-for-optasense-hdf5-format","title":"Support for OptaSense hdf5 format\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/","title":"Phase Picking and Association","text":"In\u00a0[1]: Copied! <pre>import os\n\nroot_path = \"local\"\nregion = \"demo\"\n# region = \"Ridgecrest\"\n\ncode_path = \"./QuakeFlow/slurm\"\ndef run(root_path, script):\n    cmd = f\"cd {code_path} &amp;&amp; python {script} {root_path} {region}\"\n    print(cmd)\n    os.system(cmd)\n</pre> import os  root_path = \"local\" region = \"demo\" # region = \"Ridgecrest\"  code_path = \"./QuakeFlow/slurm\" def run(root_path, script):     cmd = f\"cd {code_path} &amp;&amp; python {script} {root_path} {region}\"     print(cmd)     os.system(cmd) In\u00a0[2]: Copied! <pre>run(root_path, \"set_config.py\")\n</pre> run(root_path, \"set_config.py\") <pre>cd ./QuakeFlow/slurm &amp;&amp; python set_config.py local demo\n{\n    \"num_nodes\": 1,\n    \"sampling_rate\": 100,\n    \"degree2km\": 111.1949,\n    \"channel\": \"HH*,BH*,EH*,HN*\",\n    \"level\": \"response\",\n    \"gamma\": {\n        \"zmin_km\": 0,\n        \"zmax_km\": 60\n    },\n    \"cctorch\": {\n        \"sampling_rate\": 100,\n        \"time_before\": 0.25,\n        \"time_after\": 1.0,\n        \"min_pair_dist_km\": 10,\n        \"components\": \"ENZ123\",\n        \"component_mapping\": {\n            \"3\": 0,\n            \"2\": 1,\n            \"1\": 2,\n            \"E\": 0,\n            \"N\": 1,\n            \"Z\": 2\n        }\n    },\n    \"longitude0\": -117.504,\n    \"latitude0\": 35.705,\n    \"maxradius_degree\": 0.5,\n    \"minlatitude\": 35.205,\n    \"maxlatitude\": 36.205,\n    \"minlongitude\": -118.004,\n    \"maxlongitude\": -117.004,\n    \"starttime\": \"2019-07-04T17:00:00\",\n    \"endtime\": \"2019-07-05T00:00:00\",\n    \"provider\": [\n        \"SCEDC\"\n    ],\n    \"network\": \"CI\"\n}\n</pre> In\u00a0[3]: Copied! <pre>run(root_path, \"download_station.py\")\n\nrun(root_path, \"download_catalog.py\")\n</pre> run(root_path, \"download_station.py\")  run(root_path, \"download_catalog.py\") <pre>cd ./QuakeFlow/slurm &amp;&amp; python download_station.py local demo\n{\n    \"num_nodes\": 1,\n    \"sampling_rate\": 100,\n    \"degree2km\": 111.1949,\n    \"channel\": \"HH*,BH*,EH*,HN*\",\n    \"level\": \"response\",\n    \"gamma\": {\n        \"zmin_km\": 0,\n        \"zmax_km\": 60\n    },\n    \"cctorch\": {\n        \"sampling_rate\": 100,\n        \"time_before\": 0.25,\n        \"time_after\": 1.0,\n        \"min_pair_dist_km\": 10,\n        \"components\": \"ENZ123\",\n        \"component_mapping\": {\n            \"3\": 0,\n            \"2\": 1,\n            \"1\": 2,\n            \"E\": 0,\n            \"N\": 1,\n            \"Z\": 2\n        }\n    },\n    \"longitude0\": -117.504,\n    \"latitude0\": 35.705,\n    \"maxradius_degree\": 0.5,\n    \"minlatitude\": 35.205,\n    \"maxlatitude\": 36.205,\n    \"minlongitude\": -118.004,\n    \"maxlongitude\": -117.004,\n    \"starttime\": \"2019-07-04T17:00:00\",\n    \"endtime\": \"2019-07-05T00:00:00\",\n    \"provider\": [\n        \"SCEDC\"\n    ],\n    \"network\": \"CI\"\n}\nDownloading station response...\nLoading existing local/demo/obspy/inventory_scedc.xml\nParse 150 channels of SCEDC into csv\nParse 52 stations of SCEDC into json\nMerged 150 channels\nMerged 52 stations\ncd ./QuakeFlow/slurm &amp;&amp; python download_catalog.py local demo\n{\n    \"num_nodes\": 1,\n    \"sampling_rate\": 100,\n    \"degree2km\": 111.1949,\n    \"channel\": \"HH*,BH*,EH*,HN*\",\n    \"level\": \"response\",\n    \"gamma\": {\n        \"zmin_km\": 0,\n        \"zmax_km\": 60\n    },\n    \"cctorch\": {\n        \"sampling_rate\": 100,\n        \"time_before\": 0.25,\n        \"time_after\": 1.0,\n        \"min_pair_dist_km\": 10,\n        \"components\": \"ENZ123\",\n        \"component_mapping\": {\n            \"3\": 0,\n            \"2\": 1,\n            \"1\": 2,\n            \"E\": 0,\n            \"N\": 1,\n            \"Z\": 2\n        }\n    },\n    \"longitude0\": -117.504,\n    \"latitude0\": 35.705,\n    \"maxradius_degree\": 0.5,\n    \"minlatitude\": 35.205,\n    \"maxlatitude\": 36.205,\n    \"minlongitude\": -118.004,\n    \"maxlongitude\": -117.004,\n    \"starttime\": \"2019-07-04T17:00:00\",\n    \"endtime\": \"2019-07-05T00:00:00\",\n    \"provider\": [\n        \"SCEDC\"\n    ],\n    \"network\": \"CI\"\n}\nDownloading standard catalog...\nDowloaded 703 events from scedc\n</pre> In\u00a0[4]: Copied! <pre>run(root_path, \"download_waveform_v2.py\")\n</pre> run(root_path, \"download_waveform_v2.py\") <pre>cd ./QuakeFlow/slurm &amp;&amp; python download_waveform_v2.py local demo\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWRV2_HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWRV2_HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWRV2_HNZ_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWVP2_HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWVP2_HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWVP2_HNZ_2C2019185.ms\nlocal/demo/waveforms/2019-185/19/CI.WVP2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WVP2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WMF..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WMF..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WMF..BHZ.mseed already exists. Skip.\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWMF__HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWBM__HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWMF__HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWBM__HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWMF__HNZ_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWBM__HNZ_2C2019185.ms\nlocal/demo/waveforms/2019-185/19/CI.MPM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.MPM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.MPM..HHZ.mseed already exists. Skip.\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIQ0072HNE_012019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIQ0072HNN_012019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIQ0072HNZ_012019185.ms\nlocal/demo/waveforms/2019-185/18/CI.WCS2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WCS2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WCS2..HNZ.mseed already exists. Skip.\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWNM__HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWNM__HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CIWNM__HNZ_2C2019185.ms\nlocal/demo/waveforms/2019-185/19/CI.WBM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WBM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WBM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.TOW2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.TOW2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.TOW2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WMF..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WMF..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WMF..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SRT..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SRT..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SRT..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SRT..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SRT..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SRT..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WBM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WBM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WBM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.LRL..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.LRL..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.LRL..BHZ.mseed already exists. Skip.\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CILRL__HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CILRL__HNE_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CILRL__HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CILRL__HNN_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CILRL__HNZ_2C2019185.ms\nFailed to download scedc-pds/continuous_waveforms/2019/2019_185/CILRL__HNZ_2C2019185.ms\nlocal/demo/waveforms/2019-185/18/CI.DTP..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.DTP..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.DTP..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.MPM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.MPM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.MPM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.WRV2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.TOW2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.TOW2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.TOW2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WVP2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.DTP..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.DTP..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.DTP..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.LRL..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.LRL..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.LRL..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SLA..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SLA..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SLA..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WBM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WBM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WBM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SRT..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SRT..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SRT..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CCC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CCC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CCC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.DTP..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.DTP..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.DTP..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SLA..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SLA..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.SLA..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WMF..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WMF..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WMF..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRV2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.LRL..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.LRL..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.LRL..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CLC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CLC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CLC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.MPM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.MPM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.MPM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRV2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRV2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRV2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WCS2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.WRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CLC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CLC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/19/CI.CLC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.CLC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.CLC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.CLC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.DTP..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.DTP..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.DTP..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.LRL..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.LRL..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.CLC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.CLC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.CLC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/18/CI.SLA..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WNM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WNM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WNM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRV2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CLC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WNM..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.LRL..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WVP2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WVP2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WVP2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WVP2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.MPM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.CCC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.TOW2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WBM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WCS2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SLA..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.JRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WMF..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.SRT..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.DTP..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRV2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRV2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/20/CI.WRV2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.TOW2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WNM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WNM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WNM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WMF..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SLA..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRV2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CLC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.CCC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRV2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRV2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRV2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.MPM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WVP2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WVP2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WVP2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.JRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.SRT..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WBM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WNM..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.LRL..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WVP2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.WCS2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/21/CI.DTP..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WVP2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRV2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRV2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRV2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WRV2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.LRL..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SLA..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.JRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.MPM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WVP2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WVP2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WVP2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WNM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WNM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WNM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CLC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.CCC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.SRT..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WBM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WCS2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WMF..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.DTP..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.WNM..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/22/CI.TOW2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRV2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRV2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRV2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WVP2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRV2..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WBM..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WNM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WNM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WNM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CCC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WMF..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WCS2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.MPM..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.DTP..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WVP2..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WVP2..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WVP2..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SRT..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..HNE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..HNN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.LRL..HNZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.CLC..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.SLA..HHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WNM..EHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.WRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..BHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..BHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.JRC2..BHZ.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..HHE.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..HHN.mseed already exists. Skip.\nlocal/demo/waveforms/2019-185/23/CI.TOW2..HHZ.mseed already exists. Skip.\n</pre> In\u00a0[5]: Copied! <pre>run(root_path, \"run_phasenet.py\")\n# run(root_path, \"run_phasenet_v2.py\")\n</pre> run(root_path, \"run_phasenet.py\") # run(root_path, \"run_phasenet_v2.py\") <pre>cd ./QuakeFlow/slurm &amp;&amp; python run_phasenet_v2.py local demo\n</pre> <pre>/usr/local/python/3.10.8/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n  return component_factory.create_component_from_func(\n</pre> <pre>num_gpu = 0\npython ../EQNet/predict.py --model phasenet --add_polarity --add_event --format mseed --data_list=local/demo/phasenet/mseed_list_000.csv --response_xml=local/demo/obspy/inventory.xml --result_path=local/demo/phasenet --batch_size 1 --workers 1 --folder_depth 3 --device=cpu\nNot using distributed mode\nNamespace(model='phasenet', resume='', backbone='unet', phases=['P', 'S'], device='cpu', workers=1, batch_size=1, use_deterministic_algorithms=False, amp=False, world_size=1, dist_url='env://', data_path='./', data_list='local/demo/phasenet/mseed_list_000.csv', hdf5_file=None, prefix='', format='mseed', dataset='das', result_path='local/demo/phasenet', plot_figure=False, min_prob=0.3, add_polarity=True, add_event=True, highpass_filter=0.0, response_xml='local/demo/obspy/inventory.xml', folder_depth=3, cut_patch=False, nt=20480, nx=5120, resample_time=False, resample_space=False, system=None, location=None, skip_existing=False, distributed=False)\nTotal samples:  ./.mseed : 315 files\n</pre> <pre>Downloading: \"https://github.com/AI4EPS/models/releases/download/PhaseNet-Polarity-v3/model_99.pth\" to ./model_99.pth\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18.1M/18.1M [00:00&lt;00:00, 72.8MB/s]\nPredicting:   0%|          | 0/315 [00:00&lt;?, ?it/s]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:   0%|          | 1/315 [00:05&lt;29:34,  5.65s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:   1%|\u258f         | 4/315 [00:09&lt;09:51,  1.90s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:   2%|\u258f         | 7/315 [00:13&lt;07:34,  1.47s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:   3%|\u258e         | 10/315 [00:17&lt;06:57,  1.37s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:   4%|\u258d         | 13/315 [00:21&lt;06:56,  1.38s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:   5%|\u258c         | 16/315 [00:25&lt;06:40,  1.34s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:   6%|\u258c         | 19/315 [00:29&lt;06:32,  1.33s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:   7%|\u258b         | 22/315 [00:33&lt;06:24,  1.31s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:   8%|\u258a         | 25/315 [00:37&lt;06:22,  1.32s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:   9%|\u2589         | 28/315 [00:41&lt;06:19,  1.32s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  10%|\u2589         | 31/315 [00:45&lt;06:08,  1.30s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  11%|\u2588\u258f        | 36/315 [00:51&lt;06:06,  1.31s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting:  14%|\u2588\u258e        | 43/315 [01:01&lt;05:55,  1.31s/it]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:  15%|\u2588\u258d        | 46/315 [01:04&lt;05:55,  1.32s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:  16%|\u2588\u258c        | 49/315 [01:08&lt;05:49,  1.32s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:  17%|\u2588\u258b        | 52/315 [01:12&lt;05:44,  1.31s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:  17%|\u2588\u258b        | 55/315 [01:16&lt;05:42,  1.32s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:  18%|\u2588\u258a        | 58/315 [01:20&lt;05:34,  1.30s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:  19%|\u2588\u2589        | 61/315 [01:24&lt;05:26,  1.28s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:  20%|\u2588\u2588        | 64/315 [01:28&lt;05:27,  1.31s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:  21%|\u2588\u2588\u258f       | 67/315 [01:32&lt;05:24,  1.31s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:  22%|\u2588\u2588\u258f       | 70/315 [01:36&lt;05:19,  1.31s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:  23%|\u2588\u2588\u258e       | 73/315 [01:40&lt;05:19,  1.32s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  24%|\u2588\u2588\u258d       | 76/315 [01:44&lt;05:12,  1.31s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  26%|\u2588\u2588\u258c       | 81/315 [01:50&lt;05:07,  1.31s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting:  28%|\u2588\u2588\u258a       | 88/315 [02:00&lt;05:01,  1.33s/it]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:  29%|\u2588\u2588\u2589       | 91/315 [02:04&lt;04:57,  1.33s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:  30%|\u2588\u2588\u2589       | 94/315 [02:07&lt;04:50,  1.31s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:  31%|\u2588\u2588\u2588       | 97/315 [02:11&lt;04:45,  1.31s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:  32%|\u2588\u2588\u2588\u258f      | 100/315 [02:15&lt;04:39,  1.30s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:  33%|\u2588\u2588\u2588\u258e      | 103/315 [02:19&lt;04:42,  1.33s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:  34%|\u2588\u2588\u2588\u258e      | 106/315 [02:23&lt;04:35,  1.32s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:  35%|\u2588\u2588\u2588\u258d      | 109/315 [02:27&lt;04:29,  1.31s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:  36%|\u2588\u2588\u2588\u258c      | 112/315 [02:31&lt;04:29,  1.33s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:  37%|\u2588\u2588\u2588\u258b      | 115/315 [02:35&lt;04:23,  1.32s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:  37%|\u2588\u2588\u2588\u258b      | 118/315 [02:39&lt;04:19,  1.32s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  38%|\u2588\u2588\u2588\u258a      | 121/315 [02:43&lt;04:14,  1.31s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  40%|\u2588\u2588\u2588\u2588      | 126/315 [02:50&lt;04:13,  1.34s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting:  42%|\u2588\u2588\u2588\u2588\u258f     | 133/315 [02:59&lt;03:56,  1.30s/it]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:  43%|\u2588\u2588\u2588\u2588\u258e     | 136/315 [03:03&lt;03:51,  1.29s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:  44%|\u2588\u2588\u2588\u2588\u258d     | 139/315 [03:07&lt;03:49,  1.30s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:  45%|\u2588\u2588\u2588\u2588\u258c     | 142/315 [03:11&lt;03:47,  1.32s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:  46%|\u2588\u2588\u2588\u2588\u258c     | 145/315 [03:15&lt;03:43,  1.31s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:  47%|\u2588\u2588\u2588\u2588\u258b     | 148/315 [03:19&lt;03:41,  1.32s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:  48%|\u2588\u2588\u2588\u2588\u258a     | 151/315 [03:22&lt;03:35,  1.31s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:  49%|\u2588\u2588\u2588\u2588\u2589     | 154/315 [03:26&lt;03:29,  1.30s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:  50%|\u2588\u2588\u2588\u2588\u2589     | 157/315 [03:30&lt;03:28,  1.32s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:  51%|\u2588\u2588\u2588\u2588\u2588     | 160/315 [03:34&lt;03:25,  1.33s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 163/315 [03:38&lt;03:18,  1.30s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 166/315 [03:42&lt;03:17,  1.32s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 171/315 [03:49&lt;03:10,  1.32s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 178/315 [03:58&lt;02:59,  1.31s/it]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 181/315 [04:02&lt;02:57,  1.32s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 184/315 [04:06&lt;02:52,  1.31s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 187/315 [04:10&lt;02:47,  1.31s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 190/315 [04:14&lt;02:44,  1.32s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 193/315 [04:18&lt;02:39,  1.31s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 196/315 [04:22&lt;02:37,  1.32s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 199/315 [04:26&lt;02:32,  1.31s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 202/315 [04:30&lt;02:27,  1.30s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 205/315 [04:34&lt;02:26,  1.33s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 208/315 [04:38&lt;02:21,  1.32s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 211/315 [04:42&lt;02:18,  1.33s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 216/315 [04:48&lt;02:09,  1.31s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 223/315 [04:57&lt;02:00,  1.31s/it]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 226/315 [05:01&lt;01:55,  1.30s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 229/315 [05:05&lt;01:53,  1.32s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 232/315 [05:09&lt;01:49,  1.31s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 235/315 [05:13&lt;01:43,  1.30s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 238/315 [05:17&lt;01:41,  1.31s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 241/315 [05:21&lt;01:37,  1.32s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 244/315 [05:25&lt;01:33,  1.32s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 247/315 [05:29&lt;01:28,  1.30s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 250/315 [05:33&lt;01:25,  1.31s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 253/315 [05:37&lt;01:23,  1.35s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 256/315 [05:41&lt;01:18,  1.33s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 261/315 [05:47&lt;01:11,  1.32s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 268/315 [05:56&lt;01:01,  1.31s/it]WARNING:root:Resampling CI.CCC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CCC..BHZ from 40.0 to 100 Hz\nPredicting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 271/315 [06:00&lt;00:57,  1.31s/it]WARNING:root:Resampling CI.CLC..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.CLC..BHZ from 40.0 to 100 Hz\nPredicting:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 274/315 [06:04&lt;00:55,  1.36s/it]WARNING:root:Resampling CI.DTP..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.DTP..BHZ from 40.0 to 100 Hz\nPredicting:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 277/315 [06:08&lt;00:50,  1.33s/it]WARNING:root:Resampling CI.JRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.JRC2..BHZ from 40.0 to 100 Hz\nPredicting:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 280/315 [06:12&lt;00:46,  1.32s/it]WARNING:root:Resampling CI.LRL..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.LRL..BHZ from 40.0 to 100 Hz\nPredicting:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 283/315 [06:16&lt;00:42,  1.33s/it]WARNING:root:Resampling CI.MPM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.MPM..BHZ from 40.0 to 100 Hz\nPredicting:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 286/315 [06:20&lt;00:37,  1.30s/it]WARNING:root:Resampling CI.SLA..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SLA..BHZ from 40.0 to 100 Hz\nPredicting:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 289/315 [06:24&lt;00:33,  1.30s/it]WARNING:root:Resampling CI.SRT..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.SRT..BHZ from 40.0 to 100 Hz\nPredicting:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 292/315 [06:28&lt;00:29,  1.30s/it]WARNING:root:Resampling CI.TOW2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.TOW2..BHZ from 40.0 to 100 Hz\nPredicting:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 295/315 [06:32&lt;00:26,  1.32s/it]WARNING:root:Resampling CI.WBM..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WBM..BHZ from 40.0 to 100 Hz\nPredicting:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 298/315 [06:36&lt;00:22,  1.31s/it]WARNING:root:Resampling CI.WCS2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WCS2..BHZ from 40.0 to 100 Hz\nPredicting:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 301/315 [06:40&lt;00:18,  1.32s/it]WARNING:root:Resampling CI.WMF..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WMF..BHZ from 40.0 to 100 Hz\nPredicting:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 306/315 [06:46&lt;00:11,  1.30s/it]WARNING:root:Resampling CI.WRC2..BHE from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHN from 40.0 to 100 Hz\nWARNING:root:Resampling CI.WRC2..BHZ from 40.0 to 100 Hz\nPredicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 315/315 [07:02&lt;00:00,  1.34s/it]\nMerging picks_phasenet: 315it [00:00, 573.57it/s]\n</pre> <pre>Number of P picks: 35300\nNumber of S picks: 36384\n</pre> <pre>Merging events_phasenet: 315it [00:00, 647.84it/s]\n</pre> In\u00a0[6]: Copied! <pre>run(root_path, \"run_gamma.py\")\n</pre> run(root_path, \"run_gamma.py\") <pre>cd ./QuakeFlow/slurm &amp;&amp; python run_gamma.py local demo\nnum_nodes: 1\nsampling_rate: 100\ndegree2km: 111.1949\nchannel: HH*,BH*,EH*,HN*\nlevel: response\ngamma: {'zmin_km': 0, 'zmax_km': 60}\ncctorch: {'sampling_rate': 100, 'time_before': 0.25, 'time_after': 1.0, 'min_pair_dist_km': 10, 'components': 'ENZ123', 'component_mapping': {'3': 0, '2': 1, '1': 2, 'E': 0, 'N': 1, 'Z': 2}}\nlongitude0: -117.504\nlatitude0: 35.705\nmaxradius_degree: 0.5\nminlatitude: 35.205\nmaxlatitude: 36.205\nminlongitude: -118.004\nmaxlongitude: -117.004\nstarttime: 2019-07-04T17:00:00\nendtime: 2019-07-05T00:00:00\nprovider: ['SCEDC']\nnetwork: CI\nuse_dbscan: True\nuse_amplitude: True\nmethod: BGMM\noversample_factor: 5\nvel: {'p': 6.0, 's': 3.4285714285714284}\ndims: ['x(km)', 'y(km)', 'z(km)']\nx(km): [-45.14694189  45.14694189]\ny(km): [-55.59745  55.59745]\nz(km): (0, 30)\nbfgs_bounds: ((-46.14694188923431, 46.14694188923431), (-56.59745, 56.59745), (0, 31), (None, None))\ndbscan_eps: 7.240160594465104\ndbscan_min_samples: 3\nmin_picks_per_eq: 5\nmin_p_picks_per_eq: 0\nmin_s_picks_per_eq: 0\nmax_sigma11: 2.0\nmax_sigma22: 1.0\nmax_sigma12: 1.0\nAssociating 506 clusters with 3 CPUs\n.....................................\nAssociated 100 events\n..........................\nInitialization 1 did not converge.\n\nAssociated 200 events\n......................\nAssociated 300 events\n......................\nAssociated 400 events\n.....................\nAssociated 500 events\n.....................................\nAssociated 600 events\n.......................\nAssociated 700 events\n...................................\nAssociated 800 events\n.......................\nAssociated 900 events\n..........................\nAssociated 1000 events\n..........................\nAssociated 1100 events\n..........................\nAssociated 1200 events\n.....................\nAssociated 1300 events\n................\nAssociated 1400 events\n..........\nInitialization 1 did not converge.\n.......\nAssociated 1500 events\n...........................\nAssociated 1600 events\n..\nInitialization 1 did not converge.\n...................\nAssociated 1700 events\n....................\nAssociated 1800 events\n............................\nAssociated 1900 events\n...........................\nAssociated 2000 events\n.....</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport obspy\n</pre> import pandas as pd import json import matplotlib.pyplot as plt import numpy as np import obspy <p>Read Catalog</p> In\u00a0[8]: Copied! <pre>os.chdir(code_path)\n\nwith open(f\"{root_path}/{region}/config.json\") as f:\n    config = json.load(f)\nplot_standard_catalog = True\n\ngamma_events = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_events.csv\", parse_dates=[\"time\"])\ngamma_events.sort_values(\"gamma_score\", ascending=False, inplace=True)\ngamma_picks = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_picks.csv\", parse_dates=[\"phase_time\"])\ngamma_picks[\"phase_time\"] = gamma_picks[\"phase_time\"].dt.tz_localize(\"UTC\")\nstations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\")\nstations[\"location\"] = stations[\"location\"].fillna(\"\")\nif plot_standard_catalog:\n    standard_catalog = pd.read_csv(f\"{root_path}/{region}/obspy/catalog.csv\", parse_dates=[\"time\"])\n</pre> os.chdir(code_path)  with open(f\"{root_path}/{region}/config.json\") as f:     config = json.load(f) plot_standard_catalog = True  gamma_events = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_events.csv\", parse_dates=[\"time\"]) gamma_events.sort_values(\"gamma_score\", ascending=False, inplace=True) gamma_picks = pd.read_csv(f\"{root_path}/{region}/gamma/gamma_picks.csv\", parse_dates=[\"phase_time\"]) gamma_picks[\"phase_time\"] = gamma_picks[\"phase_time\"].dt.tz_localize(\"UTC\") stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\") stations[\"location\"] = stations[\"location\"].fillna(\"\") if plot_standard_catalog:     standard_catalog = pd.read_csv(f\"{root_path}/{region}/obspy/catalog.csv\", parse_dates=[\"time\"]) <p>Select event and picks for visulization</p> In\u00a0[9]: Copied! <pre>event = gamma_events.iloc[0]\npicks = gamma_picks[gamma_picks.event_index == event.event_index]\nstations[\"id\"] = stations.network + \".\" + stations.station + \".\" + stations.location + \".\" + stations.instrument\nstations[\"dist_km\"] = np.sqrt((stations.latitude - event.latitude)**2 + (stations.longitude - event.longitude)**2 * np.cos(event.latitude)**2) * 111.19\n</pre> event = gamma_events.iloc[0] picks = gamma_picks[gamma_picks.event_index == event.event_index] stations[\"id\"] = stations.network + \".\" + stations.station + \".\" + stations.location + \".\" + stations.instrument stations[\"dist_km\"] = np.sqrt((stations.latitude - event.latitude)**2 + (stations.longitude - event.longitude)**2 * np.cos(event.latitude)**2) * 111.19  <p>Load waveforms</p> In\u00a0[10]: Copied! <pre>begin_time = event.time - pd.Timedelta(seconds=0)\nend_time = event.time + pd.Timedelta(seconds=30)\nwaveforms = obspy.Stream()\ntry:\n    waveforms += obspy.read(f\"{root_path}/{region}/waveforms/{begin_time.year}-{begin_time.dayofyear:03d}/{begin_time.hour:02d}/*.mseed\")\nexcept Exception as e:\n    print(e)\nif end_time.hour != begin_time.hour:\n    try:\n        waveforms += obspy.read(f\"{root_path}/{region}/waveforms/{end_time.year}-{end_time.dayofyear:03d}/{end_time.hour:02d}/*.mseed\")\n    except Exception as e:\n        print(e)\nstream = waveforms.copy()\nstream.trim(obspy.UTCDateTime(begin_time), obspy.UTCDateTime(end_time), pad=True, fill_value=0);\n</pre> begin_time = event.time - pd.Timedelta(seconds=0) end_time = event.time + pd.Timedelta(seconds=30) waveforms = obspy.Stream() try:     waveforms += obspy.read(f\"{root_path}/{region}/waveforms/{begin_time.year}-{begin_time.dayofyear:03d}/{begin_time.hour:02d}/*.mseed\") except Exception as e:     print(e) if end_time.hour != begin_time.hour:     try:         waveforms += obspy.read(f\"{root_path}/{region}/waveforms/{end_time.year}-{end_time.dayofyear:03d}/{end_time.hour:02d}/*.mseed\")     except Exception as e:         print(e) stream = waveforms.copy() stream.trim(obspy.UTCDateTime(begin_time), obspy.UTCDateTime(end_time), pad=True, fill_value=0); <p>Plotting picks</p> In\u00a0[11]: Copied! <pre>component = \"Z\"\nstations = stations[stations[\"component\"] == component]\nstations.sort_values(\"dist_km\", inplace=True)\nstations.reset_index(inplace=True, drop=True)\nnormalize = lambda x: (x-np.mean(x))/(np.std(x) + np.finfo(float).eps)/4\nfig, ax = plt.subplots(figsize=(10, 8))\n\nfor i, station in stations.iterrows():\n    dist_km = station.dist_km\n    tr = stream.select(id=station.id+component)\n    if len(tr) == 1:\n        tr = tr[0]\n        # ax.plot(tr.times(\"matplotlib\") , normalize(tr.data) + dist_km, \"k\", linewidth=0.5)\n        ax.plot(tr.times(\"matplotlib\") , normalize(tr.data) + i, \"k\", linewidth=0.5)\n\ncolor = {\"P\": \"r\", \"S\": \"b\"}\nfor _, pick in picks.iterrows():\n    y = stations[stations.id == pick.station_id].index.values[0]\n    ax.plot([pick.phase_time, pick.phase_time], [y-1, y+1], color[pick.phase_type], linewidth=2)\nax.plot([], [], \"r\", label=\"P\")\nax.plot([], [], \"b\", label=\"S\")\nax.legend(loc=\"lower right\")\nax.xaxis_date()\nax.set_xlim(begin_time, end_time)\nax.set_ylim(-1, len(stations))\nplt.show()\n</pre> component = \"Z\" stations = stations[stations[\"component\"] == component] stations.sort_values(\"dist_km\", inplace=True) stations.reset_index(inplace=True, drop=True) normalize = lambda x: (x-np.mean(x))/(np.std(x) + np.finfo(float).eps)/4 fig, ax = plt.subplots(figsize=(10, 8))  for i, station in stations.iterrows():     dist_km = station.dist_km     tr = stream.select(id=station.id+component)     if len(tr) == 1:         tr = tr[0]         # ax.plot(tr.times(\"matplotlib\") , normalize(tr.data) + dist_km, \"k\", linewidth=0.5)         ax.plot(tr.times(\"matplotlib\") , normalize(tr.data) + i, \"k\", linewidth=0.5)  color = {\"P\": \"r\", \"S\": \"b\"} for _, pick in picks.iterrows():     y = stations[stations.id == pick.station_id].index.values[0]     ax.plot([pick.phase_time, pick.phase_time], [y-1, y+1], color[pick.phase_type], linewidth=2) ax.plot([], [], \"r\", label=\"P\") ax.plot([], [], \"b\", label=\"S\") ax.legend(loc=\"lower right\") ax.xaxis_date() ax.set_xlim(begin_time, end_time) ax.set_ylim(-1, len(stations)) plt.show() <p>Plotting catalog</p> In\u00a0[12]: Copied! <pre>gamma_label = \"GaMMA\"\nstandard_label = \"Standard\"\n\nbins = pd.date_range(start=config[\"starttime\"], end=config[\"endtime\"], periods=30)\nplt.figure(figsize=plt.rcParams[\"figure.figsize\"] * np.array([1.5, 1.0]))\nplt.hist(\n    gamma_events[\"time\"],\n    bins=bins,\n    edgecolor=\"k\",\n    alpha=1.0,\n    linewidth=0.5,\n    label=f\"{gamma_label}: {len(gamma_events['time'])}\",\n)\nif plot_standard_catalog:\n    plt.hist(\n        standard_catalog[\"time\"],\n        bins=bins,\n        edgecolor=\"k\",\n        alpha=0.6,\n        linewidth=0.5,\n        label=f\"{standard_label}: {len(standard_catalog['time'])}\",\n    )\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Date\")\nplt.gca().autoscale(enable=True, axis=\"x\", tight=True)\n# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\nplt.gcf().autofmt_xdate()\nplt.legend()\nplt.grid(linestyle=\"--\", linewidth=0.5, alpha=0.5)\nplt.show()\n\ngamma_markersize = min(5, 1e5 / len(gamma_events))\nstandard_markersize = min(5, 1e4 / len(standard_catalog))\nplt.figure(figsize=plt.rcParams[\"figure.figsize\"] * np.array([1.5, 1.0]))\n\nplt.scatter(\n    gamma_events[\"time\"], gamma_events[\"magnitude\"], s=gamma_markersize, alpha=1.0, linewidth=0, rasterized=True\n)\nif plot_standard_catalog:\n    plt.scatter(\n        standard_catalog[\"time\"],\n        standard_catalog[\"magnitude\"],\n        s=standard_markersize,\n        alpha=1.0,\n        linewidth=0,\n        rasterized=True,\n    )\n\nplt.ylabel(\"Magnitude\")\nplt.xlabel(\"Date\")\nplt.gcf().autofmt_xdate()\n\nplt.gca().set_prop_cycle(None)\nxlim = plt.xlim()\nylim = plt.ylim()\nplt.plot(xlim[0] - 10, ylim[0] - 10, \".\", markersize=15, alpha=1.0, label=f\"{gamma_label}: {len(gamma_events)}\")\nplt.plot(xlim[0] - 10, ylim[0] - 10, \".\", markersize=15, alpha=1.0, label=f\"{standard_label}: {len(standard_catalog)}\")\nplt.legend(loc=\"lower right\")\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.grid(linestyle=\"--\", linewidth=0.5, alpha=0.5)\nplt.show()\n</pre> gamma_label = \"GaMMA\" standard_label = \"Standard\"  bins = pd.date_range(start=config[\"starttime\"], end=config[\"endtime\"], periods=30) plt.figure(figsize=plt.rcParams[\"figure.figsize\"] * np.array([1.5, 1.0])) plt.hist(     gamma_events[\"time\"],     bins=bins,     edgecolor=\"k\",     alpha=1.0,     linewidth=0.5,     label=f\"{gamma_label}: {len(gamma_events['time'])}\", ) if plot_standard_catalog:     plt.hist(         standard_catalog[\"time\"],         bins=bins,         edgecolor=\"k\",         alpha=0.6,         linewidth=0.5,         label=f\"{standard_label}: {len(standard_catalog['time'])}\",     ) plt.ylabel(\"Frequency\") plt.xlabel(\"Date\") plt.gca().autoscale(enable=True, axis=\"x\", tight=True) # plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H')) plt.gcf().autofmt_xdate() plt.legend() plt.grid(linestyle=\"--\", linewidth=0.5, alpha=0.5) plt.show()  gamma_markersize = min(5, 1e5 / len(gamma_events)) standard_markersize = min(5, 1e4 / len(standard_catalog)) plt.figure(figsize=plt.rcParams[\"figure.figsize\"] * np.array([1.5, 1.0]))  plt.scatter(     gamma_events[\"time\"], gamma_events[\"magnitude\"], s=gamma_markersize, alpha=1.0, linewidth=0, rasterized=True ) if plot_standard_catalog:     plt.scatter(         standard_catalog[\"time\"],         standard_catalog[\"magnitude\"],         s=standard_markersize,         alpha=1.0,         linewidth=0,         rasterized=True,     )  plt.ylabel(\"Magnitude\") plt.xlabel(\"Date\") plt.gcf().autofmt_xdate()  plt.gca().set_prop_cycle(None) xlim = plt.xlim() ylim = plt.ylim() plt.plot(xlim[0] - 10, ylim[0] - 10, \".\", markersize=15, alpha=1.0, label=f\"{gamma_label}: {len(gamma_events)}\") plt.plot(xlim[0] - 10, ylim[0] - 10, \".\", markersize=15, alpha=1.0, label=f\"{standard_label}: {len(standard_catalog)}\") plt.legend(loc=\"lower right\") plt.xlim(xlim) plt.ylim(ylim) plt.grid(linestyle=\"--\", linewidth=0.5, alpha=0.5) plt.show() In\u00a0[13]: Copied! <pre>import cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport cartopy.io.img_tiles as cimgt\n\n# gamma_events = gamma_events[gamma_events[\"gamma_score\"] &gt; 30]\n\nfig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"] * np.array([1.5, 1]))\nbox = dict(boxstyle=\"round\", facecolor=\"white\", alpha=1)\ntext_loc = [0.05, 0.92]\n\ngamma_markersize = min(5, 1e5 / len(gamma_events[\"latitude\"]))\nstandard_markersize = min(5, 1e4 / len(standard_catalog[\"latitude\"]))\nalpha = 0.3\ngrd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1, 1])\nax1 = fig.add_subplot(grd[:, 0], projection=ccrs.PlateCarree())\nax1.set_extent(\n    [config[\"minlongitude\"], config[\"maxlongitude\"], config[\"minlatitude\"], config[\"maxlatitude\"]],\n    crs=ccrs.PlateCarree(),\n)\n\n\nax1.coastlines(resolution=\"10m\", color=\"gray\", linewidth=0.5)\ngl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=0.5, color=\"gray\", alpha=0.5, linestyle=\"--\")\ngl.top_labels = False\ngl.right_labels = False\nax1.scatter(gamma_events[\"longitude\"], gamma_events[\"latitude\"], s=gamma_markersize, linewidth=0, alpha=alpha)\nif plot_standard_catalog:\n    ax1.scatter(\n        standard_catalog[\"longitude\"], standard_catalog[\"latitude\"], s=standard_markersize, linewidth=0, alpha=alpha\n    )\n# ax1.axis(\"scaled\")\n# ax1.set_xlim([config[\"minlongitude\"], config[\"maxlongitude\"]])\n# ax1.set_ylim([config[\"minlatitude\"], config[\"maxlatitude\"]])\n# ax1.set_xlabel(\"Latitude\")\n# ax1.set_ylabel(\"Longitude\")\nax1.set_prop_cycle(None)\nax1.plot(\n    config[\"minlongitude\"] - 10, config[\"minlatitude\"] - 10, \".\", markersize=10, label=f\"{gamma_label}\", rasterized=True\n)\nax1.plot(\n    config[\"minlongitude\"] - 10,\n    config[\"minlatitude\"] - 10,\n    \".\",\n    markersize=10,\n    label=f\"{standard_label}\",\n    rasterized=True,\n)\nax1.plot(stations[\"longitude\"], stations[\"latitude\"], \"k^\", markersize=5, alpha=0.7, label=\"Stations\")\nax1.text(\n    text_loc[0],\n    text_loc[1],\n    \"(i)\",\n    horizontalalignment=\"left\",\n    verticalalignment=\"top\",\n    transform=plt.gca().transAxes,\n    fontsize=\"large\",\n    fontweight=\"normal\",\n    bbox=box,\n)\nplt.legend(loc=\"lower right\")\n\nax2 = fig.add_subplot(grd[0, 1])\nax2.scatter(\n    gamma_events[\"longitude\"],\n    gamma_events[\"depth_km\"],\n    s=gamma_markersize,\n    linewidth=0,\n    alpha=alpha,\n    rasterized=True,\n)\nif plot_standard_catalog:\n    ax2.scatter(\n        standard_catalog[\"longitude\"],\n        standard_catalog[\"depth_km\"],\n        s=standard_markersize,\n        linewidth=0,\n        alpha=alpha,\n        rasterized=True,\n    )\nax2.set_xlim([config[\"minlongitude\"], config[\"maxlongitude\"]])\nax2.set_ylim([0, 21])\nax2.invert_yaxis()\nax2.set_xlabel(\"Longitude\")\nax2.set_ylabel(\"Depth (km)\")\nax2.set_prop_cycle(None)\nax2.plot(config[\"minlongitude\"] - 10, 31, \".\", markersize=10, label=f\"{gamma_label}\")\nax2.plot(31, 31, \".\", markersize=10, label=f\"{standard_label}\")\nax2.text(\n    text_loc[0],\n    text_loc[1],\n    \"(ii)\",\n    horizontalalignment=\"left\",\n    verticalalignment=\"top\",\n    transform=plt.gca().transAxes,\n    fontsize=\"large\",\n    fontweight=\"normal\",\n    bbox=box,\n)\nplt.legend(loc=\"lower right\")\n\n\nfig.add_subplot(grd[1, 1])\nplt.scatter(\n    gamma_events[\"latitude\"],\n    gamma_events[\"depth_km\"],\n    s=gamma_markersize,\n    linewidth=0,\n    alpha=alpha,\n    rasterized=True,\n)\nif plot_standard_catalog:\n    plt.scatter(\n        standard_catalog[\"latitude\"],\n        standard_catalog[\"depth_km\"],\n        s=standard_markersize,\n        linewidth=0,\n        alpha=alpha,\n        rasterized=True,\n    )\nplt.xlim([config[\"minlatitude\"], config[\"maxlatitude\"]])\nplt.ylim([0, 21])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Depth (km)\")\nplt.gca().set_prop_cycle(None)\nplt.plot(config[\"minlatitude\"] - 10, 31, \".\", markersize=10, label=f\"{gamma_label}\")\nplt.plot(31, 31, \".\", markersize=10, label=f\"{standard_label}\")\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.text(\n    text_loc[0],\n    text_loc[1],\n    \"(iii)\",\n    horizontalalignment=\"left\",\n    verticalalignment=\"top\",\n    transform=plt.gca().transAxes,\n    fontsize=\"large\",\n    fontweight=\"normal\",\n    bbox=box,\n)\nplt.show()\n</pre> import cartopy.crs as ccrs import cartopy.feature as cfeature import cartopy.io.img_tiles as cimgt  # gamma_events = gamma_events[gamma_events[\"gamma_score\"] &gt; 30]  fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"] * np.array([1.5, 1])) box = dict(boxstyle=\"round\", facecolor=\"white\", alpha=1) text_loc = [0.05, 0.92]  gamma_markersize = min(5, 1e5 / len(gamma_events[\"latitude\"])) standard_markersize = min(5, 1e4 / len(standard_catalog[\"latitude\"])) alpha = 0.3 grd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1, 1]) ax1 = fig.add_subplot(grd[:, 0], projection=ccrs.PlateCarree()) ax1.set_extent(     [config[\"minlongitude\"], config[\"maxlongitude\"], config[\"minlatitude\"], config[\"maxlatitude\"]],     crs=ccrs.PlateCarree(), )   ax1.coastlines(resolution=\"10m\", color=\"gray\", linewidth=0.5) gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=0.5, color=\"gray\", alpha=0.5, linestyle=\"--\") gl.top_labels = False gl.right_labels = False ax1.scatter(gamma_events[\"longitude\"], gamma_events[\"latitude\"], s=gamma_markersize, linewidth=0, alpha=alpha) if plot_standard_catalog:     ax1.scatter(         standard_catalog[\"longitude\"], standard_catalog[\"latitude\"], s=standard_markersize, linewidth=0, alpha=alpha     ) # ax1.axis(\"scaled\") # ax1.set_xlim([config[\"minlongitude\"], config[\"maxlongitude\"]]) # ax1.set_ylim([config[\"minlatitude\"], config[\"maxlatitude\"]]) # ax1.set_xlabel(\"Latitude\") # ax1.set_ylabel(\"Longitude\") ax1.set_prop_cycle(None) ax1.plot(     config[\"minlongitude\"] - 10, config[\"minlatitude\"] - 10, \".\", markersize=10, label=f\"{gamma_label}\", rasterized=True ) ax1.plot(     config[\"minlongitude\"] - 10,     config[\"minlatitude\"] - 10,     \".\",     markersize=10,     label=f\"{standard_label}\",     rasterized=True, ) ax1.plot(stations[\"longitude\"], stations[\"latitude\"], \"k^\", markersize=5, alpha=0.7, label=\"Stations\") ax1.text(     text_loc[0],     text_loc[1],     \"(i)\",     horizontalalignment=\"left\",     verticalalignment=\"top\",     transform=plt.gca().transAxes,     fontsize=\"large\",     fontweight=\"normal\",     bbox=box, ) plt.legend(loc=\"lower right\")  ax2 = fig.add_subplot(grd[0, 1]) ax2.scatter(     gamma_events[\"longitude\"],     gamma_events[\"depth_km\"],     s=gamma_markersize,     linewidth=0,     alpha=alpha,     rasterized=True, ) if plot_standard_catalog:     ax2.scatter(         standard_catalog[\"longitude\"],         standard_catalog[\"depth_km\"],         s=standard_markersize,         linewidth=0,         alpha=alpha,         rasterized=True,     ) ax2.set_xlim([config[\"minlongitude\"], config[\"maxlongitude\"]]) ax2.set_ylim([0, 21]) ax2.invert_yaxis() ax2.set_xlabel(\"Longitude\") ax2.set_ylabel(\"Depth (km)\") ax2.set_prop_cycle(None) ax2.plot(config[\"minlongitude\"] - 10, 31, \".\", markersize=10, label=f\"{gamma_label}\") ax2.plot(31, 31, \".\", markersize=10, label=f\"{standard_label}\") ax2.text(     text_loc[0],     text_loc[1],     \"(ii)\",     horizontalalignment=\"left\",     verticalalignment=\"top\",     transform=plt.gca().transAxes,     fontsize=\"large\",     fontweight=\"normal\",     bbox=box, ) plt.legend(loc=\"lower right\")   fig.add_subplot(grd[1, 1]) plt.scatter(     gamma_events[\"latitude\"],     gamma_events[\"depth_km\"],     s=gamma_markersize,     linewidth=0,     alpha=alpha,     rasterized=True, ) if plot_standard_catalog:     plt.scatter(         standard_catalog[\"latitude\"],         standard_catalog[\"depth_km\"],         s=standard_markersize,         linewidth=0,         alpha=alpha,         rasterized=True,     ) plt.xlim([config[\"minlatitude\"], config[\"maxlatitude\"]]) plt.ylim([0, 21]) plt.gca().invert_yaxis() plt.xlabel(\"Latitude\") plt.ylabel(\"Depth (km)\") plt.gca().set_prop_cycle(None) plt.plot(config[\"minlatitude\"] - 10, 31, \".\", markersize=10, label=f\"{gamma_label}\") plt.plot(31, 31, \".\", markersize=10, label=f\"{standard_label}\") plt.legend(loc=\"lower right\") plt.tight_layout() plt.text(     text_loc[0],     text_loc[1],     \"(iii)\",     horizontalalignment=\"left\",     verticalalignment=\"top\",     transform=plt.gca().transAxes,     fontsize=\"large\",     fontweight=\"normal\",     bbox=box, ) plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#quakeflow-demo","title":"QuakeFlow demo\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#set-configuration-parameters","title":"Set configuration parameters\u00b6","text":"<p>Edit the config.json file to add parameters of studying region</p> <p>You can search for earthquake information here: USGS</p> <p>You can search for seismic network information here: IRIS</p>"},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#download-stations-and-events","title":"Download stations and events\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#download-continuous-waveform-data","title":"Download continuous waveform data\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#run-phase-picking-phasenet","title":"Run Phase Picking: PhaseNet\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#run-phase-association-gamma","title":"Run Phase Association: GaMMA\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/quakeflow_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/","title":"Signal Processing","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, ifft, fftshift\nfrom scipy.signal import butter, lfilter, filtfilt, freqz\n</pre> import numpy as np import matplotlib.pyplot as plt from scipy.fft import fft, ifft, fftshift from scipy.signal import butter, lfilter, filtfilt, freqz In\u00a0[2]: Copied! <pre>fs = 100  # Sampling frequency\nt = np.linspace(0, 1, fs, endpoint=False)  # Time vector\nf = 5  # Frequency of the sine wave\nx = np.sin(2 * np.pi * f * t)  # Sinusoidal signal\n\nplt.plot(t, x)\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.title('Sine Wave')\nplt.grid(True)\nplt.show()\n</pre> fs = 100  # Sampling frequency t = np.linspace(0, 1, fs, endpoint=False)  # Time vector f = 5  # Frequency of the sine wave x = np.sin(2 * np.pi * f * t)  # Sinusoidal signal  plt.plot(t, x) plt.xlabel('Time [s]') plt.ylabel('Amplitude') plt.title('Sine Wave') plt.grid(True) plt.show()  In\u00a0[3]: Copied! <pre>f1, f2 = 5, 30\nx1 = np.sin(2 * np.pi * f1 * t)\nx2 = 0.5 * np.sin(2 * np.pi * f2 * t)\nx_comp = x1 + x2\n\nX = fft(x_comp)\nfreqs = np.fft.fftfreq(len(X), 1/fs)\n\nplt.plot(t, x_comp)\nplt.plot(t, x1, '--', linewidth=1)\nplt.plot(t, x2, '-.', linewidth=1)\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.title('5Hz + 30Hz Sine Wave')\nplt.grid(True)\nplt.show()\n\nplt.stem(freqs, np.abs(X), use_line_collection=True)\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('Magnitude')\nplt.title('Frequency Spectrum')\nplt.grid(True)\nplt.show()\n</pre> f1, f2 = 5, 30 x1 = np.sin(2 * np.pi * f1 * t) x2 = 0.5 * np.sin(2 * np.pi * f2 * t) x_comp = x1 + x2  X = fft(x_comp) freqs = np.fft.fftfreq(len(X), 1/fs)  plt.plot(t, x_comp) plt.plot(t, x1, '--', linewidth=1) plt.plot(t, x2, '-.', linewidth=1) plt.xlabel('Time [s]') plt.ylabel('Amplitude') plt.title('5Hz + 30Hz Sine Wave') plt.grid(True) plt.show()  plt.stem(freqs, np.abs(X), use_line_collection=True) plt.xlabel('Frequency [Hz]') plt.ylabel('Magnitude') plt.title('Frequency Spectrum') plt.grid(True) plt.show() <pre>/tmp/ipykernel_3866/1887783249.py:18: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n  plt.stem(freqs, np.abs(X), use_line_collection=True)\n</pre> <ul> <li>TODO: Implement Fourier Transform of a box function</li> </ul> In\u00a0[4]: Copied! <pre>def butter_highpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n    return b, a\n\ndef butter_highpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_highpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\ncutoff_high = 20  # Desired cutoff frequency for high-pass filter\nx_highpass = butter_highpass_filter(x_comp, cutoff_high, fs, order=3)\n\nplt.plot(t, x_comp, label='Original Signal')\nplt.plot(t, x_highpass, label='High-pass Filtered Signal', linestyle='dashed')\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.title('High-pass Filter')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> def butter_highpass(cutoff, fs, order=5):     nyq = 0.5 * fs     normal_cutoff = cutoff / nyq     b, a = butter(order, normal_cutoff, btype='high', analog=False)     return b, a  def butter_highpass_filter(data, cutoff, fs, order=5):     b, a = butter_highpass(cutoff, fs, order=order)     y = lfilter(b, a, data)     return y  cutoff_high = 20  # Desired cutoff frequency for high-pass filter x_highpass = butter_highpass_filter(x_comp, cutoff_high, fs, order=3)  plt.plot(t, x_comp, label='Original Signal') plt.plot(t, x_highpass, label='High-pass Filtered Signal', linestyle='dashed') plt.xlabel('Time [s]') plt.ylabel('Amplitude') plt.title('High-pass Filter') plt.legend() plt.grid(True) plt.show() <ul> <li>TODO: Implement a low-pass filter</li> </ul> In\u00a0[5]: Copied! <pre>def butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef butter_lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\n# cutoff = 10  # Desired cutoff frequency\n</pre> def butter_lowpass(cutoff, fs, order=5):     nyq = 0.5 * fs     normal_cutoff = cutoff / nyq     b, a = butter(order, normal_cutoff, btype='low', analog=False)     return b, a  def butter_lowpass_filter(data, cutoff, fs, order=5):     b, a = butter_lowpass(cutoff, fs, order=order)     y = lfilter(b, a, data)     return y  # cutoff = 10  # Desired cutoff frequency <p>What problem do you notice with the low-pass filter? How can you fix it?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[6]: Copied! <pre>f = 5\nx = np.sin(2 * np.pi * f * t)  \n\nphase_shift_rad = - np.pi/4  # 45 degree phase shift\nx_shifted = np.sin(2 * np.pi * f * t + phase_shift_rad)\n\nplt.plot(t, x, label='Original Sine Wave')\nplt.plot(t, x_shifted, label='Phase Shifted Sine Wave', linestyle='dashed')\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.title('Sine Wave with Phase Shift')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> f = 5 x = np.sin(2 * np.pi * f * t)    phase_shift_rad = - np.pi/4  # 45 degree phase shift x_shifted = np.sin(2 * np.pi * f * t + phase_shift_rad)  plt.plot(t, x, label='Original Sine Wave') plt.plot(t, x_shifted, label='Phase Shifted Sine Wave', linestyle='dashed') plt.xlabel('Time [s]') plt.ylabel('Amplitude') plt.title('Sine Wave with Phase Shift') plt.legend() plt.grid(True) plt.show() In\u00a0[8]: Copied! <pre># Compute the frequency response of the filter\ncutoff = 10\nb, a = butter_lowpass(cutoff, fs, order=3)\nw, h = freqz(b, a, worN=8000)\n\n\n# Plot the phase response of the filter\nplt.figure()\nplt.plot(0.5*fs*w/np.pi, np.unwrap(np.angle(h)), 'r')\nplt.title('Phase Response of the Filter')\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('Phase [radians]')\nplt.grid()\nplt.show()\n</pre> # Compute the frequency response of the filter cutoff = 10 b, a = butter_lowpass(cutoff, fs, order=3) w, h = freqz(b, a, worN=8000)   # Plot the phase response of the filter plt.figure() plt.plot(0.5*fs*w/np.pi, np.unwrap(np.angle(h)), 'r') plt.title('Phase Response of the Filter') plt.xlabel('Frequency [Hz]') plt.ylabel('Phase [radians]') plt.grid() plt.show() In\u00a0[9]: Copied! <pre>def butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef zero_phase_lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = filtfilt(b, a, data)  # Apply the filter forwards and then backwards\n    return y\n\ncutoff = 10  # Desired cutoff frequency\nx_filtered_zero_phase = zero_phase_lowpass_filter(x_comp, cutoff, fs, order=3)\n\nplt.plot(t, x_comp, label='Original Signal')\nplt.plot(t, x_filtered_zero_phase, label='Zero-phase Low-pass Filtered Signal', linestyle='dashed')\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.title('Zero-phase Low-pass Filter')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> def butter_lowpass(cutoff, fs, order=5):     nyq = 0.5 * fs     normal_cutoff = cutoff / nyq     b, a = butter(order, normal_cutoff, btype='low', analog=False)     return b, a  def zero_phase_lowpass_filter(data, cutoff, fs, order=5):     b, a = butter_lowpass(cutoff, fs, order=order)     y = filtfilt(b, a, data)  # Apply the filter forwards and then backwards     return y  cutoff = 10  # Desired cutoff frequency x_filtered_zero_phase = zero_phase_lowpass_filter(x_comp, cutoff, fs, order=3)  plt.plot(t, x_comp, label='Original Signal') plt.plot(t, x_filtered_zero_phase, label='Zero-phase Low-pass Filtered Signal', linestyle='dashed') plt.xlabel('Time [s]') plt.ylabel('Amplitude') plt.title('Zero-phase Low-pass Filter') plt.legend() plt.grid(True) plt.show() In\u00a0[10]: Copied! <pre># Generate signals\nt = np.linspace(-10, 10, 500)\nrectangular_pulse = np.where((t &gt;= -5) &amp; (t &lt;= 5), 1, 0)\ntriangular_pulse = np.where(t &lt; -3, 0, 0.5 - t/6)\ntriangular_pulse = np.where(np.abs(t) &lt;= 3, triangular_pulse, 0)\n\n# Convolve in time domain\nconvolved_time = np.convolve(rectangular_pulse, triangular_pulse, 'same')\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(t, rectangular_pulse, label='Rectangular Pulse')\nplt.plot(t, triangular_pulse, label='Triangular Pulse')\nplt.legend()\nplt.title('Original Signals')\n\nplt.subplot(1, 2, 2)\nplt.plot(t, convolved_time, label='Convolution')\nplt.legend()\nplt.title('Convolution in Time Domain')\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate signals t = np.linspace(-10, 10, 500) rectangular_pulse = np.where((t &gt;= -5) &amp; (t &lt;= 5), 1, 0) triangular_pulse = np.where(t &lt; -3, 0, 0.5 - t/6) triangular_pulse = np.where(np.abs(t) &lt;= 3, triangular_pulse, 0)  # Convolve in time domain convolved_time = np.convolve(rectangular_pulse, triangular_pulse, 'same')  plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.plot(t, rectangular_pulse, label='Rectangular Pulse') plt.plot(t, triangular_pulse, label='Triangular Pulse') plt.legend() plt.title('Original Signals')  plt.subplot(1, 2, 2) plt.plot(t, convolved_time, label='Convolution') plt.legend() plt.title('Convolution in Time Domain')  plt.tight_layout() plt.show()  <ul> <li>TODO: Add convolution in frequency domain</li> </ul> In\u00a0[11]: Copied! <pre># Compute FFTs of the signals\nF_rect = fft(rectangular_pulse)\nF_tri = fft(triangular_pulse)\n\n# Convolution in Frequency domain\nmultiplied_spectrum = None\n\nif multiplied_spectrum is not None:\n    # Convert back to time domain using IFFT\n    convolved_freq = fftshift(ifft(multiplied_spectrum).real)\n\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(t, rectangular_pulse, label='Rectangular Pulse')\n    plt.plot(t, triangular_pulse, label='Triangular Pulse')\n    plt.legend()\n    plt.title('Original Signals')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(t, convolved_freq, label='Convolution (Frequency Domain)')\n    plt.legend()\n    plt.title('Convolution Result (IFFT)')\n\n    plt.tight_layout()\n    plt.show()\n</pre> # Compute FFTs of the signals F_rect = fft(rectangular_pulse) F_tri = fft(triangular_pulse)  # Convolution in Frequency domain multiplied_spectrum = None  if multiplied_spectrum is not None:     # Convert back to time domain using IFFT     convolved_freq = fftshift(ifft(multiplied_spectrum).real)      plt.figure(figsize=(10, 4))     plt.subplot(1, 2, 1)     plt.plot(t, rectangular_pulse, label='Rectangular Pulse')     plt.plot(t, triangular_pulse, label='Triangular Pulse')     plt.legend()     plt.title('Original Signals')      plt.subplot(1, 2, 2)     plt.plot(t, convolved_freq, label='Convolution (Frequency Domain)')     plt.legend()     plt.title('Convolution Result (IFFT)')      plt.tight_layout()     plt.show()  In\u00a0[12]: Copied! <pre># Generate signals\nt = np.linspace(-10, 10, 500)\nrectangular_pulse = np.where((t &gt;= -5) &amp; (t &lt;= 5), 1, 0)\ntriangular_pulse = np.where(t &lt; -3, 0, 0.5 - t/6)\ntriangular_pulse = np.where(np.abs(t) &lt;= 3, triangular_pulse, 0)\n\n# Compute cross-correlation\ncross_correlation = np.correlate(rectangular_pulse, triangular_pulse, 'same')\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(t, rectangular_pulse, label='Rectangular Pulse')\nplt.plot(t, triangular_pulse, label='Triangular Pulse')\nplt.legend()\nplt.title('Original Signals')\n\nplt.subplot(1, 2, 2)\nplt.plot(t, cross_correlation, label='Cross-correlated Signal')\nplt.legend()\nplt.title('Convolution in Time Domain')\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate signals t = np.linspace(-10, 10, 500) rectangular_pulse = np.where((t &gt;= -5) &amp; (t &lt;= 5), 1, 0) triangular_pulse = np.where(t &lt; -3, 0, 0.5 - t/6) triangular_pulse = np.where(np.abs(t) &lt;= 3, triangular_pulse, 0)  # Compute cross-correlation cross_correlation = np.correlate(rectangular_pulse, triangular_pulse, 'same')  plt.figure(figsize=(10, 4))  plt.subplot(1, 2, 1) plt.plot(t, rectangular_pulse, label='Rectangular Pulse') plt.plot(t, triangular_pulse, label='Triangular Pulse') plt.legend() plt.title('Original Signals')  plt.subplot(1, 2, 2) plt.plot(t, cross_correlation, label='Cross-correlated Signal') plt.legend() plt.title('Convolution in Time Domain')  plt.tight_layout() plt.show() <ul> <li>TODO: Add cross-corraltion in frequency domain</li> </ul> In\u00a0[13]: Copied! <pre># Compute FFTs of the signals\nF_rect = fft(rectangular_pulse)\nF_tri = fft(triangular_pulse)\n\n# Convolution in Frequency domain\nmultiplied_spectrum = None\n\nif multiplied_spectrum is not None:\n    # Convert back to time domain using IFFT\n    convolved_freq = fftshift(ifft(multiplied_spectrum).real)\n\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(t, rectangular_pulse, label='Rectangular Pulse')\n    plt.plot(t, triangular_pulse, label='Triangular Pulse')\n    plt.legend()\n    plt.title('Original Signals')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(t, convolved_freq, label='Cross-correlation (Frequency Domain)')\n    plt.legend()\n    plt.title('Convolution Result (IFFT)')\n\n    plt.tight_layout()\n    plt.show()\n</pre> # Compute FFTs of the signals F_rect = fft(rectangular_pulse) F_tri = fft(triangular_pulse)  # Convolution in Frequency domain multiplied_spectrum = None  if multiplied_spectrum is not None:     # Convert back to time domain using IFFT     convolved_freq = fftshift(ifft(multiplied_spectrum).real)      plt.figure(figsize=(10, 4))     plt.subplot(1, 2, 1)     plt.plot(t, rectangular_pulse, label='Rectangular Pulse')     plt.plot(t, triangular_pulse, label='Triangular Pulse')     plt.legend()     plt.title('Original Signals')      plt.subplot(1, 2, 2)     plt.plot(t, convolved_freq, label='Cross-correlation (Frequency Domain)')     plt.legend()     plt.title('Convolution Result (IFFT)')      plt.tight_layout()     plt.show() In\u00a0[14]: Copied! <pre># Create a simple signal\nt = np.linspace(0, 1, 1000, endpoint=False)\noriginal_signal = np.sin(2 * np.pi * 5 * t)\n\n# Add noise to the signal\nnoise = np.random.normal(0, 0.5, original_signal.shape)\nnoisy_signal = original_signal + noise\n\n# Wiener filter in frequency domain\ndef wiener_filter(noisy_signal, original_signal, noise_power):\n    # Compute power spectral density of the signal and noise\n    Sx = fft(original_signal) * np.conj(fft(original_signal)) / len(original_signal)\n    Sn = noise_power\n    \n    # Compute the Wiener filter\n    H = Sx / (Sx + Sn)\n    wiener_output = ifft(H * fft(noisy_signal)).real\n    return wiener_output\n\n# Apply the Wiener filter\nnoise_power = np.var(noise)\nestimated_signal = wiener_filter(noisy_signal, original_signal, noise_power)\n\nplt.figure(figsize=(10, 5))\nplt.plot(t, original_signal, 'b', label=\"Original Signal\")\nplt.plot(t, noisy_signal, 'c', label=\"Noisy Signal\")\nplt.plot(t, estimated_signal, 'r', label=\"Wiener Filter Output\")\nplt.legend()\nplt.title(\"Wiener Filter Example\")\nplt.grid(True)\nplt.show()\n</pre> # Create a simple signal t = np.linspace(0, 1, 1000, endpoint=False) original_signal = np.sin(2 * np.pi * 5 * t)  # Add noise to the signal noise = np.random.normal(0, 0.5, original_signal.shape) noisy_signal = original_signal + noise  # Wiener filter in frequency domain def wiener_filter(noisy_signal, original_signal, noise_power):     # Compute power spectral density of the signal and noise     Sx = fft(original_signal) * np.conj(fft(original_signal)) / len(original_signal)     Sn = noise_power          # Compute the Wiener filter     H = Sx / (Sx + Sn)     wiener_output = ifft(H * fft(noisy_signal)).real     return wiener_output  # Apply the Wiener filter noise_power = np.var(noise) estimated_signal = wiener_filter(noisy_signal, original_signal, noise_power)  plt.figure(figsize=(10, 5)) plt.plot(t, original_signal, 'b', label=\"Original Signal\") plt.plot(t, noisy_signal, 'c', label=\"Noisy Signal\") plt.plot(t, estimated_signal, 'r', label=\"Wiener Filter Output\") plt.legend() plt.title(\"Wiener Filter Example\") plt.grid(True) plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#signal-processing-101","title":"Signal Processing 101\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#basic-sinusoidal-signal","title":"Basic Sinusoidal Signal\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#fourier-transform","title":"Fourier Transform\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#filtering","title":"Filtering\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#phase-shift","title":"Phase Shift\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#convolution","title":"Convolution\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#cross-correlation","title":"Cross-Correlation\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/signal_processing/#further-reading-wiener-filter","title":"Further Reading: Wiener Filter\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/spring_slider/","title":"Spring Slider","text":"<p>\\begin{align}  \\tau &amp;=\\tau_0 - n V - \\kappa \\delta \\\\  \u03b7 &amp;= \\frac{\\mu}{2c} \\\\ \\kappa &amp;= \\frac{2\\mu}{\\pi L} \\\\ \\end{align}</p> <p>$$ \\tau_{lock} = \\tau_0 - \\kappa \\delta \\\\ $$ $$ \\tau_{str}(\\delta)= \\begin{cases}\\tau_p-\\left(\\tau_p-\\tau_r\\right) \\delta / D_c, &amp; \\delta \\le D_c \\\\ \\tau_r &amp; \\delta&gt;D_c\\end{cases} $$</p> <p>\\begin{align} f &amp;= f_s - (f_s - f_d) * \\min(\\delta, D_c) / D_c \\nonumber \\\\ \\tau_{str} &amp;= f \\cdot \u03c3_n \\nonumber \\end{align}</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\n</pre> import numpy as np import matplotlib.pyplot as plt from IPython.display import display, clear_output In\u00a0[2]: Copied! <pre>L = 1                   #fault length (m)\n\u03bc = 30e3                #shear modulus (MPa)\nc = 3e3                 #shear wave speed (m/s)\n\u03c0 = np.pi\n\u03ba = 2.0 * \u03bc / (\u03c0 * L)   #stiffness (MPa/m)\n\u03b7 = 0.5 * \u03bc / c         #radiation-damping coefficient\n</pre> L = 1                   #fault length (m) \u03bc = 30e3                #shear modulus (MPa) c = 3e3                 #shear wave speed (m/s) \u03c0 = np.pi \u03ba = 2.0 * \u03bc / (\u03c0 * L)   #stiffness (MPa/m) \u03b7 = 0.5 * \u03bc / c         #radiation-damping coefficient In\u00a0[3]: Copied! <pre>fs = 0.8        #static friction coefficient\nfd = 0.7        #dynamic friction coefficient\nDc = 10e-6      #slip weakening distance (m)\n</pre> fs = 0.8        #static friction coefficient fd = 0.7        #dynamic friction coefficient Dc = 10e-6      #slip weakening distance (m) In\u00a0[4]: Copied! <pre>\u03c3n = 100.0      #effective normal stress (MPa)\n\u03c40 = fs * \u03c3n    #initial shear stress (MPa)\nd\u03c4dt = 1e-6     #shear stress rate (MPa/s)\n</pre> \u03c3n = 100.0      #effective normal stress (MPa) \u03c40 = fs * \u03c3n    #initial shear stress (MPa) d\u03c4dt = 1e-6     #shear stress rate (MPa/s) In\u00a0[5]: Copied! <pre>\u03b4 = np.linspace(0, 2 * Dc, 100)                 #slip (m)\nf = fs - (fs - fd) * np.minimum(\u03b4, Dc) / Dc     #slip-weakening friction\n\u03c4_str = f * \u03c3n                                  #fault strength (MPa)\n</pre> \u03b4 = np.linspace(0, 2 * Dc, 100)                 #slip (m) f = fs - (fs - fd) * np.minimum(\u03b4, Dc) / Dc     #slip-weakening friction \u03c4_str = f * \u03c3n                                  #fault strength (MPa) In\u00a0[6]: Copied! <pre>plt.figure()\nplt.plot(\u03b4, \u03c4_str)\nplt.vlines(Dc, 0, fd * \u03c3n, linestyle=\"--\", label=\"Dc\")\nplt.legend()\nplt.ylim(60)\nplt.xlabel(\"Slip-weakening distance (\u03b4)\")\nplt.ylabel(\"Fault strength (\u03c4_str)\")\n</pre> plt.figure() plt.plot(\u03b4, \u03c4_str) plt.vlines(Dc, 0, fd * \u03c3n, linestyle=\"--\", label=\"Dc\") plt.legend() plt.ylim(60) plt.xlabel(\"Slip-weakening distance (\u03b4)\") plt.ylabel(\"Fault strength (\u03c4_str)\") Out[6]: <pre>Text(0, 0.5, 'Fault strength (\u03c4_str)')</pre> In\u00a0[7]: Copied! <pre>v_max = (fs - fd) * \u03c3n / \u03b7\n\u03c4_min = fd * \u03c3n\n\u03c4_max = fs * \u03c3n\n\u03b4_max = (\u03c4_max - \u03c4_min) / \u03ba\nt_max = \u03b7 / \u03ba * 2.0\nL_min = 2.0 * \u03bc / \u03c0 * Dc / (\u03c4_max - \u03c4_min)\n\nnt = 10000\ndt = 1e-7\n</pre> v_max = (fs - fd) * \u03c3n / \u03b7 \u03c4_min = fd * \u03c3n \u03c4_max = fs * \u03c3n \u03b4_max = (\u03c4_max - \u03c4_min) / \u03ba t_max = \u03b7 / \u03ba * 2.0 L_min = 2.0 * \u03bc / \u03c0 * Dc / (\u03c4_max - \u03c4_min)  nt = 10000 dt = 1e-7 In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(3, 2, figsize=(8, 10))\nt_hist = []\n\u03c4_hist = []\nv_hist = []\n\u03b4_hist = []\n\n\u03b4 = 0\nfor i in range(nt):\n\n    \u03c40 += d\u03c4dt * dt\n    \u0394\u03c4 = -\u03ba * \u03b4\n    \u03c4_lock = \u03c40 + \u0394\u03c4\n\n    f = fs - (fs - fd) * min(\u03b4, Dc) / Dc\n    \u03c4_str = f * \u03c3n\n\n    if \u03c4_lock &lt; \u03c4_str:\n        \u03c4 = \u03c4_lock\n        v = 0\n    else:\n        \u03c4 = \u03c4_str\n        v = -(\u03c4_str - \u03c4_lock) / \u03b7\n\n    \u03b4 = \u03b4 + dt * v\n    t_hist.append(i * dt)\n    \u03c4_hist.append(\u03c4)\n    v_hist.append(v)\n    \u03b4_hist.append(\u03b4)\n\n    if i % 100 == 0:\n        [axi.clear() for axi in ax.ravel()]\n        ax[0, 0].plot(t_hist, v_hist, lw=2)\n        ax[0, 1].plot(t_hist, v_hist, lw=2)\n        ax[0, 1].set_ylim(0, v_max)\n        ax[0, 1].set_title(\"Fixed ylim\")\n        ax[0, 0].set_ylabel(\"Velocity (v)\")\n        ax[1, 0].plot(t_hist, \u03c4_hist, lw=2)\n        ax[1, 1].plot(t_hist, \u03c4_hist, lw=2)\n        ax[1, 1].set_ylim(\u03c4_min, \u03c4_max)\n        ax[1, 0].set_ylabel(\"Fault stength (\u03c4)\")\n        ax[2, 0].plot(t_hist, \u03b4_hist, lw=2)\n        ax[2, 1].plot(t_hist, \u03b4_hist, lw=2)\n        ax[2, 1].set_ylim(0, \u03b4_max)\n        ax[2, 0].set_ylabel(\"Slip (\u03b4)\")\n        ax[2, 1].ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0, 0))\n        [axi.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0, 0)) for axi in ax.ravel()]\n        ax[2, 0].set_xlabel(\"Time (s)\")\n        ax[2, 1].set_xlabel(\"Time (s)\")\n        \n        display(fig)\n        clear_output(wait=True)\n</pre> fig, ax = plt.subplots(3, 2, figsize=(8, 10)) t_hist = [] \u03c4_hist = [] v_hist = [] \u03b4_hist = []  \u03b4 = 0 for i in range(nt):      \u03c40 += d\u03c4dt * dt     \u0394\u03c4 = -\u03ba * \u03b4     \u03c4_lock = \u03c40 + \u0394\u03c4      f = fs - (fs - fd) * min(\u03b4, Dc) / Dc     \u03c4_str = f * \u03c3n      if \u03c4_lock &lt; \u03c4_str:         \u03c4 = \u03c4_lock         v = 0     else:         \u03c4 = \u03c4_str         v = -(\u03c4_str - \u03c4_lock) / \u03b7      \u03b4 = \u03b4 + dt * v     t_hist.append(i * dt)     \u03c4_hist.append(\u03c4)     v_hist.append(v)     \u03b4_hist.append(\u03b4)      if i % 100 == 0:         [axi.clear() for axi in ax.ravel()]         ax[0, 0].plot(t_hist, v_hist, lw=2)         ax[0, 1].plot(t_hist, v_hist, lw=2)         ax[0, 1].set_ylim(0, v_max)         ax[0, 1].set_title(\"Fixed ylim\")         ax[0, 0].set_ylabel(\"Velocity (v)\")         ax[1, 0].plot(t_hist, \u03c4_hist, lw=2)         ax[1, 1].plot(t_hist, \u03c4_hist, lw=2)         ax[1, 1].set_ylim(\u03c4_min, \u03c4_max)         ax[1, 0].set_ylabel(\"Fault stength (\u03c4)\")         ax[2, 0].plot(t_hist, \u03b4_hist, lw=2)         ax[2, 1].plot(t_hist, \u03b4_hist, lw=2)         ax[2, 1].set_ylim(0, \u03b4_max)         ax[2, 0].set_ylabel(\"Slip (\u03b4)\")         ax[2, 1].ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0, 0))         [axi.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0, 0)) for axi in ax.ravel()]         ax[2, 0].set_xlabel(\"Time (s)\")         ax[2, 1].set_xlabel(\"Time (s)\")                  display(fig)         clear_output(wait=True)"},{"location":"eps207-observational-seismology/lectures/codes/spring_slider/#single-degree-of-freedom-spring-slider-system","title":"Single degree of freedom spring-slider system\u00b6","text":"<p>(P333, Segal 2010)</p>"},{"location":"eps207-observational-seismology/lectures/codes/wave_propagation/","title":"1D seismic wave","text":"In\u00a0[15]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\n</pre> import numpy as np import matplotlib.pyplot as plt from IPython.display import display, clear_output In\u00a0[16]: Copied! <pre>\u03bc0 = 30e3                #shear modulus (MPa)\nc0 = 3.0                 #shear wave speed (km/s)\n\u03c10 = \u03bc0 / (c0**2)        #density (kg/m^3)\nprint(f\"\u03c10 = {\u03c10:.2e} kg/m^3\")\n</pre> \u03bc0 = 30e3                #shear modulus (MPa) c0 = 3.0                 #shear wave speed (km/s) \u03c10 = \u03bc0 / (c0**2)        #density (kg/m^3) print(f\"\u03c10 = {\u03c10:.2e} kg/m^3\") <pre>\u03c10 = 3.33e+03 kg/m^3\n</pre> In\u00a0[17]: Copied! <pre>NX = 80                 ## number of grid points\nNT = 1500               ## number of time steps\n\u0394t = 1.0/NT             ## time step\n\u0394x = 1.0/NX             ## grid spacing\nNB = 30                 ## absorbing boundary\nN = NX + 2 * NB + 1     ## total grid points\nxv = np.arange(0, N) * \u0394x - NB * \u0394x\nx\u03c4 = (xv[1:] + xv[:-1]) / 2.0 ## staggered grid\n</pre> NX = 80                 ## number of grid points NT = 1500               ## number of time steps \u0394t = 1.0/NT             ## time step \u0394x = 1.0/NX             ## grid spacing NB = 30                 ## absorbing boundary N = NX + 2 * NB + 1     ## total grid points xv = np.arange(0, N) * \u0394x - NB * \u0394x x\u03c4 = (xv[1:] + xv[:-1]) / 2.0 ## staggered grid  In\u00a0[18]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 1))\nax.plot(xv[::20], np.zeros_like(xv)[::20], 'o-')\nax.plot(x\u03c4[10::20], np.zeros_like(x\u03c4)[10::20], 'o--')\nfor i in range(0, N, 20):\n    ax.text(xv[i], 0.008, r'$v$', ha='center', va='bottom')\nfor i in range(10, N, 20):\n    ax.text(x\u03c4[i], 0.008, r'$\\tau$', ha='center', va='bottom')\nax.set_xlabel(r'$x$')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(8, 1)) ax.plot(xv[::20], np.zeros_like(xv)[::20], 'o-') ax.plot(x\u03c4[10::20], np.zeros_like(x\u03c4)[10::20], 'o--') for i in range(0, N, 20):     ax.text(xv[i], 0.008, r'$v$', ha='center', va='bottom') for i in range(10, N, 20):     ax.text(x\u03c4[i], 0.008, r'$\\tau$', ha='center', va='bottom') ax.set_xlabel(r'$x$') plt.show() In\u00a0[19]: Copied! <pre>C = 1000.0\ncv = np.zeros(N)\nfor i in range(NB+1):\n    d = i * \u0394x\n    cv[NB + NX + i] = C * (d / (NB * \u0394x)) ** 3\n    cv[NB - i] = C * (d / (NB * \u0394x)) ** 3\n\nc\u03c4 = np.zeros(N-1)\nfor i in range(NB):\n    d = (i + 0.5) * \u0394x\n    c\u03c4[NB + NX + i] = C * (d / (NB * \u0394x)) ** 3\n    c\u03c4[NB - i -1] = C * (d / (NB * \u0394x)) ** 3\n</pre> C = 1000.0 cv = np.zeros(N) for i in range(NB+1):     d = i * \u0394x     cv[NB + NX + i] = C * (d / (NB * \u0394x)) ** 3     cv[NB - i] = C * (d / (NB * \u0394x)) ** 3  c\u03c4 = np.zeros(N-1) for i in range(NB):     d = (i + 0.5) * \u0394x     c\u03c4[NB + NX + i] = C * (d / (NB * \u0394x)) ** 3     c\u03c4[NB - i -1] = C * (d / (NB * \u0394x)) ** 3  In\u00a0[20]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 1))\nax.plot(xv, cv, ls='-',label=r'c$v$')\nax.plot(x\u03c4, c\u03c4, ls='--', label=r'c$\\tau$')\nax.legend()\nax.set_xlabel(r'$x$')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(8, 1)) ax.plot(xv, cv, ls='-',label=r'c$v$') ax.plot(x\u03c4, c\u03c4, ls='--', label=r'c$\\tau$') ax.legend() ax.set_xlabel(r'$x$') plt.show() In\u00a0[21]: Copied! <pre>def source(\u0394t, f0 = 5.0):\n    t = np.arange(-1.0/f0, 1.0/f0, \u0394t)\n    b = (np.pi * f0 * t) ** 2 \n    # return (1.0 - 2.0 * b) * np.exp(-b) ## Ricker\n    return np.exp(-b) ## Gaussian\n\nf = source(\u0394t) * 1e4\nif len(f) &lt; NT:\n    f = np.pad(f, (0, NT - len(f)), 'constant')\n\n## source location\nmask = np.zeros(N-2)\nmask[NB + 1] = 1.0\n</pre> def source(\u0394t, f0 = 5.0):     t = np.arange(-1.0/f0, 1.0/f0, \u0394t)     b = (np.pi * f0 * t) ** 2      # return (1.0 - 2.0 * b) * np.exp(-b) ## Ricker     return np.exp(-b) ## Gaussian  f = source(\u0394t) * 1e4 if len(f) &lt; NT:     f = np.pad(f, (0, NT - len(f)), 'constant')  ## source location mask = np.zeros(N-2) mask[NB + 1] = 1.0 In\u00a0[22]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 1))\nax.plot(np.arange(NT) * \u0394t, f)\nax.set_xlabel(r'$t$')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(8, 1)) ax.plot(np.arange(NT) * \u0394t, f) ax.set_xlabel(r'$t$') plt.show() In\u00a0[23]: Copied! <pre>fig, ax = plt.subplots(2, 1, figsize=(8, 4))\nv = np.zeros(N)\n\u03c4 = np.zeros(N-1)\n\n\u03c1 = np.ones(N-2) * \u03c10\n\u03bc = np.ones(N-1) * \u03bc0\n\n\u03c4_hist = []\nv_hist = []\nvmax = 0.007\n\u03c4max = -70\n\nfor i in range(NT):\n\n    \u0394v = 1.0/\u03c1 * ((\u03c4[1:]-\u03c4[:-1])/\u0394x  + f[i] * mask) - cv[1:-1]*v[1:-1]\n    v[1:-1] += \u0394v * \u0394t\n\n    \u0394\u03c4 = \u03bc * (v[1:]-v[:-1])/\u0394x - c\u03c4*\u03c4\n    \u03c4 += \u0394\u03c4 * \u0394t\n\n    \u03c4_hist.append(\u03c4.copy())\n    v_hist.append(v.copy())\n\n    if i % 20 == 0:\n        [axi.clear() for axi in ax.ravel()]\n        ax[0].plot(xv, v, lw=2, label=r'$v$')\n        ax[0].set_ylim(-vmax/10, vmax)\n        ax[0].set_xlabel(r'$x$')\n        ax[0].legend()\n        ax[1].plot(x\u03c4, \u03c4, lw=2, label=r'$\\tau$')\n        ax[1].set_ylim(-\u03c4max, \u03c4max)\n        ax[1].set_xlabel(r'$x$')\n        ax[1].legend()\n        display(fig)\n        clear_output(wait=True)\n</pre> fig, ax = plt.subplots(2, 1, figsize=(8, 4)) v = np.zeros(N) \u03c4 = np.zeros(N-1)  \u03c1 = np.ones(N-2) * \u03c10 \u03bc = np.ones(N-1) * \u03bc0  \u03c4_hist = [] v_hist = [] vmax = 0.007 \u03c4max = -70  for i in range(NT):      \u0394v = 1.0/\u03c1 * ((\u03c4[1:]-\u03c4[:-1])/\u0394x  + f[i] * mask) - cv[1:-1]*v[1:-1]     v[1:-1] += \u0394v * \u0394t      \u0394\u03c4 = \u03bc * (v[1:]-v[:-1])/\u0394x - c\u03c4*\u03c4     \u03c4 += \u0394\u03c4 * \u0394t      \u03c4_hist.append(\u03c4.copy())     v_hist.append(v.copy())      if i % 20 == 0:         [axi.clear() for axi in ax.ravel()]         ax[0].plot(xv, v, lw=2, label=r'$v$')         ax[0].set_ylim(-vmax/10, vmax)         ax[0].set_xlabel(r'$x$')         ax[0].legend()         ax[1].plot(x\u03c4, \u03c4, lw=2, label=r'$\\tau$')         ax[1].set_ylim(-\u03c4max, \u03c4max)         ax[1].set_xlabel(r'$x$')         ax[1].legend()         display(fig)         clear_output(wait=True) In\u00a0[24]: Copied! <pre>fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nvmax = np.max(np.abs(v_hist))\nim = ax[0].pcolormesh(xv, np.arange(NT)*\u0394t, np.array(v_hist), vmin=-vmax, vmax=vmax, cmap='seismic', shading='nearest', rasterized=True)\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$t$')\nax[0].set_title(r'$v$')\nfig.colorbar(im, ax=ax[0])\nvmax = np.max(np.abs(\u03c4_hist))\nim = ax[1].pcolormesh(x\u03c4, np.arange(NT)*\u0394t, np.array(\u03c4_hist), vmin=-vmax, vmax=vmax, cmap='seismic', shading='nearest', rasterized=True)\nax[1].set_xlabel(r'$x$')\nax[1].set_ylabel(r'$t$')\nax[1].set_title(r'$\\tau$')\nfig.colorbar(im, ax=ax[1])\nplt.show()\n</pre> fig, ax = plt.subplots(1, 2, figsize=(10, 3)) vmax = np.max(np.abs(v_hist)) im = ax[0].pcolormesh(xv, np.arange(NT)*\u0394t, np.array(v_hist), vmin=-vmax, vmax=vmax, cmap='seismic', shading='nearest', rasterized=True) ax[0].set_xlabel(r'$x$') ax[0].set_ylabel(r'$t$') ax[0].set_title(r'$v$') fig.colorbar(im, ax=ax[0]) vmax = np.max(np.abs(\u03c4_hist)) im = ax[1].pcolormesh(x\u03c4, np.arange(NT)*\u0394t, np.array(\u03c4_hist), vmin=-vmax, vmax=vmax, cmap='seismic', shading='nearest', rasterized=True) ax[1].set_xlabel(r'$x$') ax[1].set_ylabel(r'$t$') ax[1].set_title(r'$\\tau$') fig.colorbar(im, ax=ax[1]) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/lectures/codes/wave_propagation/#one-dimensional-wave-propagation","title":"One-dimensional wave propagation\u00b6","text":"<p>\\begin{align}     \\rho \\frac{\\partial^2 \\delta}{\\partial t^2} = \\frac{\\partial}{\\partial x} \\left( \\mu \\frac{\\partial \\delta}{\\partial x} \\right) + f \\end{align}</p> <p>\\begin{align}     \\rho \\frac{\\partial v}{\\partial t} &amp;=  \\frac{\\partial \\tau}{\\partial x} + f \\\\     \\frac{\\partial \\tau}{\\partial t} &amp;= \\mu \\frac{\\partial v}{\\partial x} \\\\     v &amp;= \\frac{\\partial \\delta}{\\partial t} \\end{align}</p>"},{"location":"eps207-observational-seismology/lectures/codes/wave_propagation/#define-grid-points","title":"Define grid points\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/wave_propagation/#define-absorbing-boundary-conditions","title":"Define absorbing boundary conditions\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/wave_propagation/#define-source-function","title":"Define source function\u00b6","text":""},{"location":"eps207-observational-seismology/lectures/codes/wave_propagation/#conduct-finite-difference-time-stepping","title":"Conduct finite difference time stepping\u00b6","text":""},{"location":"eps207-observational-seismology/obspy/animation_seedlink/","title":"Animation seedlink","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\nimport numpy as np\nfrom collections import deque\nimport obspy\nfrom obspy.clients.seedlink.easyseedlink import create_client\n# %matplotlib widget\n</pre> import matplotlib.pyplot as plt from IPython.display import display, clear_output import numpy as np from collections import deque import obspy from obspy.clients.seedlink.easyseedlink import create_client # %matplotlib widget In\u00a0[2]: Copied! <pre>num = 2000\ndt = 0.01\nx = np.arange(0, num, 1) * dt\nwaveform = deque([0] * num, maxlen=num)\n# normalize = lambda x: (x-np.mean(x))/np.std(x)\nnormalize = lambda x: 2*(x-np.min(x))/(np.max(x)-np.min(x))-1.0\n\nfig, ax = plt.subplots(1,1, figsize=(12,4))\nline, = ax.plot(x, np.array(waveform))\nax.set_ylim([-1, 1])\nax.set_xlabel(\"T (time)\")\nax.set_ylabel(\"Amplitude\")\n\ndef update(trace):\n    \n    waveform.extendleft(list(trace.data))\n    line.set_ydata(normalize(np.array(waveform)))\n    \n    clear_output(wait=True)\n    print(f'Received new data:\\n {trace}')\n    display(fig)\n    \ndef error():\n    pass\n\n# ip = \"dhcp-74-224.caltech.edu\"\nip = \"131.215.64.159\"\nnetwork_code = \"AM\"\nstation_code = \"RE569\"\nchannel_code = \"EHZ\"\nurl = f\"{ip}:18000\"\n\nclient = create_client(url, on_data=update, on_seedlink_error=error)\nclient.select_stream(network_code, station_code, channel_code)\nclient.run()\n</pre> num = 2000 dt = 0.01 x = np.arange(0, num, 1) * dt waveform = deque([0] * num, maxlen=num) # normalize = lambda x: (x-np.mean(x))/np.std(x) normalize = lambda x: 2*(x-np.min(x))/(np.max(x)-np.min(x))-1.0  fig, ax = plt.subplots(1,1, figsize=(12,4)) line, = ax.plot(x, np.array(waveform)) ax.set_ylim([-1, 1]) ax.set_xlabel(\"T (time)\") ax.set_ylabel(\"Amplitude\")  def update(trace):          waveform.extendleft(list(trace.data))     line.set_ydata(normalize(np.array(waveform)))          clear_output(wait=True)     print(f'Received new data:\\n {trace}')     display(fig)      def error():     pass  # ip = \"dhcp-74-224.caltech.edu\" ip = \"131.215.64.159\" network_code = \"AM\" station_code = \"RE569\" channel_code = \"EHZ\" url = f\"{ip}:18000\"  client = create_client(url, on_data=update, on_seedlink_error=error) client.select_stream(network_code, station_code, channel_code) client.run() <pre>Received new data:\n AM.RE569.00.EHZ | 2022-06-29T17:16:52.666999Z - 2022-06-29T17:16:54.716999Z | 100.0 Hz, 206 samples\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/var/folders/gc/lpnp82h92tv35c_7v97w97cm0000gn/T/ipykernel_21654/183904081.py in &lt;module&gt;\n     33 client = create_client(url, on_data=update, on_seedlink_error=error)\n     34 client.select_stream(network_code, station_code, channel_code)\n---&gt; 35 client.run()\n\n~/.local/miniconda3/lib/python3.8/site-packages/obspy/clients/seedlink/easyseedlink.py in run(self)\n    393         # Start the collection loop\n    394         while True:\n--&gt; 395             data = self.conn.collect()\n    396 \n    397             if data == SLPacket.SLTERMINATE:\n\n~/.local/miniconda3/lib/python3.8/site-packages/obspy/clients/seedlink/client/seedlinkconnection.py in collect(self)\n    957                 bytesread = None\n    958                 try:\n--&gt; 959                     bytesread = self.receive_data(self.state.bytes_remaining(),\n    960                                                   self.sladdr)\n    961                 except IOError as ioe:\n\n~/.local/miniconda3/lib/python3.8/site-packages/obspy/clients/seedlink/client/seedlinkconnection.py in receive_data(self, maxbytes, code)\n   1195         try:\n   1196             # self.socket.setblocking(0)\n-&gt; 1197             bytesread = self.socket.recv(maxbytes)\n   1198             # self.socket.setblocking(1)\n   1199         except IOError as ioe:\n\nKeyboardInterrupt: </pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/obspy/animation_seedlink/#real-time-visualization-of-raspberry-shake-using-seedlink-and-obspy","title":"Real-time visualization of Raspberry Shake using Seedlink and Obspy\u00b6","text":""},{"location":"eps207-observational-seismology/obspy/comcat_catalog/","title":"Comcat catalog","text":"In\u00a0[1]: Copied! <pre>import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nfrom libcomcat.dataframes import get_phase_dataframe, get_magnitude_data_frame, get_detail_data_frame, get_history_data_frame\nfrom libcomcat.search import get_event_by_id\nfrom libcomcat.search import search\nfrom libcomcat.classes import DetailEvent, SummaryEvent\nfrom libcomcat.utils import HEADERS, TIMEOUT\n\nfrom obspy.io.quakeml.core import Unpickler\n\nimport pickle\nimport json\nfrom pathlib import Path\nimport requests\n</pre> import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from datetime import datetime  from libcomcat.dataframes import get_phase_dataframe, get_magnitude_data_frame, get_detail_data_frame, get_history_data_frame from libcomcat.search import get_event_by_id from libcomcat.search import search from libcomcat.classes import DetailEvent, SummaryEvent from libcomcat.utils import HEADERS, TIMEOUT  from obspy.io.quakeml.core import Unpickler  import pickle import json from pathlib import Path import requests  In\u00a0[2]: Copied! <pre>phase_path = Path(\"phase\")\nif not phase_path.exists():\n    phase_path.mkdir()\nevent_path = Path(\"event\")\nif not event_path.exists():\n    event_path.mkdir()\nraw_event_path = Path(\"raw_event\")\nif not raw_event_path.exists():\n    raw_event_path.mkdir()\nraw_phase_path = Path(\"raw_phase\")\nif not raw_phase_path.exists():\n    raw_phase_path.mkdir()\n</pre> phase_path = Path(\"phase\") if not phase_path.exists():     phase_path.mkdir() event_path = Path(\"event\") if not event_path.exists():     event_path.mkdir() raw_event_path = Path(\"raw_event\") if not raw_event_path.exists():     raw_event_path.mkdir() raw_phase_path = Path(\"raw_phase\") if not raw_phase_path.exists():     raw_phase_path.mkdir() In\u00a0[3]: Copied! <pre>event_id = 'nc73201181'\ndetail = get_event_by_id(event_id, includesuperseded=True)\n</pre> event_id = 'nc73201181' detail = get_event_by_id(event_id, includesuperseded=True) In\u00a0[4]: Copied! <pre>with open(raw_event_path / f\"{event_id}.pkl\", \"wb\") as f:\n    pickle.dump(detail, f)\n\n# print(detail)\n# with open(raw_event_path / f\"{event_id}.pkl\", \"rb\") as f:\n#     x = pickle.load(f)\n# print(x)\n</pre> with open(raw_event_path / f\"{event_id}.pkl\", \"wb\") as f:     pickle.dump(detail, f)  # print(detail) # with open(raw_event_path / f\"{event_id}.pkl\", \"rb\") as f: #     x = pickle.load(f) # print(x) In\u00a0[5]: Copied! <pre>def parse_pick(pick, type='pick'):\n    tmp_pick = {}\n    if type == 'pick':\n        tmp_pick[\"resource_id\"] = pick.resource_id\n        tmp_pick[\"network\"] = pick.waveform_id.network_code\n        tmp_pick[\"station\"] = pick.waveform_id.station_code\n        tmp_pick[\"channel\"] = pick.waveform_id.channel_code\n        tmp_pick[\"location\"] = pick.waveform_id.location_code\n        tmp_pick[\"phase_time\"] = pick.time.datetime.isoformat(timespec='milliseconds')\n        tmp_pick[\"oneset\"] = pick.onset\n        tmp_pick[\"polarity\"] = pick.polarity\n        tmp_pick[\"evaluation_mode\"] = pick.evaluation_mode\n        tmp_pick[\"evaluation_status\"] = pick.evaluation_status\n    elif type == 'arrival':\n        tmp_pick[\"resource_id\"] = pick.pick_id\n        tmp_pick[\"phase_type\"] = pick.phase\n        tmp_pick[\"azimuth\"] = pick.azimuth\n        tmp_pick[\"distance\"] = pick.distance\n        tmp_pick[\"takeoff_angle\"] = pick.takeoff_angle\n        tmp_pick[\"time_residual\"] = pick.time_residual\n        tmp_pick[\"time_weight\"] = pick.time_weight\n        tmp_pick[\"time_correction\"] = pick.time_correction\n    else:\n        raise ValueError(\"type must be 'pick' or 'arrival'\")\n\n    return tmp_pick\n\ndef add_pick(pick_dict, pick):\n\n    if pick[\"resource_id\"] not in pick_dict:\n        pick_dict[pick[\"resource_id\"]] = pick\n    else:\n        pick_dict[pick[\"resource_id\"]].update(pick)\n    \n    return pick_dict\n</pre> def parse_pick(pick, type='pick'):     tmp_pick = {}     if type == 'pick':         tmp_pick[\"resource_id\"] = pick.resource_id         tmp_pick[\"network\"] = pick.waveform_id.network_code         tmp_pick[\"station\"] = pick.waveform_id.station_code         tmp_pick[\"channel\"] = pick.waveform_id.channel_code         tmp_pick[\"location\"] = pick.waveform_id.location_code         tmp_pick[\"phase_time\"] = pick.time.datetime.isoformat(timespec='milliseconds')         tmp_pick[\"oneset\"] = pick.onset         tmp_pick[\"polarity\"] = pick.polarity         tmp_pick[\"evaluation_mode\"] = pick.evaluation_mode         tmp_pick[\"evaluation_status\"] = pick.evaluation_status     elif type == 'arrival':         tmp_pick[\"resource_id\"] = pick.pick_id         tmp_pick[\"phase_type\"] = pick.phase         tmp_pick[\"azimuth\"] = pick.azimuth         tmp_pick[\"distance\"] = pick.distance         tmp_pick[\"takeoff_angle\"] = pick.takeoff_angle         tmp_pick[\"time_residual\"] = pick.time_residual         tmp_pick[\"time_weight\"] = pick.time_weight         tmp_pick[\"time_correction\"] = pick.time_correction     else:         raise ValueError(\"type must be 'pick' or 'arrival'\")      return tmp_pick  def add_pick(pick_dict, pick):      if pick[\"resource_id\"] not in pick_dict:         pick_dict[pick[\"resource_id\"]] = pick     else:         pick_dict[pick[\"resource_id\"]].update(pick)          return pick_dict In\u00a0[6]: Copied! <pre>pick_df = []\n\norigins_phase = detail.getProducts('phase-data', source=\"all\")\n# for origin in origins_phase:\n#     for k in origin.properties:\n#         print(k, origin[k])\n\nfor origin in origins_phase:\n    # for k in origin.properties:\n    #     print(k, origin[k])\n    \n    quakeurl = origin.getContentURL('quakeml.xml')\n\n    with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"wb\") as f:\n        pickle.dump(origin, f)\n\n    # print(origin)\n    # with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"rb\") as f:\n    #     x = pickle.load(f)\n    # print(x)\n\n    try:\n        response = requests.get(quakeurl, timeout=TIMEOUT, headers=HEADERS)\n        data = response.text.encode('utf-8')\n    except Exception:\n        continue\n\n    unpickler = Unpickler()\n    try:\n        catalog = unpickler.loads(data)\n    except Exception as e:\n        fmt = 'Could not parse QuakeML from %s due to error: %s'\n        continue\n    \n    pick_dict = {}\n    for catevent in catalog.events:\n        for pick in catevent.picks:\n            pick = parse_pick(pick, type=\"pick\")\n            add_pick(pick_dict, pick)\n        for tmp_origin in catevent.origins:\n            for pick in tmp_origin.arrivals:\n                pick = parse_pick(pick, type=\"arrival\")\n                add_pick(pick_dict, pick)\n    pick_df.append(pd.DataFrame.from_dict(pick_dict, orient='index'))\n\npick_df = pd.concat(pick_df)\n</pre> pick_df = []  origins_phase = detail.getProducts('phase-data', source=\"all\") # for origin in origins_phase: #     for k in origin.properties: #         print(k, origin[k])  for origin in origins_phase:     # for k in origin.properties:     #     print(k, origin[k])          quakeurl = origin.getContentURL('quakeml.xml')      with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"wb\") as f:         pickle.dump(origin, f)      # print(origin)     # with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"rb\") as f:     #     x = pickle.load(f)     # print(x)      try:         response = requests.get(quakeurl, timeout=TIMEOUT, headers=HEADERS)         data = response.text.encode('utf-8')     except Exception:         continue      unpickler = Unpickler()     try:         catalog = unpickler.loads(data)     except Exception as e:         fmt = 'Could not parse QuakeML from %s due to error: %s'         continue          pick_dict = {}     for catevent in catalog.events:         for pick in catevent.picks:             pick = parse_pick(pick, type=\"pick\")             add_pick(pick_dict, pick)         for tmp_origin in catevent.origins:             for pick in tmp_origin.arrivals:                 pick = parse_pick(pick, type=\"arrival\")                 add_pick(pick_dict, pick)     pick_df.append(pd.DataFrame.from_dict(pick_dict, orient='index'))  pick_df = pd.concat(pick_df) In\u00a0[7]: Copied! <pre>pick_df.to_csv(phase_path/f'{event_id}.csv', index=False)\n</pre> pick_df.to_csv(phase_path/f'{event_id}.csv', index=False) In\u00a0[8]: Copied! <pre>event_dict = {}\n\nfor k in detail.properties:\n    if k != \"products\":\n        # print(k, detail[k])\n        event_dict[k] = detail[k]\n</pre> event_dict = {}  for k in detail.properties:     if k != \"products\":         # print(k, detail[k])         event_dict[k] = detail[k]  In\u00a0[9]: Copied! <pre>origins_fc = detail.getProducts('focal-mechanism')\nfor origin in origins_fc:\n    for k in origin.properties:\n        # print(k, origin[k])\n        event_dict[k] = origin[k]\n</pre> origins_fc = detail.getProducts('focal-mechanism') for origin in origins_fc:     for k in origin.properties:         # print(k, origin[k])         event_dict[k] = origin[k] In\u00a0[10]: Copied! <pre>origins_mt = detail.getProducts('moment-tensor')\nfor origin in origins_mt:\n    for k in origin.properties:\n        # print(k, origin[k])\n        event_dict[k] = origin[k]\n</pre> origins_mt = detail.getProducts('moment-tensor') for origin in origins_mt:     for k in origin.properties:         # print(k, origin[k])         event_dict[k] = origin[k] In\u00a0[11]: Copied! <pre>event_df = pd.DataFrame.from_dict(event_dict, orient='index').T\nevent_df.to_csv(event_path/f'{event_id}.csv', index=False)\n</pre> event_df = pd.DataFrame.from_dict(event_dict, orient='index').T event_df.to_csv(event_path/f'{event_id}.csv', index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[12]: Copied! <pre># print(get_phase_dataframe(detail))\n</pre> # print(get_phase_dataframe(detail)) In\u00a0[13]: Copied! <pre># print(get_magnitude_data_frame(detail, catalog=\"us\", magtype=\"ml\"))\n</pre> # print(get_magnitude_data_frame(detail, catalog=\"us\", magtype=\"ml\")) In\u00a0[14]: Copied! <pre># print(get_history_data_frame(detail))\n</pre> # print(get_history_data_frame(detail)) In\u00a0[15]: Copied! <pre># summary_events = search(starttime=datetime(1994, 1, 17, 12, 30), endtime=datetime(1994, 1, 18, 12, 35),\n#                    maxradiuskm=2, latitude=34.213, longitude=-118.537)\n# detail_df = get_detail_data_frame(summary_events)\n# print(detail_df)\n</pre> # summary_events = search(starttime=datetime(1994, 1, 17, 12, 30), endtime=datetime(1994, 1, 18, 12, 35), #                    maxradiuskm=2, latitude=34.213, longitude=-118.537) # detail_df = get_detail_data_frame(summary_events) # print(detail_df) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/obspy/comcat_catalog/","title":"Comcat catalog","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nfrom libcomcat.dataframes import get_phase_dataframe, get_magnitude_data_frame, get_detail_data_frame, get_history_data_frame\nfrom libcomcat.search import get_event_by_id\nfrom libcomcat.search import search\nfrom libcomcat.classes import DetailEvent, SummaryEvent\nfrom libcomcat.utils import HEADERS, TIMEOUT\nTIMEOUT = 60 * 60\n\nfrom obspy.io.quakeml.core import Unpickler\n\nimport pickle\nimport json\nfrom pathlib import Path\nimport requests\nimport json\nimport functools\nimport multiprocessing as mp\nfrom tqdm import tqdm\nimport time\nimport random\n</pre> import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from datetime import datetime  from libcomcat.dataframes import get_phase_dataframe, get_magnitude_data_frame, get_detail_data_frame, get_history_data_frame from libcomcat.search import get_event_by_id from libcomcat.search import search from libcomcat.classes import DetailEvent, SummaryEvent from libcomcat.utils import HEADERS, TIMEOUT TIMEOUT = 60 * 60  from obspy.io.quakeml.core import Unpickler  import pickle import json from pathlib import Path import requests import json import functools import multiprocessing as mp from tqdm import tqdm import time import random In\u00a0[\u00a0]: Copied! <pre>def parse_pick(pick, type='pick'):\n    tmp_pick = {}\n    if type == 'pick':\n        tmp_pick[\"resource_id\"] = pick.resource_id\n        tmp_pick[\"network\"] = pick.waveform_id.network_code\n        tmp_pick[\"station\"] = pick.waveform_id.station_code\n        tmp_pick[\"channel\"] = pick.waveform_id.channel_code\n        tmp_pick[\"location\"] = pick.waveform_id.location_code\n        tmp_pick[\"phase_time\"] = pick.time.datetime.isoformat(timespec='milliseconds')\n        tmp_pick[\"oneset\"] = pick.onset\n        tmp_pick[\"polarity\"] = pick.polarity\n        tmp_pick[\"evaluation_mode\"] = pick.evaluation_mode\n        tmp_pick[\"evaluation_status\"] = pick.evaluation_status\n    elif type == 'arrival':\n        tmp_pick[\"resource_id\"] = pick.pick_id\n        tmp_pick[\"phase_type\"] = pick.phase\n        tmp_pick[\"azimuth\"] = pick.azimuth\n        tmp_pick[\"distance\"] = pick.distance\n        tmp_pick[\"takeoff_angle\"] = pick.takeoff_angle\n        tmp_pick[\"time_residual\"] = pick.time_residual\n        tmp_pick[\"time_weight\"] = pick.time_weight\n        tmp_pick[\"time_correction\"] = pick.time_correction\n    else:\n        raise ValueError(\"type must be 'pick' or 'arrival'\")\n\n    return tmp_pick\n\ndef add_pick(pick_dict, pick):\n\n    if pick[\"resource_id\"] not in pick_dict:\n        pick_dict[pick[\"resource_id\"]] = pick\n    else:\n        pick_dict[pick[\"resource_id\"]].update(pick)\n    \n    return pick_dict\n\ndef download_catalog(event_id, event_path, phase_path, raw_event_path, raw_phase_path):\n\n    if (event_path/f'{event_id}.json').exists():\n        print(f\"{event_id}.json already exists\")\n        return 0\n\n    try:\n        detail = get_event_by_id(event_id, includesuperseded=True)\n        print(f\"vv Success: {event_id}\")\n    except Exception as e:\n        print(f\"xx Failed: {event_id}\")\n        # print(f\"{e}\")\n        time.sleep(1)\n        return -1\n\n    # tires = 0\n    # max_tires = 10\n    # while tires &lt; max_tires:\n    #     try:\n    #         detail = get_event_by_id(event_id, includesuperseded=True)\n    #         break\n    #     except Exception as e:\n    #         print(f\"{tires}: {e}\")\n    #         tires += 1\n    #         time.sleep(60)\n</pre> def parse_pick(pick, type='pick'):     tmp_pick = {}     if type == 'pick':         tmp_pick[\"resource_id\"] = pick.resource_id         tmp_pick[\"network\"] = pick.waveform_id.network_code         tmp_pick[\"station\"] = pick.waveform_id.station_code         tmp_pick[\"channel\"] = pick.waveform_id.channel_code         tmp_pick[\"location\"] = pick.waveform_id.location_code         tmp_pick[\"phase_time\"] = pick.time.datetime.isoformat(timespec='milliseconds')         tmp_pick[\"oneset\"] = pick.onset         tmp_pick[\"polarity\"] = pick.polarity         tmp_pick[\"evaluation_mode\"] = pick.evaluation_mode         tmp_pick[\"evaluation_status\"] = pick.evaluation_status     elif type == 'arrival':         tmp_pick[\"resource_id\"] = pick.pick_id         tmp_pick[\"phase_type\"] = pick.phase         tmp_pick[\"azimuth\"] = pick.azimuth         tmp_pick[\"distance\"] = pick.distance         tmp_pick[\"takeoff_angle\"] = pick.takeoff_angle         tmp_pick[\"time_residual\"] = pick.time_residual         tmp_pick[\"time_weight\"] = pick.time_weight         tmp_pick[\"time_correction\"] = pick.time_correction     else:         raise ValueError(\"type must be 'pick' or 'arrival'\")      return tmp_pick  def add_pick(pick_dict, pick):      if pick[\"resource_id\"] not in pick_dict:         pick_dict[pick[\"resource_id\"]] = pick     else:         pick_dict[pick[\"resource_id\"]].update(pick)          return pick_dict  def download_catalog(event_id, event_path, phase_path, raw_event_path, raw_phase_path):      if (event_path/f'{event_id}.json').exists():         print(f\"{event_id}.json already exists\")         return 0      try:         detail = get_event_by_id(event_id, includesuperseded=True)         print(f\"vv Success: {event_id}\")     except Exception as e:         print(f\"xx Failed: {event_id}\")         # print(f\"{e}\")         time.sleep(1)         return -1      # tires = 0     # max_tires = 10     # while tires &lt; max_tires:     #     try:     #         detail = get_event_by_id(event_id, includesuperseded=True)     #         break     #     except Exception as e:     #         print(f\"{tires}: {e}\")     #         tires += 1     #         time.sleep(60) In\u00a0[\u00a0]: Copied! <pre>    with open(raw_event_path / f\"{event_id}.pkl\", \"wb\") as f:\n        pickle.dump(detail, f)\n\n    # print(detail)\n    # with open(raw_event_path / f\"{event_id}.pkl\", \"rb\") as f:\n    #     x = pickle.load(f)\n    # print(x)\n</pre>     with open(raw_event_path / f\"{event_id}.pkl\", \"wb\") as f:         pickle.dump(detail, f)      # print(detail)     # with open(raw_event_path / f\"{event_id}.pkl\", \"rb\") as f:     #     x = pickle.load(f)     # print(x) In\u00a0[\u00a0]: Copied! <pre>    pick_df = []\n\n    try:\n        origins_phase = detail.getProducts('phase-data', source=\"all\")\n    except Exception as e:\n        print(f\"xx Failed: {event_id} {e}\")\n        # print(f\"{e}\")\n        time.sleep(1)\n        return -1\n    # for origin in origins_phase:\n    #     for k in origin.properties:\n    #         print(k, origin[k])\n\n    for origin in origins_phase:\n        # for k in origin.properties:\n        #     print(k, origin[k])\n        \n        quakeurl = origin.getContentURL('quakeml.xml')\n\n        with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"wb\") as f:\n            pickle.dump(origin, f)\n\n        # print(origin)\n        # with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"rb\") as f:\n        #     x = pickle.load(f)\n        # print(x)\n\n        try:\n            response = requests.get(quakeurl, timeout=TIMEOUT, headers=HEADERS)\n            data = response.text.encode('utf-8')\n        except Exception:\n            continue\n\n        unpickler = Unpickler()\n        try:\n            catalog = unpickler.loads(data)\n        except Exception as e:\n            fmt = 'Could not parse QuakeML from %s due to error: %s'\n            continue\n        \n        pick_dict = {}\n        for catevent in catalog.events:\n            for pick in catevent.picks:\n                pick = parse_pick(pick, type=\"pick\")\n                add_pick(pick_dict, pick)\n            for tmp_origin in catevent.origins:\n                for pick in tmp_origin.arrivals:\n                    pick = parse_pick(pick, type=\"arrival\")\n                    add_pick(pick_dict, pick)\n        pick_df.append(pd.DataFrame.from_dict(pick_dict, orient='index'))\n\n    if len(pick_df) == 0:\n        print(f\"xx Failed to download picks: {event_id}\")\n        return -1\n\n    try:\n        pick_df = pd.concat(pick_df)\n        pick_df.sort_values(by=[\"network\",\"station\",\"location\",\"channel\",\"phase_type\"], inplace=True)\n        pick_df.to_csv(phase_path/f'{event_id}.csv', index=False)\n    except Exception as e:\n        print(f\"xx Failed to download picks: {event_id}\")\n        time.sleep(1)\n        return -1\n</pre>     pick_df = []      try:         origins_phase = detail.getProducts('phase-data', source=\"all\")     except Exception as e:         print(f\"xx Failed: {event_id} {e}\")         # print(f\"{e}\")         time.sleep(1)         return -1     # for origin in origins_phase:     #     for k in origin.properties:     #         print(k, origin[k])      for origin in origins_phase:         # for k in origin.properties:         #     print(k, origin[k])                  quakeurl = origin.getContentURL('quakeml.xml')          with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"wb\") as f:             pickle.dump(origin, f)          # print(origin)         # with open(raw_phase_path / f\"{event_id}_{origin.source}.pkl\", \"rb\") as f:         #     x = pickle.load(f)         # print(x)          try:             response = requests.get(quakeurl, timeout=TIMEOUT, headers=HEADERS)             data = response.text.encode('utf-8')         except Exception:             continue          unpickler = Unpickler()         try:             catalog = unpickler.loads(data)         except Exception as e:             fmt = 'Could not parse QuakeML from %s due to error: %s'             continue                  pick_dict = {}         for catevent in catalog.events:             for pick in catevent.picks:                 pick = parse_pick(pick, type=\"pick\")                 add_pick(pick_dict, pick)             for tmp_origin in catevent.origins:                 for pick in tmp_origin.arrivals:                     pick = parse_pick(pick, type=\"arrival\")                     add_pick(pick_dict, pick)         pick_df.append(pd.DataFrame.from_dict(pick_dict, orient='index'))      if len(pick_df) == 0:         print(f\"xx Failed to download picks: {event_id}\")         return -1      try:         pick_df = pd.concat(pick_df)         pick_df.sort_values(by=[\"network\",\"station\",\"location\",\"channel\",\"phase_type\"], inplace=True)         pick_df.to_csv(phase_path/f'{event_id}.csv', index=False)     except Exception as e:         print(f\"xx Failed to download picks: {event_id}\")         time.sleep(1)         return -1 In\u00a0[\u00a0]: Copied! <pre>    event_dict = {}\n\n    for k in detail.properties:\n        if k != \"products\":\n            # print(k, detail[k])\n            event_dict[k] = detail[k]\n</pre>     event_dict = {}      for k in detail.properties:         if k != \"products\":             # print(k, detail[k])             event_dict[k] = detail[k] In\u00a0[\u00a0]: Copied! <pre>    try:\n        origins_fc = detail.getProducts('focal-mechanism')\n        for origin in origins_fc:\n            for k in origin.properties:\n                # print(k, origin[k])\n                event_dict[k] = origin[k]\n    except Exception as e:\n        # print(f\"{e}\")\n        pass\n</pre>     try:         origins_fc = detail.getProducts('focal-mechanism')         for origin in origins_fc:             for k in origin.properties:                 # print(k, origin[k])                 event_dict[k] = origin[k]     except Exception as e:         # print(f\"{e}\")         pass In\u00a0[\u00a0]: Copied! <pre>    try:\n        origins_mt = detail.getProducts('moment-tensor')\n        for origin in origins_mt:\n            for k in origin.properties:\n                # print(k, origin[k])\n                event_dict[k] = origin[k]\n    except Exception as e:\n        # print(f\"{e}\")\n        pass\n</pre>     try:         origins_mt = detail.getProducts('moment-tensor')         for origin in origins_mt:             for k in origin.properties:                 # print(k, origin[k])                 event_dict[k] = origin[k]     except Exception as e:         # print(f\"{e}\")         pass In\u00a0[\u00a0]: Copied! <pre>    # event_df = pd.DataFrame.from_dict(event_dict, orient='index').T\n    # event_df.to_csv(event_path/f'{event_id}.csv', index=False)\n    with open(event_path/f'{event_id}.json', 'w') as f:\n        json.dump(event_dict, f, indent=2)\n\n    # time.sleep(1)\n\n    return 0\n</pre>     # event_df = pd.DataFrame.from_dict(event_dict, orient='index').T     # event_df.to_csv(event_path/f'{event_id}.csv', index=False)     with open(event_path/f'{event_id}.json', 'w') as f:         json.dump(event_dict, f, indent=2)      # time.sleep(1)      return 0 In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n</pre>  if __name__ == \"__main__\": In\u00a0[\u00a0]: Copied! <pre>    phase_path = Path(\"phase\")\n    if not phase_path.exists():\n        phase_path.mkdir()\n    event_path = Path(\"event\")\n    if not event_path.exists():\n        event_path.mkdir()\n    raw_event_path = Path(\"raw_event\")\n    if not raw_event_path.exists():\n        raw_event_path.mkdir()\n    raw_phase_path = Path(\"raw_phase\")\n    if not raw_phase_path.exists():\n        raw_phase_path.mkdir()\n</pre>     phase_path = Path(\"phase\")     if not phase_path.exists():         phase_path.mkdir()     event_path = Path(\"event\")     if not event_path.exists():         event_path.mkdir()     raw_event_path = Path(\"raw_event\")     if not raw_event_path.exists():         raw_event_path.mkdir()     raw_phase_path = Path(\"raw_phase\")     if not raw_phase_path.exists():         raw_phase_path.mkdir() In\u00a0[\u00a0]: Copied! <pre>    # event_id = 'nc73201181'\n    # download_catalog(event_id, phase_path=phase_path, event_path=event_path, raw_event_path=raw_event_path, raw_phase_path=raw_phase_path)\n</pre>     # event_id = 'nc73201181'     # download_catalog(event_id, phase_path=phase_path, event_path=event_path, raw_event_path=raw_event_path, raw_phase_path=raw_phase_path) In\u00a0[\u00a0]: Copied! <pre>    with open(\"event_id.json\", \"r\") as f:\n        tmp = json.load(f)\n    event_ids = [\"nc\"+tmp[k] for k in tmp]\n    event_ids = sorted(event_ids)[::-1]\n    \n    download_catalog_ = functools.partial(download_catalog, phase_path=phase_path, event_path=event_path, raw_event_path=raw_event_path, raw_phase_path=raw_phase_path)\n    # download_catalog_(event_id)\n\n    for _ in range(10):\n        random.shuffle(event_ids)\n        for event_id in tqdm(event_ids, mininterval=100):\n            download_catalog(event_id, phase_path=phase_path, event_path=event_path, raw_event_path=raw_event_path, raw_phase_path=raw_phase_path)\n        #   download_catalog_(event_id)\n        #     break\n\n    # ncpu = mp.cpu_count()\n    # ncpu = 4\n    # with mp.Pool(ncpu) as p:\n    #     p.map(download_catalog_, event_ids)\n</pre>     with open(\"event_id.json\", \"r\") as f:         tmp = json.load(f)     event_ids = [\"nc\"+tmp[k] for k in tmp]     event_ids = sorted(event_ids)[::-1]          download_catalog_ = functools.partial(download_catalog, phase_path=phase_path, event_path=event_path, raw_event_path=raw_event_path, raw_phase_path=raw_phase_path)     # download_catalog_(event_id)      for _ in range(10):         random.shuffle(event_ids)         for event_id in tqdm(event_ids, mininterval=100):             download_catalog(event_id, phase_path=phase_path, event_path=event_path, raw_event_path=raw_event_path, raw_phase_path=raw_phase_path)         #   download_catalog_(event_id)         #     break      # ncpu = mp.cpu_count()     # ncpu = 4     # with mp.Pool(ncpu) as p:     #     p.map(download_catalog_, event_ids) In\u00a0[\u00a0]: Copied! <pre># print(get_phase_dataframe(detail))\n</pre> # print(get_phase_dataframe(detail)) In\u00a0[\u00a0]: Copied! <pre># print(get_magnitude_data_frame(detail, catalog=\"us\", magtype=\"ml\"))\n</pre> # print(get_magnitude_data_frame(detail, catalog=\"us\", magtype=\"ml\")) In\u00a0[\u00a0]: Copied! <pre># print(get_history_data_frame(detail))\n</pre> # print(get_history_data_frame(detail)) In\u00a0[\u00a0]: Copied! <pre># summary_events = search(starttime=datetime(1994, 1, 17, 12, 30), endtime=datetime(1994, 1, 18, 12, 35),\n#                    maxradiuskm=2, latitude=34.213, longitude=-118.537)\n# detail_df = get_detail_data_frame(summary_events)\n# print(detail_df)\n</pre> # summary_events = search(starttime=datetime(1994, 1, 17, 12, 30), endtime=datetime(1994, 1, 18, 12, 35), #                    maxradiuskm=2, latitude=34.213, longitude=-118.537) # detail_df = get_detail_data_frame(summary_events) # print(detail_df) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/obspy/download_catalog/","title":"Download catalog","text":"In\u00a0[1]: Copied! <pre>import obspy\nfrom obspy.clients.fdsn.client import Client\n</pre> import obspy from obspy.clients.fdsn.client import Client In\u00a0[2]: Copied! <pre>def print_event(event):\n    event.plot(\"ortho\")\n    print(\"-------------------------------------\")\n    print(event[0])\n    print(\"-------------------------------------\")\n    print(event[0].origins[0])\n    print(\"-------------------------------------\")\n    print(event[0].magnitudes[0])\n    print(\"-------------------------------------\")\n    try:\n        print(event[0].amplitudes[0])\n    except:\n        pass\n    print(event[0].picks[0])\n    print(\"-------------------------------------\")\n    print(event[0].origins[0].arrivals[0])\n    print(\"=====================================\")\n\n    \n</pre> def print_event(event):     event.plot(\"ortho\")     print(\"-------------------------------------\")     print(event[0])     print(\"-------------------------------------\")     print(event[0].origins[0])     print(\"-------------------------------------\")     print(event[0].magnitudes[0])     print(\"-------------------------------------\")     try:         print(event[0].amplitudes[0])     except:         pass     print(event[0].picks[0])     print(\"-------------------------------------\")     print(event[0].origins[0].arrivals[0])     print(\"=====================================\")       In\u00a0[3]: Copied! <pre>client = Client(\"USGS\")\nevent = client.get_events(eventid=\"ak01862cwg6j\", includeallorigins=True)\nprint_event(event)\n</pre> client = Client(\"USGS\") event = client.get_events(eventid=\"ak01862cwg6j\", includeallorigins=True) print_event(event)  <pre>-------------------------------------\nEvent:\t2018-05-12T03:29:57.233000Z | +56.322, -148.419 | 3.3  ml | manual\n\n\t            resource_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/event/01862cwg6j\")\n\t             event_type: 'earthquake'\n\t          creation_info: CreationInfo(agency_id='AK', author='Alaska Earthquake Center, UAF/GI', creation_time=UTCDateTime(2019, 6, 16, 22, 7, 52, 899000), version='2')\n\t    preferred_origin_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/origin/01862cwg6j\")\n\t preferred_magnitude_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/magnitude/01862cwg6j/ml\")\n\t                   ---------\n\t                  picks: 78 Elements\n\t                origins: 1 Elements\n\t             magnitudes: 1 Elements\n-------------------------------------\nOrigin\n\t        resource_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/origin/01862cwg6j\")\n\t               time: UTCDateTime(2018, 5, 12, 3, 29, 57, 233000)\n\t          longitude: -148.4188\n\t           latitude: 56.322\n\t              depth: 12000.0 [uncertainty=5400.0]\n\t          method_id: ResourceIdentifier(id=\"quakeml:anss.org/cube/locationMethod/D\")\n\t            quality: OriginQuality(used_phase_count=78, standard_error=0.64)\n\t origin_uncertainty: OriginUncertainty(min_horizontal_uncertainty=2300.0, max_horizontal_uncertainty=7400.0, azimuth_max_horizontal_uncertainty=337.79, preferred_description='uncertainty ellipse')\n\t    evaluation_mode: 'manual'\n\t               ---------\n\t           arrivals: 78 Elements\n-------------------------------------\nMagnitude\n\t    resource_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/magnitude/01862cwg6j/ml\")\n\t            mag: 3.3\n\t magnitude_type: 'ml'\n\t      origin_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/origin/01862cwg6j\")\n\t      method_id: ResourceIdentifier(id=\"quakeml:anss.org/cube/magnitudeType/L\")\n-------------------------------------\nPick\n\t     resource_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/pick/36793\")\n\t            time: UTCDateTime(2018, 5, 12, 3, 31, 13, 280000)\n\t     waveform_id: WaveformStreamID(network_code='AK', station_code='CHX', channel_code='SHZ', location_code='--')\n\t evaluation_mode: 'manual'\n\t   creation_info: CreationInfo(agency_id='AK', author='Alaska Earthquake Center, UAF/GI', creation_time=UTCDateTime(2018, 5, 14, 21, 55, 53, 273000))\n-------------------------------------\nArrival\n\t   resource_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/arrival/36793\")\n\t       pick_id: ResourceIdentifier(id=\"quakeml:earthquake.alaska.edu/pick/36793\")\n\t         phase: 'P'\n\t       azimuth: 42.75\n\t      distance: 5.361\n\t time_residual: 0.778\n\t   time_weight: 0.035\n\t creation_info: CreationInfo(agency_id='AK', author='Alaska Earthquake Center, UAF/GI', creation_time=UTCDateTime(2018, 5, 14, 21, 55, 53, 273000))\n=====================================\n</pre> In\u00a0[4]: Copied! <pre>client = Client(\"USGS\")\nevent = client.get_events(eventid=\"ci40403984\", includeallorigins=True)\nprint_event(event)\n</pre> client = Client(\"USGS\") event = client.get_events(eventid=\"ci40403984\", includeallorigins=True) print_event(event)  <pre>-------------------------------------\nEvent:\t2023-01-19T13:58:40.370000Z | +35.858, -117.700 | 3.35 Ml | manual\n\n\t            resource_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?eventid=40403984\")\n\t             event_type: 'earthquake'\n\t          creation_info: CreationInfo(agency_id='CI', agency_uri=ResourceIdentifier(id=\"quakeml:doi.org/10.7909/C3WD3xH1\"), creation_time=UTCDateTime(2023, 1, 19, 14, 53, 42), version='7')\n\t    preferred_origin_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?originid=107958068\")\n\t preferred_magnitude_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?magnitudeid=110413716\")\n\t                   ---------\n\t     event_descriptions: 1 Elements\n\t                  picks: 177 Elements\n\t             amplitudes: 907 Elements\n\t                origins: 2 Elements\n\t             magnitudes: 3 Elements\n\t     station_magnitudes: 1423 Elements\n-------------------------------------\nOrigin\n\t        resource_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?originid=107749416\")\n\t               time: UTCDateTime(2023, 1, 19, 13, 58, 40, 430000)\n\t          longitude: -117.6988297\n\t           latitude: 35.8568344\n\t              depth: 8390.0 [uncertainty=540.0]\n\t         depth_type: 'from location'\n\t         time_fixed: False\n\t    epicenter_fixed: False\n\t          method_id: ResourceIdentifier(id=\"smi:ci.anss.org/origin/BINDER\")\n\t            quality: OriginQuality(associated_phase_count=111, used_phase_count=53, associated_station_count=91, used_station_count=46, standard_error=0.17, azimuthal_gap=36.0, secondary_azimuthal_gap=385.0, minimum_distance=0.07212, maximum_distance=3.36, median_distance=1.27)\n\t        origin_type: 'hypocenter'\n\t origin_uncertainty: OriginUncertainty(horizontal_uncertainty=250.0, confidence_ellipsoid=ConfidenceEllipsoid(semi_major_axis_length=1296.0, semi_minor_axis_length=480.0, semi_intermediate_axis_length=600.0, major_axis_plunge=83.0, major_axis_azimuth=6.0, major_axis_rotation=68.0), preferred_description='confidence ellipsoid', confidence_level=95.0)\n\t    evaluation_mode: 'automatic'\n\t  evaluation_status: 'preliminary'\n\t      creation_info: CreationInfo(agency_id='CI', creation_time=UTCDateTime(2023, 1, 19, 13, 59, 50))\n\t               ---------\n\t           arrivals: 111 Elements\n-------------------------------------\nMagnitude\n\t                     resource_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?magnitudeid=110413716\")\n\t                             mag: 3.35 [uncertainty=0.126]\n\t                  magnitude_type: 'Ml'\n\t                       origin_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?originid=107958068\")\n\t                       method_id: ResourceIdentifier(id=\"smi:ci.anss.org/magnitude/CISNml2\")\n\t                   station_count: 85\n\t                   azimuthal_gap: 36.6\n\t                 evaluation_mode: 'manual'\n\t               evaluation_status: 'reviewed'\n\t                   creation_info: CreationInfo(agency_id='CI', creation_time=UTCDateTime(2023, 1, 19, 14, 53, 42))\n\t                            ---------\n\t station_magnitude_contributions: 280 Elements\n-------------------------------------\nAmplitude\n\t       resource_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?ampid=835541668\")\n\t generic_amplitude: 0.00024812\n\t              type: 'AML'\n\t          category: 'point'\n\t              unit: 'm'\n\t         method_id: ResourceIdentifier(id=\"smi:ci.anss.org/amp/WASF\")\n\t               snr: 176.04141171767506\n\t       waveform_id: WaveformStreamID(network_code='AZ', station_code='CRY', channel_code='HHE', location_code='')\n\t      scaling_time: UTCDateTime(2023, 1, 19, 13, 59, 58, 950000)\n\t    magnitude_hint: 'ML'\n\t   evaluation_mode: 'manual'\n\t evaluation_status: 'reviewed'\nPick\n\t       resource_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?arid=316114080\")\n\t              time: UTCDateTime(2023, 1, 19, 13, 59, 22, 750000)\n\t       waveform_id: WaveformStreamID(network_code='BK', station_code='HELL', channel_code='HHE', location_code='00')\n\t          polarity: 'undecidable'\n\t   evaluation_mode: 'automatic'\n\t evaluation_status: 'preliminary'\n\t     creation_info: CreationInfo(agency_id='CI', creation_time=UTCDateTime(2023, 1, 19, 13, 59, 50))\n-------------------------------------\nArrival\n\t     resource_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?assocaro=316114080\")\n\t         pick_id: ResourceIdentifier(id=\"quakeml:service.scedc.caltech.edu/fdsnws/event/1/query?arid=316114080\")\n\t           phase: 'S'\n\t time_correction: -0.25\n\t         azimuth: 308.0\n\t        distance: 1.349\n\t   takeoff_angle: 53.0\n\t   time_residual: 1.39\n\t     time_weight: 0.0\n\t   creation_info: CreationInfo(agency_id='CI', creation_time=UTCDateTime(2023, 1, 19, 13, 59, 50))\n=====================================\n</pre> In\u00a0[5]: Copied! <pre>client = Client(\"USGS\")\nevent = client.get_events(eventid=\"nc73836266\", includeallorigins=True)\nprint_event(event)\n</pre> client = Client(\"USGS\") event = client.get_events(eventid=\"nc73836266\", includeallorigins=True) print_event(event) <pre>-------------------------------------\nEvent:\t2023-01-23T13:58:02.950000Z | +37.109, -121.517 | 3.62 Ml | manual\n\n\t            resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Event/NC/73836266\")\n\t             event_type: 'earthquake'\n\t          creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 17), version='5')\n\t    preferred_origin_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Origin/NC/14070484\")\n\t preferred_magnitude_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Netmag/NC/6018609\")\n\t                   ---------\n\t     event_descriptions: 1 Elements\n\t                  picks: 216 Elements\n\t             amplitudes: 1009 Elements\n\t                origins: 1 Elements\n\t             magnitudes: 2 Elements\n\t     station_magnitudes: 1009 Elements\n-------------------------------------\nOrigin\n\t        resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Origin/NC/14070484\")\n\t               time: UTCDateTime(2023, 1, 23, 13, 58, 2, 950000)\n\t          longitude: -121.5168333\n\t           latitude: 37.1093333\n\t              depth: 5650.0 [uncertainty=350.0]\n\t         depth_type: 'from location'\n\t         time_fixed: False\n\t    epicenter_fixed: False\n\t          method_id: ResourceIdentifier(id=\"smi:nc.anss.org/origin/HYP2000_m2g\")\n\t            quality: OriginQuality(associated_phase_count=129, used_phase_count=115, associated_station_count=199, used_station_count=105, standard_error=0.1, azimuthal_gap=31.0, secondary_azimuthal_gap=309.0, minimum_distance=0.05538, maximum_distance=1.962, median_distance=0.7316)\n\t        origin_type: 'hypocenter'\n\t origin_uncertainty: OriginUncertainty(horizontal_uncertainty=100.0, confidence_ellipsoid=ConfidenceEllipsoid(semi_major_axis_length=840.0, semi_minor_axis_length=216.0, semi_intermediate_axis_length=240.0, major_axis_plunge=88.0, major_axis_azimuth=113.0, major_axis_rotation=308.0), preferred_description='confidence ellipsoid', confidence_level=95.0)\n\t    evaluation_mode: 'manual'\n\t  evaluation_status: 'final'\n\t      creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 13))\n\t               ---------\n\t           arrivals: 216 Elements\n-------------------------------------\nMagnitude\n\t                     resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Netmag/NC/6018609\")\n\t                             mag: 3.62 [uncertainty=0.149]\n\t                  magnitude_type: 'Ml'\n\t                       origin_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Origin/NC/14070484\")\n\t                       method_id: ResourceIdentifier(id=\"smi:nc.anss.org/magnitude/CISNml2\")\n\t                   station_count: 160\n\t                   azimuthal_gap: 44.1\n\t                 evaluation_mode: 'manual'\n\t               evaluation_status: 'reviewed'\n\t                   creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 14))\n\t                            ---------\n\t station_magnitude_contributions: 871 Elements\n-------------------------------------\nAmplitude\n\t       resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Amp/NC/216036034\")\n\t generic_amplitude: 0.00015137\n\t              type: 'AML'\n\t          category: 'point'\n\t              unit: 'm'\n\t         method_id: ResourceIdentifier(id=\"smi:nc.anss.org/amp/WASF\")\n\t               snr: 14.359942828984583\n\t       waveform_id: WaveformStreamID(network_code='BG', station_code='ACR', channel_code='CNE', location_code='')\n\t      scaling_time: UTCDateTime(2023, 1, 23, 13, 59, 7, 460000)\n\t    magnitude_hint: 'ML'\n\t   evaluation_mode: 'manual'\n\t evaluation_status: 'reviewed'\nPick\n\t       resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Arrival/NC/118864909\")\n\t              time: UTCDateTime(2023, 1, 23, 13, 58, 4, 310000) [uncertainty=0.02]\n\t       waveform_id: WaveformStreamID(network_code='NC', station_code='HGS', channel_code='EHZ', location_code='03')\n\t             onset: 'impulsive'\n\t          polarity: 'positive'\n\t   evaluation_mode: 'manual'\n\t evaluation_status: 'reviewed'\n\t     creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 13))\n-------------------------------------\nArrival\n\t     resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/AssocArO/NC/118864909\")\n\t         pick_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Arrival/NC/118864909\")\n\t           phase: 'P'\n\t time_correction: -0.41\n\t         azimuth: 104.5\n\t        distance: 0.05538\n\t   takeoff_angle: 128.0\n\t   time_residual: 0.19\n\t     time_weight: 0.24\n\t   creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 14))\n=====================================\n</pre> In\u00a0[6]: Copied! <pre>client = Client(\"NCEDC\")\nevent = client.get_events(eventid=\"73836266\", includearrivals=True, includeallmagnitudes=True)\nprint_event(event)\n</pre> client = Client(\"NCEDC\") event = client.get_events(eventid=\"73836266\", includearrivals=True, includeallmagnitudes=True) print_event(event) <pre>-------------------------------------\nEvent:\t2023-01-23T13:58:02.950000Z | +37.109, -121.517 | 3.62 Ml | manual\n\n\t            resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Event/NC/73836266\")\n\t             event_type: 'earthquake'\n\t          creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 17), version='6')\n\t    preferred_origin_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Origin/NC/14070484\")\n\t preferred_magnitude_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Netmag/NC/6018609\")\n\t                   ---------\n\t                  picks: 216 Elements\n\t             amplitudes: 1009 Elements\n\t                origins: 1 Elements\n\t             magnitudes: 2 Elements\n\t     station_magnitudes: 1009 Elements\n-------------------------------------\nOrigin\n\t        resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Origin/NC/14070484\")\n\t               time: UTCDateTime(2023, 1, 23, 13, 58, 2, 950000)\n\t          longitude: -121.5168333\n\t           latitude: 37.1093333\n\t              depth: 5650.0 [uncertainty=350.0]\n\t         depth_type: 'from location'\n\t         time_fixed: False\n\t    epicenter_fixed: False\n\t          method_id: ResourceIdentifier(id=\"smi:nc.anss.org/origin/HYP2000_m2g\")\n\t            quality: OriginQuality(associated_phase_count=129, used_phase_count=115, associated_station_count=199, used_station_count=105, standard_error=0.1, azimuthal_gap=31.0, secondary_azimuthal_gap=309.0, minimum_distance=0.05538, maximum_distance=1.962, median_distance=0.7316)\n\t        origin_type: 'hypocenter'\n\t origin_uncertainty: OriginUncertainty(horizontal_uncertainty=100.0, confidence_ellipsoid=ConfidenceEllipsoid(semi_major_axis_length=840.0, semi_minor_axis_length=216.0, semi_intermediate_axis_length=240.0, major_axis_plunge=88.0, major_axis_azimuth=113.0, major_axis_rotation=308.0), preferred_description='confidence ellipsoid', confidence_level=95.0)\n\t    evaluation_mode: 'manual'\n\t  evaluation_status: 'final'\n\t      creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 13))\n\t               ---------\n\t           arrivals: 216 Elements\n-------------------------------------\nMagnitude\n\t                     resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Netmag/NC/6018609\")\n\t                             mag: 3.62 [uncertainty=0.149]\n\t                  magnitude_type: 'Ml'\n\t                       origin_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Origin/NC/14070484\")\n\t                       method_id: ResourceIdentifier(id=\"smi:nc.anss.org/magnitude/CISNml2\")\n\t                   station_count: 160\n\t                   azimuthal_gap: 44.1\n\t                 evaluation_mode: 'manual'\n\t               evaluation_status: 'reviewed'\n\t                   creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 14))\n\t                            ---------\n\t station_magnitude_contributions: 871 Elements\n-------------------------------------\nAmplitude\n\t       resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Amp/NC/216036034\")\n\t generic_amplitude: 0.00015137\n\t              type: 'AML'\n\t          category: 'point'\n\t              unit: 'm'\n\t         method_id: ResourceIdentifier(id=\"smi:nc.anss.org/amp/WASF\")\n\t               snr: 14.359942828984583\n\t       waveform_id: WaveformStreamID(network_code='BG', station_code='ACR', channel_code='CNE', location_code='')\n\t      scaling_time: UTCDateTime(2023, 1, 23, 13, 59, 7, 460000)\n\t    magnitude_hint: 'ML'\n\t   evaluation_mode: 'manual'\n\t evaluation_status: 'reviewed'\nPick\n\t       resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Arrival/NC/118864909\")\n\t              time: UTCDateTime(2023, 1, 23, 13, 58, 4, 310000) [uncertainty=0.02]\n\t       waveform_id: WaveformStreamID(network_code='NC', station_code='HGS', channel_code='EHZ', location_code='03')\n\t             onset: 'impulsive'\n\t          polarity: 'positive'\n\t   evaluation_mode: 'manual'\n\t evaluation_status: 'reviewed'\n\t     creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 13))\n-------------------------------------\nArrival\n\t     resource_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/AssocArO/NC/118864909\")\n\t         pick_id: ResourceIdentifier(id=\"quakeml:nc.anss.org/Arrival/NC/118864909\")\n\t           phase: 'P'\n\t time_correction: -0.41\n\t         azimuth: 104.5\n\t        distance: 0.05538\n\t   takeoff_angle: 128.0\n\t   time_residual: 0.19\n\t     time_weight: 0.24\n\t   creation_info: CreationInfo(agency_id='NC', creation_time=UTCDateTime(2023, 1, 23, 16, 53, 14))\n=====================================\n</pre> <p>Focal Mechanism</p> <p>https://earthquake.usgs.gov/earthquakes/eventpage/nc73836266/focal-mechanism</p> <p>https://earthquake.usgs.gov/realtime/product/focal-mechanism/nc73836266_fm1/nc/1674493123260/quakeml.xml</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/obspy/install/","title":"Python Environment","text":"<p>In this class, we will learn to use Obspy to access and analyze seismic waveforms. </p> <p>We will use Anaconda to set up the python environment, which comes with many commonly used python packages.</p>"},{"location":"eps207-observational-seismology/obspy/install/#anaconda","title":"Anaconda","text":"<p>First, download and install Anaconda to your laptop, which can take a while.</p>"},{"location":"eps207-observational-seismology/obspy/install/#jupyter","title":"Jupyter","text":"<p>Second, open jupyter notebook either in a terminal or from anaconda navigator.</p>"},{"location":"eps207-observational-seismology/obspy/install/#obspy","title":"Obspy","text":"<p>Third, install Obspy using the commands:</p> <pre><code>pip install obspy\n</code></pre> <p>or </p> <pre><code>conda install obspy -c conda-forge\n</code></pre>"},{"location":"eps207-observational-seismology/obspy/install/#other-packages","title":"Other packages","text":"<ul> <li>cartopy for plotting geospatial data </li> </ul> <pre><code>conda install cartopy -c conda-forge\n</code></pre>"},{"location":"eps207-observational-seismology/obspy/signal_processing/","title":"Signal processing","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport obspy\n# %matplotlib widget\n</pre> import numpy as np import matplotlib.pyplot as plt import obspy # %matplotlib widget In\u00a0[2]: Copied! <pre># Read the seismogram\nst = obspy.read(\"waveforms.mseed\")\nst = st.merge(fill_value=0)\n\ntr = st[0]\n\n# Filtering with a lowpass on a copy of the original Trace\ntr_filt = tr.copy()\ntr_filt.filter('bandpass', freqmin=1.0, freqmax=5, corners=4, zerophase=True)\n</pre> # Read the seismogram st = obspy.read(\"waveforms.mseed\") st = st.merge(fill_value=0)  tr = st[0]  # Filtering with a lowpass on a copy of the original Trace tr_filt = tr.copy() tr_filt.filter('bandpass', freqmin=1.0, freqmax=5, corners=4, zerophase=True) Out[2]: <pre>CI.FMP..HHE | 2022-06-03T12:05:21.058393Z - 2022-06-03T12:06:21.048393Z | 100.0 Hz, 6000 samples</pre> In\u00a0[3]: Copied! <pre># Now let's plot the raw and filtered data...\nt = np.arange(len(tr.data)) * tr.stats.delta\nplt.subplot(211)\nplt.plot(t, tr.data, 'k')\nplt.ylabel('Raw Data')\nplt.subplot(212)\nplt.plot(t, tr_filt.data, 'k')\nplt.ylabel('Lowpassed Data')\nplt.xlabel('Time [s]')\nplt.suptitle(tr.stats.starttime)\nplt.show()\n</pre> # Now let's plot the raw and filtered data... t = np.arange(len(tr.data)) * tr.stats.delta plt.subplot(211) plt.plot(t, tr.data, 'k') plt.ylabel('Raw Data') plt.subplot(212) plt.plot(t, tr_filt.data, 'k') plt.ylabel('Lowpassed Data') plt.xlabel('Time [s]') plt.suptitle(tr.stats.starttime) plt.show() In\u00a0[4]: Copied! <pre>tr.spectrogram(log=True);\ntr.spectrogram(log=False);\n</pre>  tr.spectrogram(log=True); tr.spectrogram(log=False); <ul> <li>Calculate spectrogram using scipy:</li> </ul> In\u00a0[54]: Copied! <pre>import scipy.signal\nf_, t_, spectrogram = scipy.signal.stft(tr.data, fs=tr.stats.sampling_rate, nperseg=100, noverlap=80)\n</pre> import scipy.signal f_, t_, spectrogram = scipy.signal.stft(tr.data, fs=tr.stats.sampling_rate, nperseg=100, noverlap=80) In\u00a0[55]: Copied! <pre>fig, ax = plt.subplots(1, 1)\nax.pcolormesh(t_, f_, np.abs(spectrogram), shading='gouraud')\nax.set_title('STFT Magnitude')\nax.set_ylabel('Frequency [Hz]')\nax.set_xlabel('Time [sec]')\nax.set_yscale('symlog')\nplt.show(fig)\n</pre> fig, ax = plt.subplots(1, 1) ax.pcolormesh(t_, f_, np.abs(spectrogram), shading='gouraud') ax.set_title('STFT Magnitude') ax.set_ylabel('Frequency [Hz]') ax.set_xlabel('Time [sec]') ax.set_yscale('symlog') plt.show(fig) In\u00a0[56]: Copied! <pre>fig, ax = plt.subplots(1, 1)\nax.pcolormesh(t_, f_, np.abs(spectrogram), shading='gouraud')\nax.set_title('STFT Magnitude')\nax.set_ylabel('Frequency [Hz]')\nax.set_xlabel('Time [sec]')\nplt.show(fig)\n</pre> fig, ax = plt.subplots(1, 1) ax.pcolormesh(t_, f_, np.abs(spectrogram), shading='gouraud') ax.set_title('STFT Magnitude') ax.set_ylabel('Frequency [Hz]') ax.set_xlabel('Time [sec]') plt.show(fig) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps207-observational-seismology/obspy/signal_processing/#seismic-signal-processing","title":"Seismic signal processing\u00b6","text":""},{"location":"eps207-observational-seismology/obspy/signal_processing/#filtering","title":"Filtering\u00b6","text":"<p>Note: Either download a test mseed file yourself or run the \"Download seismic data\" section first to download the waveforms.mseed file</p> <p>Example from obspy tutoral:</p> <p>The following script shows how to filter a seismogram. The example uses a zero-phase-shift low-pass filter with a corner frequency of 1 Hz using 2 corners. This is done in two runs forward and backward, so we end up with 4 corners de facto.</p> <p>The available filters are: bandpass, bandstop, lowpass, highpass</p>"},{"location":"eps207-observational-seismology/obspy/signal_processing/#spectrogam","title":"Spectrogam\u00b6","text":"<p>How to find the proper filtering frequencies? We can take a look at the spectrogram.</p> <ul> <li>Using Obspy's default function:</li> </ul>"},{"location":"eps207-observational-seismology/obspy/scripts/animation_seedlink/","title":"Animation seedlink","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.signal\nimport streamlit as st\nfrom collections import deque\nimport obspy\nfrom obspy.clients.seedlink.easyseedlink import create_client\n</pre> import matplotlib.pyplot as plt import numpy as np import scipy.signal import streamlit as st from collections import deque import obspy from obspy.clients.seedlink.easyseedlink import create_client In\u00a0[\u00a0]: Copied! <pre>num = 1000\ndt = 0.01\nx = np.arange(0, num, 1) * dt\nwaveform = deque([np.nan] * num, maxlen=num)\nst.set_page_config(layout=\"wide\")\nst_line = st.line_chart(np.array(waveform))\n# normalize = lambda x: (x - np.min(x))/(np.max(x) - np.min(x))\n# f, t, spectrogram = scipy.signal.stft(np.array(waveform))\n# st_ft = st.image(normalize(spectrogram).T)\n</pre> num = 1000 dt = 0.01 x = np.arange(0, num, 1) * dt waveform = deque([np.nan] * num, maxlen=num) st.set_page_config(layout=\"wide\") st_line = st.line_chart(np.array(waveform)) # normalize = lambda x: (x - np.min(x))/(np.max(x) - np.min(x)) # f, t, spectrogram = scipy.signal.stft(np.array(waveform)) # st_ft = st.image(normalize(spectrogram).T) In\u00a0[\u00a0]: Copied! <pre>def update(trace):\n    print(f'Received new data:\\n {trace}')\n    waveform.extendleft(list(trace.data))\n    st_line.line_chart(np.array(waveform))\n    # f, t, spectrogram = scipy.signal.stft(np.array(waveform))\n    # st_ft.image(normalize(spectrogram).T)\n</pre> def update(trace):     print(f'Received new data:\\n {trace}')     waveform.extendleft(list(trace.data))     st_line.line_chart(np.array(waveform))     # f, t, spectrogram = scipy.signal.stft(np.array(waveform))     # st_ft.image(normalize(spectrogram).T) In\u00a0[\u00a0]: Copied! <pre>def error():\n    pass\n</pre> def error():     pass In\u00a0[\u00a0]: Copied! <pre>url = \"rs.local:18000/\"\nclient = create_client(url, on_data=update, on_seedlink_error=error)\nclient.select_stream('AM', 'RE569', 'EHZ')\nclient.run()\n</pre> url = \"rs.local:18000/\" client = create_client(url, on_data=update, on_seedlink_error=error) client.select_stream('AM', 'RE569', 'EHZ') client.run()"},{"location":"eps207-observational-seismology/visualization/projection/","title":"Projection","text":"In\u00a0[1]: Copied! <pre>from pyproj import Proj\nlat0 = 36.20\nlon0 = -138.25\nR = 6371.0 * 1e3 #km\n</pre> from pyproj import Proj lat0 = 36.20 lon0 = -138.25 R = 6371.0 * 1e3 #km In\u00a0[2]: Copied! <pre>proj = Proj(f\"+proj=sterea +lat_0={lat0} +lon_0={lon0} +R={R} +units=km\")\nx,y = proj(longitude=lon0+1, latitude=lat0+1)\nprint(f\"x = {x:.2f}, y = {y:.2f}\")\nlat, lon = proj(longitude=x, latitude=y, inverse=True)\nprint(f\"lon = {lon:.2f}, lat = {lat:.2f}\")\n</pre> proj = Proj(f\"+proj=sterea +lat_0={lat0} +lon_0={lon0} +R={R} +units=km\") x,y = proj(longitude=lon0+1, latitude=lat0+1) print(f\"x = {x:.2f}, y = {y:.2f}\") lat, lon = proj(longitude=x, latitude=y, inverse=True) print(f\"lon = {lon:.2f}, lat = {lat:.2f}\") <pre>x = 88.58, y = 111.66\nlon = 37.20, lat = -137.25\n</pre> In\u00a0[3]: Copied! <pre>proj = Proj(f\"+proj=stere +lat_0={lat0} +lon_0={lon0} +R={R} +units=km\")\nx,y = proj(longitude=lon0+1, latitude=lat0+1)\nprint(f\"x = {x:.2f}, y = {y:.2f}\")\nlat, lon = proj(longitude=x, latitude=y, inverse=True)\nprint(f\"lon = {lon:.2f}, lat = {lat:.2f}\")\n</pre> proj = Proj(f\"+proj=stere +lat_0={lat0} +lon_0={lon0} +R={R} +units=km\") x,y = proj(longitude=lon0+1, latitude=lat0+1) print(f\"x = {x:.2f}, y = {y:.2f}\") lat, lon = proj(longitude=x, latitude=y, inverse=True) print(f\"lon = {lon:.2f}, lat = {lat:.2f}\") <pre>x = 88.58, y = 111.66\nlon = 37.20, lat = -137.25\n</pre> In\u00a0[4]: Copied! <pre>proj = Proj(f\"+proj=gnom +lat_0={lat0} +lon_0={lon0} +R={R} +units=km\")\nx,y = proj(longitude=lon0+1, latitude=lat0+1)\nprint(f\"x = {x:.2f}, y = {y:.2f}\")\nlat, lon = proj(longitude=x, latitude=y, inverse=True)\nprint(f\"lon = {lon:.2f}, lat = {lat:.2f}\")\n</pre> proj = Proj(f\"+proj=gnom +lat_0={lat0} +lon_0={lon0} +R={R} +units=km\") x,y = proj(longitude=lon0+1, latitude=lat0+1) print(f\"x = {x:.2f}, y = {y:.2f}\") lat, lon = proj(longitude=x, latitude=y, inverse=True) print(f\"lon = {lon:.2f}, lat = {lat:.2f}\") <pre>x = 88.59, y = 111.67\nlon = 37.20, lat = -137.25\n</pre> In\u00a0[5]: Copied! <pre>proj = Proj(f\"+proj=merc +lon_0={lon0} +R={R} +units=km\")\nx,y = proj(longitude=lon0+1, latitude=lat0+1)\nprint(f\"x = {x:.2f}, y = {y:.2f}\")\nlat, lon = proj(longitude=x, latitude=y, inverse=True)\nprint(f\"lon = {lon:.2f}, lat = {lat:.2f}\")\n</pre> proj = Proj(f\"+proj=merc +lon_0={lon0} +R={R} +units=km\") x,y = proj(longitude=lon0+1, latitude=lat0+1) print(f\"x = {x:.2f}, y = {y:.2f}\") lat, lon = proj(longitude=x, latitude=y, inverse=True) print(f\"lon = {lon:.2f}, lat = {lat:.2f}\") <pre>x = 111.19, y = 4462.02\nlon = 37.20, lat = -137.25\n</pre>"},{"location":"eps207-observational-seismology/visualization/projection/#coordinate-transformation","title":"Coordinate Transformation\u00b6","text":"<p>In many seismic studies, we only work on a small area of few degrees, converting lat/lon to x/y coordinates is much easier for data analysis. Here are examples to use pyproj for cartographic projections.</p> <p>The Mercator projection (merc) is commonly used for plotting maps.</p> <p>For simple projection purpose, we will mainly use local projections, such as Stereographic projection (stere), Oblique Stereographic Alternative projection (sterea), and Gnomonic projection (gnom).</p> <p>An overview of different projection views can also be found in cartopy</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/","title":"PyEarth: A Python Introduction to Earth Science","text":"<p>Welcome to PyEarth, a course designed to introduce students to the intersection of Python programming and Earth Science. This course is tailored for students interested in applying computational methods to understand our planet. The course uses Python/Jupyter Notebook and real-world observations to introduce students to various Earth phenomena and their underlying physics. </p> <p>The class is designed for undergraduate students, and no prior knowledge of Earth Science is required.  In this course, you will:</p> <ul> <li>Learn the basics of Python programming</li> <li>Explore fundamental Earth Science concepts</li> <li>Apply data analysis techniques to real-world Earth Science problems</li> <li>Gain hands-on experience with popular Python libraries such as NumPy, Pandas, Matplotlib, and scikit-learn</li> <li>Develop skills in machine learning and its applications in Earth Science</li> <li>Work on practical projects to reinforce your learning</li> </ul> <p>The class will consist of a combination of lectures and hands-on exercises from the basics of Python to advanced topics like neural networks and their applications in Earth Science.  By the end of this course, you'll have a foundation in both Python programming and Earth Science analysis techniques.</p> <p>Like to Data 8: </p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/#time-and-location","title":"Time and Location","text":"<ul> <li>Lecture: Monday 12:00 PM - 2:00 PM in McCone 265</li> <li>Office Hour:</li> <li>Monday 2:00 PM - 3:00 PM in McCone 265</li> <li>Thursday 10:00 AM - 11:00 AM in McCone 285</li> </ul> <p>Instructor: Weiqiang Zhu (zhuwq@berkeley.edu)</p> <p>Graduate student reader:  Shivangi Tikekar (shivangi.tikekar@berkeley.edu)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/#datahub-link","title":"DataHub Link","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/#final-projects","title":"Final Projects","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/#schedule","title":"Schedule","text":"Date Topic Links 09/02 Labor Day 09/09 [Introduction &amp;&amp; Python 101] slides, assignment 09/16 [Numpy &amp; Pandas] slides, assignment 09/23 [Matplotlib &amp; Cartopy &amp; PyGMT] slides, assignment 09/30 [SkLearn: Supervised Learning: Regression 1] slides, assignment 10/07 [Sklearn: Supervised Learning: Regression 2] slides, assignment 10/14 [Sklearn: Supervised Learning: Classification 1] slides, assignment 10/21 [Sklearn: Supervised Learning: Classification 2] slides, assignment 10/28 Midterm review 11/04 [Sklearn: Unsupervised Learning: Clustering] slides, assignment 11/11 Veterans Day 11/18 [Probabilites] slides, assignment 11/25 [PyTorch: Neural Networks 1] slides1, slides2, assignment 12/02 [PyTorch: Neural Networks 2] slides1, slides2, assignment1, assignment2 12/09 Final Project (RRR Week)"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/kaggle/","title":"Kaggle","text":"<p>Introduction &amp; Python 101 (09/09): For this introductory class, you might want to start with a simple dataset. Here are some options:</p> <p>\"Earth Surface Temperature Data\": https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data \"World Happiness Report\": https://www.kaggle.com/datasets/unsdsn/world-happiness</p> <p>Numpy &amp; Pandas (09/16): These libraries are great for data manipulation and analysis. Consider:</p> <p>\"Global Land Temperatures by City\": https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data \"Global Earthquake Dataset\": https://www.kaggle.com/datasets/usgs/earthquake-database</p> <p>Matplotlib &amp; PyGMT (09/23): For visualization, you'll want datasets with geographical components:</p> <p>\"Natural Earth Data\": https://www.kaggle.com/datasets/max-mind/world-cities-database \"Global Volcanic Eruptions\": https://www.kaggle.com/datasets/jessemostipak/volcano-eruptions</p> <p>SkLearn: Supervised Learning (09/30):</p> <p>\"Predict CO2 Emissions in Rwanda\": https://www.kaggle.com/datasets/mnassrib/predict-co2-emissions-in-rwanda \"Forest Cover Type Prediction\": https://www.kaggle.com/c/forest-cover-type-prediction/data</p> <p>SkLearn: Advanced models (10/07):</p> <p>\"Satellite Image Classification\": https://www.kaggle.com/datasets/Crawford/deepsat-sat4 \"Landslide Prediction\": https://www.kaggle.com/datasets/nasa/landslide-events</p> <p>SkLearn: Model selection (10/14):</p> <p>\"Air Quality in Madrid\": https://www.kaggle.com/datasets/decide-soluciones/air-quality-madrid \"Global Water Security Issues\": https://www.kaggle.com/datasets/taranvee/smart-water-quality-prediction-dataset</p> <p>SkLearn: Unsupervised Learning (10/21):</p> <p>\"Earth's Surface Water Change\": https://www.kaggle.com/datasets/navinmundhra/surface-water-change \"Climate Change: Earth Surface Temperature Data\": https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data</p> <p>SkLearn: Bayes optimization, Uncertainty, MCMC (10/28):</p> <p>\"Soil Properties\": https://www.kaggle.com/datasets/crawford/soil-properties \"Global Commodity Trade Statistics\": https://www.kaggle.com/datasets/unitednations/global-commodity-trade-statistics</p> <p>PyTorch: Neural Networks (11/04):</p> <p>\"Satellite Images of Hurricane Damage\": https://www.kaggle.com/datasets/kmader/satellite-images-of-hurricane-damage \"38-Cloud: Cloud Segmentation in Satellite Images\": https://www.kaggle.com/datasets/cordmaur/38cloud-cloud-segmentation-in-satellite-images</p> <p>PyTorch: Training &amp; Optimization (11/18):</p> <p>\"Planet: Understanding the Amazon from Space\": https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data \"Droughts in the US\": https://www.kaggle.com/datasets/cdminix/us-drought-meteorological-data</p> <p>PyTorch: Applications (11/25):</p> <p>\"NOAA Weather Data\": https://www.kaggle.com/datasets/noaa/noaa-global-surface-summary-of-the-day \"Earth Science Data from NASA\": https://www.kaggle.com/datasets/nasa/nasa-earth-science-data</p> <p>Project (12/02 &amp; 12/09): For the project, students might want to choose from any of the above datasets or explore other earth science-related datasets on Kaggle based on their specific interests.</p> <p>Remember to verify the licenses and usage terms for each dataset before incorporating them into your coursework. Also, encourage students to explore Kaggle's Earth Science category for more options: https://www.kaggle.com/search?q=tag%3A%22earth+science%22 Would you like me to elaborate on any of these datasets or suggest alternatives for specific topics?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/","title":"00 introduction python101","text":"In\u00a0[1]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied!"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#pyearth-a-python-introduction-to-earth-science","title":"PyEarth: A Python Introduction to Earth Science\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#python101-exercises","title":"Python101 Exercises\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#exercise-1-variables-and-data-types","title":"Exercise 1: Variables and Data Types\u00b6","text":"<p>Create variables for the following Earth-related information:</p> <ul> <li>Name of our planet</li> <li>Earth's radius in kilometers</li> <li>Earth's average surface temperature in Celsius</li> <li>Whether Earth has a moon (use a boolean)</li> <li>A list of Earth's layers (inner core, outer core, mantle, crust)</li> </ul> <p>Print all these variables.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#exercise-2-arithmetic-operations","title":"Exercise 2: Arithmetic Operations\u00b6","text":"<p>Calculate the following:</p> <ul> <li>The circumference of Earth (use the radius from Exercise 1 and the formula 2 * \u03c0 * r)</li> <li>The difference between the boiling point of water (100\u00b0C) and Earth's average surface temperature</li> <li>The number of times Earth's diameter (use the radius from Exercise 1) can fit between Earth and the Moon (average distance: 384,400 km)</li> </ul> <p>Print the results.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#exercise-3-control-flow","title":"Exercise 3: Control Flow\u00b6","text":"<p>Create a function that takes a temperature in Celsius and returns a description of Earth's temperature:</p> <ul> <li>If temp &lt; 0: \"Earth is in an ice age\"</li> <li>If 0 &lt;= temp &lt; 15: \"Earth is cool\"</li> <li>If 15 &lt;= temp &lt; 25: \"Earth is moderate\"</li> <li>If temp &gt;= 25: \"Earth is warm\"</li> </ul> <p>Test your function with different temperatures.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#exercise-4-lists-and-loops","title":"Exercise 4: Lists and Loops\u00b6","text":"<p>Given the list of planets: <code>[\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]</code></p> <p>Write a loop that prints each planet's name and its position from the Sun.</p> <p>Example output: \"Mercury is the 1st planet from the Sun\"</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#exercise-5-functions","title":"Exercise 5: Functions\u00b6","text":"<p>Write a function that converts kilometers to miles (1 km = 0.621371 miles). Use this function to convert Earth's radius to miles.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/00_introduction_python101/#exercise-6-dictionaries","title":"Exercise 6: Dictionaries\u00b6","text":"<p>Create a dictionary for Earth with the following keys and values:</p> <ul> <li>name: \"Earth\"</li> <li>radius_km: (use the value from Exercise 1)</li> <li>has_moon: (use the value from Exercise 1)</li> <li>atmosphere_composition: {\"nitrogen\": 78, \"oxygen\": 21, \"other\": 1}</li> </ul> <p>Write a function that takes this dictionary and prints a summary of Earth's properties.</p> <p>Example output:</p> <p>\"Earth has a radius of X km and has/doesn't have a moon. Its atmosphere is composed of 78% nitrogen, 21% oxygen, and 1% other gases.\"</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/01_numpy_pandas/","title":"01 numpy pandas","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Create a 3x3 matrix with values ranging from 0 to 8</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Generate a random number between 0 and 1 (hint: import np.random)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Extract all odd numbers from the following array:</li> </ul> In\u00a0[2]: Copied! <pre>arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n</pre> arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) <ul> <li>Replace all odd numbers in the following array with -1:</li> </ul> In\u00a0[3]: Copied! <pre>arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n</pre> arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) <ul> <li>Stack two arrays vertically and horizontally:</li> </ul> In\u00a0[4]: Copied! <pre>a = np.arange(10).reshape(2,-1)\nb = np.repeat(1, 10).reshape(2,-1)\n</pre> a = np.arange(10).reshape(2,-1) b = np.repeat(1, 10).reshape(2,-1) In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Set the index of the DataFrame to be ['X', 'Y', 'Z']</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Add a new column 'D' which is the sum of columns A and B</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Drop column 'C' from the DataFrame</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Create a DataFrame from the following dictionary of lists:</li> </ul> In\u00a0[5]: Copied! <pre>data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n        'name': ['Muffy', 'Nyarko', 'Sasha', 'Aura', 'Sunny', 'Zoro', 'Sassy', 'Penny', 'Kody', 'Cooper'],\n        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n</pre> data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],         'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],         'name': ['Muffy', 'Nyarko', 'Sasha', 'Aura', 'Sunny', 'Zoro', 'Sassy', 'Penny', 'Kody', 'Cooper'],         'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],         'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}  <ul> <li>From the DataFrame above, select all rows where the animal is a cat and the age is less than 3</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>From the DataFrame above, calculate the mean age for each animal type</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>For the DataFrame above, sort the DataFrame first by the values in the 'age' column in descending order, then by the value in the 'visits' column in ascending order</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>For the DataFrame above, replace any NaN values in the 'age' column with the median age</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/01_numpy_pandas/#pyearth-a-python-introduction-to-earth-science","title":"PyEarth: A Python Introduction to Earth Science\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/01_numpy_pandas/#numpy-and-pandas-exercises","title":"NumPy and Pandas Exercises\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/01_numpy_pandas/#numpy-exercises","title":"NumPy Exercises\u00b6","text":"<ul> <li>Create a 1D array of numbers from 0 to 9</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/01_numpy_pandas/#pandas-exercises","title":"Pandas Exercises\u00b6","text":"<ul> <li>Create a DataFrame with 3 columns (A, B, C) and 3 rows of random numbers</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/02_matplotlib_cartopy/","title":"02 matplotlib cartopy","text":"In\u00a0[35]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import cartopy.crs as ccrs import cartopy.feature as cfeature In\u00a0[36]: Copied! <pre># Load temperature data\ntemp_data = pd.read_csv('data/global_temperature.csv')\ntemp_data['date'] = pd.to_datetime(temp_data['date'])\n</pre> # Load temperature data temp_data = pd.read_csv('data/global_temperature.csv') temp_data['date'] = pd.to_datetime(temp_data['date']) <ul> <li>Create a line plot of global temperature over time</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Set xlim to show the last 5 years of data</li> </ul> <p>(hint: <code>plt.xlim(pd.Timestamp('BEGIN_DATE'), pd.Timestamp('END_DATE'))</code>)</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Create a scatter plot of temperature vs. year, color by temperature</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Set xlim to show the last 5 years of data</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Create a histogram of temperature</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[37]: Copied! <pre># Load the earthquake data\nearthquakes = pd.read_csv('data/earthquakes.csv')\n</pre> # Load the earthquake data earthquakes = pd.read_csv('data/earthquakes.csv') <ul> <li>Create a global map of earthquakes using Matplotlib</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Create the same map using Cartopy</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Plot the topography of California using PyGMT</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Plot the topography of California using Cartopy</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ul> <li>Add Earthquake data to the map</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/02_matplotlib_cartopy/#pyearth-a-python-introduction-to-earth-science","title":"PyEarth: A Python Introduction to Earth Science\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/02_matplotlib_cartopy/#lecture-3-exercise-matplotlib-cartopy-and-pygmt","title":"Lecture 3 Exercise: Matplotlib, Cartopy, and PyGMT\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/02_matplotlib_cartopy/#exercise-1-basic-matplotlib-plotting","title":"Exercise 1: Basic Matplotlib plotting\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/02_matplotlib_cartopy/#exercise-2-earthquake-data-visualization","title":"Exercise 2: Earthquake data visualization\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/03_regression/","title":"03 regression","text":"In\u00a0[1]: Copied! <pre>import json\nimport pandas as pd\nimport fsspec\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> import json import pandas as pd import fsspec import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre># M6.0 earthquake used in class\n# json_url = \"https://earthquake.usgs.gov/product/shakemap/ci38443183/atlas/1594160017984/download/stationlist.json\"\n\n# M7.1 earthquake for assignment\njson_url = \"https://earthquake.usgs.gov/product/shakemap/ci38457511/atlas/1594160054783/download/stationlist.json\"\n\nwith fsspec.open(json_url) as f:\n    data = json.load(f)\n\ndef parse_data(data):\n    rows = []\n    for line in data[\"features\"]:\n        rows.append({\n            \"station_id\": line[\"id\"],\n            \"longitude\": line[\"geometry\"][\"coordinates\"][0],\n            \"latitude\": line[\"geometry\"][\"coordinates\"][1],\n            \"pga\": line[\"properties\"][\"pga\"], # unit: %g\n            \"pgv\": line[\"properties\"][\"pgv\"], # unit: cm/s\n            \"distance\": line[\"properties\"][\"distance\"],\n        }\n    )\n    return pd.DataFrame(rows)\n\ndata = parse_data(data)\ndata = data[(data[\"pga\"] != \"null\") &amp; (data[\"pgv\"] != \"null\")]\ndata = data[~data[\"station_id\"].str.startswith(\"DYFI\")]\ndata = data.dropna()\ndata = data.sort_values(by=\"distance\", ascending=True)\ndata[\"logR\"] = data[\"distance\"].apply(lambda x: np.log10(float(x)))\ndata[\"logPGA\"] = data[\"pga\"].apply(lambda x: np.log10(float(x)))\ndata[\"logPGV\"] = data[\"pgv\"].apply(lambda x: np.log10(float(x)))\n</pre> # M6.0 earthquake used in class # json_url = \"https://earthquake.usgs.gov/product/shakemap/ci38443183/atlas/1594160017984/download/stationlist.json\"  # M7.1 earthquake for assignment json_url = \"https://earthquake.usgs.gov/product/shakemap/ci38457511/atlas/1594160054783/download/stationlist.json\"  with fsspec.open(json_url) as f:     data = json.load(f)  def parse_data(data):     rows = []     for line in data[\"features\"]:         rows.append({             \"station_id\": line[\"id\"],             \"longitude\": line[\"geometry\"][\"coordinates\"][0],             \"latitude\": line[\"geometry\"][\"coordinates\"][1],             \"pga\": line[\"properties\"][\"pga\"], # unit: %g             \"pgv\": line[\"properties\"][\"pgv\"], # unit: cm/s             \"distance\": line[\"properties\"][\"distance\"],         }     )     return pd.DataFrame(rows)  data = parse_data(data) data = data[(data[\"pga\"] != \"null\") &amp; (data[\"pgv\"] != \"null\")] data = data[~data[\"station_id\"].str.startswith(\"DYFI\")] data = data.dropna() data = data.sort_values(by=\"distance\", ascending=True) data[\"logR\"] = data[\"distance\"].apply(lambda x: np.log10(float(x))) data[\"logPGA\"] = data[\"pga\"].apply(lambda x: np.log10(float(x))) data[\"logPGV\"] = data[\"pgv\"].apply(lambda x: np.log10(float(x))) <ol> <li>Use pandas to print the first few rows of the data. Understand what each column means.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Create a scatter plot of latitude vs. longitude, with point colors representing PGA values.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Calculate and print the mean and standard deviation of PGA values.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Use scikit-learn to fit a linear regression model to this data.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Print the slope and intercept of the fitted line.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Calculate and print the R-squared value of the model.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Create a residual plot residuals vs. log(R).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Comment on any patterns you observe in the residual plot.</li> </ol> <ol> <li>Find the actual PGA value recorded in Los Angeles for the Ridgecrest earthquake using the USGS ShakeMap.</li> </ol> <p>USGS ShakeMap: https://earthquake.usgs.gov/earthquakes/eventpage/ci38457511/shakemap/pga</p> <p>Note that we are working on the M7.1 earthquake.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Use your model to predict the PGA in the Los Angeles basin (hint: use distance of the station you selected).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Compare this value to your model's prediction. Discuss possible reasons for any differences.</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/03_regression/#pyearth-a-python-introduction-to-earth-science","title":"PyEarth: A Python Introduction to Earth Science\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/03_regression/#linear-regression-in-earth-science","title":"Linear Regression in Earth Science\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/03_regression/#part-1-data-exploration","title":"Part 1: Data Exploration\u00b6","text":"<ol> <li>Load the Ridgecrest earthquake data.</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/03_regression/#part-2-simple-linear-regression","title":"Part 2: Simple Linear Regression\u00b6","text":"<ol> <li>Create a scatter plot of log(PGA) vs. log(R).</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/03_regression/#part-3-residual-analysis","title":"Part 3: Residual Analysis\u00b6","text":"<ol> <li>Calculate the residuals of your model.</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/","title":"04 regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures In\u00a0[\u00a0]: Copied! <pre># Load the data/Freedman2000_Supernova1a.csv file\nSupernova_data = \nSupernova_data.head()\n</pre> # Load the data/Freedman2000_Supernova1a.csv file Supernova_data =  Supernova_data.head() <p>The <code>VCMB</code> column is velocity relative to the cosmic microwave background in $km \\cdot s^{-1}$ .</p> <p>The <code>D(Mpc)</code> column is the distance in Mpc which is the unit typically used for these measurements. 1 Mpc =  3.09 x $10^{19}$ km</p> <p>Go ahead and double-click on this cell to see how I am getting labels that have the proper superscripts.</p> <p>To create nice labels with superscripts, we can use latex formatting, which can also be done in a markdown cell.  For a superscript, first we need to encase the text in dollar signs and then use the ^ symbol to make the following text a superscript. If there is more than one number in the superscript, you must enclose what you want as the superscript in curly braces. For example, to print $10^3$, we use $10^3$ and for 'per second' ($s^{-1}$)</p> In\u00a0[\u00a0]: Copied! <pre># Create a scatter plot of the data with Distance on the x-axis and Velocity on the y-axis\n</pre> # Create a scatter plot of the data with Distance on the x-axis and Velocity on the y-axis  In\u00a0[\u00a0]: Copied! <pre># Fit a linear regression model to the data\n# X = Supernova_data[['D(Mpc)']].values\n# y = Supernova_data['VCMB'].values\n</pre> # Fit a linear regression model to the data # X = Supernova_data[['D(Mpc)']].values # y = Supernova_data['VCMB'].values  In\u00a0[\u00a0]: Copied! <pre># Print the slope and intercept of the model\n</pre> # Print the slope and intercept of the model <p>So $H_o$, the slope of the best-fit line, is 67.5 (in the odd units of kilometers per second per megaparsec).</p> <p>Let's plot the best fit line on our graph.</p> In\u00a0[\u00a0]: Copied! <pre># Predict the model on the data\ny_pred = \n\n\n# Add the linear fit to the scatter plot\n</pre> # Predict the model on the data y_pred =    # Add the linear fit to the scatter plot  In\u00a0[\u00a0]: Copied! <pre># Predict the velocity at 350 Mpc\ny_350 = \nprint('Predicted velocity at 350 Mpc:',y_350)\n</pre> # Predict the velocity at 350 Mpc y_350 =  print('Predicted velocity at 350 Mpc:',y_350) <p>And use it, to get what is normally called the $R^2$ value, which when 1. represents perfect agreement.</p> <p>Pearson correlation coefficient between several example X,Y sets. Source: https://en.wikipedia.org/wiki/Correlation_and_dependence</p> In\u00a0[\u00a0]: Copied! <pre># Calculate the R^2 value of the model; and Print the R^2 value\nss_res =\nss_tot =\nr2 = \n\nprint('R^2:',r2)\n</pre> # Calculate the R^2 value of the model; and Print the R^2 value ss_res = ss_tot = r2 =   print('R^2:',r2) <p>Not a bad fit!  We can have confidence that there is a strong correlation between distance and velocity. The universe is expanding.</p> In\u00a0[\u00a0]: Copied! <pre># Calculate the residuals of the model predictions\nres = \n</pre> # Calculate the residuals of the model predictions res =  In\u00a0[\u00a0]: Copied! <pre># Create a scatter plot of the residuals\n</pre> # Create a scatter plot of the residuals  <p>The residual plot of a good regression shows no pattern. The residuals look about the same, above and below the horizontal line at 0, across the range of the predictor variable.</p> In\u00a0[\u00a0]: Copied! <pre># Extract the Hubble constant H0 from the model\nH0 =\n</pre> # Extract the Hubble constant H0 from the model H0 = <p>Write a function that takes in a Hubble constant value and calculates the age of the Universe in billions of year</p> In\u00a0[\u00a0]: Copied! <pre># Complete the function to calculate the age of the universe\ndef age_of_universe(Hubble_constant):\n\n    \n    return age_byr\n\nprint(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\")\n</pre> # Complete the function to calculate the age of the universe def age_of_universe(Hubble_constant):           return age_byr  print(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\") <p>Exercises:</p> <ul> <li>Import the 'Data/Freedman2000_IBandTullyFisher.csv' file.</li> </ul> In\u00a0[18]: Copied! <pre># Load the data/Freedman2000_IBandTullyFisher.csv file\ndata = \ndata.head()\n</pre> # Load the data/Freedman2000_IBandTullyFisher.csv file data =  data.head() <ul> <li>Make a linear fit to determine the slope between <code>VCMB</code> and <code>D(Mpc)</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Fit a linear regression model to the data\n</pre> # Fit a linear regression model to the data <ul> <li>Calculate the implied age of the universe from these TF galaxy data alone. Reuse the function you wrote above to calculate the age of the universe.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>H0 = \nprint(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\")\n</pre> H0 =  print(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\") In\u00a0[\u00a0]: Copied! <pre># Load the data/mu_z.csv file; Hint: use the header=1 option to skip the first row\nBetoule_data = \nBetoule_data.head()\n</pre> # Load the data/mu_z.csv file; Hint: use the header=1 option to skip the first row Betoule_data =  Betoule_data.head() <p>Now we can plot it the same way as the cosmologists did in the paper, using $\\mu$ and $\\log z$:</p> In\u00a0[\u00a0]: Copied! <pre># Create a scatter plot of the data with z on the x-axis and mu on the y-axis\n</pre> # Create a scatter plot of the data with z on the x-axis and mu on the y-axis  <p>To compare these new data with the previous considered data, we must do the following:</p> <ul> <li>Transform $z$  to velocity</li> <li>Transform $\\mu$ to distance using the equations provided.</li> <li>Truncate the new dataset which goes to much farther distances than the 'old' data set</li> </ul> In\u00a0[23]: Copied! <pre># speed of light in km/s\nc = 2.9979e8 / 1000 \n\n# the formula for v from z (and c)\nBetoule_data['velocity'] = c * (((Betoule_data['z']+1.)**2-1.)/((Betoule_data['z']+1.)**2+1.)) \n\n# convert mu to Gpc\nBetoule_data['distance'] = 10000*(10.**((Betoule_data['mu'])/5.))*1e-9\n</pre> # speed of light in km/s c = 2.9979e8 / 1000   # the formula for v from z (and c) Betoule_data['velocity'] = c * (((Betoule_data['z']+1.)**2-1.)/((Betoule_data['z']+1.)**2+1.))   # convert mu to Gpc Betoule_data['distance'] = 10000*(10.**((Betoule_data['mu'])/5.))*1e-9 In\u00a0[\u00a0]: Copied! <pre>## Create a scatter plot of the Betoule data and the Supernova data on the same plot\n</pre> ## Create a scatter plot of the Betoule data and the Supernova data on the same plot  <p>These data sets are similar to one another for the \"close\" objects, but we can see that a linear model doesn't work well for objects that are at greater distances.</p> <p>To visualize this reality, let's plot the fit to the Freedman et al. 2000 data atop this plot.</p> In\u00a0[\u00a0]: Copied! <pre># Fit a linear regression model to the Supernova data using Supernova_data[['D(Mpc)']] as the X values and Supernova_data['VCMB'] as the y values\n</pre> # Fit a linear regression model to the Supernova data using Supernova_data[['D(Mpc)']] as the X values and Supernova_data['VCMB'] as the y values  In\u00a0[\u00a0]: Copied! <pre># Calculate the residuals of the model predictions for the Betoule data\n</pre> # Calculate the residuals of the model predictions for the Betoule data  In\u00a0[\u00a0]: Copied! <pre># Add the linear fit to the scatter plot; And create a scatter plot of the residuals\n# Hint: use plt.subplot(2,1,1) to create a 2x1 plot\n</pre> # Add the linear fit to the scatter plot; And create a scatter plot of the residuals # Hint: use plt.subplot(2,1,1) to create a 2x1 plot  <p>Clearly this fit is quite poor.</p> <p>Let's make a first-order polynomial fit to all the Betoule data and then plot the residual:</p> In\u00a0[\u00a0]: Copied! <pre># Fit a linear regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values\n</pre> # Fit a linear regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values  In\u00a0[\u00a0]: Copied! <pre># Calculate the residuals of the model predictions for the Betoule data\n</pre> # Calculate the residuals of the model predictions for the Betoule data  In\u00a0[3]: Copied! <pre># Add the linear fit to the scatter plot and create a scatter plot of the residuals\n</pre> # Add the linear fit to the scatter plot and create a scatter plot of the residuals  <p>There is a lot of structure to the residual of this degree 1 fit. Let's try a degree 2 polynomial fit (known as quadratic):</p> <p>$f(x)=ax^2+bx+c$</p> In\u00a0[\u00a0]: Copied! <pre># Fit a polynomial regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values\n# Use a polynomial degree of 2\n</pre> # Fit a polynomial regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values # Use a polynomial degree of 2  In\u00a0[\u00a0]: Copied! <pre># Calculate the residuals of the model predictions for the Betoule data\n</pre> # Calculate the residuals of the model predictions for the Betoule data  In\u00a0[\u00a0]: Copied! <pre># Add the polynomial fit to the scatter plot and create a scatter plot of the residuals\n</pre> # Add the polynomial fit to the scatter plot and create a scatter plot of the residuals  <p>There is a lot of structure to the residuals of this degree 2 fit (and the residuals are still high). Let's try a degree 3 polynomial fit (known as cubic):</p> <p>$f(x)=ax^3+bx^2+cx+d$</p> In\u00a0[\u00a0]: Copied! <pre># Fit a polynomial regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values\n# Use a polynomial degree of 3\n</pre> # Fit a polynomial regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values # Use a polynomial degree of 3  In\u00a0[\u00a0]: Copied! <pre># Calculate the residuals of the model predictions for the Betoule data\n</pre> # Calculate the residuals of the model predictions for the Betoule data  In\u00a0[\u00a0]: Copied! <pre># Add the polynomial fit to the scatter plot and create a scatter plot of the residuals\n</pre> # Add the polynomial fit to the scatter plot and create a scatter plot of the residuals  <p>Can a degree 4 polynomial fit do better?</p> <p>$f(x)=ax^4+bx^3+cx^2+dx+e$</p> In\u00a0[\u00a0]: Copied! <pre># Fit a polynomial regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values\n# Use a polynomial degree of 4\n</pre> # Fit a polynomial regression model to the Betoule data using Betoule_data[['distance']] as the X values and Betoule_data['velocity'] as the y values # Use a polynomial degree of 4  In\u00a0[\u00a0]: Copied! <pre># Calculate the residuals of the model predictions for the Betoule data\n</pre> # Calculate the residuals of the model predictions for the Betoule data  In\u00a0[\u00a0]: Copied! <pre># Add the polynomial fit to the scatter plot and create a scatter plot of the residuals\n</pre> # Add the polynomial fit to the scatter plot and create a scatter plot of the residuals  <p>That looks about the same as the cubic so might as well stick with that one as a working model.</p> <p>That the velocity-distance relationship is not linear is taken as evidence that the expansion of the universe is accelerating. This acceleration is attributed to dark energy:</p> <p>In a matter-dominated universe, the expansion velocity of the Universe slows down over time owing to the attractive force of gravity. However, a decade ago two independent groups (Perlmutter et al. 1999, Riess et al. 1998) found that supernovae at z \u223c 0.5 appear to be about 10% fainter than those observed locally, consistent instead with models in which the expansion velocity is increasing; that is, a universe that is accelerating in its expansion. Combined with independent estimates of the matter density, these results are consistent with a universe in which one-third of the overall density is in the form of matter (ordinary plus dark), and two-thirds is in a form having a large, negative pressure, termed dark energy. Freedman and Madore (2010)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#regression-and-the-age-of-the-universe","title":"Regression and the age of the universe\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#import-scientific-python-packages","title":"Import scientific python packages\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#bivariate-data","title":"Bivariate data\u00b6","text":"<p>There are many examples in Earth and Planetary Science where we are interested in the dependence of one set of data on another (bivariate data), such as the distance of the last geomagnetic reversal from the ridge crest to get spreading rate and the difference in arrival times of the $P$ and $S$ seismic waves, which is related to distance from the source to the receiver.</p> <p>Today we will be focused on methods that allow us to investigate potential associations and relationships between variables. And using a classic problem from astrophysics to do so. The inspiration for this exercise came from Lecture 16 of Lisa Tauxe's Python for Earth Science Students class and some of the material is modified from those materials (https://github.com/ltauxe/Python-for-Earth-Science-Students).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#age-of-the-universe","title":"Age of the universe\u00b6","text":"<p>Today, we will focus on using the retreat velocity of galaxies and supernova as a function of their distance as our example data set. Such data underlies what has come to be known as \"Hubble's Law\" (same Hubble as for the Hubble telescope). Hubble published these results in 1929 [Hubble, E. P. (1929) Proc. Natl. Acad. Sci., 15, 168\u2013173.]</p> <p>At the time,  it was unclear whether the universe was static, expanding, or collapsing. Hubble hypothesized that if the universe were expanding, then everything in it would be moving away from us. The greater the distance between the Earth and the galaxy, the faster it must be moving.  So all that had to be done was to measure the distance and velocity of distant galaxies.  Easy-peasy - right?</p> <p>To measure velocity, Hubble made use of the doppler shift. To understand how this works, recall that the pitch you hear as an ambulance approaches changes. During doppler shift, the ambulance's pitch changes from high (as it approaches) to low (as it recedes). The pitch changes  because the relative frequency of the sound waves changes. The frequency increases as the ambulance approaches, leading to a higher pitch, and then decreases as it moves away, resulting in a lower pitch.</p> <p>Just in case you haven't had this life experience, let's listen to such a siren here: https://www.youtube.com/watch?v=imoxDcn2Sgo</p> <p>The same principle applies to light, but rather than hear a change in frequency, we observe a shift in the wavelength (the color) emitted by the galaxy. If a star or galaxy is moving away from us, its absorption bands are shifted towards longer wavelengths - the red end of the visible spectrum. The faster the star or galaxy travels away from the observer, the greater the shift will be to the red:</p> <p>So a star (or galaxy) moving away from us will have a red shift with the wavelength being spread out.</p> <p>Figures from http://www.a-levelphysicstutor.com/wav-doppler.php</p> <p>Hubble measured the red shift of different galaxies and converted them to velocities. He then estimated the distance to these objects, which is harder to do (and he was pretty far off).</p> <p>Improving such data was a major motivation of the Hubble Space Telescope. Those data and continued improvement to approaches for estimating these distances and velocities and investigating additional types of celestial objects is a major focus of ongoing research.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#type-1a-supernovae-data","title":"Type 1a supernovae data\u00b6","text":"<p>Let's import data from Freedman et al. (2000) of the distance and retreat velocity of type 1a supernovae. These supernovae are described as follows in a review paper that Freedman wrote in 2010 (https://doi.org/10.1146/annurev-astro-082708-101829):</p> <p>One of the most accurate means of measuring cosmological distances out into the Hubble flow utilizes the peak brightness of SNe Ia. The potential of supernovae for measuring distances was clear to early researchers (e.g., Baade, Minkowski, Zwicky), but it was the Hubble diagram of Kowal (1968) that set the modern course for this field, followed by decades of work by Sandage, Tammann, and collaborators (e.g., Sandage &amp; Tammann 1982, 1990; see also the review by Branch 1998). Analysis by Pskovskii (1984), followed by Phillips (1993), established a correlation between the magnitude of a SN Ia at peak brightness and the rate at which it declines, thus allowing supernova luminosities to be \u201cstandardized.\u201d This method currently probes farthest into the unperturbed Hubble flow, and it possesses very low intrinsic scatter: Freedman and Madore (2010) who then go onto describe how using Cepheid variable stars (a type of pulsating star) has allowed for the distances to be better calibrated.</p> <p>SNe Ia result from the thermonuclear runaway explosions of stars. From observations alone, the presence of SNe Ia in elliptical galaxies suggests that they do not come from massive stars. Many details of the explosion are not yet well understood, but the generally accepted view is that of a carbon-oxygen, electron-degenerate, nearly-Chandrasekharmass white dwarf orbiting in a binary system with a close companion Freedman and Madore (2010)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#determining-the-slope-of-this-line-the-hubble-constant","title":"Determining the slope of this line (the Hubble constant)\u00b6","text":"<p>We have distance on the x-axis in megaparsecs and velocity on the y-axis in km/s. The slope of this line is the Hubble constant:</p> <p>$v = H_o d$</p> <p>where $v$ is velocity, $d$ is distance, and $H_o$ is the Hubble constant.</p> <p>This looks a lot like the equation for a line through the data ($y=Ax + b$) where $A$ is the slope and $b$ is the y-intercept.  In this case, the y-intercept should be 0 or nearly so, and $m$ is $H_o$.</p> <p>So how do we find the slope?</p> <p>Here is where we can use linear regression to find the \"best fit\" line through the data. The approach is to minimize the sum of the squares of the distances (residuals) between the points and a line through them. In this illustration below, the residuals are the vertical distance between each data point and the line:</p> <p>The approach in linear regression is to find the line that minimizes the squared value of these distances all added up.</p> <p>We determine the best-fit line through this least squares approach using scikit-learn.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#fitting-a-line-with-scikit-learn","title":"Fitting a line with scikit-learn\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#using-this-linear-model-for-prediction","title":"Using this linear model for prediction\u00b6","text":"<p>What would we predict that the velocity would be for a supernova that happened to be 350 Mpc?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#evaluating-model-fit","title":"Evaluating model fit\u00b6","text":"<p>We'd also like to know who well this model fits our data (i.e. how correlated the data are). We'll use the $R^{2}$ correlation coefficient for this. $R^{2}$ is zero for uncorrelated data, and 1 for perfectly linear data (so no misfit between the model line and data).</p> <p>Review how to calculate $R^{2}$:</p> <p>$R^{2} = 1 - \\frac{SS_{res}}{SS_{tot}}$</p> <p>where $SS_{res}$ is the sum of the squares of the residuals $SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$,</p> <p>and $SS_{tot}$ is the total sum of the squares of the data $SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#evaluting-the-fit-through-plotting-residuals","title":"Evaluting the fit through plotting residuals\u00b6","text":"<p>To see how well the regression performs, the data scientist must measure how far off the estimates are from the actual values. These differences are called residuals.</p> <p>$$ \\epsilon_i = y_i - \\hat{y}_i $$</p> <p>where $\\epsilon_i$ is the residual for the $i$-th data point, $y_i$ is the observed value, and $\\hat{y}_i$ is the regression estimate.</p> <p>A residual is what's left over \u2013 the residue \u2013 after estimation.</p> <p>Residuals are the vertical distances of the points from the regression line. There is one residual for each point in the scatter plot. The residual is the difference between the observed value of $y$ and the fitted value of $y$, so for the point $(x, y)$,</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#estimating-the-age-of-the-universe","title":"Estimating the age of the universe\u00b6","text":"<p>To calculate the age of the universe, we can use Hubble's law:</p> <p>We had $v=H_o d$ as Hubble's law and we know that distance = velocity x time, or,  $d=vt$.  So, if we divide both sides by $v$ and  we get:</p> <p>$1 = H_o t$.</p> <p>Solving for $t$ (the age of the universe), we get</p> <p>$t=1/H_o$ [in some weird units.]</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#using-other-data-sets-to-estimate-the-hubble-constant","title":"Using other data sets to estimate the Hubble constant\u00b6","text":"<p>Determining the Hubble constant continues to be a major avenue of astrophysical research. In fact, Wendy Freedman's group published another study (https://arxiv.org/abs/1907.05922) that is summarized in this short video:</p> <p>https://www.youtube.com/watch?v=awcnVykOKZY</p> <p>From that paper here is a visualization of Hubble constant determinations over the past 18 years:</p> <p>Let's look at another data set from the 2000 study to see how different data sets can lead to different answers.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#tully-fisher-relation-galaxy-data","title":"Tully-Fisher Relation galaxy data\u00b6","text":"<p>The total luminosity of a spiral galaxy (corrected to face-on inclination to account for extinction) is strongly correlated with the galaxy\u2019s maximum (corrected to edge-on inclination) rotation velocity. This relation, calibrated via the Leavitt Law or TRGB, becomes a powerful means of determining extragalactic distances (Tully&amp;Fisher 1977, Aaronson et al. 1986, Pierce&amp;Tully 1988, Giovanelli et al. 1997). The TF relation at present is one of the most widely applied methods for distance measurements Freedman and Madore (2010)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/04_regression/#going-even-further-out-into-the-universe","title":"Going even further out into the universe\u00b6","text":"<p>Let's look at new data sets available for the classic Hubble problem.  I found one published by Betoule et al. in 2014 http://dx.doi.org/10.1051/0004-6361/201423413.</p> <p>In this paper, data are plotted using the parameters $z$ and $\\mu$ which are related to the red shift velocity and distance.  $z$ is the fractional shift in the spectral wavelength and $\\mu$ is related to distance.</p> <p>Here is a plot from the Betoule et al. paper:</p> <p>[Figure from Betoule et al., 2014.]  These data are type Ia supernova from different observation collaborations</p> <p>Notice that they plotted the data on a log scale. (This hides some surprising things.)</p> <p>It turns out that we have been looking at data that are low-z (that is relatively close and low red shift). We  need to convert $z$ and $\\mu$ to distance and velocity to compare to the results we have considered thus far.</p> <p>According to http://hyperphysics.phy-astr.gsu.edu/hbase/Astro/hubble.html</p> <p>velocity $v$ (as fraction of the speed of light, $c$) is given by</p> <p>${v\\over c}= {{(z+1)^2-1}  \\over {(z+1)^2+1}}$</p> <p>where $c=3 \\times 10^8$ $m s^{-1}$.</p> <p>And according to the Betoule et al. (2014) paper, $\\mu$ relates to distance in parsecs $d$ like this:</p> <p>$\\mu = 5 \\log \\frac{d}{10}$</p> <p>Let's read in the data (available from this website:  http://cdsarc.u-strasbg.fr/viz-bin/qcat?J/A+A/568/A22#sRM2.2), which are averages of the data shown in the figure above,and take a peek.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/","title":"05 classification","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap In\u00a0[\u00a0]: Copied! <pre>## Load the file data/Vermeesch2006.csv of the basalt data into a pandas DataFrame\nbasalt_data = \n</pre> ## Load the file data/Vermeesch2006.csv of the basalt data into a pandas DataFrame basalt_data =  In\u00a0[29]: Copied! <pre>## Take a look at the first few rows of the DataFrame\n</pre> ## Take a look at the first few rows of the DataFrame  In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 6))\n\n## Loop through the unique values of the 'affinity' column to plot each group\n\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> plt.figure(figsize=(8, 6))  ## Loop through the unique values of the 'affinity' column to plot each group   plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() In\u00a0[\u00a0]: Copied! <pre>## Use the groupby and describe methods to get a summary of the TiO2 content for each affinity group\n</pre> ## Use the groupby and describe methods to get a summary of the TiO2 content for each affinity group  <p>Try the other column of 'V_ppm'</p> In\u00a0[\u00a0]: Copied! <pre>## Use the groupby and describe methods to get a summary of the V content for each affinity group\n</pre> ## Use the groupby and describe methods to get a summary of the V content for each affinity group  <p>Can you differentiate between the different affinities on titanium or vanadium concentration alone?</p> <p>Let's plot the data on the TiO2 vs V plot and see if we can visually classify the data.</p> In\u00a0[7]: Copied! <pre>sample1 = {'TiO2_wt_percent': 4, 'V_ppm': 300}\nsample2 = {'TiO2_wt_percent': 1, 'V_ppm': 350}\nsample3 = {'TiO2_wt_percent': 1.6, 'V_ppm': 280}\n</pre> sample1 = {'TiO2_wt_percent': 4, 'V_ppm': 300} sample2 = {'TiO2_wt_percent': 1, 'V_ppm': 350} sample3 = {'TiO2_wt_percent': 1.6, 'V_ppm': 280} In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 6))\n\nfor affinity in basalt_data['affinity'].unique():\n    subset = basalt_data[basalt_data['affinity'] == affinity]\n    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n\n## Add the unknown samples to the plot using different markers and labels\n\n\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(8, 6))  for affinity in basalt_data['affinity'].unique():     subset = basalt_data[basalt_data['affinity'] == affinity]     plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)  ## Add the unknown samples to the plot using different markers and labels   plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.legend() plt.show() In\u00a0[9]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n</pre> from sklearn.linear_model import LogisticRegression In\u00a0[10]: Copied! <pre>## Define the classifier using the LogisticRegression class from scikit-learn. Hint: use the 'liblinear' solver\nclassifier_linear = \n</pre> ## Define the classifier using the LogisticRegression class from scikit-learn. Hint: use the 'liblinear' solver classifier_linear =  In\u00a0[11]: Copied! <pre>## Let's create a new DataFrame with only the samples that have both TiO2 and V data available. Hint: use the .notna() method\nbasalt_data_Ti_V = \n</pre> ## Let's create a new DataFrame with only the samples that have both TiO2 and V data available. Hint: use the .notna() method basalt_data_Ti_V =  In\u00a0[\u00a0]: Copied! <pre>## Print the number of samples in the original DataFrame basalt_data and the new DataFrame basalt_data_Ti_V\n</pre> ## Print the number of samples in the original DataFrame basalt_data and the new DataFrame basalt_data_Ti_V  <p>In machine learning literature and code conventions, uppercase \"X\" is often used to represent the matrix of feature variables (predictors), while lowercase \"y\" is used to represent the target variable (response).</p> <p>This notation is used to visually distinguish between the two variables and indicate that \"X\" is a matrix (usually with multiple columns for features), while \"y\" is a vector (usually with a single column representing the target variable).</p> <p>The uppercase \"X\" signifies a multi-dimensional data structure, and the lowercase \"y\" signifies a one-dimensional data structure.</p> In\u00a0[13]: Copied! <pre>## Let's define X as the features of ['TiO2_wt_percent', 'V_ppm'] and y as the target variable 'affinity'\nX = \ny = \n</pre> ## Let's define X as the features of ['TiO2_wt_percent', 'V_ppm'] and y as the target variable 'affinity' X =  y =  <p>The categorical variables that represent different categories that we have here are: 'MORB', 'IAB', and 'OIB'. However, most machine learning algorithms require numerical inputs.</p> <p>Label encoding is a technique that transforms the categorical variables into numerical labels. We can use the <code>sklearn.preprocessing</code> <code>LabelEncoder</code> function to do this task for us.</p> In\u00a0[14]: Copied! <pre>from sklearn.preprocessing import LabelEncoder\n\n## let's encode the target variable y using the LabelEncoder class from scikit-learn\nle = LabelEncoder()\ny_encoded = \n</pre> from sklearn.preprocessing import LabelEncoder  ## let's encode the target variable y using the LabelEncoder class from scikit-learn le = LabelEncoder() y_encoded =  <p>Let's take a look at the original 'affinity' categories and the encoded categories.</p> In\u00a0[\u00a0]: Copied! <pre>## Print the original and encoded affinity values. Hint: use the np.unique() function\n</pre> ## Print the original and encoded affinity values. Hint: use the np.unique() function  In\u00a0[\u00a0]: Copied! <pre>## Fit the classifier using the fit method with X and y_encoded as arguments\n</pre> ## Fit the classifier using the fit method with X and y_encoded as arguments  In\u00a0[\u00a0]: Copied! <pre># Generate a grid of points over the feature space\nxx, yy = np.meshgrid(np.linspace(0, max(basalt_data_Ti_V['TiO2_wt_percent']), 101),\n                     np.linspace(0, max(basalt_data_Ti_V['V_ppm']), 101))\ngrid = np.array([xx.ravel(), yy.ravel()]).T\n\nplt.figure(figsize=(4, 3))\nplt.scatter(xx, yy, s=1)\nplt.tight_layout()\n</pre> # Generate a grid of points over the feature space xx, yy = np.meshgrid(np.linspace(0, max(basalt_data_Ti_V['TiO2_wt_percent']), 101),                      np.linspace(0, max(basalt_data_Ti_V['V_ppm']), 101)) grid = np.array([xx.ravel(), yy.ravel()]).T  plt.figure(figsize=(4, 3)) plt.scatter(xx, yy, s=1) plt.tight_layout() In\u00a0[18]: Copied! <pre>grid_classes = classifier_linear.predict(grid)\n</pre> grid_classes = classifier_linear.predict(grid) <p>We can now plot up those grid with the actual data</p> In\u00a0[\u00a0]: Copied! <pre>grid_classes = grid_classes.reshape(xx.shape)\ncmap = ListedColormap(['C2', 'C0', 'C1']) # Define the colors to make the decision boundary and the data points with the same colors\n\nplt.figure(figsize=(8, 6))\n\n## Plot the decision boundary; Hint: use the plt.contourf() function\n\n## Add real data points\n\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> grid_classes = grid_classes.reshape(xx.shape) cmap = ListedColormap(['C2', 'C0', 'C1']) # Define the colors to make the decision boundary and the data points with the same colors  plt.figure(figsize=(8, 6))  ## Plot the decision boundary; Hint: use the plt.contourf() function  ## Add real data points  plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <p>We can now plot the unknown points onto this classified grid and see what their assignment would be.</p> In\u00a0[\u00a0]: Copied! <pre>cmap = ListedColormap(['C2', 'C0', 'C1'])\ngrid_classes = grid_classes.reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\n\n## Plot the decision boundary; Hint: use the plt.contourf() function\n\n## Add real data points\n\n## Add the unknow points\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> cmap = ListedColormap(['C2', 'C0', 'C1']) grid_classes = grid_classes.reshape(xx.shape)  plt.figure(figsize=(8, 6))  ## Plot the decision boundary; Hint: use the plt.contourf() function  ## Add real data points  ## Add the unknow points  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <p>While we can visually see where the points fall, we can also ask the classifier to predict the values of these unknown points using <code>classifier_svc_linear.predict()</code>.</p> <p>We can return the actual labels of the data (rather than the encoded numbers) by using <code>le.inverse_transform()</code></p> In\u00a0[\u00a0]: Copied! <pre>## Predict the class of the unknown points\nprediction_point1_encoded = \nprediction_point2_encoded = \nprediction_point3_encoded = \n\n## Decode the predicted classes into the original labels; Hint: use the inverse_transform method of the LabelEncoder class\nprediction_point1 = \nprediction_point2 = \nprediction_point3 = \n\n## Print the results\nprint(f\"Sample 1 {sample1} is classified as class {prediction_point1_encoded}, which is {prediction_point1}\")\nprint(f\"Sample 2 {sample2} is classified as class {prediction_point2_encoded}, which is {prediction_point2}\")\nprint(f\"Sample 3 {sample3} is classified as class {prediction_point3_encoded}, which is {prediction_point3}\")\n</pre> ## Predict the class of the unknown points prediction_point1_encoded =  prediction_point2_encoded =  prediction_point3_encoded =   ## Decode the predicted classes into the original labels; Hint: use the inverse_transform method of the LabelEncoder class prediction_point1 =  prediction_point2 =  prediction_point3 =   ## Print the results print(f\"Sample 1 {sample1} is classified as class {prediction_point1_encoded}, which is {prediction_point1}\") print(f\"Sample 2 {sample2} is classified as class {prediction_point2_encoded}, which is {prediction_point2}\") print(f\"Sample 3 {sample3} is classified as class {prediction_point3_encoded}, which is {prediction_point3}\") In\u00a0[22]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n</pre> from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score In\u00a0[23]: Copied! <pre>## Let's split the data into training and testing sets using the train_test_split function from scikit-learn with a test size of 30%\nX_train, X_test, y_train, y_test = \n</pre> ## Let's split the data into training and testing sets using the train_test_split function from scikit-learn with a test size of 30% X_train, X_test, y_train, y_test =  In\u00a0[\u00a0]: Copied! <pre>## Print the number of samples in the training and testing sets\n</pre> ## Print the number of samples in the training and testing sets  In\u00a0[\u00a0]: Copied! <pre>## Fit the classifier using the training data\n</pre> ## Fit the classifier using the training data  In\u00a0[\u00a0]: Copied! <pre>## Make predictions on the test set\ny_pred = \n</pre> ## Make predictions on the test set y_pred =  In\u00a0[\u00a0]: Copied! <pre>## Calculate the accuracy using the accuracy_score function\naccuracy = \nprint(f\"The accuracy of the classifier is {accuracy:.2f}\")\n</pre> ## Calculate the accuracy using the accuracy_score function accuracy =  print(f\"The accuracy of the classifier is {accuracy:.2f}\") <p>The above <code>accuracy_score</code> computes the proportion of correct predictions out of the total number of predictions. The accuracy score ranges from 0 to 1, where a higher score indicates better classification performance.</p> <p>We can make a plot that is the classification based on the training data and can then plot the test data. The fraction of points that are plotting within the correct classification corresponds to the accuracy score.</p> In\u00a0[\u00a0]: Copied! <pre>grid_classes = classifier_linear.predict(grid)\ngrid_classes = grid_classes.reshape(xx.shape)\n\ncmap = ListedColormap(['C2', 'C0', 'C1'])\n\nplt.figure()\nplt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolor='k', s=50)\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> grid_classes = classifier_linear.predict(grid) grid_classes = grid_classes.reshape(xx.shape)  cmap = ListedColormap(['C2', 'C0', 'C1'])  plt.figure() plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6) plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolor='k', s=50) plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <p>How well did the linear classifier do?</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n## Calculate the confusion matrix using the confusion_matrix function\ncm =\n\nfig, ax = plt.subplots(figsize=(4, 4))\nclass_names = le.inverse_transform(np.unique(y_test)).astype(str)\n\n## Plot the confusion matrix using ConfusionMatrixDisplay\ndisp = \ndisp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d', colorbar=False)\n\nax.set_title('Confusion Matrix')\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nplt.show()\n</pre> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  ## Calculate the confusion matrix using the confusion_matrix function cm =  fig, ax = plt.subplots(figsize=(4, 4)) class_names = le.inverse_transform(np.unique(y_test)).astype(str)  ## Plot the confusion matrix using ConfusionMatrixDisplay disp =  disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d', colorbar=False)  ax.set_title('Confusion Matrix') ax.set_xlabel('Predicted') ax.set_ylabel('True') plt.show()  <p>Can you interpret the confusion matrix? What does it tell you about the performance of the classifier?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#introduction-to-machine-learning-classification-of-basalt-source","title":"Introduction to machine learning: classification of basalt source\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#import-scientific-python-libraries","title":"Import scientific python libraries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#machine-learning","title":"Machine learning\u00b6","text":"<p>Text from: https://scikit-learn.org/stable/tutorial/basic/tutorial.html</p> <p>In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.</p> <p>Learning problems fall into a few categories:</p> <ul> <li><p>supervised learning, in which the data comes with additional attributes that we want to predict (https://scikit-learn.org/stable/supervised_learning.html). This problem can be either:</p> <ul> <li>regression: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.</li> <li>classification: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of a classification problem would be handwritten digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.</li> </ul> </li> <li><p>unsupervised learning, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization (https://scikit-learn.org/stable/unsupervised_learning.html).</p> </li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#training-set-and-testing-set","title":"Training set and testing set\u00b6","text":"<p>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the training set, on which we learn some properties; we call the other set the testing set, on which we test the learned properties.</p> <p>Today we will focus on classification through a supervised learning approach</p> <p>Systems doing this type of analysis are all around us. Consider a spam filter for example</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#classifying-volcanic-rocks","title":"Classifying volcanic rocks\u00b6","text":"<p>Today we are going to deal with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a week ago. During class today, we will focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).</p> <p>Igneous rocks form in a wide variety of tectonic settings, including mid-ocean ridges, ocean islands, and volcanic arcs. It is a problem of great interest to igneous petrologists to recover the original tectonic setting of mafic rocks of the past. When the geological setting alone cannot unambiguously resolve this question, the chemical composition of these rocks might contain the answer. The major, minor, and trace elemental composition of basalts shows large variations, for example as a function of formation depth (e.g., Kushiro and Kuno, 1963) --- Vermeesch (2006)</p> <p>For this analysis, we are going to use a dataset that was compiled in</p> <p>Vermeesch (2006) Tectonic discrimination of basalts with classification trees, Geochimica et Cosmochimica Acta https://doi.org/10.1016/j.gca.2005.12.016</p> <p>These data were grouped into 3 categories:</p> <ul> <li>256 Island arc basalts (IAB) from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.</li> <li>241 Mid-ocean ridge (MORB) samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.</li> <li>259 Ocean-island (OIB) samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.</li> </ul> <p>Let's look at the illustration above and determine where each of these settings are within a plate tectonic context</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#import-data","title":"Import data\u00b6","text":"<p>The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#can-geochemical-data-be-used-to-classify-the-tectonic-setting","title":"Can geochemical data be used to classify the tectonic setting?\u00b6","text":"<p>These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?</p> <p>A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. For the moment, you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.</p> <p>Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas Earth and Planetary Science Letters https://doi.org/10.1016/0012-821X(82)90120-0</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#plot-tio2-wt-vs-v-ppm","title":"Plot TiO2 (wt%) vs V (ppm)\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#use-the-pandas-groupby-function-to-group-by-affinity-and-describe-the-values-of-column-tio2_wt_percent","title":"Use the pandas groupby function to group by affinity and describe the values of column 'TiO2_wt_percent'\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#eye-test-classification-method","title":"Eye test classification method\u00b6","text":"<p>In order to classify the basalt into their affinity based on titanium and vanadium concentrations, we can use a classification method.</p> <p>The goal here is to be able to make an inference of what environment an unknown basalt formed in based on comparison to these data.</p> <p>Let's say that we have three points where their affinity is unknown.</p> <ul> <li>point 1 has TiO2 of 4% and V concentration of 300 ppm</li> <li>point 2 has TiO2 of 1% and V concentration of 350 ppm</li> <li>point 3 has TiO2 of 1.9% and V concentration of 200 ppm</li> </ul> <p>What do you think the classification of these three points should be?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#a-linear-classification","title":"A linear classification\u00b6","text":"<p>An approach that has been taken in volcanic geochemistry is to draw lines to use for classification.</p> <p>We are using the package scikit-learn in order to implement such a classification. Scikit-learn is a widely-used Python library for machine learning and data analysis. It provides a wide range of tools for data preprocessing, model selection, and evaluation. Its user-friendly interface and extensive documentation make it a popular choice for researchers, data analysts, and machine learning practitioners.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#import-sci-kit-learn","title":"Import sci-kit learn\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#define-our-classifier","title":"Define our classifier\u00b6","text":"<p>We can use <code>LogisticRegression</code> from scikit-learn as a classifier. The algorithm finds the best straight line, also called a hyperplane, to separate different groups of data.</p> <p>Once the lines have been found they can be used predict the group of new data points based on which side of the line they fall on.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#preparing-the-data-for-classification","title":"Preparing the data for classification\u00b6","text":"<p>We need to do a bit of prep work on the data first as not all of the data have Ti (wt %) and V (ppm) data.</p> <p>Let's define a new dataframe <code>basalt_data_Ti_V</code> that has the rows that contain both values. This will result in us using fewer data than is in the total dataset.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#fittrain-the-classifier","title":"Fit/train the classifier\u00b6","text":"<p>Now that we have <code>X</code> as DataFrame of <code>['TiO2_wt_percent', 'V_ppm']</code> and <code>y_encoded</code> as numerical representation of the categories we can fit the classifier to the data.</p> <p>To do this, we feed the DataFrame of the data and the array of the classification into a <code>.fit</code> function preformed on the classifier object.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#visualizing-the-decision-boundaries","title":"Visualizing the decision boundaries\u00b6","text":"<p>Let's make a 101 x 101 grid of x and y values between 0 and the maximum values.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#classify-the-grid","title":"Classify the grid\u00b6","text":"<p>We can then predict the class labels for each point in the grid.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#training-and-testing","title":"Training and testing\u00b6","text":"<p>How good is our linear classifier? To answer this we'll need to find out how frequently our classifications are correct.</p> <p>Discussion question</p> <p>How should we determine the accuracy of this classification scheme using the data that we already have?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#why-do-we-need-to-set-training-validation-and-test-datasets","title":"Why Do We Need to Set Training, Validation, and Test Datasets?\u00b6","text":"<p>When building a machine learning model, it's crucial to evaluate its performance accurately and ensure it generalizes well to new, unseen data. To achieve this, we typically split our dataset into three parts: training, validation, and test sets.</p> <ul> <li>The training set is used to train the model. The model learns the patterns and relationships within this data. The goal is to minimize the error on the training set by adjusting the model's parameters.</li> <li>The validation set is used to tune the model's hyperparameters. It helps in assessing the model's performance during the training phase and prevents overfitting. By evaluating the model on the validation set, we can choose the best model configuration.</li> <li>The test set is used to evaluate the final model's performance. It provides an unbiased estimate of how well the model will perform on new, unseen data. The test set should only be used once the model is fully trained and all hyperparameters are tuned.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#variance-and-bias-trade-off","title":"Variance and Bias Trade-Off\u00b6","text":"<p>In machine learning, the variance and bias trade-off is a fundamental concept that affects model performance.</p> <ul> <li>Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).</li> <li>Variance refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).</li> </ul> <p>Trade-Off:</p> <ul> <li>High Bias, Low Variance: The model is too simple and cannot capture the underlying patterns of the data (underfitting).</li> <li>Low Bias, High Variance: The model is too complex and captures noise in the training data (overfitting).</li> <li>Optimal Balance: The goal is to find a balance where the model has low bias and low variance, meaning it generalizes well to new data.</li> </ul> <p>By using training, validation, and test sets, we can monitor and adjust our model to achieve this balance, ensuring it performs well on unseen data.</p> <p>Let's try it in our classification problem.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#split-the-data-into-training-and-testing-sets","title":"Split the data into training and testing sets\u00b6","text":"<p>Because we are using a linear classifier, there are not many hyperparameters to tune. For simplicity, we will only divide the data into training and testing sets, using 70% of the data for training and 30% for testing.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#fit-the-model-to-the-training-data","title":"Fit the model to the training data\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#make-predictions-on-the-testing-data","title":"Make predictions on the testing data\u00b6","text":"<p>The test set was held back from training. We can use it to evaluate the model. How often are the categorizations correction? To do this evaluation, we can predict the categories using <code>classifier_svc_linear.predict()</code> and the compare those to the actual labels using <code>accuracy_score()</code></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/05_classification/#visualizing-the-classification-using-a-confusion-matrix","title":"Visualizing the classification using a \"confusion matrix\"\u00b6","text":"<p>A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It visually displays the accuracy of a classifier by comparing its predicted labels against the true labels. The matrix consists of rows and columns that represent the true and predicted classes, respectively. Each cell in the matrix corresponds to the number of samples for a specific combination of true and predicted class labels.</p> <p>The main diagonal of the matrix represents the correctly classified instances, while the off-diagonal elements represent the misclassified instances. By analyzing the confusion matrix, you can gain insights into the performance of the classifier and identify where it gets \"confused.\"</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/","title":"06 classification","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import pandas as pd In\u00a0[2]: Copied! <pre>basalt_data = pd.read_csv('data/Vermeesch2006.csv')\nbasalt_data.tail()\n</pre> basalt_data = pd.read_csv('data/Vermeesch2006.csv') basalt_data.tail() Out[2]: affinity SiO2_wt_percent TiO2_wt_percent Al2O3_wt_percent Fe2O3_wt_percent FeO_wt_percent CaO_wt_percent MgO_wt_percent MnO_wt_percent K2O_wt_percent Na2O_wt_percent P2O5(wt%) La_ppm Ce_ppm Pr_ppm Nd_ppm Sm_ppm Eu_ppm Gd_ppm Tb_ppm Dy_ppm Ho_ppm Er_ppm Tm_ppm Yb_ppm Lu_ppm Sc_ppm V_ppm Cr_ppm Co_ppm Ni_ppm Cu_ppm Zn_ppm Ga_ppm Rb_ppm Sr_ppm Y_ppm Zr_ppm Nb_ppm Sn_ppm Cs_ppm Ba_ppm Hf_ppm Ta_ppm Pb_ppm Th_ppm U_ppm 143Nd/144Nd 87Sr/86Sr 206Pb/204Pb 207Pb/204Pb 208Pb/204Pb 751 IAB 50.97 0.78 18.86 NaN NaN 10.85 4.71 0.16 0.60 2.38 0.13 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 36.0 34.0 152.0 NaN NaN 7.5 371.0 19.3 56.0 NaN NaN NaN 130.0 NaN NaN NaN NaN NaN NaN NaN 18.82 15.556 38.389 752 IAB 51.00 1.41 17.06 3.80 7.04 9.97 4.96 0.17 0.73 2.56 0.28 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 13.0 395.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70348 NaN NaN NaN 753 IAB 52.56 1.21 17.74 2.28 7.53 10.48 5.57 0.24 0.29 2.27 0.13 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 163.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70362 NaN NaN NaN 754 IAB 52.59 1.50 16.88 2.41 7.90 10.83 4.91 0.26 0.54 1.63 0.08 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 151.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70363 NaN NaN NaN 755 IAB 52.96 1.27 15.65 2.91 9.32 9.78 4.24 0.23 0.46 2.54 0.15 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 254.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70352 NaN NaN NaN In\u00a0[3]: Copied! <pre>print(basalt_data.columns)\n</pre> print(basalt_data.columns) <pre>Index(['affinity', 'SiO2_wt_percent', 'TiO2_wt_percent', 'Al2O3_wt_percent',\n       'Fe2O3_wt_percent', 'FeO_wt_percent', 'CaO_wt_percent',\n       'MgO_wt_percent', 'MnO_wt_percent', 'K2O_wt_percent', 'Na2O_wt_percent',\n       'P2O5(wt%)', 'La_ppm', 'Ce_ppm', 'Pr_ppm', 'Nd_ppm', 'Sm_ppm', 'Eu_ppm',\n       'Gd_ppm', 'Tb_ppm', 'Dy_ppm', 'Ho_ppm', 'Er_ppm', 'Tm_ppm', 'Yb_ppm',\n       'Lu_ppm', 'Sc_ppm', 'V_ppm', 'Cr_ppm', 'Co_ppm', 'Ni_ppm', 'Cu_ppm',\n       'Zn_ppm', 'Ga_ppm', 'Rb_ppm', 'Sr_ppm', 'Y_ppm', 'Zr_ppm', 'Nb_ppm',\n       'Sn_ppm', 'Cs_ppm', 'Ba_ppm', 'Hf_ppm', 'Ta_ppm', 'Pb_ppm', 'Th_ppm',\n       'U_ppm', '143Nd/144Nd', '87Sr/86Sr', '206Pb/204Pb', '207Pb/204Pb',\n       '208Pb/204Pb'],\n      dtype='object')\n</pre> In\u00a0[4]: Copied! <pre>plt.figure(figsize=(8, 6))\n\nfor affinity in basalt_data['affinity'].unique():\n    subset = basalt_data[basalt_data['affinity'] == affinity]\n    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> plt.figure(figsize=(8, 6))  for affinity in basalt_data['affinity'].unique():     subset = basalt_data[basalt_data['affinity'] == affinity]     plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <pre>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n</pre> In\u00a0[5]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.impute import SimpleImputer\n</pre> from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay from sklearn.impute import SimpleImputer  In\u00a0[6]: Copied! <pre>## Prepare the data to define the features X and the target y\nbasalt_data_Ti_V = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['V_ppm'].isna())]\nX = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values\ny = basalt_data_Ti_V['affinity'].values\n\n## Encode the target variable from string to integer\nle = LabelEncoder()\ny = le.fit_transform(y)\n</pre> ## Prepare the data to define the features X and the target y basalt_data_Ti_V = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['V_ppm'].isna())] X = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values y = basalt_data_Ti_V['affinity'].values  ## Encode the target variable from string to integer le = LabelEncoder() y = le.fit_transform(y)  In\u00a0[7]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = \n\n## Train the model\nmodel = \nmodel.fit(\n\n# Predict the test set\ny_pred = \n\n# Calculate the accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.2f}')\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False)\n</pre> from sklearn.linear_model import LogisticRegression  ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test =   ## Train the model model =  model.fit(  # Predict the test set y_pred =   # Calculate the accuracy accuracy =  print(f'Accuracy: {accuracy:.2f}')  # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False) <pre>Accuracy: 0.85\n</pre> <p>While there is a nice simplicity to the linear classifier approach comparing the $TiO_2$ vs V data, we have a lot more information from other aspects of the geochemistry. We might as well use that information as well.</p> <p>Let's use all the data we can.</p> In\u00a0[8]: Copied! <pre>basalt_data.head(1)\n</pre> basalt_data.head(1) Out[8]: affinity SiO2_wt_percent TiO2_wt_percent Al2O3_wt_percent Fe2O3_wt_percent FeO_wt_percent CaO_wt_percent MgO_wt_percent MnO_wt_percent K2O_wt_percent Na2O_wt_percent P2O5(wt%) La_ppm Ce_ppm Pr_ppm Nd_ppm Sm_ppm Eu_ppm Gd_ppm Tb_ppm Dy_ppm Ho_ppm Er_ppm Tm_ppm Yb_ppm Lu_ppm Sc_ppm V_ppm Cr_ppm Co_ppm Ni_ppm Cu_ppm Zn_ppm Ga_ppm Rb_ppm Sr_ppm Y_ppm Zr_ppm Nb_ppm Sn_ppm Cs_ppm Ba_ppm Hf_ppm Ta_ppm Pb_ppm Th_ppm U_ppm 143Nd/144Nd 87Sr/86Sr 206Pb/204Pb 207Pb/204Pb 208Pb/204Pb 0 MORB 48.2 2.52 15.2 2.31 8.56 9.69 7.15 0.17 0.9 3.79 0.44 18.48 41.19 NaN NaN 6.67 2.02 NaN 1.17 NaN NaN NaN NaN 3.68 0.54 28.8 297.0 196.0 43.0 113.0 44.0 97.0 NaN 13.09 342.0 NaN 289.0 NaN NaN NaN NaN 5.2 NaN 2.15 1.4881 0.4941 NaN 0.703 NaN NaN NaN In\u00a0[9]: Copied! <pre>## Prepare the data into features (X) and target (y)\nX = basalt_data.drop('affinity', axis=1)\ny = basalt_data['affinity']\n\n## We will keep the column names for later use\ncolumns = X.columns\n\n## Encode the target variable\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n## Impute missing values using median imputation\nimputer = \nX = \n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = \n</pre> ## Prepare the data into features (X) and target (y) X = basalt_data.drop('affinity', axis=1) y = basalt_data['affinity']  ## We will keep the column names for later use columns = X.columns  ## Encode the target variable le = LabelEncoder() y = le.fit_transform(y)  ## Impute missing values using median imputation imputer =  X =   ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test =  <p>Let's try to apply the same classification methods to the data using all the features.</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(\n\n# Predict the test set\ny_pred = \n\n# Calculate the accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.2f}')\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> # Train the model model = LogisticRegression(solver=\"liblinear\") model.fit(  # Predict the test set y_pred =   # Calculate the accuracy accuracy =  print(f'Accuracy: {accuracy:.2f}')  # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <p>How do the results compare to the linear logistic regression using only TiO2 and V? Comment on the results below.</p> In\u00a0[10]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n## Define the classifier using make_pipeline\nclassifier = \n\n## Train the model\nclassifier.fit(\n\n## Predict the test set\ny_pred = \n\n## Calculate the accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.2f}')\n\n## Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> from sklearn.svm import SVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler  ## Define the classifier using make_pipeline classifier =   ## Train the model classifier.fit(  ## Predict the test set y_pred =   ## Calculate the accuracy accuracy =  print(f'Accuracy: {accuracy:.2f}')  ## Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.91\n</pre> <p>Let's try to classify the data using a non-linear kernel. We will use the radial basis function (RBF) kernel, which is the default kernel for the <code>SVC</code> class. The RBF kernel is defined as:</p> <p>$$K(x, x') = \\exp(-\\gamma ||x - x'||^2)$$</p> <p>where $\\gamma$ is a hyperparameter that controls the smoothness of the decision boundary. A larger value of $\\gamma$ will result in a more complex decision boundary, which can lead to overfitting. We will use the default value of $\\gamma$ for now, but we can tune it later if needed.</p> In\u00a0[11]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n## Define the classifier using make_pipeline and the RBF kernel\ngamma = 0.03 ## gamma is a hyperparameter of the RBF kernel\nclassifier = \n\n## Train the model\nclassifier.fit(\n\n## Predict the test set\ny_pred = \n\n## Calculate the accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.2f}')\n\n## Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> from sklearn.svm import SVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler  ## Define the classifier using make_pipeline and the RBF kernel gamma = 0.03 ## gamma is a hyperparameter of the RBF kernel classifier =   ## Train the model classifier.fit(  ## Predict the test set y_pred =   ## Calculate the accuracy accuracy =  print(f'Accuracy: {accuracy:.2f}')  ## Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.94\n</pre> <p>Let's try a Decision Trees approach which is another supervised machine learning algorithm for classification</p> <p>Decision Trees are a type of flowchart-like structure where internal nodes represent decisions based on the input features, branches represent the outcome of these decisions, and leaf nodes represent the final output or class label. The primary goal of a decision tree is to recursively split the data into subsets based on feature values that maximize the separation between the classes.</p> <p>Why use Decision Trees?</p> <ul> <li>Easy to understand and interpret: Decision Trees are human-readable and can be visualized, making them easy to understand and interpret even for those with limited machine learning experience.</li> <li>Minimal data preprocessing: Decision Trees do not require extensive data preprocessing, such as scaling or normalization, as they can handle both numerical and categorical features.</li> <li>Non-linear relationships: Decision Trees can model complex, non-linear relationships between features and target variables.</li> <li>Feature importance: Decision Trees can provide insights into feature importance, helping to identify the most relevant features for the problem at hand.</li> </ul> In\u00a0[12]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\n\n## Define the decision tree classifier\nclassifier = \n\n## Train the model\nclassifier.fit(\n\n## Make predictions on the test set\ny_pred = \n\n## Evaluate the classifier\naccuracy = \nprint(f\"Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.tree import DecisionTreeClassifier  ## Define the decision tree classifier classifier =   ## Train the model classifier.fit(  ## Make predictions on the test set y_pred =   ## Evaluate the classifier accuracy =  print(f\"Accuracy: {accuracy:.2f}\") <pre>Accuracy: 0.9118942731277533\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.tree import plot_tree\n\nfig, ax = plt.subplots(figsize=(30, 20))\n\n## Get the original class names\nclass_names = le.inverse_transform(np.unique(y)).astype(str)\n\n## Visualize the decision tree\nplot_tree(classifier, filled=True, feature_names=X.columns, class_names=class_names, ax=ax);\n</pre> from sklearn.tree import plot_tree  fig, ax = plt.subplots(figsize=(30, 20))  ## Get the original class names class_names = le.inverse_transform(np.unique(y)).astype(str)  ## Visualize the decision tree plot_tree(classifier, filled=True, feature_names=X.columns, class_names=class_names, ax=ax); In\u00a0[\u00a0]: Copied! <pre>## Get the feature importances from the classifier\nimportances = classifier.feature_importances_\n\n## Pair the feature names with their corresponding importances\nfeature_importances = list(zip(columns, importances))\n\n## Create a DataFrame from the feature importances\ndf_feature_importances = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])\n\n## Sort the feature importances in descending order\ndf_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)\n\n## Reset the index and drop the old index column\ndf_feature_importances.reset_index(drop=True, inplace=True)\n\n## Display the sorted feature importances\ndisplay(df_feature_importances)\n</pre> ## Get the feature importances from the classifier importances = classifier.feature_importances_  ## Pair the feature names with their corresponding importances feature_importances = list(zip(columns, importances))  ## Create a DataFrame from the feature importances df_feature_importances = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])  ## Sort the feature importances in descending order df_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)  ## Reset the index and drop the old index column df_feature_importances.reset_index(drop=True, inplace=True)  ## Display the sorted feature importances display(df_feature_importances)  <p>If we were going to build a classifier on just two variables, which should be pick? Add your answer to the markdown cell below.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.neighbors import KNeighborsClassifier\n\n## Extract the two most important features from the data; Filter out rows with nan values\nbasalt_data = \n\n## Prepare the data into features (X) and target (y); Encode the target variable\nX = \ny = \n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test =\n\n## Define the KNN classifier with 3 neighbors using make_pipeline with StandardScaler and KNeighborsClassifier \nclassifier = \n\n## Train the model\nclassifier.fit(\n\n## Make predictions on the test set\ny_pred = \n\n## Evaluate the classifier\naccuracy = \nprint(f\"Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.neighbors import KNeighborsClassifier  ## Extract the two most important features from the data; Filter out rows with nan values basalt_data =   ## Prepare the data into features (X) and target (y); Encode the target variable X =  y =   ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test =  ## Define the KNN classifier with 3 neighbors using make_pipeline with StandardScaler and KNeighborsClassifier  classifier =   ## Train the model classifier.fit(  ## Make predictions on the test set y_pred =   ## Evaluate the classifier accuracy =  print(f\"Accuracy: {accuracy:.2f}\") <p>Let's plot the decision boundary for the KNN classifier. The decision boundary is the line that separates the different classes in the feature space.</p> In\u00a0[\u00a0]: Copied! <pre># Create a meshgrid \nx_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\ny_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 20), np.linspace(y_min, y_max, 20))\n\n# Classify the grid points using the classifier\ngrid = np.c_[xx.ravel(), yy.ravel()]\ngrid_classes = classifier.predict(grid)\n\n# Reshape the predicted class labels to match the shape of the input grid\ngrid_classes = grid_classes.reshape(xx.shape)\n\n# Plot the decision boundary and the test data points\ncmap = ListedColormap(['C2', 'C0', 'C1'])\nplt.figure()\nplt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolors='k', marker='o', s=50)\n\n# Add a legend\ncbar = plt.colorbar(ticks=[0.375, 1., 1.625])\ncbar.set_ticklabels(le.inverse_transform([0, 1, 2]))\n\nplt.xlabel('Normalized TiO2_wt_percent')\nplt.ylabel('Normalized Sr_ppm')\nplt.title('Classifier (Test Data with Decision Boundary)')\nplt.show()\n</pre> # Create a meshgrid  x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1 y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 20), np.linspace(y_min, y_max, 20))  # Classify the grid points using the classifier grid = np.c_[xx.ravel(), yy.ravel()] grid_classes = classifier.predict(grid)  # Reshape the predicted class labels to match the shape of the input grid grid_classes = grid_classes.reshape(xx.shape)  # Plot the decision boundary and the test data points cmap = ListedColormap(['C2', 'C0', 'C1']) plt.figure() plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6) plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolors='k', marker='o', s=50)  # Add a legend cbar = plt.colorbar(ticks=[0.375, 1., 1.625]) cbar.set_ticklabels(le.inverse_transform([0, 1, 2]))  plt.xlabel('Normalized TiO2_wt_percent') plt.ylabel('Normalized Sr_ppm') plt.title('Classifier (Test Data with Decision Boundary)') plt.show()"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#introduction-to-machine-learning-classification-of-basalt-source","title":"Introduction to machine learning: classification of basalt source\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#import-scientific-python-libraries","title":"Import scientific python libraries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#classifying-volcanic-rocks","title":"Classifying volcanic rocks\u00b6","text":"<p>Today we will continue to deal with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a week ago. During class today, we will focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).</p> <p>Igneous rocks form in a wide variety of tectonic settings, including mid-ocean ridges, ocean islands, and volcanic arcs. It is a problem of great interest to igneous petrologists to recover the original tectonic setting of mafic rocks of the past. When the geological setting alone cannot unambiguously resolve this question, the chemical composition of these rocks might contain the answer. The major, minor, and trace elemental composition of basalts shows large variations, for example as a function of formation depth (e.g., Kushiro and Kuno, 1963) --- Vermeesch (2006)</p> <p>For this analysis, we are going to use a dataset that was compiled in</p> <p>Vermeesch (2006) Tectonic discrimination of basalts with classification trees, Geochimica et Cosmochimica Acta https://doi.org/10.1016/j.gca.2005.12.016</p> <p>These data were grouped into 3 categories:</p> <ul> <li>256 Island arc basalts (IAB) from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.</li> <li>241 Mid-ocean ridge (MORB) samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.</li> <li>259 Ocean-island (OIB) samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.</li> </ul> <p>Let's look at the illustration above and determine where each of these settings are within a plate tectonic context</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#import-data","title":"Import data\u00b6","text":"<p>The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#can-geochemical-data-be-used-to-classify-the-tectonic-setting","title":"Can geochemical data be used to classify the tectonic setting?\u00b6","text":"<p>These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?</p> <p>A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. For the moment, you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.</p> <p>Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas Earth and Planetary Science Letters https://doi.org/10.1016/0012-821X(82)90120-0</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#plot-tio2-wt-vs-v-ppm","title":"Plot TiO2 (wt%) vs V (ppm)\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#training-and-testing","title":"Training and testing\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#import-more-sklearn-tools","title":"Import more <code>sklearn</code> tools\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#review-linear-logistic-regression","title":"Review: Linear logistic regression\u00b6","text":"<ul> <li>Using only $TiO_2$ and V, we can use logistic regression to classify the data into MORB, OIB, and IAB.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#more-classification-methods","title":"More classification methods\u00b6","text":"<p>Today we will explore a few more classification methods:</p> <ul> <li>Support vector machines</li> <li>Decision trees (Random forests)</li> <li>Nearest neighbors</li> <li>Neural networks</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#preparing-the-data-to-use-features-other-than-tio2-and-v","title":"Preparing the data to use features other than TiO2 and V\u00b6","text":"<ol> <li><p>Encode the target variable 'affinity' using LabelEncoder: The target variable 'affinity' contains categorical data, which needs to be encoded as numerical values for the decision tree classifier. The LabelEncoder from scikit-learn is used to transform the 'affinity' column into numerical labels.</p> </li> <li><p>Split the data into features (X) and target (y): The dataset is split into two parts, features (X) and the target variable (y). Features are the input variables that the classifier will use to make predictions, and the target variable is the output we want the classifier to predict.</p> </li> <li><p>Impute missing values using median imputation: most classifiers cannot handle missing values in the input data. Therefore, missing values in the dataset need to be imputed (filled in) before training the classifier. Let's use median imputation which replaces the missing values with the median of the non-missing values in the same column. We can import and use the <code>SimpleImputer</code> function.</p> </li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#support-vector-machines-svm","title":"Support Vector Machines (SVM)\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#what-is-a-support-vector-machine","title":"What is a support vector machine?\u00b6","text":"<p>Support vector machines are a type of supervised learning model that can be used for both classification and regression. The basic idea is to find the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the support vectors, which are the points that are closest to the hyperplane. The distance between the support vectors and the hyperplane is called the margin. The goal of the SVM is to maximize the margin, which is done by minimizing the norm of the weight vector. The SVM can be used for both linear and non-linear classification by using different kernel functions.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#how-does-the-svm-work","title":"How does the SVM work?\u00b6","text":"<p>Support vector machines work by finding the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the support vectors, which are the points that are closest to the hyperplane. The distance between the support vectors and the hyperplane is called the margin. The goal of the SVM is to maximize the margin, which is done by minimizing the norm of the weight vector. The SVM can be used for both linear and non-linear classification by using different kernel functions.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#what-is-a-kernel-function","title":"What is a kernel function?\u00b6","text":"<p>A kernel function is a function that takes two input vectors and returns a scalar value. The kernel function is used to map the input vectors into a higher-dimensional space where the classes are linearly separable. The most common kernel functions are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#lets-try-a-support-vector-machine-classifier-for-our-data","title":"Let's try a support vector machine classifier for our data\u00b6","text":"<p>First, we will use a linear kernel to classify the data. Same as before, we will use the <code>SVC</code> class from the <code>sklearn.svm</code> module. We will use the <code>fit</code> method to train the model on the training data and the <code>predict</code> method to make predictions on the test data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#decision-tree","title":"Decision tree\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#implement-the-decisiontreeclassifier","title":"Implement the <code>DecisionTreeClassifier</code>\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#plot-the-decision-tree","title":"Plot the decision tree\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#what-aspects-of-the-data-are-important-for-the-classification","title":"What aspects of the data are important for the classification?\u00b6","text":"<p>We want to be able to readily determine what data fields are the most important for the decision tree. We can do that by determining \"feature importance.\" The code below extracts and displays the importance score, also known as Gini importance or Mean Decrease Impurity, which is a measure of how much a feature contributes to the decision-making process of the decision tree model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#k-nearest-neighbors","title":"K-nearest neighbors\u00b6","text":"<p>Let's try another classification method called K-nearest neighbors (KNN) which is a simple and intuitive algorithm for classification. The basic idea behind KNN is to classify a new data point based on the majority class of its K nearest neighbors in the feature space.</p> <p>Apply the <code>KNeighborsClassifier</code> to the two most important features from the decision tree classifier.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/06_classification/#a-word-of-warning","title":"A word of warning\u00b6","text":"<p>As a word of warning, we shouldn't get too carried away. Clearly, there are complexities related to this approach (our accuracy scores aren't that high). There are other types of contextual data that can give insight. For example, Shervais (1982) notes that:</p> <p>\"More specific evaluation of the tectonic setting of these and other ophiolites requires application of detailed geologic and petrologic data as well as geochemistry. The Ti/V discrimination diagram, however, is a potentially powerful adjunct to these techniques.\"</p> <p>Vermeesch adds to this point by writing:</p> <p>\"no classification method based solely on geochemical data will ever be able to perfectly determine the tectonic affinity of basaltic rocks (or other rocks for that matter) simply because there is a lot of actual overlap between the geochemistry of the different tectonic settings. Notably IABs have a much wider range of compositions than either MORBs or OIBs. Therefore, geochemical classification should never be the only basis for determining tectonic affinity. This is especially the case for rocks that have undergone alteration. In such cases, mobile elements such as Sr, which have great discriminative power, cannot be used.\"</p> <p>Additionally, we would like to be able to assign physical processes to any classification given that we are seeking insight into how the Earth works.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/","title":"07 clustering","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering In\u00a0[5]: Copied! <pre># Load rock sample data\ndata = pd.read_csv('data/rock_samples.csv')\n</pre> # Load rock sample data data = pd.read_csv('data/rock_samples.csv') In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[3]: Copied! <pre># Load true rock types\nrock_types = pd.read_csv('data/rock_types.csv')\n\n#\n</pre> # Load true rock types rock_types = pd.read_csv('data/rock_types.csv')  # In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#pyearth-a-python-introduction-to-earth-science","title":"PyEarth: A Python Introduction to Earth Science\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#sklearn-unsupervised-learning-clustering","title":"Sklearn: Unsupervised Learning: Clustering\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#1-data-exploration","title":"1. Data Exploration\u00b6","text":"<ul> <li>Create a scatter plot of two randomly chosen components</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#2-apply-pca","title":"2. Apply PCA\u00b6","text":"<ul> <li>Standardize the features</li> <li>Apply PCA to reduce dimensionality to 2 components</li> <li>Plot rock samples using the first two principal components</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#3-apply-k-means-clustering-method","title":"3. Apply K-means clustering method\u00b6","text":"<ul> <li>Apply K-Means to the Rock samples (hint: apply to the original rocks samples not the transformed data)</li> <li>Create a plot to visualize the results (hint: use the first two principal components from PCA as x and y coordinates)</li> <li>How many clusters are there? And what parameters did you use to determine this?</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#4-apply-dbscan-clustering-method","title":"4. Apply DBSCAN clustering method\u00b6","text":"<ul> <li>Apply DBSCAN to the Rock samples</li> <li>Create a plot to visualize the results</li> <li>How many clusters are there? And what parameters did you use to determine this?</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#5-apply-hierarchical-clustering-method","title":"5. Apply Hierarchical clustering method\u00b6","text":"<ul> <li>Apply Hierarchical clustering to the Rock samples</li> <li>Create a plot to visualize the results</li> <li>How many clusters are there? And what parameters did you use to determine this?</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#6-compare-with-true-rock-types","title":"6. Compare with true rock types\u00b6","text":"<ul> <li>Load the true rock types</li> <li>Create a plot of the actual rock types (hint: loop through the rock types to plot)</li> <li>Comment on how well the clustering methods performed</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#7-discussion","title":"7. Discussion\u00b6","text":"<ul> <li>Write a brief discussion about the clustering results and how they compare to the true rock types</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/07_clustering/#bonus-experiment-with-different-parameters","title":"Bonus: Experiment with different parameters\u00b6","text":"<ul> <li>Try different parameters for each clustering method and observe the changes in results</li> <li>Compare clustering results on the original scaled data and the PCA-reduced data</li> <li>Comment on the changes in results</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/","title":"08 probabilities","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom math import factorial\n</pre> import matplotlib.pyplot as plt import pandas as pd import numpy as np from math import factorial In\u00a0[\u00a0]: Copied! <pre># np.random.seed(0)\nfor flip in range(0,10):\n    ## simulate a coin flip using np.random.choice\n    flip_result = \n    print(f\"Flip {flip} result: {flip_result}\")\n</pre> # np.random.seed(0) for flip in range(0,10):     ## simulate a coin flip using np.random.choice     flip_result =      print(f\"Flip {flip} result: {flip_result}\") <p>What does <code>np.random.seed(0)</code> do? Try running the code block above without it and with it. What do you notice?</p> <p>Now let's record how many times the result was heads. We will make a list called <code>flip_results</code> and have it be blank to start. Each time we go through the code we will append the result to the list:</p> In\u00a0[\u00a0]: Copied! <pre>flip_results = []\n\nfor flip in range(0,10):\n    ## simulate a coin flip using np.random.choice\n    flip_result = \n    flip_results.append(\n    \nprint(f\"{flip_results = }\")\n</pre> flip_results = []  for flip in range(0,10):     ## simulate a coin flip using np.random.choice     flip_result =      flip_results.append(      print(f\"{flip_results = }\") <p>We can calculate how many times were heads by taking the sum of the list:</p> In\u00a0[\u00a0]: Copied! <pre>## use sum() to calculate the number of heads\nnumber_heads = \nprint(f\"{number_heads = }\")\n</pre> ## use sum() to calculate the number of heads number_heads =  print(f\"{number_heads = }\") <p>Now let's flip the coin 10 times and do that 10 times. Each time we flip it, let's record how many heads resulted from the flip.</p> In\u00a0[\u00a0]: Copied! <pre>number_heads = []\n\n## do 100 experiments\nnumber_experiments = 100\nnumber_flips = 10\nfor flip_experiment in range(number_experiments):\n\n    flip_results = []\n    \n    ## each experiment, do 10 flips\n    for flip in range(0,number_flips):\n        flip_result = \n        flip_results.append(\n\n    ## count the number of heads in the current experiment using sum()\n    current_experiment = \n    \n    ## save the result\n    number_heads.append(\n\n## print the number of heads\nprint(f\"{number_heads[:10] = }\")\n</pre> number_heads = []  ## do 100 experiments number_experiments = 100 number_flips = 10 for flip_experiment in range(number_experiments):      flip_results = []          ## each experiment, do 10 flips     for flip in range(0,number_flips):         flip_result =          flip_results.append(      ## count the number of heads in the current experiment using sum()     current_experiment =           ## save the result     number_heads.append(  ## print the number of heads print(f\"{number_heads[:10] = }\") <p>Let's visualize the distribution of the number of heads.</p> In\u00a0[\u00a0]: Copied! <pre>## plot the histogram of the number of heads using plt.hist\nplt.figure()\nbins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered\nplt.\nplt.show()\n</pre> ## plot the histogram of the number of heads using plt.hist plt.figure() bins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered plt. plt.show() <p>Instead of 10 experiments, let's do 1000 experiments. Plot the histogram of the result and check how the distribution changes.</p> In\u00a0[\u00a0]: Copied! <pre>number_heads = []\n\n## do 10 experiments\nnumber_experiments = 1000\nnumber_flips = 10\nfor flip_experiment in range(number_experiments):\n\n    flip_results = []\n    \n    ## each experiment, do 10 flips\n    for flip in range(0,number_flips):\n        flip_result = \n        flip_results.append(\n\n    ## count the number of heads in the current experiment\n    current_experiment =\n    \n    ## save the result\n    number_heads.append(\n\n## print the number of heads\nprint(f\"{number_heads[:10] = }\")\n</pre> number_heads = []  ## do 10 experiments number_experiments = 1000 number_flips = 10 for flip_experiment in range(number_experiments):      flip_results = []          ## each experiment, do 10 flips     for flip in range(0,number_flips):         flip_result =          flip_results.append(      ## count the number of heads in the current experiment     current_experiment =          ## save the result     number_heads.append(  ## print the number of heads print(f\"{number_heads[:10] = }\") In\u00a0[\u00a0]: Copied! <pre>## plot the histogram of the number of heads using plt.hist\nplt.figure()\nbins = np.arange(0, number_flips+1, 1) - 0.5 # make the bins centered\nplt.\nplt.show()\n</pre> ## plot the histogram of the number of heads using plt.hist plt.figure() bins = np.arange(0, number_flips+1, 1) - 0.5 # make the bins centered plt. plt.show()  <p>How does the distribution change as the number of experiments increases? Is the center of the distribution changing? Is the width of the distribution changing?</p> In\u00a0[9]: Copied! <pre>def binomial_probability(number_positive,positive_rate,number_attempts):\n    \"\"\"\n    This function computes the probability of getting x particular outcomes (heads) in n attempts, where p is the \n    probability of a particular outcome (head) for any given attempt (coin toss).\n    \n    Parameters\n    ----------\n    number_positive : number of a particular outcome\n    positive_rate : probability of that outcome in a given attempt\n    number_attempts : number of attempts\n    \n    Returns\n    ---------\n    prob : probability of that number of the given outcome occuring in that number of attempts\n    \"\"\"\n    x = number_positive\n    p = positive_rate\n    n = number_attempts\n\n    ## compute the binomial probability following the equation above. Hint: use the factorial() function for x! n! (n-x)!\n    prob = \n\n    return prob\n</pre> def binomial_probability(number_positive,positive_rate,number_attempts):     \"\"\"     This function computes the probability of getting x particular outcomes (heads) in n attempts, where p is the      probability of a particular outcome (head) for any given attempt (coin toss).          Parameters     ----------     number_positive : number of a particular outcome     positive_rate : probability of that outcome in a given attempt     number_attempts : number of attempts          Returns     ---------     prob : probability of that number of the given outcome occuring in that number of attempts     \"\"\"     x = number_positive     p = positive_rate     n = number_attempts      ## compute the binomial probability following the equation above. Hint: use the factorial() function for x! n! (n-x)!     prob =       return prob <p>We can use this function to calculate the probability of getting 10 heads ($x=10$) when there are 10 coin tosses ($n=10$) given with the $p$ (probability) of 0.5.</p> In\u00a0[\u00a0]: Copied! <pre>binomial_probability(10,0.5,10)\n</pre> binomial_probability(10,0.5,10) <p>Let's calculate the probability of getting [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10] heads.</p> In\u00a0[\u00a0]: Copied! <pre>## create an array of the number of heads we want to calculate the probability for\nx = np.arange(11)\n\nprob_heads = 0.5\nnumber_flips = 10\n\nprobabilities = []\nfor number_head in x:\n    ## use the binomial_probability function to calculate the probability\n    prob = \n    probabilities.append(\n    \n    print(f\"Number of heads: {number_head}, probability: {prob:.5f}\")\n</pre> ## create an array of the number of heads we want to calculate the probability for x = np.arange(11)  prob_heads = 0.5 number_flips = 10  probabilities = [] for number_head in x:     ## use the binomial_probability function to calculate the probability     prob =      probabilities.append(          print(f\"Number of heads: {number_head}, probability: {prob:.5f}\") <p>These probabilities are the theoretical probabilities of getting a particular number of heads in 10 coin flips. Let's verify that the histogram of the number of heads in 1000 coin flips matches the theoretical probabilities.</p> <p>Make a plot where you both plot the histogram from 1000 coin flips (using <code>plt.hist()</code> with <code>density=True</code>) and you plot the results head_numbers probabilities (using <code>plt.plot()</code>).</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10,5))\n\n## plot the histogram from 1000 coin flips using plt.hist()\nbins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered\nplt.\n\n## plot the theoretical probabilities calculated based on the binomial distribution using plt.plot\nplt.\n\nplt.xlabel(f'Number of heads out of {number_flips} attempts')\nplt.ylabel('Fraction of times with this number of heads') \n\nplt.title(f'Coin flip results (n={number_flips})')\n\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(10,5))  ## plot the histogram from 1000 coin flips using plt.hist() bins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered plt.  ## plot the theoretical probabilities calculated based on the binomial distribution using plt.plot plt.  plt.xlabel(f'Number of heads out of {number_flips} attempts') plt.ylabel('Fraction of times with this number of heads')   plt.title(f'Coin flip results (n={number_flips})')  plt.legend() plt.show() <p>How does the histogram of the number of heads in 1000 coin flips match the theoretical probabilities? Does this imply that the theoretical distribution i.e., the binomial distribution, is a good model for the coin flip experiment?</p> In\u00a0[\u00a0]: Copied! <pre>help(np.random.binomial)\n</pre> help(np.random.binomial) <p><code>np.random.binomial( )</code> requires 2 parameters, $n$ and $p$, with an optional keyword argument <code>size</code> (if <code>size</code> is not specified, it returns a single trial). We could have used this function earlier to get the number of heads that were flipped, but the way we did it also worked.</p> <p>Let's follow the example the is given in the <code>np.random.binomial( )</code> docstring.</p> <p>A company drills 9 wild-cat oil exploration wells (high risk drilling in unproven areas), each with an estimated probability of success of 0.1. All nine wells fail. What is the probability of that happening?</p> <p>Note that success in this context means that liquid hydocarbons came out of the well. In reality, you may not consider this a success given that the result is that more hydrocarbons will be combusted as a result, leading to higher atmospheric carbon dioxide levels and associated global warming.</p> <p>If we do <code>np.random.binomial(9, 0.1, 100)</code> we will get a list of 100 values that represent the number of wells that yielded oil when there is a 10% (p = 0.1) chance of each individual well yielding oil.</p> In\u00a0[\u00a0]: Copied! <pre>success_rate = 0.1\nnumber_wells = 9\nnumber_simulations = 100\n\n## simulate the number of successful wells using np.random.binomial\nnumber_success = \n\nprint(f\"{number_success[:10] = }\")\n</pre> success_rate = 0.1 number_wells = 9 number_simulations = 100  ## simulate the number of successful wells using np.random.binomial number_success =   print(f\"{number_success[:10] = }\")  In\u00a0[\u00a0]: Copied! <pre>## Count the number of failures\nnumber_failures = \n\n## Calculate the failure rate\nfailure_rate = \n\nprint(f\"The number of failures is {number_failures} out of {number_simulations} simulations\")\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> ## Count the number of failures number_failures =   ## Calculate the failure rate failure_rate =   print(f\"The number of failures is {number_failures} out of {number_simulations} simulations\") print(f\"The failure rate is {failure_rate:.2f}\") <p>We can write a function that uses this process to simulate fraction of times that there no successful wells for a given number of wells, a given probability and a given number of simulations;</p> In\u00a0[17]: Copied! <pre>def wildcat_failure_rate(number_wells,success_rate,number_simulations):\n    '''\n    Simulate the number of times that there are no successful wells for a given number of wells and a given probability for each well.\n    \n    Parameters\n    ----------\n    number_wells : number of wells drilled in each simulation\n    success_rate : probability that each well will be successful\n    number_simulations : number of times that drilling number_wells is simulated\n    '''\n    \n    simulations = np.random.binomial(number_wells, success_rate, number_simulations)\n    number_failures = np.sum(simulations == 0)\n    failure_rate = number_failures/number_simulations\n\n    return failure_rate\n</pre> def wildcat_failure_rate(number_wells,success_rate,number_simulations):     '''     Simulate the number of times that there are no successful wells for a given number of wells and a given probability for each well.          Parameters     ----------     number_wells : number of wells drilled in each simulation     success_rate : probability that each well will be successful     number_simulations : number of times that drilling number_wells is simulated     '''          simulations = np.random.binomial(number_wells, success_rate, number_simulations)     number_failures = np.sum(simulations == 0)     failure_rate = number_failures/number_simulations      return failure_rate <p>Put the <code>wildcat_failure_rate</code> function to use</p> <p>Use the function to simulate the failure rate for the above scenario (9 wells drilled, 0.1 probability of success for each well) and do it for 10 simulations</p> In\u00a0[\u00a0]: Copied! <pre>number_wells = \nsuccess_rate = \nnumber_simulations = \n\nfailure_rate = \n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> number_wells =  success_rate =  number_simulations =   failure_rate =   print(f\"The failure rate is {failure_rate:.2f}\") <p>Use the function to simulate the failure rate for the same scenario for 1000 simulations</p> In\u00a0[\u00a0]: Copied! <pre>number_wells = \nsuccess_rate = \nnumber_simulations = \n\nfailure_rate = \n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> number_wells =  success_rate =  number_simulations =   failure_rate =   print(f\"The failure rate is {failure_rate:.2f}\") <p>Use the function to simulate the failure rate for 100,000 simulations</p> In\u00a0[\u00a0]: Copied! <pre>number_wells = \nsuccess_rate =  \nnumber_simulations =  \n\nfailure_rate =  \n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> number_wells =  success_rate =   number_simulations =    failure_rate =    print(f\"The failure rate is {failure_rate:.2f}\") <p>Put the <code>binomial_probability</code> function to use</p> <p>Instead of the examples above we are simulating the result, we could directly use the theoretical binomial_probability distribution to calculate the probability without doing many simulations.</p> In\u00a0[\u00a0]: Copied! <pre>## calculate the probability using the binomial_probability function with number_positive=0, positive_rate=0.1, number_attempts=9\nfailure_rate = \n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> ## calculate the probability using the binomial_probability function with number_positive=0, positive_rate=0.1, number_attempts=9 failure_rate =   print(f\"The failure rate is {failure_rate:.2f}\") <p>How well does the calculated binomial_probability match the simulated wildcat_failure rates? How many times do you need to simulate the problem to get a number that matches the theoretical probability?</p> In\u00a0[22]: Copied! <pre>def poisson_probability(k,lamb):\n    \"\"\"\n    This function computes the probability of getting k particular outcomes when the expected rate is lambda.\n    \"\"\"\n    \n    ## compute the poisson probability of getting k outcomes when the expected rate is lambda using the equation above\n    prob = \n    \n    return prob\n</pre> def poisson_probability(k,lamb):     \"\"\"     This function computes the probability of getting k particular outcomes when the expected rate is lambda.     \"\"\"          ## compute the poisson probability of getting k outcomes when the expected rate is lambda using the equation above     prob =           return prob In\u00a0[\u00a0]: Copied! <pre>lamb = 5\nk = 6\n## use the poisson_probability function to calculate the probability\nprob = \nprint(f\"The probability of getting {k} outcomes when the expected rate is {lamb} is {prob*100:.3f}%\")\n</pre> lamb = 5 k = 6 ## use the poisson_probability function to calculate the probability prob =  print(f\"The probability of getting {k} outcomes when the expected rate is {lamb} is {prob*100:.3f}%\") <p>So that result tells us that there is a 14.6% chance of observing exactly 6, but it would be much more helpful to be able to visualize the probability distribution. So let's go through and calculate the probability of seeing any number between 0 and 10. First, we can make an array between 0 and 11:</p> In\u00a0[\u00a0]: Copied! <pre>## Let's calcualte the theoretical probability of seeing different numbers of meteors when the expected rate is 5 (Southern Taurids)\ntaurid_meteor_rate = 5\nnumber_meteors_seen = np.arange(0,11)\ntaurid_meteor_sighting_probability = []\n\nfor n in number_meteors_seen:\n    ## use the poisson_probability function to calculate the probability\n    prob = \n    taurid_meteor_sighting_probability.append(\n    print(f\"The probability to see {n} meteors is {prob:.3f}\")\n</pre> ## Let's calcualte the theoretical probability of seeing different numbers of meteors when the expected rate is 5 (Southern Taurids) taurid_meteor_rate = 5 number_meteors_seen = np.arange(0,11) taurid_meteor_sighting_probability = []  for n in number_meteors_seen:     ## use the poisson_probability function to calculate the probability     prob =      taurid_meteor_sighting_probability.append(     print(f\"The probability to see {n} meteors is {prob:.3f}\")  <p>Based on the probability of seeing 0 meteors, what is the theoretical probability of seeing at least 1 meteor?</p> In\u00a0[\u00a0]: Copied! <pre>## use the poisson_probability function to calculate the probability of seeing at least 1 meteor. Hint: use 1 - the probability of seeing 0 meteors\nprob_w_meteor_sighting = \nprint(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the Southern Taurids rate of {taurid_meteor_rate} meteors per hour\")\n</pre> ## use the poisson_probability function to calculate the probability of seeing at least 1 meteor. Hint: use 1 - the probability of seeing 0 meteors prob_w_meteor_sighting =  print(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the Southern Taurids rate of {taurid_meteor_rate} meteors per hour\") In\u00a0[\u00a0]: Copied! <pre>## Plot the probability distribution using plt.plot\nplt.figure()\nplt.\nplt.xlabel('Number of meteors seen')\nplt.ylabel('Probability')\nplt.legend()\nplt.show()\n</pre> ## Plot the probability distribution using plt.plot plt.figure() plt. plt.xlabel('Number of meteors seen') plt.ylabel('Probability') plt.legend() plt.show() <p>When there is not an active shower the background meteor rate is about 2 an hour (although it is variable depending on time of night and season; see more here: https://www.amsmeteors.org/meteor-showers/meteor-faq/).</p> <p>Let's update our code to calculate the probability of seeing different numbers of meteors when the background rate is 2 an hour (lambda = 2).</p> <ul> <li>Calculate the probability of seeing different numbers of meteors when the background rate is 2 an hour (lambda = 2).</li> <li>Plot these probabilities alongside the probability of seeing those same numbers during the Southern Taurids shower.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Let's calcualte the theoretical probability of seeing different numbers of meteors when the background rate is 2 an hour\n\nbackground_meteor_rate = 2\nnumber_meteors_seen = np.arange(0,11)\nbackground_sighting_probability = []\n\nfor n in number_meteors_seen:\n    ## use the poisson_probability function to calculate the probability\n    prob = \n    background_sighting_probability.append(\n\n    print(f\"The probability to see {n} meteors is {prob:.3f}\")\n</pre> ## Let's calcualte the theoretical probability of seeing different numbers of meteors when the background rate is 2 an hour  background_meteor_rate = 2 number_meteors_seen = np.arange(0,11) background_sighting_probability = []  for n in number_meteors_seen:     ## use the poisson_probability function to calculate the probability     prob =      background_sighting_probability.append(      print(f\"The probability to see {n} meteors is {prob:.3f}\") <p>Based on the probability of seeing 0 meteors, what is the theoretical probability of seeing at least 1 meteor?</p> In\u00a0[\u00a0]: Copied! <pre>## use the poisson_probability function to calculate the probability of seeing at least 1 meteor. Hint: use 1 - the probability of seeing 0 meteors\nprob_w_meteor_sighting = \nprint(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the background rate of {background_meteor_rate} meteors per hour\")\n</pre> ## use the poisson_probability function to calculate the probability of seeing at least 1 meteor. Hint: use 1 - the probability of seeing 0 meteors prob_w_meteor_sighting =  print(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the background rate of {background_meteor_rate} meteors per hour\") In\u00a0[\u00a0]: Copied! <pre>## Plot the probability distribution using plt.plot\nplt.figure()\n## plot the Southern Taurids probability distribution\nplt.\n## plot the background probability distribution\nplt.\n\nplt.ylabel('Probability')\nplt.xlabel('Number of meteors seen')\nplt.legend()\nplt.show()\n</pre> ## Plot the probability distribution using plt.plot plt.figure() ## plot the Southern Taurids probability distribution plt. ## plot the background probability distribution plt.  plt.ylabel('Probability') plt.xlabel('Number of meteors seen') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>lamb = 2\nnumber_hours = 1000\nnumber_meteors = []\n\nfor hour in np.arange(number_hours):\n    ## use the np.random.poisson function to simulate the number of meteors per hour\n    number_meteor_per_hour = \n    number_meteors.append(\n\n## Calculate the fraction of hours watched with at least 1 meteor sighting\nnumber_meteors = np.array(number_meteors)\n## use np.sum to calculate the number of hours with at least 1 meteor sighting\nnumber_hours_w_meteor_sighting = \nprob_w_meteor_sighting = \n\nprint(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\")\n</pre> lamb = 2 number_hours = 1000 number_meteors = []  for hour in np.arange(number_hours):     ## use the np.random.poisson function to simulate the number of meteors per hour     number_meteor_per_hour =      number_meteors.append(  ## Calculate the fraction of hours watched with at least 1 meteor sighting number_meteors = np.array(number_meteors) ## use np.sum to calculate the number of hours with at least 1 meteor sighting number_hours_w_meteor_sighting =  prob_w_meteor_sighting =   print(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\") <ul> <li>Do the same meteor watching simulation with $\\lambda = 5$ (the Southern Taurids rate).</li> </ul> In\u00a0[\u00a0]: Copied! <pre>lamb = 5\nnumber_hours = 1000\nnumber_meteors = []\n\nfor hour in np.arange(number_hours):\n    ## use the np.random.poisson function to simulate the number of meteors per hour\n    number_meteor_per_hour = \n    number_meteors.append(\n    \n## Calculate the fraction of hours watched with at least 1 meteor sighting\nnumber_meteors = np.array(number_meteors)\n## use np.sum to calculate the number of hours with at least 1 meteor sighting\nnumber_hours_w_meteor_sighting = \nprob_w_meteor_sighting = \n\nprint(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\")\n</pre> lamb = 5 number_hours = 1000 number_meteors = []  for hour in np.arange(number_hours):     ## use the np.random.poisson function to simulate the number of meteors per hour     number_meteor_per_hour =      number_meteors.append(      ## Calculate the fraction of hours watched with at least 1 meteor sighting number_meteors = np.array(number_meteors) ## use np.sum to calculate the number of hours with at least 1 meteor sighting number_hours_w_meteor_sighting =  prob_w_meteor_sighting =   print(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\") <p>The above estimate is based on Monte Carlo simulation of hundreds and thousands of hours. How does it compare to the theoretical probability?</p> <p>Hint: If your results are not similar, try increasing the number of hours simulated.</p> In\u00a0[\u00a0]: Copied! <pre>## Calculate the expected number of M5 earthquakes in a year\nM = 5\nlogN = \nN = \nprint(f\"The expected number of M5 earthquakes in 1 year is {N:.2f}\")\n\n## Calculate the expected number of M5 earthquakes in 30 years\nlamb = \nprint(f\"The expected number of M5 earthquakes in 30 years is {lamb:.2f}\")\n\n## Calculate the probability of observing 0-9 M5 earthquake in a year\nnumber_earthquakes = np.arange(0,10)\nearthquake_probability = []\nfor n in number_earthquakes:\n    ## use the poisson_probability function to calculate the probability\n    prob = \n    earthquake_probability.append(\n    print(f\"The probability of observing {n} M5 earthquakes in a year is {prob*100:.3f}%\")\n</pre> ## Calculate the expected number of M5 earthquakes in a year M = 5 logN =  N =  print(f\"The expected number of M5 earthquakes in 1 year is {N:.2f}\")  ## Calculate the expected number of M5 earthquakes in 30 years lamb =  print(f\"The expected number of M5 earthquakes in 30 years is {lamb:.2f}\")  ## Calculate the probability of observing 0-9 M5 earthquake in a year number_earthquakes = np.arange(0,10) earthquake_probability = [] for n in number_earthquakes:     ## use the poisson_probability function to calculate the probability     prob =      earthquake_probability.append(     print(f\"The probability of observing {n} M5 earthquakes in a year is {prob*100:.3f}%\") In\u00a0[\u00a0]: Copied! <pre>## Plot the probability distribution using plt.plot\nplt.figure()\nplt.\n\nplt.ylabel('Probability')\nplt.xlabel('Number of M5 earthquakes in 30 years')\nplt.legend()\nplt.show()\n</pre> ## Plot the probability distribution using plt.plot plt.figure() plt.  plt.ylabel('Probability') plt.xlabel('Number of M5 earthquakes in 30 years') plt.legend() plt.show()  <p>How does the probability change with the number of events? How does that reconcile with the rate of M5 earthquakes in 30 years?</p> In\u00a0[\u00a0]: Copied! <pre>## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 30 years\nmagnitudes = [4, 5, 6, 7]\nnumber_years = 30\nfor M in magnitudes:\n    logN = \n    N = \n    lamb = \n    prob0 = \n\n    ## calculate the probability of observing at least one event, hint: use 1 - prob0\n    prob = \n    print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\") \n</pre> ## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 30 years magnitudes = [4, 5, 6, 7] number_years = 30 for M in magnitudes:     logN =      N =      lamb =      prob0 =       ## calculate the probability of observing at least one event, hint: use 1 - prob0     prob =      print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\")  <p>How do the probabilities change if a 10 year period is considered?</p> In\u00a0[\u00a0]: Copied! <pre>## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 10 years\nmagnitudes = [4, 5, 6, 7]\nnumber_years = 10\nfor M in magnitudes:\n    logN = \n    N = \n    lamb = \n    prob0 = \n\n    ## calculate the probability of observing at least one event, hint: use 1 - prob0\n    prob = \n    print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\") \n</pre> ## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 10 years magnitudes = [4, 5, 6, 7] number_years = 10 for M in magnitudes:     logN =      N =      lamb =      prob0 =       ## calculate the probability of observing at least one event, hint: use 1 - prob0     prob =      print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\")"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#probability-distributions-meteor-shower-gazing","title":"Probability distributions &amp; meteor shower gazing\u00b6","text":"<p>Our goals for today:</p> <ul> <li>Discuss some key statistics topics: samples versus populations and empirical versus theorectical distributions</li> <li>Simulate a head/tail coin toss and well drilling i.e. Binomial distribution</li> <li>Simulate meteors entering Earth's atmosphere i.e. Poisson distribution</li> <li>Simulate geomagnetic polarity reversals i.e. Gamma distribution</li> <li>Use Gutenberg-Richter to assess earthquake probability</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#setup","title":"Setup\u00b6","text":"<p>Run this cell as it is to setup your environment.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#case-1-flipping-a-coin","title":"Case 1: Flipping a coin\u00b6","text":"<p>Let's pretend we are flipping a coin 10 times using <code>np.random.choice([0, 1])</code>. How many times will be get heads? 1 is heads, 0 is tails. Let's use a for loop and get Python to simulate such a coin flip scenario for us.</p> <p>What for loops do is take a chunk of code (in Python the chunk that is indented) being run multiple times. In this case, the code will get looped through 10 times -- specified by <code>range(0,10)</code>.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#binomial-distribution","title":"Binomial distribution:\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#theoretical","title":"Theoretical\u00b6","text":"<p>A relatively straight-forward distribution is the binomial distribution which describes the probability of a particular outcome when there are only two possibilities (yes or no, heads or tails, 1 or 0).</p> <p>For example, in a coin toss experiment (heads or tails), if we flip the coin  $n$ times, what is the probability of getting $x$ 'heads'?  We assume that the probability $p$ of a head for any given coin toss is 50%; put another way $p$ = 0.5.</p> <p>The binomial distribution can be described by an equation:</p> <p>$$P=f(x,p,n)= \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$$</p> <p>We can look at this kind of distribution by evaluating the probability for getting $x$ 'heads' out of $n$ attempts. We'll code the equation as a function, and calculate the probability $P$ of a particular outcome (e.g., $x$ heads in $n$ attempts).</p> <p>Note that for a coin toss, $p$ is 0.5, but other yes/no questions can be investigated as well (e.g., chance of finding a fossil in a sedimentary layer; whether or not a landslide occurs following an earthquake).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#empirical","title":"Empirical\u00b6","text":"<p>The type of sampling we were doing above where we were flipping coins is called a Monte Carlo simulation. We can use simulate data from all sorts of distributions. Let's keep focusing on the binomial distribution and look at using the <code>np.random.binomial</code> function.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#poisson-distribution","title":"Poisson distribution:\u00b6","text":"<p>A Poisson Distribution gives the probability of a number of events in an interval generated by a Poisson process: the average time between events is known, but the exact timing of events is random. The events must be independent and may occur only one at a time.</p> <p>Within Earth and Planetary Science there are many processes that approximately meet this criteria.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#theoretical","title":"Theoretical\u00b6","text":"<p>The Poisson distribution gives the probability that an event (with two possible outcomes) occurs $k$ number of times in an interval of time where $\\lambda$ is the expected rate of occurance. The Poisson distribution is the limit of the binomial distribution for large $n$. So if you take the limit of the binomial distribution as $n \\rightarrow \\infty$ you'll get the Poisson distribution:</p> <p>$$P(k) = e^{-\\lambda}\\frac{\\lambda^{k}}{k!}$$</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#observing-meteors","title":"Observing meteors\u00b6","text":"<p>From https://www.amsmeteors.org/meteor-showers/meteor-faq/:</p> <p>How big are most meteoroids? How fast do they travel? The majority of visible meteors are caused by particles ranging in size from about that of a small pebble down to a grain of sand, and generally weigh less than 1-2 grams. Those of asteroid origin can be composed of dense stony or metallic material (the minority) while those of cometary origin (the majority) have low densities and are composed of a \u201cfluffy\u201d conglomerate of material, frequently called a \u201cdustball.\u201d The brilliant flash of light from a meteor is not caused so much by the meteoroid\u2019s mass, but by its high level of kinetic energy as it collides with the atmosphere.</p> <p>Meteors enter the atmosphere at speeds ranging from 11 km/sec (25,000 mph), to 72 km/sec (160,000 mph!). When the meteoroid collides with air molecules, its high level of kinetic energy rapidly ionizes and excites a long, thin column of atmospheric atoms along the meteoroid\u2019s path, creating a flash of light visible from the ground below. This column, or meteor trail, is usually less than 1 meter in diameter, but will be tens of kilometers long.</p> <p>The wide range in meteoroid speeds is caused partly by the fact that the Earth itself is traveling at about 30 km/sec (67,000 mph) as it revolves around the sun. On the evening side, or trailing edge of the Earth, meteoroids must catch up to the earth\u2019s atmosphere to cause a meteor, and tend to be slow. On the morning side, or leading edge of the earth, meteoroids can collide head-on with the atmosphere and tend to be fast.</p> <p>What is a meteor shower? Does a shower occur \u201call at once\u201d or over a period of time? Most meteor showers have their origins with comets. Each time a comet swings by the sun, it produces copious amounts of meteoroid sized particles which will eventually spread out along the entire orbit of the comet to form a meteoroid \u201cstream.\u201d If the Earth\u2019s orbit and the comet\u2019s orbit intersect at some point, then the Earth will pass through this stream for a few days at roughly the same time each year, encountering a meteor shower. The only major shower clearly shown to be non-cometary is the Geminid shower, which share an orbit with the asteroid (3200 Phaethon): one that comes unusually close to the sun as well as passing through the earth\u2019s orbit. Most shower meteoroids appear to be \u201cfluffy\u201d, but the Geminids are much more durable as might be expected from asteroid fragments.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#observing-the-southern-taurids-meteor-shower","title":"Observing the Southern Taurids meteor shower\u00b6","text":"<p>Let's say you are planning a camping trip to go out try to see shooting stars next Fall in a rural location. You are looking at a date in October and that there is an active shower:</p> <p>Southern Taurids</p> <p>Active from September 28th to December 2, 2021. The peak is November 4-5, 2021</p> <p>The Southern Taurids are a long-lasting shower that reaches a barely noticeable maximum on October 9 or 10. The shower is active for more than two months but rarely produces more than five shower members per hour, even at maximum activity. The Taurids (both branches) are rich in fireballs and are often responsible for increased number of fireball reports from September through November. https://www.amsmeteors.org/meteor-showers/meteor-shower-calendar/</p> <p>At a rate of 5 observed meteors per hour, what is the probability of observing 6 meteors in an hour?</p> <p>We can use the Poisson probability function to answer this question:</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#simulate-meteor-observing","title":"Simulate meteor observing\u00b6","text":"<p>There are many cases where it can be useful to simulate data sets. In this case, one could simulate what your experience could be in terms of the number of hours you could spend looking at the night sky and seeing 1 meteor or more on a normal night vs. a night with the Southern Taurids shower ongoing.</p> <p>We can use the <code>np.random.poisson</code> function to simulate 'realistic' data.</p> <p>Recall the Poisson distribution represents the probability of observing $k$ events when the expected rate is $\\lambda$.</p> <p>The Poisson distribution assumes that the events are independent and occur at a constant rate. Each call to <code>np.random.poisson( )</code> is doing an experiment following the Poisson distribution to see how many events occur.</p> <p>Let's try it with $\\lambda = 2$ (the background rate) and watch the sky for 1000 hours.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#extension-earthquake-probability","title":"Extension: Earthquake Probability\u00b6","text":"<p>The occurrence of earthquakes is also a Poisson process, events occur randomly in time, and the average recurrence can be determined from Gutenberg-Richter.</p> <p>The Gutenberg-Richter statistic which gives the annual rate of earthquakes above a given magnitude.</p> <p>Assumeing the Gutenberg-Richter relationshipfor the San Franciso Bay Area: log10(N)= 3.266 - 0.797M, for each year.</p> <p>Let's apply the Poisson distribution to this problem.</p> <p>So $\\lambda = N * {\\Delta}time$, where N is the annual rate. It is common to consider ${\\Delta}time=30 yrs$.</p> <p>Use the Poisson's distribution to find the probability of 0 to 9 M5 events in a 30 year period.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/08_probabilities/#poisson-probability-of-1-or-more-earthquakes","title":"Poisson Probability of 1 or more earthquakes\u00b6","text":"<p>The Poisson probability of zero events has an interesting use in characterizing earthquake hazard.</p> <p>$P(k=0)=e^{-\\lambda}$</p> <p>The complement of the zero event probability is the probability of 1 or more earthquakes occuring in the period of time. It is this probability that is used in earthquake forecast reports. The probability of one or more events is written as;</p> <p>$P(k &gt;= 1) = 1 - e^{-\\lambda}$</p> <p>Determine the probability of 1 or more M4, M5, M6 and M7 in a 30 year period.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/09_neural_networks1/","title":"09 neural networks1","text":"<p>Created by Minh-Chien Trinh, Jeonbuk National University,</p> <p>If you want to learn more about Deep Learning (Deep Nerual Networks), you can check the Deep Learning Specialization on Coursera or watch videos on YouTube.</p> <p>In this lecture, we will use PyTorch to build and train neural networks. The pytorch library is a powerful tool for building and training neural networks. It provides a flexible and efficient library for deep learning. It is also currently the most popular library for deep learning.</p> <p> </p> <p>Let's import the necessary libraries of PyTorch and other libraries.</p> In\u00a0[\u00a0]: Copied! <pre>## First part of this semester\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n## Second part of this semester\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n## Last part of this semester\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n</pre> ## First part of this semester import matplotlib.pyplot as plt import numpy as np import pandas as pd  ## Second part of this semester from sklearn.impute import SimpleImputer from sklearn.metrics import r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder  ## Last part of this semester import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim In\u00a0[\u00a0]: Copied! <pre>## Set random seed for reproducibility\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n</pre> ## Set random seed for reproducibility torch.manual_seed(0) torch.cuda.manual_seed(0) np.random.seed(0) <ul> <li>Load the Betoule data</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Load the Betoule data\n# betoule_data = pd.read_csv('data/mu_z.csv',header=1) ## reading from local file\nbetoule_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/mu_z.csv',header=1) ## reading from github for running on colab\nbetoule_data.head()\n\n## Apply processing to convert to distance and velocity\n# speed of light in km/s\nc = 2.9979e8 / 1000 \n\n## the formula for v from z (and c)\nbetoule_data['velocity'] = c * (((betoule_data['z']+1.)**2-1.)/((betoule_data['z']+1.)**2+1.)) \n\n## convert mu to Gpc\nbetoule_data['distance'] = 10000*(10.**((betoule_data['mu'])/5.))*1e-9\n</pre> ## Load the Betoule data # betoule_data = pd.read_csv('data/mu_z.csv',header=1) ## reading from local file betoule_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/mu_z.csv',header=1) ## reading from github for running on colab betoule_data.head()  ## Apply processing to convert to distance and velocity # speed of light in km/s c = 2.9979e8 / 1000   ## the formula for v from z (and c) betoule_data['velocity'] = c * (((betoule_data['z']+1.)**2-1.)/((betoule_data['z']+1.)**2+1.))   ## convert mu to Gpc betoule_data['distance'] = 10000*(10.**((betoule_data['mu'])/5.))*1e-9 In\u00a0[\u00a0]: Copied! <pre>## Review the data\nplt.figure()\nplt.scatter(\nplt.xlabel('Distance (Mpc)')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.show()\n</pre> ## Review the data plt.figure() plt.scatter( plt.xlabel('Distance (Mpc)') plt.ylabel('Velocity (km s$^{-1}$)') plt.show()  <ul> <li>Prepare the data into features (X) and target (y). This is same as the previous lecture.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Define features (X) and target (y) variables using the distance as the feature and velocity as the target\nX = \ny = \n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = \n</pre> ## Define features (X) and target (y) variables using the distance as the feature and velocity as the target X =  y =   ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test =  <ul> <li>Let's start to build the first neural network model to fit the Betoule data.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\n## Normalize the data to make the training process more efficient\nmagnitude_X = 10**int(np.log10(X.max()))\nmagnitude_y = 10**int(np.log10(y.max()))\nX_train_tensor = X_train_tensor / magnitude_X\ny_train_tensor = y_train_tensor / magnitude_y\nX_test_tensor = X_test_tensor / magnitude_X\ny_test_tensor = y_test_tensor / magnitude_y\n\n## Define the neural network model\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size):\n        super(SimpleNN, self).__init__()\n        ## Define the neural network layers\n        self.fc1 = \n        self.fc2 =\n    \n    def forward(self, x):\n        ## Apply the neural network layers\n        x = \n        x = \n        x = \n        return x\n\n## Define the model dimensions\ninput_size = X.shape[-1]\noutput_size = 1 # Output layer for regression (1 output neuron)\nhidden_size = 16\n\n## Define the model, loss function, and optimizer. Hint: using your defined model, MSE loss, and Adam optimizer\nmodel = \ncriterion = \noptimizer = \n\n## Define fit function\ndef fit(model, X, y, epochs=100):\n    ## set the model to training mode\n    model.\n    losses = []\n    for epoch in range(epochs):\n        ## zero the gradients\n        optimizer.zero_grad()\n\n        ## get the outputs from the model\n        outputs = \n        ## calculate the loss\n        loss = \n        loss.backward()\n        ## update the weights\n        optimizer.\n\n        losses.append(loss.item())\n        if (epoch+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n    return losses\n\n## Define predict function\ndef predict(model, X):\n    ## set the model to evaluation mode\n    model.\n    with torch.no_grad():\n        ## get the outputs from the model\n        outputs = \n    return outputs\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=100)\n\n## Plot the loss during the training process\nplt.figure()\nplt.plot(\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Convert data to PyTorch tensors X_train_tensor = torch.tensor(X_train, dtype=torch.float32) y_train_tensor = torch.tensor(y_train, dtype=torch.float32) X_test_tensor = torch.tensor(X_test, dtype=torch.float32) y_test_tensor = torch.tensor(y_test, dtype=torch.float32)  ## Normalize the data to make the training process more efficient magnitude_X = 10**int(np.log10(X.max())) magnitude_y = 10**int(np.log10(y.max())) X_train_tensor = X_train_tensor / magnitude_X y_train_tensor = y_train_tensor / magnitude_y X_test_tensor = X_test_tensor / magnitude_X y_test_tensor = y_test_tensor / magnitude_y  ## Define the neural network model class SimpleNN(nn.Module):     def __init__(self, input_size, output_size, hidden_size):         super(SimpleNN, self).__init__()         ## Define the neural network layers         self.fc1 =          self.fc2 =          def forward(self, x):         ## Apply the neural network layers         x =          x =          x =          return x  ## Define the model dimensions input_size = X.shape[-1] output_size = 1 # Output layer for regression (1 output neuron) hidden_size = 16  ## Define the model, loss function, and optimizer. Hint: using your defined model, MSE loss, and Adam optimizer model =  criterion =  optimizer =   ## Define fit function def fit(model, X, y, epochs=100):     ## set the model to training mode     model.     losses = []     for epoch in range(epochs):         ## zero the gradients         optimizer.zero_grad()          ## get the outputs from the model         outputs =          ## calculate the loss         loss =          loss.backward()         ## update the weights         optimizer.          losses.append(loss.item())         if (epoch+1) % 10 == 0:             print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')     return losses  ## Define predict function def predict(model, X):     ## set the model to evaluation mode     model.     with torch.no_grad():         ## get the outputs from the model         outputs =      return outputs  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=100)  ## Plot the loss during the training process plt.figure() plt.plot( plt.xlabel('Epochs') plt.ylabel('Loss') plt.show()  <ul> <li>Evaluate the model on the test set. This is same as the previous lecture.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Predict on the test set\ny_pred_tensor = predict(\ny_pred = y_pred_tensor.numpy() * magnitude_y\n\n## Calculate R-squared metric\nr2 = \nprint(f'R-squared: {r2:.4f}')\n</pre> ## Predict on the test set y_pred_tensor = predict( y_pred = y_pred_tensor.numpy() * magnitude_y  ## Calculate R-squared metric r2 =  print(f'R-squared: {r2:.4f}') In\u00a0[\u00a0]: Copied! <pre>## Predict on the whole dataset for plotting\nX_tensor = torch.tensor(X, dtype=torch.float32)\nX_tensor = X_tensor / magnitude_X\ny_pred_tensor = predict(model, X_tensor)\ny_pred = y_pred_tensor.numpy() * magnitude_y\ny_pred = y_pred.squeeze() # remove the extra dimension\n\n## Plot the results\nplt.figure(figsize=(6, 6))\nplt.subplot(2,1,1)\n## plot the data\nplt.scatter(\n## plot the fitted line\nplt.plot(\nplt.title('data and a nerual network fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\n## plot the residuals\nplt.subplot(2,1,2)\nplt.scatter(\nplt.title('residuals of a nerual network fit')\nplt.ylabel('Residual velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.tight_layout()\nplt.show()\n</pre> ## Predict on the whole dataset for plotting X_tensor = torch.tensor(X, dtype=torch.float32) X_tensor = X_tensor / magnitude_X y_pred_tensor = predict(model, X_tensor) y_pred = y_pred_tensor.numpy() * magnitude_y y_pred = y_pred.squeeze() # remove the extra dimension  ## Plot the results plt.figure(figsize=(6, 6)) plt.subplot(2,1,1) ## plot the data plt.scatter( ## plot the fitted line plt.plot( plt.title('data and a nerual network fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  ## plot the residuals plt.subplot(2,1,2) plt.scatter( plt.title('residuals of a nerual network fit') plt.ylabel('Residual velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.tight_layout() plt.show()  <ul> <li>Compare the results with previous polynomial regression. How does the neural network perform?</li> </ul> <ul> <li>Load the basalt affinity data</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Load the basalt affinity data\n# basalt_data = pd.read_csv('data/Vermeesch2006.csv') ## reading from local file\nbasalt_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/Vermeesch2006.csv') ## reading from github for running on colab\nbasalt_data.tail()\n</pre> ## Load the basalt affinity data # basalt_data = pd.read_csv('data/Vermeesch2006.csv') ## reading from local file basalt_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/Vermeesch2006.csv') ## reading from github for running on colab basalt_data.tail() In\u00a0[\u00a0]: Copied! <pre>## Review the data\nplt.figure(figsize=(8, 6))\n\n## plot each affinity as a different color\nfor affinity in basalt_data['affinity'].unique():\n    subset = \n    plt.scatter(\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> ## Review the data plt.figure(figsize=(8, 6))  ## plot each affinity as a different color for affinity in basalt_data['affinity'].unique():     subset =      plt.scatter(  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show()  <ul> <li>Prepare the data into features (X) and target (y). This is same as the previous lecture.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Prepare the data into features (X) and target (y)\nX = \ny = \n\n## Encode the target variable\nle = LabelEncoder()\ny = \n\n## Impute missing values using median imputation\nimputer = SimpleImputer(strategy='median')\nX = \n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = \n</pre> ## Prepare the data into features (X) and target (y) X =  y =   ## Encode the target variable le = LabelEncoder() y =   ## Impute missing values using median imputation imputer = SimpleImputer(strategy='median') X =   ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test =  <ul> <li>Let's start to build the second neural network model to fit the basalt affinity data.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n## Normalize the data to make the training process more efficient\nmu = X_train_tensor.mean(dim=0, keepdim=True)\nstd = X_train_tensor.std(dim=0, keepdim=True)\nX_train_tensor = (X_train_tensor - mu) / std\nX_test_tensor = (X_test_tensor - mu) / std\n\n## Define the neural network model\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size,  output_size, hidden_size):\n        super(SimpleNN, self).__init__()\n        ## Define the neural network layers\n        self.fc1 = \n        self.fc2 = \n    \n    def forward(self, x):\n        ## Apply the neural network layers\n        x = \n        x = \n        x = \n        return x\n\n## Initialize the model, loss function, and optimizer\ninput_size = X_train.shape[-1]\noutput_size = len(le.classes_) # Output layer for classification (number of classes)\nhidden_size = 16\n\n## Define the model, loss function, and optimizer. Hint: using your defined model, CrossEntropy loss, and Adam optimizer\nmodel = \ncriterion = \noptimizer = \n\n## Define fit function\ndef fit(model, X_train, y_train, epochs=100):\n    ## set the model to training mode\n    model.\n    losses = []\n    for epoch in range(epochs):\n        ## zero the gradients\n        optimizer.zero_grad()\n\n        ## get the outputs from the model\n        outputs = \n        ## calculate the loss\n        loss = \n        loss.backward()\n        ## update the weights\n        optimizer.step()\n\n        losses.append(loss.item())\n        if (epoch+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n    return losses\n\n## Define predict function\ndef predict(model, X):\n    ## set the model to evaluation mode\n    model.\n    with torch.no_grad():\n        ## get the outputs from the model\n        outputs = \n        _, predicted = torch.max(outputs, 1)\n    return predicted\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=100)\n\n## Plot the loss\nplt.figure()\nplt.plot(\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Convert data to PyTorch tensors X_train_tensor = torch.tensor(X_train, dtype=torch.float32) y_train_tensor = torch.tensor(y_train, dtype=torch.long) X_test_tensor = torch.tensor(X_test, dtype=torch.float32) y_test_tensor = torch.tensor(y_test, dtype=torch.long)  ## Normalize the data to make the training process more efficient mu = X_train_tensor.mean(dim=0, keepdim=True) std = X_train_tensor.std(dim=0, keepdim=True) X_train_tensor = (X_train_tensor - mu) / std X_test_tensor = (X_test_tensor - mu) / std  ## Define the neural network model class SimpleNN(nn.Module):     def __init__(self, input_size,  output_size, hidden_size):         super(SimpleNN, self).__init__()         ## Define the neural network layers         self.fc1 =          self.fc2 =           def forward(self, x):         ## Apply the neural network layers         x =          x =          x =          return x  ## Initialize the model, loss function, and optimizer input_size = X_train.shape[-1] output_size = len(le.classes_) # Output layer for classification (number of classes) hidden_size = 16  ## Define the model, loss function, and optimizer. Hint: using your defined model, CrossEntropy loss, and Adam optimizer model =  criterion =  optimizer =   ## Define fit function def fit(model, X_train, y_train, epochs=100):     ## set the model to training mode     model.     losses = []     for epoch in range(epochs):         ## zero the gradients         optimizer.zero_grad()          ## get the outputs from the model         outputs =          ## calculate the loss         loss =          loss.backward()         ## update the weights         optimizer.step()          losses.append(loss.item())         if (epoch+1) % 10 == 0:             print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')     return losses  ## Define predict function def predict(model, X):     ## set the model to evaluation mode     model.     with torch.no_grad():         ## get the outputs from the model         outputs =          _, predicted = torch.max(outputs, 1)     return predicted  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=100)  ## Plot the loss plt.figure() plt.plot( plt.xlabel('Epochs') plt.ylabel('Loss') plt.show()  <ul> <li>Evaluate the model on the test set. This is same as the previous lecture.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Predict on the test set\ny_pred_tensor = \ny_pred = y_pred_tensor.numpy()\n\n## Calculate accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.4f}')\n\n## Confusion matrix; Hint: use confusion_matrix from sklearn.metrics\nconf_matrix = \ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Predict on the test set y_pred_tensor =  y_pred = y_pred_tensor.numpy()  ## Calculate accuracy accuracy =  print(f'Accuracy: {accuracy:.4f}')  ## Confusion matrix; Hint: use confusion_matrix from sklearn.metrics conf_matrix =  disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <ul> <li>Compare the results with previous classification methods. How does the neural network perform?</li> </ul> <ul> <li>Compare the two neural networks built for the regression and classification tasks. Please list the similarities and differences.</li> </ul> <ul> <li>The neural networks we built are very simple with only one hidden layer. Do you know which variable controls the complexity of the neural networks?</li> </ul> <ul> <li>If we want to build a more complex neural network, how can we do it? Think about the number of layers and neurons in each layer.</li> </ul> <p>If you are interested to build a more complex neural network, you can try the following website.</p> <p>The more layers and neurons you add, the more complex the neural network becomes, it can fit more complex data, while in the meantime, it is also more challenging to train.</p> <p>There are many hyperparameters you can tune in the online playgroud. Explore if we can find the parameters that can fit all the data distributions.</p> <p>Train a neural network online</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/09_neural_networks1/#regression-and-classification-with-neural-networks","title":"Regression and Classification with Neural Networks\u00b6","text":"Run in Google Colab"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/09_neural_networks1/#11-a-brief-history","title":"1.1. A Brief History\u00b6","text":"<p>In the 1940s, NNs were conceived.</p> <p>In the 1960s, the concept of backpropagation came, then people know how to train them.</p> <p>In 2010, NNs started winning competitions and get much attention than before.</p> <p>Since 2010, NNs have been on a meteoric rise as their magical ability to solve problems previously deemed unsolvable (i.e., image captioning, language translation, audio and video synthesis, and more).</p> <p>One important milestone is the AlexNet architecture in 2012, which won the ImageNet competition.</p> <p></p> <p></p> <p>The ImageNet competition is a benchmark for image classification, where the goal is to classify images into one of 1,000 categories.</p> <p></p> <p>You can find more information about the AlexNet model on Wikipedia. We will use the AlexNet model in the next lecture to classify images of rocks.</p> <p>Currently, NNs are the primary solution to most competitions and technological challenges like self-driving cars, calculating risk, detecting fraud, early cancer detection,\u2026</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/09_neural_networks1/#12-what-is-a-neural-network","title":"1.2. What is a Neural Network?\u00b6","text":"<p>ANNs are inspired by the organic brain, translated to the computer.</p> <p>ANNs have neurons, activations, and interconnectivities.</p> <p>NNs are considered \u201cblack boxes\u201d between inputs and outputs.</p> <p>Each connection between neurons has a weight associated with it. Weights are multiplied by corresponding input values. These multiplications flow into the neuron and are summed before being added with a bias. Weights and biases are trainable or tunable.</p> <p>$$ \\begin{aligned} output &amp; = weight \\cdot input + bias \\\\ y &amp; = a \\cdot x + b \\end{aligned} $$</p> <p>The formula should look very familiar to you. It is similar to the previous linear regression and classification models.</p> <p>Then, an activation function is applied to the output.</p> <p>$$ \\begin{aligned} output &amp; = \\sum (weight \\cdot input) + bias \\\\ output &amp; = activation (output) \\end{aligned} $$</p> <p>When a step function that mimics a neuron in the brain (i.e., \u201cfiring\u201d or not, on-off switch) is used as an activation function:</p> <ul> <li>If its output is greater than 0, the neuron fires (it would output 1).</li> <li>If its output is less than 0, the neuron does not fire and would pass along a 0.</li> </ul> <p>The input layer represents the actual input data (i.e., pixel values from an image, temperature, \u2026)</p> <ul> <li>The data can be \u201craw\u201d, should be preprocessed like normalization and scaling.</li> <li>The input needs to be in numeric form.</li> </ul> <p>The output layer is whatever the NN returns.</p> <ul> <li>In regression, the predicted value is a scalar value, the output layer has a single neuron.</li> <li>In classification, the class of the input is predicted, the output layer has as many neurons as the training dataset has classes. But can also have a single output neuron for binary (two classes) classification.</li> </ul> <p>A typical NN has thousands or even up to millions of adjustable parameters (weights and biases).</p> <p>NNs act as enormous functions with vast numbers of parameters.</p> <p>Finding the combination of parameter (weight and bias) values is the challenging part.</p> <p>The end goal for NNs is to adjust their weights and biases (the parameters), so they produce the desired output for unseen data.</p> <p>A major issue in supervised learning is overfitting, where the algorithm doesn\u2019t understand underlying input-output dependencies, just basically \u201cmemorizes\u201d the training data.</p> <p>The goal of NN is generalization, that can be obtained when separating the data into training data and validation data.</p> <p>Weights and biases are adjusted based on the error/loss presenting how \u201cwrong\u201d the algorithm in NN predicting the output.</p> <p>NNs can be used for regression (predict a scalar, singular, value), clustering (assigned unstructured data into groups), and many other tasks.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/09_neural_networks1/#applying-neural-networks-for-regression","title":"Applying Neural Networks for Regression\u00b6","text":"<p>In today's lecture, we will revisit the Betoule data and apply neural networks for regression.</p> <p>If you already forgot the background of this data, please review the lecture 04 regression.</p> <p>Remember the challenge of the Betoule data is that the velocity is non-linear with respect to the distance.</p> <p>In the previous lecture, we used sklearn to fit the linear regression model with high polynomial degrees.</p> <p>Here we will use PyTorch to fit the Betoule data and compare the results with the linear regression model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/09_neural_networks1/#applying-neural-networks-for-classification","title":"Applying Neural Networks for Classification\u00b6","text":"<p>Neural networks work well for the regression tasks, how about the classification tasks?</p> <p>Let's continue to apply neural networks for the binary classification task.</p> <p>Again, we will re-use the basalt affinity dataset that we covered in the previous lecture.</p> <p>If you already forgot the background of this data, please review the lecture 05 classification.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/","title":"10 neural networks2","text":"<p>Note: If you are running this in a colab notebook, we recommend you enable a free GPU by going:</p> <p>Runtime \u2006\u2006\u2192\u2006\u2006 Change runtime type \u2006\u2006\u2192\u2006\u2006 Hardware Accelerator: GPU</p> <p>GPUs are commonly used to accelerate training of deep learning models. If you want to learn more about GPUs or even TPUs for training nerual networks, check out this short video.</p> <p>This notebooks shows how to define and train a simple modern Neural Network with PyTorch.</p> <p>We will use the MNIST dataset, which is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.</p> <p>The dateaset is very popular and is often used as a \"Hello World\" example in the field of Machine Learning. It is a good dataset to start with, as it is small and easy to work with.</p> <p>For this small dataset, we will use the socalled \"LeNet\" architecture is used here. It is a simple convolutional neural network, which was introduced by Yann LeCun in 1998. It is a simple and effective architecture for small image datasets. You can read more about the model on Wikipedia.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay import torch from torch import nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>## Set random seed\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n</pre> ## Set random seed torch.manual_seed(0) torch.cuda.manual_seed(0) np.random.seed(0) <p>Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image.</p> <p>Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.</p> In\u00a0[\u00a0]: Copied! <pre>mnist = fetch_openml('mnist_784', as_frame=False)\n</pre> mnist = fetch_openml('mnist_784', as_frame=False) In\u00a0[\u00a0]: Copied! <pre>print(f\"MNIST data shape: {mnist.data.shape}, data type: {mnist.data.dtype}\")\nprint(f\"MNIST target shape: {mnist.target.shape}, target type: {mnist.target.dtype}\")\n</pre> print(f\"MNIST data shape: {mnist.data.shape}, data type: {mnist.data.dtype}\") print(f\"MNIST target shape: {mnist.target.shape}, target type: {mnist.target.dtype}\") In\u00a0[\u00a0]: Copied! <pre>## Make sure the data is float32 and the labels are int64\nX = mnist.data.astype('float32')\ny = mnist.target.astype('int64')\n\n## Normalize the data to [0, 1]. Hint: the raw pixel values are in [0, 255].\nX =\nprint(f\"{X.min() = }, {X.max() = }\")\n</pre> ## Make sure the data is float32 and the labels are int64 X = mnist.data.astype('float32') y = mnist.target.astype('int64')  ## Normalize the data to [0, 1]. Hint: the raw pixel values are in [0, 255]. X = print(f\"{X.min() = }, {X.max() = }\") <p>Same as prevoious lectures, let split the data into training and testing sets.</p> In\u00a0[\u00a0]: Copied! <pre>## Split data into training and testing sets using 30% of the data for testing and random seed 42\nX_train, X_test, y_train, y_test = \n</pre> ## Split data into training and testing sets using 30% of the data for testing and random seed 42 X_train, X_test, y_train, y_test =  In\u00a0[\u00a0]: Copied! <pre>## Define a function to plot a selection of images and their labels\ndef plot_example(X, y, n_samples=10):\n    \"\"\"Plot the first n_samples images and their labels in a row.\"\"\"\n    fig, axes = plt.subplots(1, n_samples, figsize=(2*n_samples, 4))\n    for i, ax in enumerate(axes):\n        ax.imshow(X[i].reshape(28, 28), cmap='gray')\n        ax.set_title(y[i], fontsize=32)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n</pre> ## Define a function to plot a selection of images and their labels def plot_example(X, y, n_samples=10):     \"\"\"Plot the first n_samples images and their labels in a row.\"\"\"     fig, axes = plt.subplots(1, n_samples, figsize=(2*n_samples, 4))     for i, ax in enumerate(axes):         ax.imshow(X[i].reshape(28, 28), cmap='gray')         ax.set_title(y[i], fontsize=32)         ax.axis('off')     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>## Plot the first 10 images from the training set\nplot_example(\n</pre> ## Plot the first 10 images from the training set plot_example( <ul> <li>Prepare the data for training and testing</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.int64)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n</pre> ## Convert data to PyTorch tensors X_train_tensor = torch.tensor(X_train, dtype=torch.float32) y_train_tensor = torch.tensor(y_train, dtype=torch.int64) X_test_tensor = torch.tensor(X_test, dtype=torch.float32) y_test_tensor = torch.tensor(y_test, dtype=torch.int64) <ul> <li>Build a simple fully connected neural network in PyTorch's framework.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Define the network architecture: A simple fully connected neural network\nclass FCN(nn.Module):\n    def __init__(\n            self,\n            input_dim=28*28,\n            hidden_dim=28*4,\n            output_dim=10,\n            dropout=0.5,\n    ):\n        super(FCN, self).__init__()\n        ## Define the neural network layers\n        self.fc1 = \n        self.fc2 = \n        self.dropout = \n\n    def forward(self, x, **kwargs):\n        ## Apply the neural network layers\n        x = \n        x = \n        x = \n        x = \n        return x\n</pre> ## Define the network architecture: A simple fully connected neural network class FCN(nn.Module):     def __init__(             self,             input_dim=28*28,             hidden_dim=28*4,             output_dim=10,             dropout=0.5,     ):         super(FCN, self).__init__()         ## Define the neural network layers         self.fc1 =          self.fc2 =          self.dropout =       def forward(self, x, **kwargs):         ## Apply the neural network layers         x =          x =          x =          x =          return x In\u00a0[\u00a0]: Copied! <pre>## Define the model dimensions\nmnist_dim = X.shape[1]\nhidden_dim = int(mnist_dim/8)\noutput_dim = len(np.unique(mnist.target))\n\n## Define the model, loss function, and optimizer\nmodel = \ncriterion = \noptimizer = \n\n## Define the device to choose the fastest for training\n## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise\ndevice = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\n## Define fit function\ndef fit(model, X_train, y_train, epochs=100):\n    dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)\n    model.to(device)\n\n    ## set the model to training mode\n    model.\n    losses = []\n    for epoch in range(epochs):\n        loss = 0\n        for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):\n            X_train = X_train.to(device)\n            y_train = y_train.to(device)\n            ## zero the gradients\n            optimizer.zero_grad()\n\n            ## get the model predictions\n            outputs = \n            ## calculate the loss\n            batch_loss = \n            batch_loss.backward()\n            ## update the weights\n            optimizer.\n            \n            loss += batch_loss.item()\n\n        # average loss per batch\n        loss = loss / len(dataloader)\n        losses.append(loss)\n\n        print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')\n\n    return losses\n\n## Define predict function\ndef predict(model, X):\n    dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)\n    ## set the model to evaluation mode\n    model.\n    device = next(model.parameters()).device\n    with torch.no_grad():\n        predicted = []\n        for X, in tqdm(dataloader, desc='Predicting'):\n            X = X.to(device)\n            \n            ## get the model predictions\n            outputs =\n\n            _, predicted_batch = torch.max(outputs, 1)\n            predicted.append(predicted_batch.cpu())\n    return torch.cat(predicted)\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=15)\n\n## Plot the loss\nplt.figure()\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Define the model dimensions mnist_dim = X.shape[1] hidden_dim = int(mnist_dim/8) output_dim = len(np.unique(mnist.target))  ## Define the model, loss function, and optimizer model =  criterion =  optimizer =   ## Define the device to choose the fastest for training ## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'  ## Define fit function def fit(model, X_train, y_train, epochs=100):     dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)     model.to(device)      ## set the model to training mode     model.     losses = []     for epoch in range(epochs):         loss = 0         for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):             X_train = X_train.to(device)             y_train = y_train.to(device)             ## zero the gradients             optimizer.zero_grad()              ## get the model predictions             outputs =              ## calculate the loss             batch_loss =              batch_loss.backward()             ## update the weights             optimizer.                          loss += batch_loss.item()          # average loss per batch         loss = loss / len(dataloader)         losses.append(loss)          print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')      return losses  ## Define predict function def predict(model, X):     dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)     ## set the model to evaluation mode     model.     device = next(model.parameters()).device     with torch.no_grad():         predicted = []         for X, in tqdm(dataloader, desc='Predicting'):             X = X.to(device)                          ## get the model predictions             outputs =              _, predicted_batch = torch.max(outputs, 1)             predicted.append(predicted_batch.cpu())     return torch.cat(predicted)  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=15)  ## Plot the loss plt.figure() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('Loss') plt.show() <ul> <li>Evaluate the model on the test set. This is same as the previous lecture.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Predict on the test set\ny_pred_tensor = \ny_pred = y_pred_tensor.numpy()\n\n## Calculate accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.4f}')\n\n## Confusion matrix\nconf_matrix = \ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y))))\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Predict on the test set y_pred_tensor =  y_pred = y_pred_tensor.numpy()  ## Calculate accuracy accuracy =  print(f'Accuracy: {accuracy:.4f}')  ## Confusion matrix conf_matrix =  disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y)))) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <p>What accuracy did you get? Is it above 95%?</p> <p>An accuracy of above 95% for a network with only one hidden layer is not too bad.</p> <p>Let's take a look at some predictions that went wrong:</p> In\u00a0[\u00a0]: Copied! <pre>error_mask_fcn = y_pred != y_test\nplot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn], n_samples=10)\n</pre> error_mask_fcn = y_pred != y_test plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn], n_samples=10) <p>Are these errors reasonable?</p> <ul> <li>Prepare the data for training and testing</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Convert data to PyTorch tensors and reshape to 4D tensor (batch_size, channel, height, width)\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1, 28, 28)\ny_train_tensor = torch.tensor(y_train, dtype=torch.int64)\ny_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n</pre> ## Convert data to PyTorch tensors and reshape to 4D tensor (batch_size, channel, height, width) X_train_tensor = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28) X_test_tensor = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1, 28, 28) y_train_tensor = torch.tensor(y_train, dtype=torch.int64) y_test_tensor = torch.tensor(y_test, dtype=torch.int64) <ul> <li>Build a simple convolutional neural network in PyTorch's framework.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Define the network architecture: A simple convolutional neural network\nclass CNN(nn.Module):\n    def __init__(self, input_dim=28*28, hidden_dim=112, output_dim=10, dropout=0.5):\n        super(CNN, self).__init__()\n        ## Define the neural network layers\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.conv1_drop = nn.Dropout2d(p=dropout)\n        self.conv2_drop = nn.Dropout2d(p=dropout)\n        num_features = 64 * int((input_dim**0.5 // 4 - 2)**2)\n        self.fc1 = nn.Linear(num_features, hidden_dim) # 1600 = number channels * width * height\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.fc1_drop = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        \n        ## convolutional layers\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.conv1_drop(x)\n\n        ## convolutional layers\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.conv2_drop(x)\n        \n        ## flatten over channel, height and width        \n        x = x.view(x.size(0), -1)\n        \n        ## fully connected layers\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc1_drop(x)\n        x = self.fc2(x)\n\n        return x\n</pre> ## Define the network architecture: A simple convolutional neural network class CNN(nn.Module):     def __init__(self, input_dim=28*28, hidden_dim=112, output_dim=10, dropout=0.5):         super(CNN, self).__init__()         ## Define the neural network layers         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)         self.conv1_drop = nn.Dropout2d(p=dropout)         self.conv2_drop = nn.Dropout2d(p=dropout)         num_features = 64 * int((input_dim**0.5 // 4 - 2)**2)         self.fc1 = nn.Linear(num_features, hidden_dim) # 1600 = number channels * width * height         self.fc2 = nn.Linear(hidden_dim, output_dim)         self.fc1_drop = nn.Dropout(p=dropout)      def forward(self, x):                  ## convolutional layers         x = self.conv1(x)         x = F.relu(x)         x = F.max_pool2d(x, 2)         x = self.conv1_drop(x)          ## convolutional layers         x = self.conv2(x)         x = F.relu(x)         x = F.max_pool2d(x, 2)         x = self.conv2_drop(x)                  ## flatten over channel, height and width                 x = x.view(x.size(0), -1)                  ## fully connected layers         x = self.fc1(x)         x = F.relu(x)         x = self.fc1_drop(x)         x = self.fc2(x)          return x In\u00a0[\u00a0]: Copied! <pre>## Define model dimensions\nmnist_dim = X.shape[1]\nhidden_dim = int(mnist_dim/8)\noutput_dim = len(np.unique(mnist.target))\n\n## Define the model, loss function, and optimizer\nmodel = \ncriterion = \noptimizer =\n\n## Define the device to choose the fastest for training\n## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise\ndevice = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\n## Define fit function\ndef fit(model, X_train, y_train, epochs=100):\n\n    dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)\n    model.to(device)\n\n    ## set the model to training mode\n    model.\n    losses = []\n    for epoch in range(epochs):\n        loss = 0\n        for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):\n            X_train = X_train.to(device)\n            y_train = y_train.to(device)\n            ## zero the gradients\n            optimizer.zero_grad()\n\n            ## get the model predictions\n            outputs = \n            ## calculate the loss\n            batch_loss = \n            batch_loss.backward()\n            ## update the weights\n            optimizer.\n\n            loss += batch_loss.item()\n\n        # average loss per batch\n        loss = loss / len(dataloader)\n        losses.append(loss)\n\n        print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')\n\n    return losses\n\n## Define predict function\ndef predict(model, X):\n\n    dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)\n\n    ## set the model to evaluation mode\n    model.\n    device = next(model.parameters()).device\n    with torch.no_grad():\n        predicted = []\n        for X, in tqdm(dataloader, desc='Predicting'):\n            X = X.to(device)\n\n            ## get the model predictions\n            outputs = \n            \n            _, predicted_batch = torch.max(outputs, 1)\n            predicted.append(predicted_batch.cpu())\n    return torch.cat(predicted)\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=15)\n\n## Plot the loss\nplt.figure()\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Define model dimensions mnist_dim = X.shape[1] hidden_dim = int(mnist_dim/8) output_dim = len(np.unique(mnist.target))  ## Define the model, loss function, and optimizer model =  criterion =  optimizer =  ## Define the device to choose the fastest for training ## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'  ## Define fit function def fit(model, X_train, y_train, epochs=100):      dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)     model.to(device)      ## set the model to training mode     model.     losses = []     for epoch in range(epochs):         loss = 0         for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):             X_train = X_train.to(device)             y_train = y_train.to(device)             ## zero the gradients             optimizer.zero_grad()              ## get the model predictions             outputs =              ## calculate the loss             batch_loss =              batch_loss.backward()             ## update the weights             optimizer.              loss += batch_loss.item()          # average loss per batch         loss = loss / len(dataloader)         losses.append(loss)          print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')      return losses  ## Define predict function def predict(model, X):      dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)      ## set the model to evaluation mode     model.     device = next(model.parameters()).device     with torch.no_grad():         predicted = []         for X, in tqdm(dataloader, desc='Predicting'):             X = X.to(device)              ## get the model predictions             outputs =                           _, predicted_batch = torch.max(outputs, 1)             predicted.append(predicted_batch.cpu())     return torch.cat(predicted)  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=15)  ## Plot the loss plt.figure() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('Loss') plt.show() In\u00a0[\u00a0]: Copied! <pre>## Predict on the test set\ny_pred_tensor = \ny_pred = y_pred_tensor.numpy()\n\n## Calculate accuracy\naccuracy = \nprint(f'Accuracy: {accuracy:.4f}')\n\n## Confusion matrix\nconf_matrix = \ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y))))\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Predict on the test set y_pred_tensor =  y_pred = y_pred_tensor.numpy()  ## Calculate accuracy accuracy =  print(f'Accuracy: {accuracy:.4f}')  ## Confusion matrix conf_matrix =  disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y)))) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <ul> <li>What accuracy did you get? Is it better than the fully connected network?</li> </ul> <p>An accuracy of &gt;98% should suffice for this example!</p> <p>Let's take a look at some predictions that went wrong:</p> In\u00a0[\u00a0]: Copied! <pre>error_mask_cnn = y_pred != y_test\nplot_example(X_test[error_mask_cnn], y_pred[error_mask_cnn], n_samples=10)\n</pre> error_mask_cnn = y_pred != y_test plot_example(X_test[error_mask_cnn], y_pred[error_mask_cnn], n_samples=10) <ul> <li>Let's further look at the accuracy of the convolutional network model (CNN) on the misclassified examples previously by the fully connected network (FCN).</li> </ul> In\u00a0[\u00a0]: Copied! <pre>accuracy = accuracy_score(y_test[error_mask_fcn], y_pred[error_mask_fcn])\nprint(f\"Accuracy: {accuracy:.4f}\")\n</pre> accuracy = accuracy_score(y_test[error_mask_fcn], y_pred[error_mask_fcn]) print(f\"Accuracy: {accuracy:.4f}\") <p>About 70% of the previously misclassified images are now correctly identified.</p> <p>Let's take a look at some of the misclassified examples before:</p> In\u00a0[\u00a0]: Copied! <pre>plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn])\n</pre> plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn]) <ul> <li>Last questions. Please take a look at the training loops of the fully connected network and the convolutional network. Comment on the similarities and differences.</li> </ul> <p>You can see although the network architecture is different, the training loops are very similar.</p> <p>This is one feature of neural networks models. You can build significantly larger models and train them efficiently with a similar training loop, as long as you have enough computational power.</p> <p>Bonus points: One challenge in deep learning training is to find the proper hyperparameters. There are many hyperparameters to tune, such as the learning rate, the number of hidden layers, the number of neurons in each layer, the batch size, the number of epochs, etc.</p> <p>Try to tune the hyperparameters of the convolutional network model to achieve a higher accuracy. For the MNIST dataset, an accuracy of &gt;99% is possible with the LeNet architecture. Could you achieve this?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/#neural-networks-with-pytorch","title":"Neural Networks with PyTorch\u00b6","text":"Run in Google Colab"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/#loading-data","title":"Loading Data\u00b6","text":"<p>Using SciKit-Learns <code>fetch_openml</code> to load MNIST data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/#preprocessing-data","title":"Preprocessing Data\u00b6","text":"<p>The above <code>featch_mldata</code> method to load MNIST returns <code>data</code> and <code>target</code> as <code>uint8</code> which we convert to <code>float32</code> and <code>int64</code> respectively.</p> <p>To avoid big weights that deal with the pixel values from between [0, 255], we scale <code>X</code> down. A commonly used range is [0, 1].</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/#visualize-a-selection-of-training-images-and-their-labels","title":"Visualize a selection of training images and their labels\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/#build-neural-network-with-pytorch","title":"Build Neural Network with PyTorch\u00b6","text":"<p>In the previous lecture, we have built a simple fully connected neural network with one hidden layer for both linear regression and classification tasks. Let's first try a similar network for the classification task of MNIST.</p> <p>Note the dataset is much larger than our previous examples, so we need to adjust the network size accordingly. (It is still tiny compared to modern standards)</p> <p>Let's think about the network architecture:</p> <ul> <li>Input layer: 784 dimensions (28x28). This is defined by the MNIST data shape.</li> <li>Hidden layer: 98 (= 784 / 8). This is a free parameter that we can choose.</li> <li>Output layer: 10 neurons, representing digits 0 - 9. This is defined by the number of classes in the dataset.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/10_neural_networks2/#convolutional-network","title":"Convolutional Network\u00b6","text":"<p>To further improve the performance, let's try a convolutional neural network (CNN) for MNIST.</p> <p>The 2D convolutional layer expects a 4 dimensional tensor as input. The dimensions represent:</p> <ul> <li>Batch size</li> <li>Number of channel</li> <li>Height</li> <li>Width</li> </ul> <p>So we need to reshape the MNIST data to have the right shape. MNIST data has only one channel. As stated above, each MNIST vector represents a 28x28 pixel image. Hence, the resulting shape for PyTorch tensor needs to be (x, 1, 28, 28).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/","title":"11 neural networks3","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\nimport os\nif not os.path.exists('LiCS'):\n    if not os.path.exists('LiCS.zip'):\n        !wget https://github.com/AI4EPS/EPS88_PyEarth/releases/download/LiCS/LiCS.zip\n    !unzip LiCS.zip\nelse:\n    print(\"File already exists.\")\n</pre> %%capture import os if not os.path.exists('LiCS'):     if not os.path.exists('LiCS.zip'):         !wget https://github.com/AI4EPS/EPS88_PyEarth/releases/download/LiCS/LiCS.zip     !unzip LiCS.zip else:     print(\"File already exists.\") In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\nimport random\n</pre> import torch import torch.nn as nn import torch.optim as optim import numpy as np from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from tqdm import tqdm from collections import Counter from torch.utils.data import Subset from sklearn.model_selection import train_test_split import random In\u00a0[\u00a0]: Copied! <pre>## Set random seed\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n</pre> ## Set random seed torch.manual_seed(0) torch.cuda.manual_seed(0) np.random.seed(0) In\u00a0[\u00a0]: Copied! <pre># We run the model on the GPU if possible\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\n# If you get errors about CUDA running out of memory, you can set it to run on the CPU by uncommenting the next line.\n# device = torch.device(\"cpu\")\nprint(device)\n</pre> # We run the model on the GPU if possible if torch.cuda.is_available():     device = torch.device(\"cuda\") elif torch.backends.mps.is_available():     device = torch.device(\"mps\") else:     device = torch.device(\"cpu\")  # If you get errors about CUDA running out of memory, you can set it to run on the CPU by uncommenting the next line. # device = torch.device(\"cpu\") print(device) In\u00a0[\u00a0]: Copied! <pre># Path to the dataset\ndataset_dir = 'LiCS'\n\n# Transformers applied to the dataset\ndata_transforms = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ndataset = datasets.ImageFolder(root=dataset_dir, transform=data_transforms)\nclass_names = dataset.classes\n\n# Show information about the dataset\nprint(f'Dataset size: {len(dataset)}')\nprint(f'Mapping from class names to indexes: {dataset.class_to_idx}')\nprint(f'Number of samples per class: {dict(Counter(dataset.targets))}')\n</pre> # Path to the dataset dataset_dir = 'LiCS'  # Transformers applied to the dataset data_transforms = transforms.Compose([     transforms.ToTensor(), ])  dataset = datasets.ImageFolder(root=dataset_dir, transform=data_transforms) class_names = dataset.classes  # Show information about the dataset print(f'Dataset size: {len(dataset)}') print(f'Mapping from class names to indexes: {dataset.class_to_idx}') print(f'Number of samples per class: {dict(Counter(dataset.targets))}') In\u00a0[\u00a0]: Copied! <pre>def plot_example(images, labels, show_labels=False):\n    num_cols = 4\n    num_rows = (len(images) - 1) // num_cols + 1\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), squeeze=False)\n    for i in range(num_rows):\n        for j in range(num_cols):\n            idx = i * num_cols + j\n            if idx &lt; len(images):\n                image = images[idx][0]\n                image = image * 2 * np.pi - np.pi\n                axs[i, j].imshow(image, cmap='jet', vmin=-np.pi, vmax=np.pi)\n                if show_labels:\n                    axs[i, j].set_title(class_names[labels[idx]])\n                axs[i, j].axis('off')\n            else:\n                axs[i, j].axis('off')\n    plt.show()\n</pre> def plot_example(images, labels, show_labels=False):     num_cols = 4     num_rows = (len(images) - 1) // num_cols + 1     fig, axs = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), squeeze=False)     for i in range(num_rows):         for j in range(num_cols):             idx = i * num_cols + j             if idx &lt; len(images):                 image = images[idx][0]                 image = image * 2 * np.pi - np.pi                 axs[i, j].imshow(image, cmap='jet', vmin=-np.pi, vmax=np.pi)                 if show_labels:                     axs[i, j].set_title(class_names[labels[idx]])                 axs[i, j].axis('off')             else:                 axs[i, j].axis('off')     plt.show() In\u00a0[\u00a0]: Copied! <pre>num_samples_per_class = 10\nnoise_indices = [i for i, label in enumerate(dataset.targets) if label == 0][:num_samples_per_class]\nvolcano_indices = [i for i, label in enumerate(dataset.targets) if label == 1][:num_samples_per_class]\nindices = noise_indices + volcano_indices\nrandom.shuffle(indices)\nimages, labels = zip(*[dataset[i] for i in indices])\nplot_example(images, labels, show_labels=False)\n</pre> num_samples_per_class = 10 noise_indices = [i for i, label in enumerate(dataset.targets) if label == 0][:num_samples_per_class] volcano_indices = [i for i, label in enumerate(dataset.targets) if label == 1][:num_samples_per_class] indices = noise_indices + volcano_indices random.shuffle(indices) images, labels = zip(*[dataset[i] for i in indices]) plot_example(images, labels, show_labels=False) <p>Question: Based on the images, can you tell which images are from the class 'volcano_deformation' and which are from the class 'background_noise'? How can you tell the difference?</p> <p>Question: Pass <code>show_labels=True</code> to the function and run the code again to show the ground truth labels of the images. Do the labels match your prediction?</p> <p>Now let's see if we can also train a neural network to differentiate between the two classes. If we can, we can use this neural network to automatically detect volcanic deformation signals in InSAR images. As you can imagine, the InSAR satellites are taking thousands of images every day, and it is impossible for a human to look at all of them. This is where the neural network and machine learning come in handy.</p> <p>Recall what we learned in the previous lectures about neural networks. We will folow a similar approach to build a neural network that can differentiate between the two classes of images for automatic detection of volcanic deformation signals.</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset_size = 2000\ntest_dataset_size = 200\n\ntargets = np.array(dataset.targets)\n# The two classes are unbalanced with 'volcano_deformation' having more images. Thus we remove some of them. \n# All the 3203 'background_noise' images are in front, followed by the 3605 'volcano_deformation' images. \n# So from the array with all the images, we can select the first 6406 images, which is all the 'background_noise' ones followed by 3203 'volcano_deformation' ones.\n# Thus, we select the first 6406 images from the dataset.\ntargets = targets[:6406]\ntrain_indices, test_indices = train_test_split(\n    np.arange(targets.shape[0]),\n    train_size=train_dataset_size,\n    test_size=test_dataset_size,\n    stratify=targets,\n)\n\n# We use the splits we obtained to create subsets of the main dataset, one for train and one for test\ntrain_dataset = Subset(dataset, indices=train_indices)\ntest_dataset = Subset(dataset, indices=test_indices)\n\n# Show information about our subsets\ntrain_classes = [dataset.targets[i] for i in train_dataset.indices]\nprint(f'Numner of samples per class for the train dataset: {dict(Counter(train_classes))}')\n\ntest_classes = [dataset.targets[i] for i in test_dataset.indices]\nprint(f'Numner of samples per class for the test dataset: {dict(Counter(test_classes))}')\n</pre> train_dataset_size = 2000 test_dataset_size = 200  targets = np.array(dataset.targets) # The two classes are unbalanced with 'volcano_deformation' having more images. Thus we remove some of them.  # All the 3203 'background_noise' images are in front, followed by the 3605 'volcano_deformation' images.  # So from the array with all the images, we can select the first 6406 images, which is all the 'background_noise' ones followed by 3203 'volcano_deformation' ones. # Thus, we select the first 6406 images from the dataset. targets = targets[:6406] train_indices, test_indices = train_test_split(     np.arange(targets.shape[0]),     train_size=train_dataset_size,     test_size=test_dataset_size,     stratify=targets, )  # We use the splits we obtained to create subsets of the main dataset, one for train and one for test train_dataset = Subset(dataset, indices=train_indices) test_dataset = Subset(dataset, indices=test_indices)  # Show information about our subsets train_classes = [dataset.targets[i] for i in train_dataset.indices] print(f'Numner of samples per class for the train dataset: {dict(Counter(train_classes))}')  test_classes = [dataset.targets[i] for i in test_dataset.indices] print(f'Numner of samples per class for the test dataset: {dict(Counter(test_classes))}') In\u00a0[\u00a0]: Copied! <pre>train_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=2,\n    shuffle=True\n)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=2,\n    # shuffle=False\n    shuffle=True ## just for visualization purposes\n)\n</pre> train_dataloader = torch.utils.data.DataLoader(     train_dataset,     batch_size=2,     shuffle=True )  test_dataloader = torch.utils.data.DataLoader(     test_dataset,     batch_size=2,     # shuffle=False     shuffle=True ## just for visualization purposes ) In\u00a0[\u00a0]: Copied! <pre>models.list_models()\n</pre> models.list_models() In\u00a0[\u00a0]: Copied! <pre>model = models.alexnet(weights='DEFAULT') # equivalent to ``models.alexnet(weights='IMAGENET1K_V1')``\nprint(model.children)\n</pre> model = models.alexnet(weights='DEFAULT') # equivalent to ``models.alexnet(weights='IMAGENET1K_V1')`` print(model.children) In\u00a0[\u00a0]: Copied! <pre># We take the last layer in the network, which is the output layer. In this case, it is the 6-th layer inside `classifier`\nprint(f'Initian output layer of the network: {model.classifier[6]}')\nin_features = model.classifier[6].in_features\nprint(f'Input features of the output layer: {in_features}')\n\n# We replace the last layer and set the output size to 2 since we have 2 classes\nnew_output_layer = nn.Linear(in_features=in_features, out_features=2)\nmodel.classifier[6] = new_output_layer\nprint(f'New output layer of the network: {model.classifier[6]}')\n</pre> # We take the last layer in the network, which is the output layer. In this case, it is the 6-th layer inside `classifier` print(f'Initian output layer of the network: {model.classifier[6]}') in_features = model.classifier[6].in_features print(f'Input features of the output layer: {in_features}')  # We replace the last layer and set the output size to 2 since we have 2 classes new_output_layer = nn.Linear(in_features=in_features, out_features=2) model.classifier[6] = new_output_layer print(f'New output layer of the network: {model.classifier[6]}') In\u00a0[\u00a0]: Copied! <pre># We move the model to the selected device (either GPU or CPU)\nmodel = \n\n# We define a loss function\ncriterion = \n\n# We define an optimizer. All parameters are being optimized\noptimizer = \n</pre> # We move the model to the selected device (either GPU or CPU) model =   # We define a loss function criterion =   # We define an optimizer. All parameters are being optimized optimizer =  In\u00a0[\u00a0]: Copied! <pre>def fit(model, criterion, optimizer, epochs=25, save_path='results/', save_name='best_model_params.pt', eval_interval=1):\n    start_time = time.time()\n    \n    if not os.path.isdir(save_path):\n        os.mkdir(save_path) \n    model_params_path = os.path.join(save_path, save_name)\n\n    torch.save(model.state_dict(), model_params_path)\n    best_acc = 0.0\n\n    for epoch in range(1, epochs + 1):\n        print(f'Epoch {epoch} / {epochs}')\n        print('*' * 20)\n\n        epoch_start = time.time()\n        # Training phase\n        # We need to set the model to training mode\n        model.\n        \n        total_loss = 0.0\n        total_correct_preds = 0\n\n        for inputs, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch} / {epochs}\"):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # get output of model\n            outputs = \n\n            # calculate the loss\n            loss = \n\n            # backpropagation\n            loss.backward()\n\n            # update the weights\n            optimizer.\n\n            # the prediction is the class with the highest probability\n            _, preds = torch.max(outputs, 1)\n\n            # Batch stats\n            total_loss += loss.item() * inputs.size(0)\n            total_correct_preds += torch.sum(preds == labels.data)\n\n\n        # Epoch stats\n        epoch_loss = total_loss / len(train_dataloader.dataset)\n        epoch_acc = total_correct_preds / len(train_dataloader.dataset)\n\n        print(f'Training Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}')\n        \n\n        if epoch % eval_interval == 0:\n            test_loss, test_acc = predict(model, criterion)\n\n            # save the model\n            if test_acc &gt; best_acc:\n                best_acc = test_acc\n                torch.save(model.state_dict(), model_params_path)\n\n        print()\n        time_elapsed = time.time() - epoch_start\n        print(f'Epoch complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n        print()\n    \n    time_elapsed = time.time() - start_time\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best evaluation Accuracy: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(torch.load(model_params_path, weights_only=True))\n    return model\n</pre> def fit(model, criterion, optimizer, epochs=25, save_path='results/', save_name='best_model_params.pt', eval_interval=1):     start_time = time.time()          if not os.path.isdir(save_path):         os.mkdir(save_path)      model_params_path = os.path.join(save_path, save_name)      torch.save(model.state_dict(), model_params_path)     best_acc = 0.0      for epoch in range(1, epochs + 1):         print(f'Epoch {epoch} / {epochs}')         print('*' * 20)          epoch_start = time.time()         # Training phase         # We need to set the model to training mode         model.                  total_loss = 0.0         total_correct_preds = 0          for inputs, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch} / {epochs}\"):             inputs = inputs.to(device)             labels = labels.to(device)              # zero the parameter gradients             optimizer.zero_grad()              # get output of model             outputs =               # calculate the loss             loss =               # backpropagation             loss.backward()              # update the weights             optimizer.              # the prediction is the class with the highest probability             _, preds = torch.max(outputs, 1)              # Batch stats             total_loss += loss.item() * inputs.size(0)             total_correct_preds += torch.sum(preds == labels.data)           # Epoch stats         epoch_loss = total_loss / len(train_dataloader.dataset)         epoch_acc = total_correct_preds / len(train_dataloader.dataset)          print(f'Training Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}')                   if epoch % eval_interval == 0:             test_loss, test_acc = predict(model, criterion)              # save the model             if test_acc &gt; best_acc:                 best_acc = test_acc                 torch.save(model.state_dict(), model_params_path)          print()         time_elapsed = time.time() - epoch_start         print(f'Epoch complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')         print()          time_elapsed = time.time() - start_time     print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')     print(f'Best evaluation Accuracy: {best_acc:4f}')      # load best model weights     model.load_state_dict(torch.load(model_params_path, weights_only=True))     return model <p>For testing, we don't need to calculate the gradient. In order to calculate the gradient, PyTorch saves all the operations performed on the data. This uses memory and is unnecessary during testing. Thus, we need to call torch.no_grad(). This tells the PyTorch library not to save the following operations.</p> In\u00a0[\u00a0]: Copied! <pre>def predict(model, criterion):\n    # Testing phase\n    # We need to set the model to evaluation mode\n    model.eval()\n    \n    total_loss = 0.0\n    total_correct_preds = 0\n\n    with torch.no_grad():  \n        for inputs, labels in test_dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n    \n            # get output of model\n            outputs =\n            \n            # calculate the loss\n            loss = \n\n            # the prediction is the class with the highest probability\n            _, preds = torch.max(outputs, 1)\n    \n            # stats\n            total_loss += loss.item() * inputs.size(0)\n            total_correct_preds += torch.sum(preds == labels.data)\n            \n    test_loss = total_loss / len(test_dataloader.dataset)\n    test_acc = total_correct_preds / len(test_dataloader.dataset)\n\n    print(f'Test Loss: {test_loss:.4f} Accuracy: {test_acc:.4f}')\n\n    return test_loss, test_acc\n</pre> def predict(model, criterion):     # Testing phase     # We need to set the model to evaluation mode     model.eval()          total_loss = 0.0     total_correct_preds = 0      with torch.no_grad():           for inputs, labels in test_dataloader:             inputs = inputs.to(device)             labels = labels.to(device)                  # get output of model             outputs =                          # calculate the loss             loss =               # the prediction is the class with the highest probability             _, preds = torch.max(outputs, 1)                  # stats             total_loss += loss.item() * inputs.size(0)             total_correct_preds += torch.sum(preds == labels.data)                  test_loss = total_loss / len(test_dataloader.dataset)     test_acc = total_correct_preds / len(test_dataloader.dataset)      print(f'Test Loss: {test_loss:.4f} Accuracy: {test_acc:.4f}')      return test_loss, test_acc In\u00a0[\u00a0]: Copied! <pre>model_conv = fit(model, criterion, optimizer, epochs=5, \n                save_path='results/', save_name='best_model_params.pt', eval_interval=1)\n\n# Evaluate the model again after training is complete\nprint()\nprint('Final results:')\ntest_loss, test_acc = predict(model, criterion)\n</pre> model_conv = fit(model, criterion, optimizer, epochs=5,                  save_path='results/', save_name='best_model_params.pt', eval_interval=1)  # Evaluate the model again after training is complete print() print('Final results:') test_loss, test_acc = predict(model, criterion) In\u00a0[\u00a0]: Copied! <pre>## Lists to save the results for visualization\ninputs = []\nlabels = []\npreds = []\n\n## Predict on the test dataset and \nmodel.eval()\nwith torch.no_grad():\n    for i, (input, label) in enumerate(test_dataloader):\n        input = input.to(device)\n        label = label.to(device)\n\n        ## Get the output of the model\n        output = \n\n        ## Get the class with the highest probability\n        pred = output.argmax(-1)\n\n        if not torch.equal(label, pred):\n            inputs.append(input)\n            labels.append(label)\n            preds.append(pred)\n\n## Concatenate the results for visualization\ninputs = torch.cat(inputs)\nlabels = torch.cat(labels)\npreds = torch.cat(preds)\n\n## Plot the results\nplot_example(inputs.cpu(), preds.cpu(), show_labels=True)\n</pre> ## Lists to save the results for visualization inputs = [] labels = [] preds = []  ## Predict on the test dataset and  model.eval() with torch.no_grad():     for i, (input, label) in enumerate(test_dataloader):         input = input.to(device)         label = label.to(device)          ## Get the output of the model         output =           ## Get the class with the highest probability         pred = output.argmax(-1)          if not torch.equal(label, pred):             inputs.append(input)             labels.append(label)             preds.append(pred)  ## Concatenate the results for visualization inputs = torch.cat(inputs) labels = torch.cat(labels) preds = torch.cat(preds)  ## Plot the results plot_example(inputs.cpu(), preds.cpu(), show_labels=True) <p>Let's compare the predictions of the model with the ground truth labels.</p> In\u00a0[\u00a0]: Copied! <pre>plot_example(inputs.cpu(), labels.cpu(), show_labels=True)\n</pre> plot_example(inputs.cpu(), labels.cpu(), show_labels=True) <p>Question: Do you think the model is doing a good job at classifying the images? For the examples where the model is wrong, can you tell why it is wrong?</p> <p>Question: Can you think of ways to improve the model?</p>  CONGRATULATIONS  <p>You finished the tutorial. If you want to see this model being run in real time, you can visit the COMET portal: https://comet.nerc.ac.uk/comet-volcano-portal/.</p> <p>Select a volcano you want and scroll to to bottom of the page. On top of identifying deformation in images, the model on the portal can also pinpoint the location of the deformation by showing which parts of the image have a higher probability.</p> <p>If you want to try your own image, you can use the code below:</p> In\u00a0[\u00a0]: Copied! <pre># If you stopped the notebook, you need to redefine the model. You can either run the code cells above that define the model\n# or uncomment the next lines\n# model = models.alexnet(weights='DEFAULT')\n# in_features = model.classifier[6].in_features\n# new_output_layer = nn.Linear(in_features=in_features, out_features=2)\n# model.classifier[6] = new_output_layer\n# model.to(device)\n\n# Uncomment the next line to load the model checkpoint\n# This only needs to be done if you closed the notebook or if you want to load a different checkpoint\n# model.load_state_dict(torch.load('results/best_model_params.pt', weights_only=True))\n\n\n# Replace this line with an image loaded by you\nimage = np.random.rand(500, 500, 3)\n\n# If your image has only one channel, uncomment the next line\n# image = torch.cat([image, image, image], dim=1)\n\n# We added a resize in case your image is not the correct size\nimage_transforms = transforms.Compose([\n    # Changes the input to a tensor, also brings the channels to the front, so image.shape = (3, 227, 227) after this operation\n    transforms.ToTensor(),\n    # In case the image is in another format\n    transforms.ConvertImageDtype(torch.float),\n    # Resize the image to the correct size\n    transforms.Resize((227, 227)),\n])\n\n# We apply the transforms to the image\nimage = image_transforms(image)\n# We change the shape of the image to 1x3x227x227 because the model needs a batch of images, so we shape our image as a batch of size 1\n# Here, image.shape = (3, 227, 227)\nimage = image.view(1, *image.shape)\n\nwith torch.no_grad():\n    image = image.to(device)\n    output = model(image)\n    preds = output.argmax(dim=-1)\n    probs = output.softmax(dim=-1)\n\n# We use image[0] to select the first image in our batch of size 1 (the only image in the batch)\nplot_example(image.cpu(), preds.cpu(), show_labels=True)\n</pre> # If you stopped the notebook, you need to redefine the model. You can either run the code cells above that define the model # or uncomment the next lines # model = models.alexnet(weights='DEFAULT') # in_features = model.classifier[6].in_features # new_output_layer = nn.Linear(in_features=in_features, out_features=2) # model.classifier[6] = new_output_layer # model.to(device)  # Uncomment the next line to load the model checkpoint # This only needs to be done if you closed the notebook or if you want to load a different checkpoint # model.load_state_dict(torch.load('results/best_model_params.pt', weights_only=True))   # Replace this line with an image loaded by you image = np.random.rand(500, 500, 3)  # If your image has only one channel, uncomment the next line # image = torch.cat([image, image, image], dim=1)  # We added a resize in case your image is not the correct size image_transforms = transforms.Compose([     # Changes the input to a tensor, also brings the channels to the front, so image.shape = (3, 227, 227) after this operation     transforms.ToTensor(),     # In case the image is in another format     transforms.ConvertImageDtype(torch.float),     # Resize the image to the correct size     transforms.Resize((227, 227)), ])  # We apply the transforms to the image image = image_transforms(image) # We change the shape of the image to 1x3x227x227 because the model needs a batch of images, so we shape our image as a batch of size 1 # Here, image.shape = (3, 227, 227) image = image.view(1, *image.shape)  with torch.no_grad():     image = image.to(device)     output = model(image)     preds = output.argmax(dim=-1)     probs = output.softmax(dim=-1)  # We use image[0] to select the first image in our batch of size 1 (the only image in the batch) plot_example(image.cpu(), preds.cpu(), show_labels=True) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#finetuning-alexnet-for-volcanic-deformation-classification","title":"Finetuning AlexNet for Volcanic Deformation Classification\u00b6","text":"Run in Google Colab <p>Note: If you are running this in a colab notebook, we recommend you enable a free GPU by going:</p> <p>Runtime \u2006\u2006\u2192\u2006\u2006 Change runtime type \u2006\u2006\u2192\u2006\u2006 Hardware Accelerator: GPU</p> <p>This tutorial demonstrates how to fine-tune a pre-trained AlexNet model on the LICS dataset, which consists of volcanic InSAR (Interferometric Synthetic Aperture Radar) images.</p> <p>As you recall, AlexNet is a convolutional neural network that won the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). You can watch this video to learn more about the AlexNet model: link.</p> <p>The dataset contains 6,808 images, divided into two categories: 3,605 images displaying deformation signals (class 1 'volcano_deformation') and 3,203 images without deformation signals (class 0 'background_noise'). Originally, the pre-trained AlexNet was designed to classify 1,000 types of natural objects, such as tables, cars, dogs, etc.</p> <p>In this tutorial, we will modify the final layer of the neural network to classify images into just two classes instead of the original 1,000.</p> <p></p> <p>Created by Robert Gabriel Popescu and Juliet Biggs, University of Bristol, UK.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#lics-dataset","title":"LiCS Dataset\u00b6","text":"<p>Download and Unzip the dataset and make sure the folder LiCS has two subfolders: background_noise and volcano_deformation.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#imports","title":"Imports\u00b6","text":"<p>We will import the necessary libraries:</p> <ul> <li>torch is the main library that will run the Artificial neural network, called PyTorch</li> <li>matplotlib is for creating graphs</li> <li>numpy is for storying arrays of data. A few other libraries work only with this (such as matplotlib)</li> <li>sklearn is a machine learning library that we will use to separate the dataset into train and test</li> <li>torchvision is a library for loading images as datasets to be used by PyTorch</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#running-the-models-on-the-gpu","title":"Running the models on the GPU\u00b6","text":"<p>We want to run the code on the GPU if possible. Artifial neural networks work faster on a GPU so we will select the device cuda if it is available. Alternatively, if you are using a Mac, you want to select mps.</p> <p>If you encounter problems with the memory, manually setting this to the CPU will solve the problem, but it will also slow down the training process.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#loading-the-dataset","title":"Loading the dataset\u00b6","text":"<p>We will load the LICS dataset that contains two folders, one with images of volcano deformation and one with images of background noise. If the dataset is not in the same folder as this notebook, change dataset_dir to the correct path of the dataset.</p> <p>We need to define the transformations that will be applied to the images. In this case, we will only transform the images into tensors, which is how PyTorch stores and uses data. This will also bring the images from the interval [0,255] (the interval used to store RGB images) to [0,1]. Neural networks work better with small numbers, ideally between the interval [-1,1] or [0,1].</p> <p>Tensors are a specialized data structure that is similar to arrays and matrices. Tensors are used to encode the inputs and outputs of a model, as well as the parameters of the model. Tensors are similar to NumPy\u2019s ndarrays, except that tensors can run on GPUs or other hardware accelerators. They are also optimized for automatic differentiation.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#visualizing-the-dataset","title":"Visualizing the dataset\u00b6","text":"<p>We will create a function to visualize some of the images inside the dataset. Then we will load some images using the train dataloader and visualize them.</p> <p>The images are wrapped InSAR interferograms. Each fringe represents 2.8 cm of displacement. The images are automatically processed from satellite data. You can read more about InSAR images here.</p> <p>Watch this video to understand how InSAR works: link</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#splitting-the-dataset-into-train-and-test","title":"Splitting the dataset into train and test\u00b6","text":"<p>PyTorch allows us to split a dataset into two parts only at random, so if we want to have the same number of images from each class into both the train and the test dataset, we need to use a function from sklearn: train_test_split.</p> <p>In general, the size of the test dataset is much smaller than the training dataset. Here we use a training dataset that is ten times bigger than the test. The optimum split depends on the use case, the model used, the size of the dataset, etc., but a generally good split is 80% train and 20% test.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#creating-a-dataloader","title":"Creating a DataLoader\u00b6","text":"<p>A DataLoader is responsible for loading the data, in our case images, from the disk into memory during training or testing.</p> <p>We need to decide how many images are loaded at once by setting batch_size. The batch_size option influences how fast and efficient the model trains, but a bigger batch size also means more memory is required. A bigger batch size can also yield higher accuracy.</p> <p>The argument shuffle decides if the order in which the images are loaded is random or not. We will make the order random for training but not for testing. Making the order random when training can boost the final accuracy of the model.</p> <p>Here, the batch size is set to 2. You can increase this number if you run the program on a GPU or a strong CPU.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#if-you-get-an-error-like-this-torchcudaoutofmemoryerror-cuda-out-of-memory-one-way-of-solving-it-is-to-choose-a-smaller-number-for-the-batch_size-option-if-this-doesnt-work-as-a-last-resort-you-can-choose-to-run-the-model-on-the-cpu-instead-of-the-gpu-from-the-second-code-cell","title":"If you get an error like this: torch.cuda.OutOfMemoryError: CUDA out of memory, one way of solving it is to choose a smaller number for the batch_size option. If this doesn't work, as a last resort, you can choose to run the model on the CPU instead of the GPU from the second code cell.\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#training-an-artificial-neural-network","title":"Training an Artificial Neural Network\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#introduction","title":"Introduction\u00b6","text":"<p>Artificial Neural Networks, or ANNs for short, are Machine Learning models inspired by biological neural networks that are found in animal brains.</p> <p>An ANN is a collection of nodes called artificial neurons that abstract the neurons found in a brain. An artificial neuron, like its biological counterpart, receives signals from other neurons, processes them and sends its own signal to other neurons connected to it. The signal, or the input, received from a connection with another neuron is a real number, and each connection has a weight associated with it that is adjusted during training. The output of the neuron is computed by applying a non-linear function to the weighted sum of the inputs. Neurons are generally grouped into layers and each layer may perform a different transformation to its inputs. The first layer is called the input layer and it has the same number of neurons as the number of dimensions of the input. The last layer is called the output layer and the layers in-between are known as hidden layers. An example of a network is presented in the figure below.</p> <p>In our case, the input is an RGB image of size 227x227. Since RBG images have 3 channels for colours, each pixel has 3 values, so our input layer has 3x227x227=154587 neurons. For the network that we will use, the hidden layers are fixed. We will modify the output layer to have 2 neurons.</p> <p>The model for one neuron is: \\begin{align*} y = \\varphi(\\sum_{i} w_i x_i - b) \\end{align*} where $x_i$ is the i<sup>th</sup> input received, $w_i$ is the weight adjusting that input, $\\varphi$ is a non-linear function, $b$ is the bias and $y$ is the output.</p> <p>The input of the network is a feature vector $\\mathbf{x} = \\mathbf{x}^0$. Denoting all the weights of a neuron as $\\mathbf{w} = [w_0, w_1, \\dots]$ and the weights of all neurons in a layer as $W = [\\mathbf{w_0}, \\mathbf{w_1}, \\dots]$, the output of a layer $l$ can be written as: \\begin{align*} \\mathbf{y}^l = \\varphi(W^l \\mathbf{x}^l - \\mathbf{b}^l) \\end{align*} The output of the network will be a vector $\\mathbf{y}^N$, with $N$ being the number of layers in the network.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#the-backpropagation-algorithm","title":"The Backpropagation Algorithm\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#intuition","title":"Intuition\u00b6","text":"<p>In order for the network to learn, an algorithm is needed to update the weights of the neurons such that the output of the network gets closer to the desired one.</p> <p>In supervised learning, each input $\\mathbf{x}^0$ is paired with an output $\\mathbf{y}^*$. In a classification problem, $\\mathbf{y}^*$ will be a one-hot vector with the bit that represents the class of the input set to $1$. You can read more about one-hot vectors here.</p> <p>The loss function or cost function $\\mathcal{L}(\\mathbf{y}^N, \\mathbf{y}^*)$ is a function that computes the distance between the current output of the network and the desired one. The gradient descent method relies on calculating the derivative of the loss function with respect to the weights of the network.</p> <p>\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}^l} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}^{l+1}} \\frac{\\partial \\mathbf{y}^{l+1}}{\\partial \\mathbf{y}^{l}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W^l} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}^l} \\frac{\\partial \\mathbf{y}^l}{\\partial W^l} \\end{align*}</p> <p>The weights can then be updated: \\begin{equation*} W^l \\leftarrow W^l - \\eta\\frac{\\partial \\mathcal{L}}{\\partial W^l} \\end{equation*} where $\\eta &gt; 0$ is a given learning rate. You can read more about the learning rate here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#loss-function","title":"Loss function\u00b6","text":"<p>The loss function needs to fulfil the following conditions:</p> <ul> <li>it has a derivative</li> <li>it is written as a function of the outputs of the network</li> <li>it can be written as an average $ \\mathcal{L}(Y^{N}, Y^{*}) = \\frac{1}{n} \\sum_{\\mathbf{y}} \\mathcal{L}(\\mathbf{y}^{N}, \\mathbf{y}^{*}) $</li> </ul> <p>The last condition exists because a network is trained, in general, with a set of samples at once, known as a batch. The aim of training the network is to achieve a loss equal to $0$ on the entire dataset, as such using the whole dataset at once to calculate the gradient would be ideal. Unfortunately, this is unfeasible on big datasets. For this reason, small batches that fit into memory are used instead. They provide a more accurate update for the weights than using one sample at a time, while not being too computationally and memory intensive. Splitting the dataset into multiple batches, and then iterating over all of them is known as an epoch.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#the-algorithm","title":"The algorithm\u00b6","text":"<p>Since calculating the gradient of the weights depends only on the output of the current and next layer, the Backpropagation algorithm calculates the updates for the entire network starting from the last layer. The current result is then used to calculate the gradients for the previous layer until the entire network is updated. The stopping criteria for the algorithm could be completing a number of epochs, achieving a certain accuracy or not producing any significant changes. The algorithm can be seen below.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#learning-methods","title":"Learning methods\u00b6","text":"<p>Online gradient descent uses one sample per iteration, which only roughly approximates aspects of the cost function. This results in a noisy gradient descent that may not find the local minimum. Deterministic gradient descent uses the entire dataset in every iteration and, given enough time, will find the true local minimum. As discussed previously, this has a high computational cost and is not usable in practice. Stochastic gradient descent (SGD) uses a batch of samples in every iteration, thus achieving a good approximation of the real gradient. This especially reduces the computational cost for high-dimensional optimization problems, in trade for a lower convergence rate.</p> <p>Many improvements have been proposed to SGD. In machine learning, in particular, the result of gradient descent relies too much on setting a good learning rate. Setting this parameter too high will result in the algorithm diverging, meanwhile setting it too low will make learning slow. Denoting $\\Delta\\mathcal{J}(X, W_t) = \\frac{\\partial \\mathcal{L}}{\\partial W_t}$ as the steepest gradient for input $X$ and weights $W_t$ at iteration $t$, the SGD learning rule can be written as:</p> <p>\\begin{equation*} W_{t+1} = W_t - \\eta\\Delta\\mathcal{J}(X, W_t) \\end{equation*}</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#choosing-a-pre-trained-network","title":"Choosing a pre-trained network\u00b6","text":"<p>We will use a pre-trained network to speed up our training time. We can use the command below to check all the available pre-trained networks in PyTorch.</p> <p>Not all networks have the same input size, so you need to check what input a network expects before using it.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#alexnet","title":"AlexNet\u00b6","text":"<p>We will choose AlexNet as our pre-trained network. AlexNet expects an RGB image of size 227x227.</p> <p>We also check the name of all the layers, as we need to know the name of the last layer. We will replace this layer since it doesn't fit our case. AlexNet was trained to classify images into 1000 classes, so the output of the network is an array of size 1000. We only have two classes, so we need to modify the output layer.</p> <p>We take a look at the layers of the AlexNet and identify the last layer. The last layer, or the output layer, is the layer numbered as 6 in the classifier part of the network. It is a Linear layer, also known as a fully connected layer, and has an input of 4096 numbers and an output of 1000 numbers, corresponding to the 1000 classes. The input of this layer comes from the output of the previous layer, so it cannot be changed, but we will change the output to just 2 numbers, to correspond to the 2 classes we have.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#replace-the-last-layer-of-the-network","title":"Replace the last layer of the network\u00b6","text":"<p>Since we have two classes, volcano_deformation and background_noise, we need a layer with 2 neurons. We create a new Linear layer with the same input size of 4096, but with an output of 2. We then set our newly created layer in position 6 in the classifier.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#loss-function-and-optimizer","title":"Loss function and Optimizer\u00b6","text":"<p>We define a loss function and an optimizer. The loss function will be Cross Entropy Loss and the optimizer will be Adam.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#cross-entropy-loss","title":"Cross Entropy Loss\u00b6","text":"<p>Cross-entropy loss measures the performance of a classification model whose output should represent a set of probabilities. Since the output of a network is not guaranteed to be a set of numbers from the interval [0,1] whose sum is 1, the output is also normalized. You can read more about Cross Entropy Loss here and here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#adam","title":"Adam\u00b6","text":"<p>Adaptive Moment Estimation (Adam) is an optimizer, where running averages of both the gradients and the second moments of the gradients are used. You can read more about Adam here and here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>For training, we get a batch of images and we run the batch through the model and get an output. We use the loss function to get a loss value.</p> <p>By calling loss.backwards(), we calculate the gradient of the loss value by using the Backpropagation algorithm. We can then call optimizer.step() to update the weights of the network and perform the gradient descent. Since the function step() is called on the optimizer, the selected optimizing algorithm is used to update the weights.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#running-the-model","title":"Running the model\u00b6","text":"<p>Now we can just call the training function to train the model on our dataset.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/11_neural_networks3/#visualizing-the-predictions-of-the-model","title":"Visualizing the predictions of the model\u00b6","text":"<p>Finally, we can visualize the predictions of the model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/","title":"Homework01","text":"In\u00a0[1]: Copied! <pre>#This cell is a code cell. It is where we can type code that \n#can be run. The hashtag at the start of this line makes it \n#so that this text is a comment not code. \n\nimport pandas as pd\npd.set_option('display.max_columns', None)\n</pre> #This cell is a code cell. It is where we can type code that  #can be run. The hashtag at the start of this line makes it  #so that this text is a comment not code.   import pandas as pd pd.set_option('display.max_columns', None) <p>The reason why we execute the code <code>import pandas as pd</code> is so that we can use the functions of the <code>pandas</code> library which provides really helpful data structures and data analysis tools. We are using the standard convention of importing it using the nickname <code>pd</code>. One of the fantastic things about doing data analysis in Python is the availability of great data analysis tools such as <code>pandas</code>. One of the frustrating things can be learning how to use these diverse tools and which to use when. You will get more and more comfortable with these tools as the term progresses.</p> In\u00a0[2]: Copied! <pre>## e.g., my_birthday = '1990-01-01'\n</pre> ## e.g., my_birthday = '1990-01-01'  <p>What we just did in the code above is to create the variable <code>my_birthday</code> and assign it to be set to be the string 'year-mm-dd'. Run a code cell with just that variable, and show the variable as the code output.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Another way to see the variable is to tell python to print it using the <code>print()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>We are going to need to tell the USGS to look for earthquakes that are bracketed between the start of the day on my birthday and the end of the day on my birthday for which we can assign a variable named <code>day_after_my_birthday</code>. Python variable names cannot have spaces in them which is why I am using underscores <code>_</code>.</p> In\u00a0[3]: Copied! <pre>## e.g., day_after_my_birthday = '1990-01-02'\n</pre> ## e.g., day_after_my_birthday = '1990-01-02'  In\u00a0[\u00a0]: Copied! <pre>standard_url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&amp;orderby=magnitude'\n\nmy_birthquake_url = (standard_url + '&amp;starttime=' + \n                       my_birthday + '&amp;endtime=' + \n                       day_after_my_birthday)\n</pre> standard_url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&amp;orderby=magnitude'  my_birthquake_url = (standard_url + '&amp;starttime=' +                         my_birthday + '&amp;endtime=' +                         day_after_my_birthday) <p>Take a look at the url that is created:</p> In\u00a0[\u00a0]: Copied! <pre>my_birthquake_url\n</pre> my_birthquake_url In\u00a0[\u00a0]: Copied! <pre>print(my_birthquake_url)\n</pre> print(my_birthquake_url) In\u00a0[6]: Copied! <pre>## e.g., my_birthday_earthquakes = pd.read_csv(my_birthquake_url)\n</pre> ## e.g., my_birthday_earthquakes = pd.read_csv(my_birthquake_url)  <p>These data are sorted by magnitude with the largest magnitude earthquakes at top. Let's look at the first 5 rows of the DataFrame using the <code>.head()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>We can just look at values in one row by applying the <code>.loc</code> function to the DataFrame and calling the index 0. Python is zero-indexed (just like human ages!) so the first row is row zero. Please apply .loc to the first row to see all the details about my birthquake.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>It can be useful to return a single value which can be done by calling both the the row and the column using the <code>.loc</code> function. Please apply .loc to the first row and the column 'mag' to see the magnitude of my birthquake.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>What is the magnitude of your birthquake? Where did it occur? Use print statements to show your answers.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>What is the largest earthquake of your birthquake day? Use '.sort_values()' function to sort the DataFrame and print the first row, which is the largest earthquake of your birthquake day.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>When working with Python for this course, you are going to get errors. I get errors almost everyday. Programming languages are not flexible and want things to be just so.</p> <p>Error messages can look quite intimidating, but they often are informative (particularly if you look at the bottom). For example, the code cell below should result in an error. Go ahead and execute it and let's have a look at the result.</p> In\u00a0[10]: Copied! <pre>my_birthday_earthquakes.loc[0]['birthday_cake']\n</pre> my_birthday_earthquakes.loc[0]['birthday_cake'] In\u00a0[\u00a0]: Copied! <pre>from IPython.lib.display import YouTubeVideo\nYouTubeVideo('kIID5FDi2JQ')\n</pre> from IPython.lib.display import YouTubeVideo YouTubeVideo('kIID5FDi2JQ') <p>all models map projections are wrong, but some are useful - Phileas Elson (SciPy 2018)</p> In\u00a0[12]: Copied! <pre>import cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n</pre> import cartopy.crs as ccrs import matplotlib.pyplot as plt <p>The syntax of using these functions takes some getting used to. Here we will make a figure, create an axis object with a defined projection, and then plot coastlines and a stock image that shows elevation.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 4))\nax = plt.axes(projection=ccrs.Mollweide())\nax.coastlines()\nax.stock_img()\nplt.show()\n</pre> plt.figure(figsize=(8, 4)) ax = plt.axes(projection=ccrs.Mollweide()) ax.coastlines() ax.stock_img() plt.show() <p>Let's plot the location of Berkeley on a map. First we want to assign the latitude and longitude of Berkeley to variables:</p> In\u00a0[14]: Copied! <pre>Berkeley_latitude = 37.8715\nBerkeley_longitude = -122.2730\n</pre> Berkeley_latitude = 37.8715 Berkeley_longitude = -122.2730 <p>Now we can use the <code>plt.scatter()</code> function to plot the location of Berkeley. <code>plt.scatter()</code> is one of the many plotting functions available using the <code>matplotlib</code> library that we imported above using <code>import matplotlib.pyplot as plt</code>.</p> <p>We give the <code>plt.scatter()</code> function <code>Berkeley_longitude</code> as the x-value, <code>Berkeley_latitude</code> as the y-value while also telling it to transform it into map coordinates (<code>transform=ccrs.PlateCarree()</code>) and to make the point red (<code>color='red'</code>).</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 4))\nax = plt.axes(projection=ccrs.Mollweide())\nax.stock_img()\nplt.scatter(Berkeley_longitude, Berkeley_latitude, \n            transform=ccrs.PlateCarree(), color='red')\nplt.show()\n</pre> plt.figure(figsize=(8, 4)) ax = plt.axes(projection=ccrs.Mollweide()) ax.stock_img() plt.scatter(Berkeley_longitude, Berkeley_latitude,              transform=ccrs.PlateCarree(), color='red') plt.show() <p>Revisit the above notebook when you found your birthquake and enter the latitude of longitude in the cell below assigning them to <code>birthquake_latitude</code> and <code>birthquake_longitude</code>.</p> In\u00a0[\u00a0]: Copied! <pre>birthquake_longitude = \nbirthquake_latitude = \n</pre> birthquake_longitude =  birthquake_latitude =  <p>Once <code>birthquake_latitude</code> and <code>birthquake_longitude</code> are defined, we can plot them instead of the position of Berkeley. Replace the ellipsis (<code>...</code>) with <code>birthquake_latitude</code> and <code>birthquake_longitude</code> in the cell below:</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 4))\nax = plt.axes(projection=ccrs.Mollweide())\nax.stock_img()\nplt.scatter(..., ..., \n            transform=ccrs.PlateCarree(), color='red')\nplt.show()\n</pre> plt.figure(figsize=(8, 4)) ax = plt.axes(projection=ccrs.Mollweide()) ax.stock_img() plt.scatter(..., ...,              transform=ccrs.PlateCarree(), color='red') plt.show() <p>Now let's plot both the location of Berkeley and the location of your birthquake. Rather than a single values for x (i.e. a single value of longitude) and a single value for y (i.e. a single value of latitude), we want there to be a list of x values and a list of y values. A list can be defined with square brackets with values separated by commas (e.g. <code>[value1, value2]</code>).</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 4))\nax = plt.axes(projection=ccrs.Robinson())\nax.stock_img()\nplt.scatter(\n    [Berkeley_longitude, birthquake_longitude],\n    [Berkeley_latitude, birthquake_latitude],\n    transform=ccrs.PlateCarree(),\n    color='red')\nplt.show()\n</pre> plt.figure(figsize=(8, 4)) ax = plt.axes(projection=ccrs.Robinson()) ax.stock_img() plt.scatter(     [Berkeley_longitude, birthquake_longitude],     [Berkeley_latitude, birthquake_latitude],     transform=ccrs.PlateCarree(),     color='red') plt.show() <p>We can save the figure using <code>plt.savefig()</code> putting the name of the file with the extension within the <code>()</code>. In this case, let's call it <code>'map_w_Berkeley_and_birthquake.png'</code>. Let's also go ahead and add a title to the plot using <code>plt.title()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 4))\nax = plt.axes(projection=ccrs.Robinson())\nax.stock_img()\nplt.scatter(\n    [Berkeley_longitude, birthquake_longitude],\n    [Berkeley_latitude, birthquake_latitude],\n    transform=ccrs.PlateCarree(),\n    color='red')\nplt.title('map with location of Berkeley and my Birthquake')\nplt.savefig('map_w_Berkeley_and_birthquake.png')\n</pre> plt.figure(figsize=(8, 4)) ax = plt.axes(projection=ccrs.Robinson()) ax.stock_img() plt.scatter(     [Berkeley_longitude, birthquake_longitude],     [Berkeley_latitude, birthquake_latitude],     transform=ccrs.PlateCarree(),     color='red') plt.title('map with location of Berkeley and my Birthquake') plt.savefig('map_w_Berkeley_and_birthquake.png') In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#homework-01-of-eps-88","title":"Homework 01 of EPS 88\u00b6","text":"<p>The goal of this course is to empower you to learn about the Earth using computation.</p> <p>The tools of data science enable us to learn about the Earth through the approaches of:</p> <ul> <li>Exploration<ul> <li>Identifying patterns in data through visualization</li> </ul> </li> <li>Inference<ul> <li>Using data to obtain reliable insights about the Earth through the application of statistics</li> </ul> </li> <li>Prediction<ul> <li>Use analysis of data we can observe to make informed predictions of things we cannot observe using machine learning</li> </ul> </li> </ul> <p>You should have taken Data 8, be taking it concurrently, or have other experience with Python outside of the context of Data 8. All Data 8 materials including lecture videos are openly available which does enable self-study: http://www.data8.org/fa24/ as is the Data 8 textbook: https://inferentialthinking.com/chapters/intro.html</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#the-jupyter-notebook-environment","title":"The Jupyter notebook environment\u00b6","text":"<p>The Jupyter notebook environment is the environment we will be habitating this semester in EPS 88.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#markdown-cells","title":"Markdown cells\u00b6","text":"<p>This cell is Markdown text. It is the cell type where we can type text that isn't code. Go ahead and double click in this cell and you will see that you can edit it. Type something here:</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#code-cells","title":"Code cells\u00b6","text":"<p>Let's get going right away by dealing with some data within this Jupyter notebook. The first bit of code you will run is in the cell below. This is a code cell rather than a markdown cell (you can change the cell type using the drop-down box above). You can either hit the play button above, or more efficiently press shift+enter on your keyboard to run the code.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#finding-birthquakes","title":"Finding Birthquakes\u00b6","text":"<p>Your birthquake is the largest magnitude earthquake that occured on the day you were born. In this in-class exercise, we are going to search an earthquake catalog and find your birthquake.</p> <p>To do so, we can going to download data from the US Geological Survey (USGS) Earthquake Hazards program. https://earthquake.usgs.gov</p> <p>We are going to use an API that lets us send an url to the USGS and get earthquake information for a set of specified parameters.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#finding-my-birthquake","title":"Finding my birthquake\u00b6","text":"<p>Let's do it first for my birthday. We will define my birthday in year-month-day format and the day after my birthday in year-month-day format in order to make a url that gets data starting on 12 am of your birthday and ending 12 am of the next day. We are putting the quote marks (' ') around the dates so that they are strings (the Python data type that is a sequence of text).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#defining-my-birthday-earthquake-url","title":"Defining my birthday earthquake URL\u00b6","text":"<p>To make a url that we can send to the USGS and get back data, we need to insert these dates into the USGS earthquake API url format (https://earthquake.usgs.gov/fdsnws/event/1/). API stands for 'application programming interface' and an API url path provides a way to access data that are available online. A nice thing about using an API url to access data is that it makes code portable and reproducible as the code can grab the data from the internet without needing a local copy.</p> <p>We will define the <code>standard_url</code> as a string and include that we want to get the data as a <code>csv</code> with <code>format=csv</code> and that we want the data to be in the order of magitude with <code>&amp;orderby=magnitude</code>. We can then add the dates that were set above to be the <code>&amp;starttime</code> and <code>&amp;endtime</code>.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#getting-mys-birthday-earthquakes","title":"Getting my's birthday earthquakes\u00b6","text":"<p>We now have a url that we can use to get data from the USGS. We could cut and paste this url into a web browser. Go ahead and give that a try.</p> <p>Alternatively, we can use functions from the <code>pandas</code> module that we imported at the top of this notebook to get these data. The standard way to use <code>pandas</code> is to import it with the shorthand <code>pd</code>. We will use the <code>pd.read_csv()</code> function which will take the data that comes from the USGS url and make it into a <code>DataFrame</code>. A <code>DataFrame</code> is a data structure with columns of different data that are defined with column names.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#map-projections-and-making-your-birthquake-map","title":"Map projections and making your birthquake map\u00b6","text":"<p>The purpose of this introduction is to give you a bit of a background on map projections and other geospatial concepts. This will help you to:</p> <ul> <li>choose map projections that are appropriate for plotting data</li> <li>understand the terms used in the <code>cartopy</code> functions which is a function library we will use for plotting geospatial data</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#the-world-is-not-flat-2d-sorry-flat-earthers","title":"The world is not flat / 2D (sorry flat-Earthers)\u00b6","text":"<p>\"Azimuthal equidistant projections of the sphere ... have been co-opted as images of the flat Earth model, depicting Antarctica as an ice wall surrounding a disk-shaped Earth.\" (Wikipedia: Flat Earth)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#most-of-our-media-for-visualization-are-flat","title":"Most of our media for visualization are flat\u00b6","text":"<p>Our two most common media are flat:</p> <ul> <li>Paper</li> <li>Screen</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#but-there-are-a-few-that-arent","title":"But there are a few that aren't...\u00b6","text":"<p>For example:</p> <ul> <li>3D rendering engine (the engine is then typically responsible for projecting the data to 2D for presentation to screen)</li> <li>A Spherical Projector...</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#map-projections-taking-us-from-spherical-to-flat","title":"[Map] Projections: Taking us from spherical to flat\u00b6","text":"<p>A map projection (or more commonly refered to as just \"projection\") is:</p> <p>a systematic transformation of the latitudes and longitudes of locations from the surface of a sphere or an ellipsoid into locations on a plane. [Wikipedia: Map projection].</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#the-major-problem-with-map-projections","title":"The major problem with map projections\u00b6","text":"<ul> <li>The surface of a sphere is topologically different to a 2D surface, therefore we have to cut the sphere somewhere</li> <li>A sphere's surface cannot be represented on a plane without distortion.</li> </ul> <p>Watch the video embedded below (click the notebook play button to embed it in the notebook or watch it at this link: https://youtu.be/kIID5FDi2JQ). This video gives an introduction (with nice accompanying visualizations) of this issue and different projections along with the positives and negatives of different commonly used ones.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#different-projections","title":"Different projections\u00b6","text":"<p>We are going to use the function library <code>cartopy</code> to make maps. <code>cartopy</code>  supports a number of different map projections which enable the 3 dimensional surface of Earth to be shown in 2 dimensions on our computer screens. Having watched the above video will give you some context to appreciate these jokes:</p> <p>You can check out the list of projections supported by <code>cartopy</code> here: https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#common-distortions-of-map-projections","title":"Common distortions of map projections\u00b6","text":"<p>Properties of maps that are often not preserved in projections:</p> <ul> <li>Area</li> <li>Shape</li> <li>Direction</li> <li>Distance</li> <li>Scale</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#classifying-projections","title":"Classifying projections\u00b6","text":"<p>Two common approaches:</p> <ol> <li>By [2D] surface classification</li> <li>By preserving a given property (metric)</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#projections-by-surface-classification","title":"Projections by surface classification\u00b6","text":"<p>Downside: Not all projections can be classified in this way -&gt; Leads to big \"pseudo\" and \"other\" groups.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#surface-classification-cylindrical","title":"Surface classification: Cylindrical\u00b6","text":"<p> Source: http://ayresriverblog.com/2011/05/19/the-world-is-flat/ </p> <ul> <li>Meridians and paralells are straight and perpendicular.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#surface-classification-azimuthal","title":"Surface classification: Azimuthal\u00b6","text":"<p> Source: http://ayresriverblog.com/2011/05/19/the-world-is-flat/ </p> <ul> <li>Parallels are complete circles</li> <li>Great circles from central point are straight lines.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#surface-classification-conic","title":"Surface classification: Conic\u00b6","text":"<p> Source: http://ayresriverblog.com/2011/05/19/the-world-is-flat/ </p> <ul> <li>Meridians are straight equally-spaced lines</li> <li>Parallels are circular arcs.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#projections-by-preserving-metric","title":"Projections by preserving metric\u00b6","text":"<p>Downside: Some projections can live in multiple groups.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#preserving-metric-conformal","title":"Preserving metric: Conformal\u00b6","text":"<p>Also known as Orthomorphic.</p> <p>These projections preserve angles locally. Implying that circles anywhere on the Earth's surface map to circles of varying size in the projected space.</p> <p>Examples of conformal projections:</p> <ul> <li>Mercator</li> <li>Transverse Mercator</li> <li>Stereographic</li> <li>Lambert conformal conic</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#preserving-metric-conformal","title":"Preserving metric: Conformal\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#use-in-large-scale-maps-zoomed-in","title":"Use in large scale maps (zoomed in)\u00b6","text":"<p>Often used to preserve shape to represent their physical counterpart. Seamless online maps like OSM/Google/Bing typically use a Mercator projection although Google Maps has begun using an 3D-rendered globe projection when the user zooms out:</p> <p>The first launch of [Google] Maps actually did not use Mercator, and streets in high latitude places like Stockholm did not meet at right angles on the map the way they do in reality. [ref]</p> <p>The major drawback: it is difficult to compare lengths or areas</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#preserving-metric-conformal","title":"Preserving metric: Conformal\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#use-in-small-scale-maps-zoomed-out","title":"Use in small scale maps (zoomed out)\u00b6","text":"<p>Maps reflecting directions, such as an [aero]nautical chart, or whose gradients are important, such as a weather maps, are often projected by conformal projections.</p> <p>Historically, many world maps are drawn by conformal projections, but the fact that the scale of the map varies by location makes it difficult to compare lengths or areas. Some have gone as far as calling the Mercator projection imperialistic and racist.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#preserving-metric-equidistant","title":"Preserving metric: Equidistant\u00b6","text":"<p>No map projection can be universally equidistant.</p> <p>Some projections preserve distance from some standard point or line.</p> <p>Examples of projections that preserve distances along meridians (but not parallels):</p> <ul> <li>Equirectangular / Plate Carree</li> <li>Azimuthal equidistant</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#preserving-metric-equal-area","title":"Preserving metric: Equal-area\u00b6","text":"<p>Equal-area maps preserve area measure, generally distorting shapes in order to do so.</p> <p>Examples of equal area projections:</p> <ul> <li>Albers conic</li> <li>Eckert IV</li> <li>Goode's homolosine</li> <li>Lambert azimuthal equal-area</li> <li>Lambert cylindrical equal-area</li> <li>Mollweide</li> <li>Sinusoidal</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#preserving-metric-compromise","title":"Preserving metric: Compromise\u00b6","text":"<p>Rather than perfectly preserving any metric properties, compromise projections aim strike a balance between distortions. These compromises are often at the cost of polar distortions.</p> <p>Examples:</p> <ul> <li>Miller</li> <li>Robinson</li> <li>Winkel Tripel</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#tissots-indicatrix","title":"Tissot's indicatrix\u00b6","text":"<p>A mathematical contrivance in order to characterize local distortions of a map projection. Multiple circles (on the sphere/ellipse) of constant area are drawn on the map. By analysing the distortions, we can identify (or more often rule-out) particular preserving metrics. You can see how dramatic the distortion is in an equirectangular projection.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#now-lets-make-your-first-map","title":"Now let's make your first map!\u00b6","text":"<p>We are going to use <code>cartopy</code> in conjunction with <code>matplotlib</code> to make maps. <code>cartopy</code> can transform points, lines and images into different map projections. <code>matplotlib</code> provides tools to visualize these projections. We will import them using the standard conventions. You must press play (or more efficiently shift+enter) on the cell that imports these function libraries for the rest of the code to work.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#make-a-map-of-5-largest-birthdate-earthquakes","title":"Make a map of 5 largest birthdate earthquakes.\u00b6","text":"<p>Use the code cells below to make another map where you plot the locations of the 5 largest magnitude earthquakes that occured on the day you were born. Choose any projection you want (https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html). You can see that the examples above use Robinson and Mollweide projections.</p> <p>Add a title to the map that has your name in it.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework01/#acknowledgments","title":"Acknowledgments\u00b6","text":"<p>This introductory text is modified by Prof. Nicholas Swanson-Hysell from a tutorial on working with geospatial data using the library <code>cartopy</code> that was presented at the 2018 Scipy conference by Phileas Elson (lots of great things to learn in this tutorial if you want to dig into it at some point):</p> <p>https://youtu.be/AmidIx6Jmn8</p> <p>https://github.com/SciTools/cartopy-tutorial</p> <p>The materials in the linked to tutorials were licensed with an open license as long as they original source is acknowledged.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/","title":"Homework02","text":"<p>Today we will be focused on putting tools to use that we have used before with a couple new tricks related to dealing with time-series data.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np <p>One of the strengths of pandas is its ability to read and write different data types. For example, we have used the <code>pd.read_csv()</code> function to import .csv files throughout the course. This function can either be pointed to a file that is one your computer or a file that is posted on the internet. There are some online databases where you can use a url to access data of your choosing using a special format (API). We took this approach to get our birthquakes earlier in the course.</p> <p>Let's import daily data since 2005 for the Visalia California GPS station. The data is in the North American tectonic plate (NAM14) reference frame which means that it takes the interior eastern part of North America functions as a fixed and stable point.</p> In\u00a0[2]: Copied! <pre>P566_GPS_data = pd.read_csv('data/P566.cwu.nam14.csv')\n</pre> P566_GPS_data = pd.read_csv('data/P566.cwu.nam14.csv') <p>Whoops. There was an error. I kept this error in here as a reminder that I get errors all of the time. Remember that the errors are informative, but can also be a bit cryptic. In this case, it says \"Expected 2 fields in line 10, saw 4.\" So it seems like there is a mismatch between the number of columns it is expecting and the number that there are.</p> <p>Let's look at the file.</p> <p>It turns out that there are a bunch of header lines and the header row that contains the column names needs to be specified (<code>header = 11</code>).</p> In\u00a0[3]: Copied! <pre>## Add header = 11 to skip the first 11 rows\n</pre> ## Add header = 11 to skip the first 11 rows  <p>We know how to take a peak at a DataFrame by applying the <code>.head()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data.head()\n</pre> P566_GPS_data.head() <p>We have done a lot where we have used extracted data from a single column. We have used the syntax <code>DataFrameName['column_name]</code>. It can be helpful to look at the available columns:</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data.columns\n</pre> P566_GPS_data.columns <p>Some columns names have spaces before and after the name. To make it easier to work with the data, let's remove the spaces from the column names.</p> In\u00a0[5]: Copied! <pre>P566_GPS_data.columns = P566_GPS_data.columns.str.strip()\n</pre> P566_GPS_data.columns = P566_GPS_data.columns.str.strip() <p>Let first look at how this point is moving north with respect to stable North America.</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data['North (mm)'].describe()\n</pre> P566_GPS_data['North (mm)'].describe() <p>By themselves these data are pretty cool. It looks like Visalia has moved north relative to stable North America by ~170 mm (17 cm) over the past 18 years (the data starts in November 2005).</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data['Date'][0]\n</pre> P566_GPS_data['Date'][0] In\u00a0[\u00a0]: Copied! <pre>type(P566_GPS_data['Date'][0])\n</pre> type(P566_GPS_data['Date'][0]) <p>Right now, pandas thinks that the values in the data column are strings (a sequence of characters) rather than datetime values. We can convert them to be datetime values using <code>pd.to_datetime</code>.</p> In\u00a0[10]: Copied! <pre>## Use the pd.to_datetime function to convert the Date column to a datetime object\nP566_GPS_data['Date'] = \n</pre> ## Use the pd.to_datetime function to convert the Date column to a datetime object P566_GPS_data['Date'] =  In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data['Date'][0]\n</pre> P566_GPS_data['Date'][0] In\u00a0[\u00a0]: Copied! <pre>type(P566_GPS_data['Date'][0])\n</pre> type(P566_GPS_data['Date'][0]) In\u00a0[13]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data.plot(x='Date', y='North (mm)')\n</pre> P566_GPS_data.plot(x='Date', y='North (mm)') In\u00a0[\u00a0]: Copied! <pre>## Create a plot between P566_GPS_data['Date'] and P566_GPS_data['North (mm)']\n</pre> ## Create a plot between P566_GPS_data['Date'] and P566_GPS_data['North (mm)']  In\u00a0[\u00a0]: Copied! <pre>## Create a plot between P566_GPS_data['Date'] and P566_GPS_data['East (mm)']\n</pre> ## Create a plot between P566_GPS_data['Date'] and P566_GPS_data['East (mm)']  <p>What is going on with that drop midway through 2019? Let's take a look.</p> <p>Through some trial and error, the drop was between indices 4900 and 4950. Let's zoom in on that drop.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10,5))\nplt.plot(P566_GPS_data['Date'][4900:4950],P566_GPS_data['East (mm)'][4900:4950],'.')\nplt.ylabel('east since start (mm)')\nplt.xlabel('date')\nplt.title('GPS data from station P566 (Visalia, CA)')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(10,5)) plt.plot(P566_GPS_data['Date'][4900:4950],P566_GPS_data['East (mm)'][4900:4950],'.') plt.ylabel('east since start (mm)') plt.xlabel('date') plt.title('GPS data from station P566 (Visalia, CA)') plt.tight_layout() plt.show() <p>What happened on July 6, 2019? Add your answer to the cell below.</p> <p>https://earthquake.usgs.gov/earthquakes/eventpage/ci38457511/executive</p> In\u00a0[48]: Copied! <pre>from sklearn.linear_model import LinearRegression\n</pre> from sklearn.linear_model import LinearRegression <p>We can calculate the number of days by making a new column in the data frame that is the 'date' value minus the initial date. This will be the number of days since the first date in the data set (Nov. 16 2005).</p> In\u00a0[20]: Copied! <pre>P566_GPS_data['days'] = (P566_GPS_data['Date'] - P566_GPS_data['Date'][0])/np.timedelta64(1,'D')\n</pre> P566_GPS_data['days'] = (P566_GPS_data['Date'] - P566_GPS_data['Date'][0])/np.timedelta64(1,'D') <p>Let's take a look at our DataFrame and make sure it has a new column <code>days</code> and that the column looks good.</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data.head()\n</pre> P566_GPS_data.head() <p>Now we can do a linear regression between the days (<code>P566_GPS_data['days']</code>) and the distance traveled north (<code>P566_GPS_data['North (mm)']</code>)</p> In\u00a0[\u00a0]: Copied! <pre>## Define and fit a linear regression model using the 'days' column as the independent variable and the 'North (mm)' column as the dependent variable\n## Hint: the independent variable should be a 2D array, you should use double brackets [[]] to select the column \nmodel_north = \nmodel_north.fit(\n</pre> ## Define and fit a linear regression model using the 'days' column as the independent variable and the 'North (mm)' column as the dependent variable ## Hint: the independent variable should be a 2D array, you should use double brackets [[]] to select the column  model_north =  model_north.fit( <p>We can get the best fitting slope and intercept of the line using the <code>.coef_</code> and <code>.intercept_</code> attributes of the <code>LinearRegression</code> object.</p> In\u00a0[23]: Copied! <pre>## Retrieve the slope and intercept from the model\nslop, intercept = \n</pre> ## Retrieve the slope and intercept from the model slop, intercept =  In\u00a0[\u00a0]: Copied! <pre>print(f'The slope is {slop:.2f} and the intercept is {intercept:.2f}')\n</pre> print(f'The slope is {slop:.2f} and the intercept is {intercept:.2f}') <p>What are the units of this slope? Write your answer in the cell below.</p> In\u00a0[\u00a0]: Copied! <pre>## Predict the dependent variable based on the independent variable 'days'\ny_pred = \n</pre> ## Predict the dependent variable based on the independent variable 'days' y_pred =   In\u00a0[50]: Copied! <pre>## Plot both the data 'North (mm)' and the model 'y_pred' on the same plot. Using the 'Date' column as the x-axis\n</pre> ## Plot both the data 'North (mm)' and the model 'y_pred' on the same plot. Using the 'Date' column as the x-axis  In\u00a0[\u00a0]: Copied! <pre>## Calculate the residuals between the data 'North (mm)' and the model prediction y_pred. \nresiduals = \n</pre> ## Calculate the residuals between the data 'North (mm)' and the model prediction y_pred.  residuals =   In\u00a0[\u00a0]: Copied! <pre>## Plot the residuals over Date.\n</pre> ## Plot the residuals over Date.  In\u00a0[\u00a0]: Copied! <pre>## Calculate the R^2 value for the model\ny_data = \ny_pred = \n\nR2 = \n\nprint(f'The R^2 value is {R2:.6f}')\n</pre> ## Calculate the R^2 value for the model y_data =  y_pred =   R2 =   print(f'The R^2 value is {R2:.6f}') In\u00a0[28]: Copied! <pre>def GPS_direction(east_magnitude, north_magnitude):\n    direction_rad = np.arctan2(east_magnitude, north_magnitude)\n    direction = np.rad2deg(direction_rad) % 360\n    return direction\n</pre> def GPS_direction(east_magnitude, north_magnitude):     direction_rad = np.arctan2(east_magnitude, north_magnitude)     direction = np.rad2deg(direction_rad) % 360     return direction In\u00a0[\u00a0]: Copied! <pre>GPS_direction(0,-1)\n</pre> GPS_direction(0,-1) <p>Let's repeat the process for the east data.</p> <p>Calculate the slope for the east data. Use this slope and the slope for the north data to calculate the direction of the motion of the station using the <code>GPS_direction</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>## Define and fit a linear regression model using the 'days' column as the independent variable and the 'East (mm)' column as the dependent variable\nmodel_east = \nmodel_east.fit(\n</pre> ## Define and fit a linear regression model using the 'days' column as the independent variable and the 'East (mm)' column as the dependent variable model_east =  model_east.fit( In\u00a0[\u00a0]: Copied! <pre>### Retrieve the slope and intercept from the model\nslop_east, intercept_east = \n\nprint(f'The slope is {slop_east:.2f} and the intercept is {intercept_east:.2f}')\n</pre> ### Retrieve the slope and intercept from the model slop_east, intercept_east =   print(f'The slope is {slop_east:.2f} and the intercept is {intercept_east:.2f}') <p>Based on estimation of east and north slope, we can calculate the direction of the motion of the station.</p> In\u00a0[\u00a0]: Copied! <pre># To make it clear, let's redefine the slope and intercept for the North component\nslop_north, intercept_north = model_north.coef_[0], model_north.intercept_\n\ndirection = GPS_direction(slop_east, slop_north)\n\nprint(f'The direction is {direction:.2f} degrees')\n</pre> # To make it clear, let's redefine the slope and intercept for the North component slop_north, intercept_north = model_north.coef_[0], model_north.intercept_  direction = GPS_direction(slop_east, slop_north)  print(f'The direction is {direction:.2f} degrees') In\u00a0[32]: Copied! <pre>P566_lat = 36.32445\nP566_lon = -119.22929\n</pre> P566_lat = 36.32445 P566_lon = -119.22929 In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport cartopy.io.img_tiles as cimgt\n\n# Replace these variables with your station's longitude and latitude\nP566_lon = -120.0  # Example longitude of station P566\nP566_lat = 37.0    # Example latitude of station P566\n\n# Add a background image from Google Maps\ntiles = cimgt.GoogleTiles()\n\n# Set up the map\nplt.figure(figsize=(10, 10))\nax = plt.axes(projection=tiles.crs)\nax.set_extent((-125, -114, 32, 42.5))  # Set the geographical extent (lon_min, lon_max, lat_min, lat_max)\n\n# Instread of adding features, we add the Google Maps image\n# ax.add_feature(cfeature.LAND)\n# ax.add_feature(cfeature.OCEAN)\n# ax.add_feature(cfeature.STATES)\n\n# Add the Google Maps image to the map\nax.add_image(tiles, 6) # The number 6 is the zoom level. The higher the number, the closer in you zoom\n\n# Plot the station's location\nax.scatter(P566_lon, P566_lat, transform=ccrs.PlateCarree(), color='red', s=50, label='P566 Station')\n\n# Annotate the station with its name\nax.text(P566_lon, P566_lat, 'P566\\nstation\\n', transform=ccrs.PlateCarree(),\n        color='red', horizontalalignment='center', verticalalignment='bottom', size=12)\n\n# Annotation for the direction of motion using an arrow\nax.arrow(P566_lon, P566_lat, np.sin(np.deg2rad(direction)), np.cos(np.deg2rad(direction)), head_width=0.1, head_length=0.1, fc='blue', ec='blue', transform=ccrs.PlateCarree())\n\nplt.legend()\nplt.show()\n</pre> import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import cartopy.io.img_tiles as cimgt  # Replace these variables with your station's longitude and latitude P566_lon = -120.0  # Example longitude of station P566 P566_lat = 37.0    # Example latitude of station P566  # Add a background image from Google Maps tiles = cimgt.GoogleTiles()  # Set up the map plt.figure(figsize=(10, 10)) ax = plt.axes(projection=tiles.crs) ax.set_extent((-125, -114, 32, 42.5))  # Set the geographical extent (lon_min, lon_max, lat_min, lat_max)  # Instread of adding features, we add the Google Maps image # ax.add_feature(cfeature.LAND) # ax.add_feature(cfeature.OCEAN) # ax.add_feature(cfeature.STATES)  # Add the Google Maps image to the map ax.add_image(tiles, 6) # The number 6 is the zoom level. The higher the number, the closer in you zoom  # Plot the station's location ax.scatter(P566_lon, P566_lat, transform=ccrs.PlateCarree(), color='red', s=50, label='P566 Station')  # Annotate the station with its name ax.text(P566_lon, P566_lat, 'P566\\nstation\\n', transform=ccrs.PlateCarree(),         color='red', horizontalalignment='center', verticalalignment='bottom', size=12)  # Annotation for the direction of motion using an arrow ax.arrow(P566_lon, P566_lat, np.sin(np.deg2rad(direction)), np.cos(np.deg2rad(direction)), head_width=0.1, head_length=0.1, fc='blue', ec='blue', transform=ccrs.PlateCarree())  plt.legend() plt.show()  <p></p> <p>Does the direction of motion of the station you calculated match the direction of the Pacific Plate relative to North America? Write your answer in the cell below.</p> In\u00a0[53]: Copied! <pre>## Create a plot between P566_GPS_data['Date'] and P566_GPS_data['Vertical (mm)']\n</pre> ## Create a plot between P566_GPS_data['Date'] and P566_GPS_data['Vertical (mm)']  <p>What do these data show? What is happening to the land surface? Why? Add your answer to the cell below.</p> <p>https://earthobservatory.nasa.gov/images/89761/san-joaquin-valley-is-still-sinking</p> <p>https://www.earthdate.org/californias-sinking-valley</p> <p>This is a big problem for the San Joaquin Valley. So is the rate of land subsidence increasing? Specifically, was the rate of land subsidence greater during the last 5 years (2018-01-01 and 2023-01-01) than it was in the first 5 years of the record (2006-01-01 and 2011-01-01)?</p> <p>To answer this question, we need to:</p> <ul> <li>Filter the DataFrame to only include those years</li> <li>Compare the the slopes between the two age ranges. Which one appears to be greater? Is this results significant or do they have overlapping confidence bounds?</li> </ul> <p>Let's look at a subset of the data for the past 5 years between 2018-01-01 and 2023-01-01. We have done a lot of this filtering using pandas. However, the syntax is hard to remember.</p> <p>It can be helpful to remember how this is actually working under the hood. When we are passing in a conditional statement like <code>P566_GPS_data['Date'] &gt;= '2006-01-01'</code> we are asking pandas to tell us, at every value in the <code>P566_GPS_data['Date']</code> column is it true or false that the date is greater than 2018-01-01?</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_data['Date'] &gt;= '2018-01-01'\n</pre> P566_GPS_data['Date'] &gt;= '2018-01-01' <p>The result is a list of true/false values. We then use these true/false values to filter the values in the DataFrame only returning those that are true. We can link multiple conditionals together with the <code>&amp;</code> symbol such as in the example below:</p> In\u00a0[\u00a0]: Copied! <pre>P566_GPS_18_23 = P566_GPS_data[(P566_GPS_data['Date'] &gt;= '2018-01-01') &amp; (P566_GPS_data['Date'] &lt; '2023-01-01')]\nP566_GPS_18_23.head()\n</pre> P566_GPS_18_23 = P566_GPS_data[(P566_GPS_data['Date'] &gt;= '2018-01-01') &amp; (P566_GPS_data['Date'] &lt; '2023-01-01')] P566_GPS_18_23.head() In\u00a0[\u00a0]: Copied! <pre>plt.figure()\nplt.plot(P566_GPS_18_23['Date'],P566_GPS_18_23['Vertical (mm)'])\nplt.ylabel('vertical since start (mm)')\nplt.xlabel('date')\nplt.title('GPS data from station P566 (Visalia, CA)')\nplt.show()\n</pre> plt.figure() plt.plot(P566_GPS_18_23['Date'],P566_GPS_18_23['Vertical (mm)']) plt.ylabel('vertical since start (mm)') plt.xlabel('date') plt.title('GPS data from station P566 (Visalia, CA)') plt.show() In\u00a0[\u00a0]: Copied! <pre>## Let's do the same for the period 2006-2011\nP566_GPS_06_11 = \nP566_GPS_06_11.head()\n</pre> ## Let's do the same for the period 2006-2011 P566_GPS_06_11 =  P566_GPS_06_11.head() <p>Now we can build a linear model for the 2006-2011 data and the 2018-2023 data. So that we can compare the slopes of these two models to see if the rate of land subsidence has increased.</p> In\u00a0[\u00a0]: Copied! <pre>## Define and fit a linear regression model using P566_GPS_18_23['days'] and P566_GPS_18_23['Vertical (mm)']\nmodel_18_23 =\nmodel_18_23.fit(\n\n## Retrieve the slope and intercept from the model\nslop_18_23, intercept_18_23 =\nprint(f'The slope is {slop_18_23:.2f} and the intercept is {intercept_18_23:.2f} for the period 2018-2023')\n</pre> ## Define and fit a linear regression model using P566_GPS_18_23['days'] and P566_GPS_18_23['Vertical (mm)'] model_18_23 = model_18_23.fit(  ## Retrieve the slope and intercept from the model slop_18_23, intercept_18_23 = print(f'The slope is {slop_18_23:.2f} and the intercept is {intercept_18_23:.2f} for the period 2018-2023') In\u00a0[\u00a0]: Copied! <pre>## Define and fit a linear regression model using P566_GPS_06_11['days'] and P566_GPS_06_11['Vertical (mm)']\nmodel_06_11 = \nmodel_06_11.fit(\n\n## Retrieve the slope and intercept from the model\nslop_06_11, intercept_06_11 =\nprint(f'The slope is {slop_06_11:.2f} and the intercept is {intercept_06_11:.2f} for the period 2006-2011')\n</pre> ## Define and fit a linear regression model using P566_GPS_06_11['days'] and P566_GPS_06_11['Vertical (mm)'] model_06_11 =  model_06_11.fit(  ## Retrieve the slope and intercept from the model slop_06_11, intercept_06_11 = print(f'The slope is {slop_06_11:.2f} and the intercept is {intercept_06_11:.2f} for the period 2006-2011') <p>Based on your results, think about the following questions and write your answers in the cell below:</p> <p>What is the main economic activity around P566? And what resources does that activity require?</p> <p>What does the vertical component of the GPS time-series tell us about the land movement in the San Joaquin Valley when comparing the periods 2006-2011 and 2018-2023?</p> <p>What are the implications of the land subsidence in the San Joaquin Valley?</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#homework-02-of-eps-88","title":"Homework 02 of EPS 88\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#gps-data-and-subsidence-of-the-san-joaquin-valley","title":"GPS data and subsidence of the San Joaquin Valley\u00b6","text":"<p>The example data we are going to use today is from continuously operating high-precision GPS stations that are operated by UNAVCO which is a non-profit university-governed consortium that facilitates geoscience research and education using geodesy.</p> <p>Let's get an introduction here: https://youtu.be/yxLMk120vMU</p> <p>This data viewer gives a great summary velocity overview of the available GPS data: https://www.unavco.org/software/visualization/GPS-Velocity-Viewer/GPS-Velocity-Viewer.html</p> <p>Let's look at data from a GPS station that is in Visalia California. Visalia is in the San Joaquin Valley between Fresno and Bakersfield.</p> <p>https://www.unavco.org/instrumentation/networks/status/pbo/overview/P566</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#using-pandas-to-import-and-filter-data","title":"Using pandas to import and filter data\u00b6","text":"<p>From the previous classes we have used the pandas library to import and filter data. The DataFrame object has been the most common way we have dealt with data.</p> <p>We have used the <code>numpy</code> library of functions to make numerical and statistical calculations. In particular we have put the numpy array data structure to work for us.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#pandas-timeseries","title":"Pandas timeseries\u00b6","text":"<p>Pandas is good at dealing with time series data. We need to make sure that the data type of the 'Date' column is a time series</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#making-plots-using-matplotlib","title":"Making plots using <code>matplotlib</code>\u00b6","text":"<p>We have relied on <code>matplotlib</code> to make plots throughout the course which we have imported as follows:</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#plotting-with-pandas","title":"Plotting with pandas\u00b6","text":"<p>One thing that you can do using pandas once you have imported matplotlib that we haven't done very much is use built-in plotting functions on the DataFrame. In this case we can use <code>.plot</code>.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#plotting-using-plt-functions","title":"Plotting using plt functions\u00b6","text":"<p>We have made a number of different plot types using <code>matplotlib</code> such as <code>plt.hist()</code>, <code>plt.plot()</code> and <code>plt.scatter()</code>. When dealing with timedate values, one can use <code>plt.plot()</code>, but not <code>plt.scatter()</code>.</p> <p>Let's visualize both the north and east columns using <code>plt.plot()</code>. The data are from every day between Nov. 16, 2005 and now.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#fitting-a-line-with-scikit-learn","title":"Fitting a line with scikit-learn\u00b6","text":"<p>Scikit-learn has a function called <code>LinearRegression</code> that can be used to calculate best fit lines using the .fit() method. We have used this to fit lines to data in the past.</p> <p>Recall from class, we can also consider higher order curves by using the <code>PolynomialFeatures</code> function in scikit-learn.</p> <p>This function can be used to transform the data into a higher order polynomial space and then use the <code>LinearRegression</code> function to fit a line to the transformed data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#make-a-plot-of-prediction","title":"Make a plot of prediction\u00b6","text":"<p>Plot a best-fit line for the data. Recall that you can use model.predict() to predict the values of the best-fit line.</p> <p>Calculate and plot the residual. Recall that the residual is the difference between the actual data and the values obtained with the linear model.</p> <p>Use the same function to predict how far north (relative to stable North America) the Visalia station will go in the next 10 years. There are 365.25 days in a year.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#evaluating-the-model-using-r2","title":"Evaluating the model using $R^{2}$\u00b6","text":"<p>We'd also like to know who well this model fits our data (i.e. how correlated the data are). The $R^{2}$ correlation coefficient can be helpful in this regard. $R^{2}$ is zero for uncorrelated data, and 1 for perfectly linear data (so no misfit between the model line and data). Let's calculate the $R^{2}$ value for our model. Recall that the $R^{2}$ value is calculated as follows:</p> <p>$$R^{2} = 1 - \\frac{\\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}$$</p> <p>where $y_{i}$ is the actual data, $\\hat{y}_{i}$ is the prediction, and $\\bar{y}$ is the mean of the actual data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#defining-a-function","title":"Defining a function\u00b6","text":"<p>When you may be doing a calculation more than once it is a good idea to define a function. Let's define a function that will take an east magnitude and a north magnitude and return a direction between 0 and 360.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#making-a-map-with-cartopy","title":"Making a map with <code>cartopy</code>\u00b6","text":"<p>At the start of the course, we made a number of maps using the <code>cartopy</code> library. Thecode below that will make a map showing the location of the P566 GPS station.</p> <p>Let's define variables giving the latitude and longitude of the P566 GPS station.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework02/#lets-take-look-at-the-vertical-component-of-the-gps-time-series","title":"Let's take look at the vertical component of the GPS time-series\u00b6","text":"<p>We have been looking at the north and east components of the GPS time-series. Let's take a look at the vertical component of the GPS time-series.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/","title":"Homework03","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd  from sklearn.impute import SimpleImputer from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier from sklearn.tree import plot_tree from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.preprocessing import LabelEncoder from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  In\u00a0[2]: Copied! <pre>## Read Doucet et al. 2022 data\nDoucet_data = pd.read_csv('data/Doucet2022.csv',header=11)\n\n## Read Vermeesch (2006) data\nVermeesch_data = pd.read_csv('data/Vermeesch2006.csv')\n</pre> ## Read Doucet et al. 2022 data Doucet_data = pd.read_csv('data/Doucet2022.csv',header=11)  ## Read Vermeesch (2006) data Vermeesch_data = pd.read_csv('data/Vermeesch2006.csv') In\u00a0[\u00a0]: Copied! <pre>## print the column names\nprint(  ## Doucet et al. 2022\nprint(  ## Vermeesch (2006)\n</pre> ## print the column names print(  ## Doucet et al. 2022 print(  ## Vermeesch (2006) <p>We can see that the column names are different. We need to make them the same.</p> <p>There are multiple ways for data cleaning. Here we will use the following steps:</p> <ol> <li>Rename the 'affinity' column to 'type' in the Vermeesch dataframe to match the column name in the Doucet dataframe</li> <li>Remove the units from the column names</li> <li>Keep only the columns that are present in both dataframes</li> </ol> In\u00a0[\u00a0]: Copied! <pre>## rename type to affinity for Doucet data\nVermeesch_data.rename(columns={'affinity':'type'},inplace=True)\nprint(Vermeesch_data.columns)\n</pre> ## rename type to affinity for Doucet data Vermeesch_data.rename(columns={'affinity':'type'},inplace=True) print(Vermeesch_data.columns) In\u00a0[\u00a0]: Copied! <pre>## remove _ppm from the column names\nDoucet_data.columns = Doucet_data.columns.str.replace('_ppm', '')\nVermeesch_data.columns = Vermeesch_data.columns.str.replace('_ppm', '')\n\n## remove _wt_percent from the column names\nDoucet_data.columns = Doucet_data.columns.str.replace('_wt_percent', '')\nVermeesch_data.columns = Vermeesch_data.columns.str.replace('_wt_percent', '')\n\nprint(Doucet_data.columns)\nprint(Vermeesch_data.columns)\n</pre> ## remove _ppm from the column names Doucet_data.columns = Doucet_data.columns.str.replace('_ppm', '') Vermeesch_data.columns = Vermeesch_data.columns.str.replace('_ppm', '')  ## remove _wt_percent from the column names Doucet_data.columns = Doucet_data.columns.str.replace('_wt_percent', '') Vermeesch_data.columns = Vermeesch_data.columns.str.replace('_wt_percent', '')  print(Doucet_data.columns) print(Vermeesch_data.columns) In\u00a0[\u00a0]: Copied! <pre>## keep common columns between Doucet and Vermeesch data\ncommon_columns = list(set(Doucet_data.columns) &amp; set(Vermeesch_data.columns))\n\nDoucet_data = Doucet_data[common_columns]\nVermeesch_data = Vermeesch_data[common_columns]\n\nprint(Doucet_data.columns)\nprint(Vermeesch_data.columns)\n</pre> ## keep common columns between Doucet and Vermeesch data common_columns = list(set(Doucet_data.columns) &amp; set(Vermeesch_data.columns))  Doucet_data = Doucet_data[common_columns] Vermeesch_data = Vermeesch_data[common_columns]  print(Doucet_data.columns) print(Vermeesch_data.columns) <p>Note that here we simply keep the common columns between the two dataframes, which loss some information. If you want to keep all the information, you could further explore how to rename the columns in a way that can be used for both dataframes.</p> <p>The Doucet et al. 2022 study includes data from additional basalt types. To test Vermeesch's hypothesis, let's filter the data to be those from:</p> <ul> <li>Island arc basalts (IAB) In the Doucet et al. dataset these are called <code>ARC-O</code> standing for oceanic arc.</li> <li>Mid-ocean ridge (MORB)</li> <li>Ocean-island (OIB)</li> </ul> <p>The code below filters to these types and creates a new dataframe</p> In\u00a0[\u00a0]: Copied! <pre>## Filter the data \nDoucet_data = Doucet_data[(Doucet_data['type']=='MORB') | (Doucet_data['type']=='OIB') | (Doucet_data['type']=='ARC-O')].reset_index(drop=True)\nDoucet_data.head()\n\n## To make the column names consistent with Vermeesch (2006), let's rename the type names \"ARC-O\" to \"IAB\"\nDoucet_data.loc[Doucet_data['type']=='ARC-O','type'] = 'IAB'\nDoucet_data.head()\n</pre> ## Filter the data  Doucet_data = Doucet_data[(Doucet_data['type']=='MORB') | (Doucet_data['type']=='OIB') | (Doucet_data['type']=='ARC-O')].reset_index(drop=True) Doucet_data.head()  ## To make the column names consistent with Vermeesch (2006), let's rename the type names \"ARC-O\" to \"IAB\" Doucet_data.loc[Doucet_data['type']=='ARC-O','type'] = 'IAB' Doucet_data.head()  In\u00a0[\u00a0]: Copied! <pre>## Encode the target variable 'type' using LabelEncoder\nle = LabelEncoder()\n# we seperate fit and transform calls, so that we can use the same label encoder for both datasets\nle.fit(\n\n## Split the data into features (X) and target (y)\nX = Doucet_data.drop('type',axis=1)\nX_columns = X.columns ## save the column names\ny = Doucet_data['type']\n\n## apply le.transform to convert y from string to integer\ny =  \n\n## Impute missing values using median imputation\nimputer = SimpleImputer(strategy='median')\n#we seperate fit and transform calls, so that we can use the same imputer for both datasets\nimputer.fit( \n#apply imputer.transform to fill in the missing values\nX = \n\n## Split the data into training and testing sets. Use 30% of the data as the testing set. Set random_state=42\nX_train, X_test, y_train, y_test = \n\n## Let's check the shape of the training and testing sets\nprint(f\"{X_train.shape = }\")\nprint(f\"{X_test.shape = }\")\n</pre> ## Encode the target variable 'type' using LabelEncoder le = LabelEncoder() # we seperate fit and transform calls, so that we can use the same label encoder for both datasets le.fit(  ## Split the data into features (X) and target (y) X = Doucet_data.drop('type',axis=1) X_columns = X.columns ## save the column names y = Doucet_data['type']  ## apply le.transform to convert y from string to integer y =    ## Impute missing values using median imputation imputer = SimpleImputer(strategy='median') #we seperate fit and transform calls, so that we can use the same imputer for both datasets imputer.fit(  #apply imputer.transform to fill in the missing values X =   ## Split the data into training and testing sets. Use 30% of the data as the testing set. Set random_state=42 X_train, X_test, y_train, y_test =   ## Let's check the shape of the training and testing sets print(f\"{X_train.shape = }\") print(f\"{X_test.shape = }\")  In\u00a0[\u00a0]: Copied! <pre>## Build a logistic regression classifier using LogisticRegression(solver=\"liblinear\")\n## remember to use make_pipeline and StandardScaler to standardize the data\nmodel_linear = \n\n## train the model\nmodel_linear.fit(\n\n## predict on the test set\ny_pred = \n\n## evaluate the model\naccuracy = \nprint(f\"{accuracy = }\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Build a logistic regression classifier using LogisticRegression(solver=\"liblinear\") ## remember to use make_pipeline and StandardScaler to standardize the data model_linear =   ## train the model model_linear.fit(  ## predict on the test set y_pred =   ## evaluate the model accuracy =  print(f\"{accuracy = }\")  ## plot confusion matrix conf_matrix = confusion_matrix(y_test,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);  In\u00a0[\u00a0]: Copied! <pre>## Build a SVM classifier using SVC(kernel=\"linear\")\n## remember to use make_pipeline and StandardScaler to standardize the data\nmodel_svm = \n\n## train the model\nmodel_svm.fit(\n\n## predict on the test set\ny_pred = \n\n## evaluate the model\naccuracy = \nprint(f\"{accuracy = }\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Build a SVM classifier using SVC(kernel=\"linear\") ## remember to use make_pipeline and StandardScaler to standardize the data model_svm =   ## train the model model_svm.fit(  ## predict on the test set y_pred =   ## evaluate the model accuracy =  print(f\"{accuracy = }\")  ## plot confusion matrix conf_matrix = confusion_matrix(y_test,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <p>As we dicussed in class, the advantage of SVM is that it can handle non-linear decision boundaries. Let's try a RBF kernel SVM.</p> <p>The RBF kernel SVM is more computationally expensive than the linear kernel SVM, but it can handle non-linear decision boundaries.</p> <p>The gamma parameter controls the smoothness of the decision boundary. A large gamma value will create a more complex decision boundary, while a small gamma value will create a simpler decision boundary.</p> <p>First build a SVM classifier using a default gamma value of 0.1.</p> <p>Then try a different gamma value and see if you can find a better model.</p> <p>Report the best gamma value and the accuracy of the model.</p> In\u00a0[\u00a0]: Copied! <pre>## Build a SVM classifier using SVC(kernel=\"rbf\",gamma=0.1)\n## Remember to use make_pipeline and StandardScaler to standardize the data\nmodel_svm = \n\n## train the model\nmodel_svm.fit(\n\n## predict on the test set\ny_pred = \n\n## evaluate the model\naccuracy = \nprint(f\"{accuracy = }\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Build a SVM classifier using SVC(kernel=\"rbf\",gamma=0.1) ## Remember to use make_pipeline and StandardScaler to standardize the data model_svm =   ## train the model model_svm.fit(  ## predict on the test set y_pred =   ## evaluate the model accuracy =  print(f\"{accuracy = }\")  ## plot confusion matrix conf_matrix = confusion_matrix(y_test,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); In\u00a0[\u00a0]: Copied! <pre>## Build a decision tree classifier with default setting DecisionTreeClassifier(max_depth=None)\n## No need to standardize the data for decision tree\nmodel_tree = \n\n## train the model\nmodel_tree.fit(\n\n## predict on the test set\ny_pred = \n\n## evaluate the model\naccuracy = \nprint(f\"{accuracy = }\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Build a decision tree classifier with default setting DecisionTreeClassifier(max_depth=None) ## No need to standardize the data for decision tree model_tree =   ## train the model model_tree.fit(  ## predict on the test set y_pred =   ## evaluate the model accuracy =  print(f\"{accuracy = }\")  ## plot confusion matrix conf_matrix = confusion_matrix(y_test,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <p>As we discussed in class, the decision tree classifier is a white box model, which means we can understand the importance of each feature in the decision making process.</p> In\u00a0[\u00a0]: Copied! <pre>## print the most important features\nfeature_importances = model_tree.feature_importances_\nfeature_df = pd.DataFrame({'feature':X_columns,'importance':feature_importances})\nfeature_df = feature_df.sort_values(by='importance',ascending=False)\nfeature_df.head(10)\n</pre> ## print the most important features feature_importances = model_tree.feature_importances_ feature_df = pd.DataFrame({'feature':X_columns,'importance':feature_importances}) feature_df = feature_df.sort_values(by='importance',ascending=False) feature_df.head(10) <p>How does the accuracy of the decision tree based on larger dataset from Doucet et al. (2022) compare to that using the smaller dataset from Vermeesch (2006)? Write your answer in the markdown cell below.</p> <p>What similarities and differences are there between the importance of different data fields (feature importance) between the decision tree built on the Vermeesch (2006) data compilation vs that built on the Doucet et al. (2022) data compilation?</p> In\u00a0[\u00a0]: Copied! <pre>## Preprocess the Vermeesch (2006) dataset\n## Split the data into features (X) and target (y)\nX = Vermeesch_data.drop('type',axis=1)\nX = X[X_columns] ## keep column in the same order as Doucet et al. (2022) dataset\ny = Vermeesch_data['type']\ny = le.transform(y) ## reuse the label encoder we defined earlier\n\n## Impute missing values using median imputation \n#use the same imputer we defined earlier\nX = imputer.transform(X) \n</pre> ## Preprocess the Vermeesch (2006) dataset ## Split the data into features (X) and target (y) X = Vermeesch_data.drop('type',axis=1) X = X[X_columns] ## keep column in the same order as Doucet et al. (2022) dataset y = Vermeesch_data['type'] y = le.transform(y) ## reuse the label encoder we defined earlier  ## Impute missing values using median imputation  #use the same imputer we defined earlier X = imputer.transform(X)   In\u00a0[\u00a0]: Copied! <pre>## Apply the linear model and evaluate the accuracy\ny_pred = \naccuracy = accuracy_score(y,y_pred)\nprint(f\"Logistic regression accuracy: {accuracy:.3f}\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Apply the linear model and evaluate the accuracy y_pred =  accuracy = accuracy_score(y,y_pred) print(f\"Logistic regression accuracy: {accuracy:.3f}\")  ## plot confusion matrix conf_matrix = confusion_matrix(y,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); In\u00a0[\u00a0]: Copied! <pre>## Apply the SVM model and evaluate the accuracy\ny_pred = \naccuracy = accuracy_score(y,y_pred)\nprint(f\"SVM accuracy: {accuracy:.3f}\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Apply the SVM model and evaluate the accuracy y_pred =  accuracy = accuracy_score(y,y_pred) print(f\"SVM accuracy: {accuracy:.3f}\")  ## plot confusion matrix conf_matrix = confusion_matrix(y,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); In\u00a0[\u00a0]: Copied! <pre>## Apply the decision tree model and evaluate the accuracy\ny_pred = \naccuracy = accuracy_score(y,y_pred)\nprint(f\"Decision tree accuracy: {accuracy:.3f}\")\n\n## plot confusion matrix\nconf_matrix = confusion_matrix(y,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Apply the decision tree model and evaluate the accuracy y_pred =  accuracy = accuracy_score(y,y_pred) print(f\"Decision tree accuracy: {accuracy:.3f}\")  ## plot confusion matrix conf_matrix = confusion_matrix(y,y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <p>Based on the accuracy, please comment on the following questions:</p> <ul> <li><p>How does the accuracy of models trained on Doucet et al. (2022) dataset compare to the model trained on Vermeesch (2006) dataset? Recall the accuracy you get in previous class using the model trained on Vermeesch (2006) dataset.</p> </li> <li><p>Among these models, which one performs the best on the Doucet et al. (2022) dataset and which one performane the worst? Similar question for the Vermeesch (2006) dataset. Are the best and worst models the same or totally different?</p> </li> <li><p>What are the possible reasons for the difference in accuracy?  What does this imply for the predictive power of machine learning models? Hint: think about model fitting and generalization.</p> </li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#homework-03-of-eps-88","title":"Homework 03 of EPS 88\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#learning-from-a-bigger-classification-of-basalt-source","title":"Learning from a bigger classification of basalt source\u00b6","text":"<p>In the 2006 paper</p> <p>Vermeesch, P. (2006). Tectonic discrimination of basalts with classification trees. Geochimica et Cosmochimica Acta, 70, 1839-1848. https://doi.org/10.1016/j.gca.2005.12.016</p> <p>Vermeesch wrote:</p> <p>\"If a much larger database were compiled, the trees would grow and their discriminative power increase, but they would still be easy to interpret\"</p> <p>In a more recent paper, Doucet et al. compiled many more data. Rather than 756 basalt data points, they compiled 29,407 of which 22,005 correspond to the categories of Vermeesch (2006).</p> <p>Doucet, L. S., Tetley, M. G., Li, Z.-X., Liu, Y., &amp; Gamaleldien, H. (2022). Geochemical fingerprinting of continental and oceanic basalts: A machine learning approach. Earth-Science Reviews, 233, https://doi.org/10.1016/j.earscirev.2022.104192</p> <p>Your task in this assignment is use the data of Doucet et al. (2022) to evaluate whether the predictive power of the classification tree approach increases within this increase in data size as predicted by Vermeesch (2006).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#import-scientific-python-libraries","title":"Import scientific Python libraries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#import-data","title":"Import data\u00b6","text":"<p>We will import the data from Doucet et al. 2022 that is provided as their supplemental table 1.</p> <p>For comparison, we will also import the data from Vermeesch (2006).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#inspect-data","title":"Inspect data\u00b6","text":"<p>As we have two dataframes, if we want to develop a machine learning model that can be applied to both datasets, we need to make sure that the column names (feature names) are the same.</p> <p>Let's first inspect the column names of the two dataframes. If they are different, we need to make them the same.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#data-preprocessing","title":"Data preprocessing\u00b6","text":"<p>Recall the data preprocessing steps we used in the previous class:</p> <ul> <li>Encode the target variable 'type' using LabelEncoder</li> <li>Split the data into features (X) and target (y)</li> <li>Impute missing values using median imputation</li> <li>Split the data into training and testing sets</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#build-a-logistic-regression-classifier","title":"Build a logistic regression classifier\u00b6","text":"<p>Review the previous lectures on logistic regression and how to build a logistic regression classifier using <code>sklearn</code>.</p> <p>Try to build a logistic regression classifier and evaluate its accuracy using Doucet et al. (2022) data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#build-a-svm-classifier","title":"Build a SVM classifier\u00b6","text":"<p>Let's try to use the more advanced machine learning models SVM to see if they can improve the accuracy.</p> <p>First, let's try a linear kernel SVM, which is similar to the logistic regression classifier we built above.</p> <p>Review the previous lectures on SVM and how to build a SVM classifier using <code>sklearn</code>.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#build-a-decision-tree-classifier","title":"Build a decision tree classifier\u00b6","text":"<p>Review the previous lectures on decision tree and how to build a decision tree classifier using <code>sklearn</code>.</p> <p>Similar to the gamma parameter in SVM, the <code>max_depth</code> parameter in decision tree can also affect the complexity of the model.</p> <p>The default setting is <code>max_depth=None</code> which means it will keep going and going until the leafs of the tree contain a single category.</p> <p>First, let's build a decision tree classifier with the default setting.</p> <p>Then try different <code>max_depth</code> values and see if you can find a better model.</p> <p>Report the best <code>max_depth</code> value and the accuracy of the model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework03/#applying-the-model-trained-on-doucet-et-al-2022-dataset-to-the-vermeesch-2006-dataset","title":"Applying the model trained on Doucet et al. (2022) dataset to the Vermeesch (2006) dataset\u00b6","text":"<p>The goal of machine learning is to generalize the model to new data. Here we will apply the decision tree classifier trained on the Doucet et al. (2022) dataset to the Vermeesch (2006) dataset to see how well the model can be applied to new data.</p> <p>Let's apply the linear model, SVM, and decision tree classifier to the Vermeesch (2006) dataset and evaluate their accuracy.</p> <p>Please note that you would need to preprocess the Vermeesch (2006) dataset in the same way as the Doucet et al. (2022) dataset.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/","title":"Homework04","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.dates as mdates\nfrom matplotlib import cm\nimport cartopy.crs as ccrs\nfrom cartopy.io.shapereader import Reader\nfrom geopy import distance\nimport datetime\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import r2_score\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.patches as mpatches import matplotlib.dates as mdates from matplotlib import cm import cartopy.crs as ccrs from cartopy.io.shapereader import Reader from geopy import distance import datetime from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline from sklearn.metrics import r2_score In\u00a0[\u00a0]: Copied! <pre>## Load the topography data using np.loadtxt\nlats = np.loadtxt(       # ./data/etopo20lats.txt\nlons = np.loadtxt(       # ./data/etopo20lons.txt\ntopo_grid = np.loadtxt(  # ./data/etopo20data.txt\n\n## Create a meshgrid for latitude and longitude using np.meshgrid\nlon_grid, lat_grid = np.meshgrid(\n\n## Check the shape of lat_grid, lon_grid, and topo_grid are the same\nprint(f\"Shape of lat_grid: {lat_grid.shape}, Shape of lon_grid: {lon_grid.shape}, Shape of topo_grid: {topo_grid.shape}\")\n\n## Downsample the data to make the plotting faster\nlon_grid = lon_grid[::2, ::2]\nlat_grid = lat_grid[::2, ::2]\ntopo_grid = topo_grid[::2, ::2]\n</pre> ## Load the topography data using np.loadtxt lats = np.loadtxt(       # ./data/etopo20lats.txt lons = np.loadtxt(       # ./data/etopo20lons.txt topo_grid = np.loadtxt(  # ./data/etopo20data.txt  ## Create a meshgrid for latitude and longitude using np.meshgrid lon_grid, lat_grid = np.meshgrid(  ## Check the shape of lat_grid, lon_grid, and topo_grid are the same print(f\"Shape of lat_grid: {lat_grid.shape}, Shape of lon_grid: {lon_grid.shape}, Shape of topo_grid: {topo_grid.shape}\")  ## Downsample the data to make the plotting faster lon_grid = lon_grid[::2, ::2] lat_grid = lat_grid[::2, ::2] topo_grid = topo_grid[::2, ::2] <ul> <li>Plot the topography using the Orthographic projection</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Set the projection\nax = plt.axes(projection=ccrs.Orthographic(central_longitude=-40.0, central_latitude=20.0))\nax.set_global()\n\n## Plot the topography using the Orthographic projection\nplt.contourf(\n\n## Add coastlines and gridlines\nax.coastlines()\nax.gridlines()\n\n## Add colorbar\ncolor_ax = plt.axes([0.95, 0.3, 0.05, 0.35])\nplt.colorbar(cax=color_ax, label='elevation/depth (m)')\n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Set the projection ax = plt.axes(projection=ccrs.Orthographic(central_longitude=-40.0, central_latitude=20.0)) ax.set_global()  ## Plot the topography using the Orthographic projection plt.contourf(  ## Add coastlines and gridlines ax.coastlines() ax.gridlines()  ## Add colorbar color_ax = plt.axes([0.95, 0.3, 0.05, 0.35]) plt.colorbar(cax=color_ax, label='elevation/depth (m)')  plt.show() In\u00a0[\u00a0]: Copied! <pre>## Load the seafloor age data\nseafloor_age_data = pd.read_csv(      # data/age.csv\n\n## extract longitude, latitude, and age\nage_longitude = seafloor_age_data['\nage_latitude = seafloor_age_data['\nage = seafloor_age_data['\n\n## reshape the data to 2D arrays. Hint: the shape of the data is (901,1801)\nage_grid = age.reshape((901,1801))\nage_long_grid = age_longitude.reshape((901,1801))\nage_lat_grid = age_latitude.reshape((901,1801))\n\n## check the shape of the data\nprint(f\"Shape of age_grid: {age_grid.shape}, Shape of age_long_grid: {age_long_grid.shape}, Shape of age_lat_grid: {age_lat_grid.shape}\")\n\n## Downsample the data to make the plotting faster\nage_long_grid = age_long_grid[::4, ::4]\nage_lat_grid = age_lat_grid[::4, ::4]\nage_grid = age_grid[::4, ::4]\n</pre> ## Load the seafloor age data seafloor_age_data = pd.read_csv(      # data/age.csv  ## extract longitude, latitude, and age age_longitude = seafloor_age_data[' age_latitude = seafloor_age_data[' age = seafloor_age_data['  ## reshape the data to 2D arrays. Hint: the shape of the data is (901,1801) age_grid = age.reshape((901,1801)) age_long_grid = age_longitude.reshape((901,1801)) age_lat_grid = age_latitude.reshape((901,1801))  ## check the shape of the data print(f\"Shape of age_grid: {age_grid.shape}, Shape of age_long_grid: {age_long_grid.shape}, Shape of age_lat_grid: {age_lat_grid.shape}\")  ## Downsample the data to make the plotting faster age_long_grid = age_long_grid[::4, ::4] age_lat_grid = age_lat_grid[::4, ::4] age_grid = age_grid[::4, ::4]  <ul> <li>Plot the seafloor age using the Orthographic projection</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Set the projection\nax = plt.axes(projection=ccrs.Orthographic(central_longitude=-40.0, central_latitude=20.0))\nax.set_global()\n\n## Plot the seafloor age using the Orthographic projection\nplt.contourf(\n\n## Add coastlines and gridlines\nax.coastlines()\nax.gridlines()\n\n## Add colorbar\ncolor_ax = plt.axes([0.95, 0.3, 0.05, 0.35])\nplt.colorbar(cax=color_ax, label='Age, Myr') \n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Set the projection ax = plt.axes(projection=ccrs.Orthographic(central_longitude=-40.0, central_latitude=20.0)) ax.set_global()  ## Plot the seafloor age using the Orthographic projection plt.contourf(  ## Add coastlines and gridlines ax.coastlines() ax.gridlines()  ## Add colorbar color_ax = plt.axes([0.95, 0.3, 0.05, 0.35]) plt.colorbar(cax=color_ax, label='Age, Myr')   plt.show() <p>Question: What patterns do you observe? Where is the youngest seafloor in relation to the seafloor ridges we observed in our map of topography? Where is the oldest seafloor?</p> <p>What was the position of the continents like when the oldest seafloor formed?</p> <ul> <li>Load the Earthquake Catalog</li> </ul> <p>Load the .csv (comma separated values) data file of all the earthquakes of magnitude 4 and higher from 2000 - 2012 in the ANSS (Advanced National Seismic System) Comprehensive Catalog or \"ComCat.\"</p> <p>The ANSS Comprehensive Catalog (ComCat) http://www.quake.geo.berkeley.edu/anss/catalog-search.html</p> <p>This data set has the following columns:</p> <p><code>DateTime, Latitude, Longitude, Depth, Magnitude, MagType, NbStations, Gap, Distance, RMS, Source, EventID</code></p> <p>Let's import it using the pandas <code>pd.read_csv()</code> function. We can see the first 5 rows of the dataframe using the <code>.head()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>## Load the earthquake data. Hint: skipping the first 7 rows using header=7\nEQ_data = pd.read_csv(   #data/ANSS_2000_2012.csv\n\n## Check the first rows of the dataframe\nEQ_data.head()\n</pre> ## Load the earthquake data. Hint: skipping the first 7 rows using header=7 EQ_data = pd.read_csv(   #data/ANSS_2000_2012.csv  ## Check the first rows of the dataframe EQ_data.head()  <p>Recall from the homework that Pandas dataframe columns can be accessed using bracket notation with the name of the column as a string:</p> In\u00a0[\u00a0]: Copied! <pre>## Access the Magnitude column\nEQ_data['\n</pre> ## Access the Magnitude column EQ_data[' <ul> <li>What is the largest magnitude earthquake in our catalog?</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Find the largest magnitude earthquake in our catalog\nlargest_magnitude = \n\nprint(f\"The largest magnitude earthquake in our catalog is {largest_magnitude}\")\n</pre> ## Find the largest magnitude earthquake in our catalog largest_magnitude =   print(f\"The largest magnitude earthquake in our catalog is {largest_magnitude}\") <ul> <li>Determining when and where the largest Earthquake happened</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Find the date of the largest earthquake. Hint: using the `==` operator, e.g. EQ_data['DateTime'][EQ_data['Magnitude'] == largest_magnitude], to filter the dataframe, and `.values[0]` to get the first value of the filtered dataframe\nlargest_eq_date = EQ_data['DateTime'][\n\nprint(f\"The largest earthquake in our catalog happened on {largest_eq_date}\")\n</pre> ## Find the date of the largest earthquake. Hint: using the `==` operator, e.g. EQ_data['DateTime'][EQ_data['Magnitude'] == largest_magnitude], to filter the dataframe, and `.values[0]` to get the first value of the filtered dataframe largest_eq_date = EQ_data['DateTime'][  print(f\"The largest earthquake in our catalog happened on {largest_eq_date}\") <ul> <li>Determine where the earthquake happened</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## Find the longitude and latitude of the largest earthquake. \nlargest_eq_lon = EQ_data['Longitude'][\nlargest_eq_lat = EQ_data['Latitude'][\n\nprint(f\"The largest earthquake in our catalog happened at {largest_eq_lon} longitude and {largest_eq_lat} latitude\")\n</pre> ## Find the longitude and latitude of the largest earthquake.  largest_eq_lon = EQ_data['Longitude'][ largest_eq_lat = EQ_data['Latitude'][  print(f\"The largest earthquake in our catalog happened at {largest_eq_lon} longitude and {largest_eq_lat} latitude\") <p>There are many colormaps available in Matplotlib: https://matplotlib.org/stable/tutorials/colors/colormaps.html</p> <p>There are many different markers available in Matplotlib</p> <p>Let's plot a red square at the location of the largest earthquake in our catalog.</p> <p>To the <code>plt.scatter</code> function, add <code>s=100</code> to adjust the size of the marker, add <code>color='red'</code> to change the color, add <code>marker=s</code> to make it a square.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Set the projection using ccrs.Robinson(central_longitude=180)\nax = plt.axes(projection=ccrs.Robinson(central_longitude=180))\nax.set_global()\n\n## Plot the largest earthquake, using s=100, marker='s', color='red'\nplt.scatter(\n\n## Add coastlines, stock image, and gridlines\nax.coastlines()\nax.stock_img()\nax.gridlines()\n\n## Add a title\nplt.title('2011 T\u014dhoku earthquake')    \n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Set the projection using ccrs.Robinson(central_longitude=180) ax = plt.axes(projection=ccrs.Robinson(central_longitude=180)) ax.set_global()  ## Plot the largest earthquake, using s=100, marker='s', color='red' plt.scatter(  ## Add coastlines, stock image, and gridlines ax.coastlines() ax.stock_img() ax.gridlines()  ## Add a title plt.title('2011 T\u014dhoku earthquake')      plt.show() <p>Question: Watch the following video and comment on the effects of this earthquake:</p> <p>https://www.youtube.com/watch?v=oWzdgBNfhQU</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Plot a histogram of earthquake magnitudes, using bins=30, edgecolor='white'\nplt.hist(\n\n## Label the axes and add a title\nplt.xlabel('\nplt.ylabel('\nplt.title('Histogram of Earthquake Magnitudes')\n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Plot a histogram of earthquake magnitudes, using bins=30, edgecolor='white' plt.hist(  ## Label the axes and add a title plt.xlabel(' plt.ylabel(' plt.title('Histogram of Earthquake Magnitudes')  plt.show() <p>There are so many small earthquakes that we can't even see a bin for the Tohoku quake.</p> <p>Let's make the histogram on a log-scale.</p> <p>For any function, we can put a question mark after it to get its docstring. Let's do this for <code>plt.hist()</code>.</p> <p>Once you execute the cell below, you will see that there are a lot of options (which you can also view here: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html).</p> <p>One of the options is to make the plot be on a log scale by setting <code>log=True</code>.</p> In\u00a0[\u00a0]: Copied! <pre>?plt.hist\n</pre> ?plt.hist <ul> <li>Make a histogram of the Earthquake magnitude data on a log-scale</li> </ul> <p>Set <code>log=True</code> within the <code>plt.hist</code> function.</p> <p>Let's make sure that this figure has labeled axes using <code>plt.xlabel()</code> and <code>plt.ylabel()</code> and we can add a title as well using <code>plt.title()</code></p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Plot a histogram of earthquake magnitudes, using bins=30, edgecolor='white', log=True\nplt.hist(\n\n## Label the axes and add a title\nplt.xlabel('\nplt.ylabel('\nplt.title('Histogram of Earthquake Magnitudes')\n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Plot a histogram of earthquake magnitudes, using bins=30, edgecolor='white', log=True plt.hist(  ## Label the axes and add a title plt.xlabel(' plt.ylabel(' plt.title('Histogram of Earthquake Magnitudes')  plt.show() In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Plot a histogram of earthquake depths, using bins=30, edgecolor='white', log=True\nplt.hist(\n\n## Label the axes and add a title\nplt.xlabel('\nplt.ylabel('\nplt.title('Histogram of Earthquake Depths')\n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Plot a histogram of earthquake depths, using bins=30, edgecolor='white', log=True plt.hist(  ## Label the axes and add a title plt.xlabel(' plt.ylabel(' plt.title('Histogram of Earthquake Depths')  plt.show() <p>Questsion: At what depth are the majority of earthquakes? How deep do they extend? How does that compare to the typical depth of the lithosphere (~100 km)?</p> <p>The map we made above is nice, but it doesn't tell us everything about our data. Let's create a map of the earthquake epicenters that colors the points by depth.</p> <p>To do this, use the same <code>plt.scatter()</code> function, but add the option to set the color by depth. You can do this by having  <code>c=EQ_data['Depth']</code> within the function.</p> <p>You can customize the output by setting the minimum value for the color bar <code>vmin=0</code> and the maximum value <code>vmax=200</code>.</p> <p>You can also customize the colormap. A perceptually uniform sequential color map like <code>cmap='magma_r'</code> works well (https://matplotlib.org/tutorials/colors/colormaps.html).</p> <p>It is also nice to make the points partially see through by setting <code>alpha=0.5</code>.</p> <p>All of these customizations can be made by adding these arguments within the <code>plt.scatter()</code> function.</p> <p>Make a map that colors points by depth by inserting these arguments in the <code>plt.scatter()</code> function in the code block below.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nfig = plt.figure(figsize=(15,15))\n\n## Set the projection using ccrs.Robinson(central_longitude=180)\nax = plt.axes(projection=ccrs.Robinson(central_longitude=180))\nax.set_global()\n\n## Plot the epicenters of the earthquakes, using c=EQ_data['Depth'], s=10, vmin=0,vmax=200,cmap='magma_r',alpha=0.5\nplt.scatter(\n\n## Add coastlines, stock image, and gridlines\nax.coastlines()\nax.stock_img()\nax.gridlines()\n\n## Add a colorbar\nplt.colorbar(shrink=0.4, label='depth (km)')\n\n## Add a title\nplt.title('Earthquake Epicenters 2000-2012')\n\nplt.show()\n</pre> ## Make a figure fig = plt.figure(figsize=(15,15))  ## Set the projection using ccrs.Robinson(central_longitude=180) ax = plt.axes(projection=ccrs.Robinson(central_longitude=180)) ax.set_global()  ## Plot the epicenters of the earthquakes, using c=EQ_data['Depth'], s=10, vmin=0,vmax=200,cmap='magma_r',alpha=0.5 plt.scatter(  ## Add coastlines, stock image, and gridlines ax.coastlines() ax.stock_img() ax.gridlines()  ## Add a colorbar plt.colorbar(shrink=0.4, label='depth (km)')  ## Add a title plt.title('Earthquake Epicenters 2000-2012')  plt.show() <p>Question: What depth of earthquakes occur at mid-ocean ridges?</p> <p>Question: What depth of earthquakes occur at trenches?</p> <p>Source: Fundamentals of Geophysics (2nd Edition) Lowrie, W.</p> <p>A cross-section through a subduction zone. Red points are earthquake focus points. The most active region is the zone of contact between the plates. There is a back-arc seismic zone in the overriding plate. Below ~70 km depth earthquakes occur within the subducting plate, this region is call the Wadati-Benioff seismic zone.</p> <p>The earthquakes at trenches (like around the Pacific ocean's 'ring of fire') get deeper in a systematic way. The deepest earthquakes are the farthest from the trench. This reveals the location of the downgoing slabs.</p> <ul> <li>The Andean subduction zone</li> </ul> <p>Let's look at a subset of this earthquake catalog across the Andes in South America. The code below is filtering the data frame to only include those between 20\u00baS and 25\u00baS latitude and 75\u00baW and 60\u00baW longitude.</p> In\u00a0[17]: Copied! <pre>## Filter the data frame to only include those between 20\u00baS and 25\u00baS latitude and 75\u00baW and 60\u00baW longitude\nmin_lat = -25\nmax_lat = -20\nmin_lon = -75\nmax_lon = -60 \nselected_quakes = EQ_data[\n</pre> ## Filter the data frame to only include those between 20\u00baS and 25\u00baS latitude and 75\u00baW and 60\u00baW longitude min_lat = -25 max_lat = -20 min_lon = -75 max_lon = -60  selected_quakes = EQ_data[ <p>Let's make a map of the epicenters of these earthquakes following the same procedure as we did above.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Set the projection using ccrs.Robinson(central_longitude=180)\nax = plt.axes(projection=ccrs.Robinson(central_longitude=180))\nax.set_global()\n\n## Plot the epicenters of the earthquakes, using s=10, marker='.', color='red'\nplt.scatter(\n\n## Add coastlines, stock image, and gridlines\nax.coastlines()\nax.stock_img()\nax.gridlines()\n\n## Add a title\nplt.title('Selected Earthquake Epicenters 2000-2012')    \n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Set the projection using ccrs.Robinson(central_longitude=180) ax = plt.axes(projection=ccrs.Robinson(central_longitude=180)) ax.set_global()  ## Plot the epicenters of the earthquakes, using s=10, marker='.', color='red' plt.scatter(  ## Add coastlines, stock image, and gridlines ax.coastlines() ax.stock_img() ax.gridlines()  ## Add a title plt.title('Selected Earthquake Epicenters 2000-2012')      plt.show() <p>Let's take all of the earthquakes within that region and plot earthquake depth on the y-axis and earthquake location on the x-axis.</p> <p>Labeling axes is super important in science! Don't make plots without labeled axes!</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Plot the epicenters of the earthquakes, using longitude on the x-axis and depth on the y-axis, using s=10, marker='.'. \n## Hint: Using -selected_quakes['Depth'] to plot deeper earthquakes lower on the y-axis\nplt.scatter(\n\n## Label the axes and add a title\nplt.xlabel('\nplt.ylabel('\nplt.title('Earthquake depths from 20\u00b0S to 25\u00b0S')\n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Plot the epicenters of the earthquakes, using longitude on the x-axis and depth on the y-axis, using s=10, marker='.'.  ## Hint: Using -selected_quakes['Depth'] to plot deeper earthquakes lower on the y-axis plt.scatter(  ## Label the axes and add a title plt.xlabel(' plt.ylabel(' plt.title('Earthquake depths from 20\u00b0S to 25\u00b0S')  plt.show() <ul> <li>Japan subduction</li> </ul> <p>Let's look at the subduction zone off the east coast of Japan.</p> <p>Please find the correct latitude and longitude range for this region and plot the earthquakes on a map and make a similar depth vs. longitude plot (or depth vs latitude plot) for another region.</p> In\u00a0[20]: Copied! <pre># Define the latitude and longitude range\nmin_lat = 32\nmax_lat = 43\nmin_lon = 135\nmax_lon = 145\n\nselected_quakes = EQ_data[\n</pre> # Define the latitude and longitude range min_lat = 32 max_lat = 43 min_lon = 135 max_lon = 145  selected_quakes = EQ_data[ In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure()\n\n## Plot the epicenters of the earthquakes, using longitude on the x-axis and depth on the y-axis, using s=10, marker='.'. \n## Hint: Using -selected_quakes['Depth'] to plot deeper earthquakes lower on the y-axis\nplt.scatter(\n\n## Label the axes and add a title\nplt.xlabel('\nplt.ylabel('\nplt.title('earthquake depths from 30\u00b0N to 40\u00b0N')\n\nplt.show()\n</pre> ## Make a figure plt.figure()  ## Plot the epicenters of the earthquakes, using longitude on the x-axis and depth on the y-axis, using s=10, marker='.'.  ## Hint: Using -selected_quakes['Depth'] to plot deeper earthquakes lower on the y-axis plt.scatter(  ## Label the axes and add a title plt.xlabel(' plt.ylabel(' plt.title('earthquake depths from 30\u00b0N to 40\u00b0N')  plt.show() <p>Question: What direction is subduction occuring below South America and Japan? Are they towards the east or west?</p> <p>In stead of using the prepared earthquake catalog, let's use the USGS API to import a global earthquake catalog (Magnitude &gt;= 6.0).</p> <p>This can be very useful if you want to get the latest earthquake data.</p> <ul> <li>Load the earthquake data from the USGS API</li> </ul> <p>Update the start_day and end_day (from 2010-01-01 to 2025-01-01) to get a query url that will return earthquakes that occured over the past 10 years.</p> In\u00a0[\u00a0]: Copied! <pre>## Define the start and end day from 2010-01-01 to 2025-01-01 and the minimum magnitude = 6.0\nstart_day = '\nend_day = '\nmin_magnitude = \n\n## Define the standard url\nstandard_url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&amp;orderby=magnitude'\n\n## Define the query url\nquery_url = f\"{standard_url}&amp;starttime={start_day}&amp;endtime={end_day}&amp;minmagnitude={min_magnitude}\"\n\n## Read the data from the query url into a dataframe\nearthquake_data = pd.read_csv(query_url)\n\n## Print the first rows of the dataframe\nearthquake_data.head()\n</pre> ## Define the start and end day from 2010-01-01 to 2025-01-01 and the minimum magnitude = 6.0 start_day = ' end_day = ' min_magnitude =   ## Define the standard url standard_url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&amp;orderby=magnitude'  ## Define the query url query_url = f\"{standard_url}&amp;starttime={start_day}&amp;endtime={end_day}&amp;minmagnitude={min_magnitude}\"  ## Read the data from the query url into a dataframe earthquake_data = pd.read_csv(query_url)  ## Print the first rows of the dataframe earthquake_data.head() <ul> <li>Plotting the plate boundaries and earthquake locations together</li> </ul> <p>In addition to plotting the earthquake locations, we can plot the location of plate boundaries.</p> <p>I took the plate boundaries provided by the US Geological Survey (USGS) and split them by their categorization into trenches (subduction zones), ridges (spreading centers) and transform (strike-slip boundaries like the San Andreas fault).</p> <p>The code below makes a map where these different plate boundaries are represented by different color lines. Please add the earthquake locations to this map as well.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure(figsize=(15,15))\n\n## Set the projection using ccrs.Robinson(central_longitude=180)\nax = plt.axes(projection=ccrs.Robinson(central_longitude=180))\nax.set_global()\n\n## Add coastlines, stock image, and gridlines\nax.coastlines()\nax.gridlines()\n\n## Read the data from the shapefile\ndata = Reader('./data/Plate_Boundaries_transform.shp')\n\n## Add the transform plate boundaries to the map\nax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(), \n                  edgecolor='orange', facecolor='none',\n                  linewidth=3)\n\n## Read the data from the shapefile\ndata = Reader('./data/Plate_Boundaries_trenches.shp')\n\n## Add the trench plate boundaries to the map\nax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(), \n                  edgecolor='darkblue', facecolor='none',\n                  linewidth=3)\n\n## Read the data from the shapefile\ndata = Reader('./data/Plate_Boundaries_ridges.shp')\n\n## Add the ridge plate boundaries to the map\nax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(), \n                  edgecolor='red', facecolor='none',\n                  linewidth=3)\n\n## Make patches to add to a legend\ntrans = mpatches.Rectangle((0, 0), 1, 1, facecolor=\"orange\")\ncon = mpatches.Rectangle((0, 0), 1, 1, facecolor=\"darkblue\")\ndiv = mpatches.Rectangle((0, 0), 1, 1, facecolor=\"red\")\nlabels = ['Transform','Convergent','Divergent']\nplt.legend([trans, con, div], labels)\n\n## Add a title\nplt.title('Map of plate boundaries (red=ridge; blue=trench; orange=transform)')\n\n## Plot the earthquake locations on the map. Adjust the color and size of the points as need to make it look nice.\nplt.scatter(\n\nplt.colorbar(shrink=0.4, label='depth (km)')\n\nplt.show()\n</pre> ## Make a figure plt.figure(figsize=(15,15))  ## Set the projection using ccrs.Robinson(central_longitude=180) ax = plt.axes(projection=ccrs.Robinson(central_longitude=180)) ax.set_global()  ## Add coastlines, stock image, and gridlines ax.coastlines() ax.gridlines()  ## Read the data from the shapefile data = Reader('./data/Plate_Boundaries_transform.shp')  ## Add the transform plate boundaries to the map ax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(),                    edgecolor='orange', facecolor='none',                   linewidth=3)  ## Read the data from the shapefile data = Reader('./data/Plate_Boundaries_trenches.shp')  ## Add the trench plate boundaries to the map ax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(),                    edgecolor='darkblue', facecolor='none',                   linewidth=3)  ## Read the data from the shapefile data = Reader('./data/Plate_Boundaries_ridges.shp')  ## Add the ridge plate boundaries to the map ax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(),                    edgecolor='red', facecolor='none',                   linewidth=3)  ## Make patches to add to a legend trans = mpatches.Rectangle((0, 0), 1, 1, facecolor=\"orange\") con = mpatches.Rectangle((0, 0), 1, 1, facecolor=\"darkblue\") div = mpatches.Rectangle((0, 0), 1, 1, facecolor=\"red\") labels = ['Transform','Convergent','Divergent'] plt.legend([trans, con, div], labels)  ## Add a title plt.title('Map of plate boundaries (red=ridge; blue=trench; orange=transform)')  ## Plot the earthquake locations on the map. Adjust the color and size of the points as need to make it look nice. plt.scatter(  plt.colorbar(shrink=0.4, label='depth (km)')  plt.show() <p>Question: Where do the majority of earthquakes occur?</p> <p>Question: What do those locations correspond with? Trenches, ridges, or transform boundaries?</p> In\u00a0[\u00a0]: Copied! <pre>## Define the earthquake location of the 2020 Alaska earthquake\nEarthquake_lat = 55.0683\nEarthquake_lon = -158.5543\n\n# Define the station location at Columbia College, Columbia, CA, USA\nstation_lat = 38.03455\nstation_lon = -120.38651\n\n## Make a figure\nplt.figure(1,(10,10))\n\n## Set the projection using ccrs.Orthographic(central_longitude=-130,central_latitude=60)\nax = plt.axes(projection=ccrs.Orthographic(central_longitude=-130,central_latitude=60))\nax.set_global()\n\n## Read the data from the shapefile\ndata = Reader('./data/Plate_Boundaries_transform.shp')\n\n## Add the transform plate boundaries to the map\nax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(), \n                  edgecolor='orange', facecolor='none',\n                  linewidth=3)\n\n## Read the data from the shapefile\ndata = Reader('./data/Plate_Boundaries_trenches.shp')\n\n## Add the trench plate boundaries to the map\nax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(), \n                  edgecolor='darkblue', facecolor='none',\n                  linewidth=3)\n\n## Read the data from the shapefile\ndata = Reader('./data/Plate_Boundaries_ridges.shp')\n\n## Add the ridge plate boundaries to the map\nax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(), \n                  edgecolor='red', facecolor='none',\n                  linewidth=3)\n\n## Plot the earthquake location on the map\nplt.scatter(Earthquake_lon,Earthquake_lat,s=100,marker='*',\n            color='red', edgecolor='black',transform=ccrs.PlateCarree())\n\n## Add a text annotation for the earthquake location\nplt.text(Earthquake_lon+5,Earthquake_lat,'Earthquake',fontsize=14,color='red',\n         transform=ccrs.PlateCarree())\n\n## Plot the seismic station location on the map\nplt.scatter(station_lon,station_lat,s=100,marker='^',\n            color='green', edgecolor='black',transform=ccrs.PlateCarree())\n\n## Add a text annotation for the seismic station location\nplt.text(station_lon+5,station_lat,'Columbia College',fontsize=12,color='green',\n         transform=ccrs.PlateCarree())\n\n## Plot the line connecting the earthquake and the seismic station\nplt.plot([Earthquake_lon,station_lon],[Earthquake_lat,station_lat],\n         color='red',transform=ccrs.Geodetic())\n\n## Add coastlines, stock image, and gridlines\nax.coastlines()\nax.stock_img()\nax.gridlines()\n\n## Add a title\nplt.title('The 2020 Alaska Earthquake and the Columbia College Seismic Station')\n\nplt.show()\n</pre> ## Define the earthquake location of the 2020 Alaska earthquake Earthquake_lat = 55.0683 Earthquake_lon = -158.5543  # Define the station location at Columbia College, Columbia, CA, USA station_lat = 38.03455 station_lon = -120.38651  ## Make a figure plt.figure(1,(10,10))  ## Set the projection using ccrs.Orthographic(central_longitude=-130,central_latitude=60) ax = plt.axes(projection=ccrs.Orthographic(central_longitude=-130,central_latitude=60)) ax.set_global()  ## Read the data from the shapefile data = Reader('./data/Plate_Boundaries_transform.shp')  ## Add the transform plate boundaries to the map ax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(),                    edgecolor='orange', facecolor='none',                   linewidth=3)  ## Read the data from the shapefile data = Reader('./data/Plate_Boundaries_trenches.shp')  ## Add the trench plate boundaries to the map ax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(),                    edgecolor='darkblue', facecolor='none',                   linewidth=3)  ## Read the data from the shapefile data = Reader('./data/Plate_Boundaries_ridges.shp')  ## Add the ridge plate boundaries to the map ax.add_geometries(data.geometries(), crs=ccrs.PlateCarree(),                    edgecolor='red', facecolor='none',                   linewidth=3)  ## Plot the earthquake location on the map plt.scatter(Earthquake_lon,Earthquake_lat,s=100,marker='*',             color='red', edgecolor='black',transform=ccrs.PlateCarree())  ## Add a text annotation for the earthquake location plt.text(Earthquake_lon+5,Earthquake_lat,'Earthquake',fontsize=14,color='red',          transform=ccrs.PlateCarree())  ## Plot the seismic station location on the map plt.scatter(station_lon,station_lat,s=100,marker='^',             color='green', edgecolor='black',transform=ccrs.PlateCarree())  ## Add a text annotation for the seismic station location plt.text(station_lon+5,station_lat,'Columbia College',fontsize=12,color='green',          transform=ccrs.PlateCarree())  ## Plot the line connecting the earthquake and the seismic station plt.plot([Earthquake_lon,station_lon],[Earthquake_lat,station_lat],          color='red',transform=ccrs.Geodetic())  ## Add coastlines, stock image, and gridlines ax.coastlines() ax.stock_img() ax.gridlines()  ## Add a title plt.title('The 2020 Alaska Earthquake and the Columbia College Seismic Station')  plt.show() <p>Question: At what type of plate boundary did this earthquake occur?</p> <p>More geologic context about this quake can be found here: https://www.iris.edu/hq/files/programs/education_and_outreach/retm/tm_200722_alaska/200722_Alaska.pdf</p> In\u00a0[\u00a0]: Copied! <pre>## define the seismic station location (latitude, longitude)\nseismic_station_location = (\n\n## define the earthquake location (latitude, longitude)\nearthquake_location = (\n\n## calculate the distance between the earthquake and the seismic station. Hint: using distance.distance(location1, location2).km\nearthquake_seismograph_distance_km = distance.distance(\n\nprint(f'The distance between the earthquake and the seismic station is {earthquake_seismograph_distance_km:.2f} km')\n</pre> ## define the seismic station location (latitude, longitude) seismic_station_location = (  ## define the earthquake location (latitude, longitude) earthquake_location = (  ## calculate the distance between the earthquake and the seismic station. Hint: using distance.distance(location1, location2).km earthquake_seismograph_distance_km = distance.distance(  print(f'The distance between the earthquake and the seismic station is {earthquake_seismograph_distance_km:.2f} km') <p>Let's load the .csv (Comma Separated Variable) data file of this seismogram as recorded at the Columbia College, Columbia, CA, USA seismic station.</p> <p>Samples were taken every 0.025 seconds (40 Hz) and the record starts 60 seconds before the arrival of the first wave which is called the P wave.</p> <p>https://www.iris.edu/app/station_monitor/#2020-07-22T06:12:44/BK-CMB/webicorder/BK-CMB%7C11273635</p> In\u00a0[\u00a0]: Copied! <pre>## Read the seismogram data. Hint: add header=9, names=['Time','Sample'] to the function\nseismogram = pd.read_csv(   # ./data/BK.CMB.00.BHZ.Q.2020-07-22T061756.019538.csv\n\n## Preview format of the seismogram data\nseismogram.head()\n</pre> ## Read the seismogram data. Hint: add header=9, names=['Time','Sample'] to the function seismogram = pd.read_csv(   # ./data/BK.CMB.00.BHZ.Q.2020-07-22T061756.019538.csv  ## Preview format of the seismogram data seismogram.head() <p>Let's take a look at the seismogram data.</p> <p>The <code>seismogram['Time']</code> column is a time series of the time of the samples. We need to convert this to a datetime object using <code>pd.to_datetime()</code>.</p> <p>The <code>seismogram['Sample']</code> column is a time series of the velocity of the ground motion at the location of the seismic station due to this earthquake.</p> In\u00a0[27]: Copied! <pre>## Extract time from seismogram dataframe and assign to time variable. \ntime = \n\n## Use pd.to_datetime() to convert the time series to a datetime object\ntime = pd.to_datetime(\n\n## Earthquake origin time from USGS: https://earthquake.usgs.gov/earthquakes/eventpage/us7000asvb/executive\nearthquake_origin_time = pd.to_datetime(\"2020-07-22T06:12.44Z\")\n\n## Calculate the travel time in minutes\ntraveltime = (time - earthquake_origin_time).dt.total_seconds() / 60\n\n## Extract velocity from seismogram dataframe and assign to velocity variable\nvelocity = \n</pre> ## Extract time from seismogram dataframe and assign to time variable.  time =   ## Use pd.to_datetime() to convert the time series to a datetime object time = pd.to_datetime(  ## Earthquake origin time from USGS: https://earthquake.usgs.gov/earthquakes/eventpage/us7000asvb/executive earthquake_origin_time = pd.to_datetime(\"2020-07-22T06:12.44Z\")  ## Calculate the travel time in minutes traveltime = (time - earthquake_origin_time).dt.total_seconds() / 60  ## Extract velocity from seismogram dataframe and assign to velocity variable velocity =   <p>Let's plot the seismogram with <code>time</code> on the x-axis and <code>velocity</code> on the y-axis.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure with a size of 10x5\nplt.figure(figsize=(10,5))\n\n## Plot the seismogram between the travel time and velocity\nplt.plot(\n\n## Add x-axis label and y-axis label\nplt.xlabel('\nplt.ylabel('\n\n## Add a title\nplt.title('Seismogram of 2020-07-22 Alaskan earthquake recorded at Columbia College, CA')\n\n## Add gridlines\nplt.grid(True)\n\nplt.show()\n</pre>  ## Make a figure with a size of 10x5 plt.figure(figsize=(10,5))  ## Plot the seismogram between the travel time and velocity plt.plot(  ## Add x-axis label and y-axis label plt.xlabel(' plt.ylabel('  ## Add a title plt.title('Seismogram of 2020-07-22 Alaskan earthquake recorded at Columbia College, CA')  ## Add gridlines plt.grid(True)  plt.show() <p>Seismographs record the arrival of multiple types of waves. P (primary) waves are compressional waves that arrive first. S (secondary/shear) waves are shear waves that arrive next as illustrated in the example seismograph below.</p> <p>Following the P and S waves are the high amplitude surface waves:</p> <p>Let's see from the P and S wave arrival times on the seismograph, if we can determine the distance to the earthquake.</p> <p>To do this, we need to pick the arrival time of the S wave. From the seismograph, read the travel time of the P and S waves.</p> <p>Assign the travel time of the P wave to a variable called <code>p_wave_travel_time</code> and the travel time of the S wave to a variable called <code>s_wave_travel_time</code>.</p> In\u00a0[29]: Copied! <pre>p_wave_travel_time =  #minutes                         \ns_wave_travel_time =  #minutes\n</pre> p_wave_travel_time =  #minutes                          s_wave_travel_time =  #minutes <p>Let's plot the seismograph with annotations for the P and S wave arrivals to see if your picks are correct.</p> <p>Using the annotations, we can indicate when the P wave arrived and when the S wave arrived on the seismograph. Take your code from above that plots the seismograph and add this code to also plot annotations:</p> <pre><code>ax.annotate('P wave', (mdates.date2num(time[psamp]), velocity[psamp]), xytext=(-10, 35), \n            textcoords='offset points', arrowprops=dict(arrowstyle='-|&gt;'))\nax.annotate('S wave', (mdates.date2num(time[ssamp]), velocity[ssamp]), xytext=(-10, 35), \n            textcoords='offset points', arrowprops=dict(arrowstyle='-|&gt;'))\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>## Make a figure with a size of 10x5\nplt.figure(figsize=(10,5))\n## Plot the seismograph\nplt.plot(\n\n## Plot the P wave annotation\nplt.annotate('P wave', (p_wave_travel_time, 0), xytext=(-10, 35), \n            textcoords='offset points', arrowprops=dict(arrowstyle='-|&gt;'))\n\n## Plot the S wave annotation\nplt.annotate('S wave', (s_wave_travel_time, 0), xytext=(-10, 35), \n            textcoords='offset points', arrowprops=dict(arrowstyle='-|&gt;'))\n\n## Add x-axis label and y-axis label\nplt.xlabel('\nplt.ylabel('\n\n## Add a title\nplt.title('Seismogram of 2020-07-22 Alaskan earthquake recorded at Columbia College, CA')\n\n## Add gridlines\nplt.grid(True)\n\nplt.show()\n</pre> ## Make a figure with a size of 10x5 plt.figure(figsize=(10,5)) ## Plot the seismograph plt.plot(  ## Plot the P wave annotation plt.annotate('P wave', (p_wave_travel_time, 0), xytext=(-10, 35),              textcoords='offset points', arrowprops=dict(arrowstyle='-|&gt;'))  ## Plot the S wave annotation plt.annotate('S wave', (s_wave_travel_time, 0), xytext=(-10, 35),              textcoords='offset points', arrowprops=dict(arrowstyle='-|&gt;'))  ## Add x-axis label and y-axis label plt.xlabel(' plt.ylabel('  ## Add a title plt.title('Seismogram of 2020-07-22 Alaskan earthquake recorded at Columbia College, CA')  ## Add gridlines plt.grid(True)  plt.show()  <p>If your picks are not correct, go back up and adjust the P and S wave arrival times until you are happy with them.</p> <p>Once you have made this plot, go back up and adjust the S wave arrival time (which is with respect to the start of the record in fractional minutes). Keep adjusting that value and moving you S wave arrival pick until it as at a spot in the record that you feel happy with. You should be looking for when the record transitions from the amplitude charecteristic of the P wave to a higher amplitude (but an amplitude that is still significantly less than that associated with the surface waves). Look at the example above for guidance.</p> <p>Keep adjusting and rerunning the code until you are happy with the S wave pick</p> <p>Once you have your s wave pick subtract the <code>p_wave_travel_time</code> from the <code>s_wave_travel_time</code> and assign that difference to a variable <code>seismogram_s_p_difference</code> in the code cell below:</p> In\u00a0[31]: Copied! <pre>## Calculate the S-P time difference using the variables p_wave_travel_time and s_wave_travel_time\nseismogram_s_p_difference = \n</pre> ## Calculate the S-P time difference using the variables p_wave_travel_time and s_wave_travel_time seismogram_s_p_difference =  <ul> <li>Estimate distance based on S-P time difference</li> </ul> <p>The difference in P and S wave arrival times can be used to determine the distance from the recording station to the earthquake using a travel time curve if we know the velocities of the waves through the Earth.  So first we need to know how these two waves behave \u2014 particularly their velocities. Check out this short video demonstration:</p> <p>https://www.iris.edu/hq/inclass/uploads/videos/A_6_seismictraveltimeirisbounc.mp4</p> <p>https://www.iris.edu/hq/inclass/animation/traveltime_curves_how_they_are_created</p> <p>Calculated travel times based on a standard earth models are in the the data folder as <code>arrival_times.csv</code>. The time unit is minutes. Let's import them as a dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>## Read the travel time table\ntravel_time_table = pd.read_csv('./data/arrival_times.csv')\n\n## Preview the travel time table\ntravel_time_table.head()\n</pre> ## Read the travel time table travel_time_table = pd.read_csv('./data/arrival_times.csv')  ## Preview the travel time table travel_time_table.head() <p>We will use the S-P time difference to estimate the distance between the earthquake and the seismograph.</p> <p>So let's create a new column in the travel time table that is the S-P time difference. We want distance in km, so we need to convert the degrees to km.</p> <p>To convert the degrees to km, we need to know the radius of the Earth. The radius of the Earth is 6371 km.</p> <p>The circumference of the Earth is $2\\pi r$ for 360 degrees, so 1 degree is $2\\pi r / 360$ km.</p> In\u00a0[33]: Copied! <pre>## Create a new column in the travel time table that is the S-P time difference\ntravel_time_table['S-P_difference'] = \n\n## Convert the degrees to km\nradius_earth = 6371\ndegrees_to_km = 2 * np.pi * radius_earth / 360\ntravel_time_table['distance_km'] = travel_time_table['degrees_from_quake'] * degrees_to_km\n</pre> ## Create a new column in the travel time table that is the S-P time difference travel_time_table['S-P_difference'] =   ## Convert the degrees to km radius_earth = 6371 degrees_to_km = 2 * np.pi * radius_earth / 360 travel_time_table['distance_km'] = travel_time_table['degrees_from_quake'] * degrees_to_km  <p>Let's plot the S-P time difference vs distance in km from the earthquake.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure()\n\n## Plot the S-P time difference vs distance\nplt.plot(\n\n## Add x-axis label and y-axis label\nplt.ylabel('\nplt.xlabel('\n\n## Add legend and grid\nplt.legend()\nplt.grid()\n\nplt.show()\n</pre> plt.figure()  ## Plot the S-P time difference vs distance plt.plot(  ## Add x-axis label and y-axis label plt.ylabel(' plt.xlabel('  ## Add legend and grid plt.legend() plt.grid()  plt.show() <p>Based on the timetable between S-P time difference and distance, we can build a linear relationship between the two.</p> <p>The model will take the S-P time difference as input and return the distance as output.</p> In\u00a0[\u00a0]: Copied! <pre>## Define the input and output variables. Hint: use the `[[]]` to extract S-P time difference as a column. And add `.values` to convert to a numpy array\nX = travel_time_table[['S-P_difference']].values\ny = travel_time_table[['distance_km']].values\n\n## Fit a linear model using LinearRegression() from sklearn\nmodel = \nmodel.fit(\n\n## Get prediction from the model\ny_pred = \n\n## Evaluate the model using the r2_score function\nR_squared = \nprint(f'The model R-squared is {R_squared:.2f}')\n\n## Plot the prediction\nplt.figure()\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_pred, color='red', label='Linear Regression')\nplt.xlabel('S-P time difference (minutes)')\nplt.ylabel('Distance (km)')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> ## Define the input and output variables. Hint: use the `[[]]` to extract S-P time difference as a column. And add `.values` to convert to a numpy array X = travel_time_table[['S-P_difference']].values y = travel_time_table[['distance_km']].values  ## Fit a linear model using LinearRegression() from sklearn model =  model.fit(  ## Get prediction from the model y_pred =   ## Evaluate the model using the r2_score function R_squared =  print(f'The model R-squared is {R_squared:.2f}')  ## Plot the prediction plt.figure() plt.scatter(X, y, label='Data') plt.plot(X, y_pred, color='red', label='Linear Regression') plt.xlabel('S-P time difference (minutes)') plt.ylabel('Distance (km)') plt.legend() plt.grid() plt.show() <p>Question: Is the linear relationship between the S-P time difference and distance fit the data well?</p> <p>Let's see if we can add higher order terms to the model to improve the fit.</p> <p>Recall the <code>PolynomialFeatures</code> function from sklearn, which we used in the linear regression lecture.</p> <p>We can use the <code>make_pipeline</code> function to add the polynomial features to the model, as we did in the logistic regression/classification lecture.</p> In\u00a0[\u00a0]: Copied! <pre>## Define the input and output variables. Hint: use the `[[]]` to extract S-P time difference as a column\nX = travel_time_table[['S-P_difference']].values\ny = travel_time_table[['distance_km']].values\n\n\n## Fit a linear model using make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\nmodel = make_pipeline(\nmodel.fit(\n\n## Get prediction from the model\ny_pred = \n\n## Evaluate the model using r2_score\nR_squared = \nprint(f'The model R-squared is {R_squared:.2f}')\n\n## Plot the prediction\nplt.figure()\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_pred, color='red', label='Polynomial Regression')\nplt.xlabel('S-P time difference (minutes)')\nplt.ylabel('Distance (km)')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> ## Define the input and output variables. Hint: use the `[[]]` to extract S-P time difference as a column X = travel_time_table[['S-P_difference']].values y = travel_time_table[['distance_km']].values   ## Fit a linear model using make_pipeline(PolynomialFeatures(degree=2), LinearRegression()) model = make_pipeline( model.fit(  ## Get prediction from the model y_pred =   ## Evaluate the model using r2_score R_squared =  print(f'The model R-squared is {R_squared:.2f}')  ## Plot the prediction plt.figure() plt.scatter(X, y, label='Data') plt.plot(X, y_pred, color='red', label='Polynomial Regression') plt.xlabel('S-P time difference (minutes)') plt.ylabel('Distance (km)') plt.legend() plt.grid() plt.show() <p>It seems that the polynomial model fits the data better than the linear model, resulting in a higher R-squared value.</p> <p>Now we can use the polynomial model to estimate the distance between the earthquake and the seismograph based on the S-P time difference you picked.</p> In\u00a0[\u00a0]: Copied! <pre>## Use the model to predict the distance. Hint: use the `[[ ]]` to pass in the S-P time difference as a vertical array\ndistance_km = model.predict(\n\n## Print the estimated distance\nprint(f'The estimated distance is {distance_km[0][0]:.2f} km')\n</pre> ## Use the model to predict the distance. Hint: use the `[[ ]]` to pass in the S-P time difference as a vertical array distance_km = model.predict(  ## Print the estimated distance print(f'The estimated distance is {distance_km[0][0]:.2f} km')  <p>From reading the seismograph, you can successfully pick the P and S wave arrival times and use them to estimate the distance to the earthquake.</p> <p>How does the estimate of distance compare to the true earthquake epicenter distance that you calculated using the <code>geopy</code> <code>distance.distance()</code>?</p> <p>If the errors are within 15%, you did a good job picking the P and S wave arrival times. If they are very different you may want to reconsider your P and S wave arrival times and recompute.</p> <p>In the above example, we only used one seismograph station to estimate the distance to the earthquake. In practice, we use data from many stations to estimate the earthquake location.</p> <p>The earthquake locations that we used in the first part of the assignment are determined thorugh computing the distance from the earthquake for at least three stations as illustrated below.</p> <p>With three stations, we can form a triangle and determine the location of the earthquake by finding the point that is equidistant from all three stations.</p> <p>Let's look at earthquakes that occurred close to Berkeley.</p> <p>In particular, on October 17 at 5:15pm PDT (October 18 1989 at 04:15am UTC) the M6.9 Loma Prieta earthquake occurred in the Santa Cruz mountains approximately 80 km southwest of the Berkeley Campus.</p> <p>We will use the earthquake catalog in the Bay Area and aftershocks of the Loma Prieta earthquake to examine the earthquake statistics.</p> <p>https://en.wikipedia.org/wiki/1989_Loma_Prieta_earthquake</p> <ul> <li>Load the Earthquake Catalog</li> </ul> <p>Load the .csv data file of all the earthquakes from January 01,1989 to December 31, 1995 in the ANSS (Advanced National Seismic System) catalog from between latitudes 36.0-38.5\u00b0 and longitude -123.0 to -121.0\u00b0 (http://ncedc.org/anss/catalog-search.html).</p> In\u00a0[\u00a0]: Copied! <pre>## Load the Earthquake Catalog\ncatalog = pd.read_csv(    #data/bay_area_anss_1989_1995.csv'\n\n## Convert the DateTime column to a datetime object\ncatalog['DateTime'] = pd.to_datetime(\n\n## Preview the catalog\ncatalog.head()\n</pre> ## Load the Earthquake Catalog catalog = pd.read_csv(    #data/bay_area_anss_1989_1995.csv'  ## Convert the DateTime column to a datetime object catalog['DateTime'] = pd.to_datetime(  ## Preview the catalog catalog.head() <p>As usual, we will start by plotting the earthquake catalog on a map.</p> In\u00a0[\u00a0]: Copied! <pre>## Coordinates for UC Berkeley\nBerk_lat = 37.8716\nBerk_lon = -122.2727\n\n## Coordinates for the map\nlat0=36.0\nlat1=38.5\nlon0=-123.0\nlon1=-121.0\n\n## Make a map using cartopy\nplt.figure(figsize=(10, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n\n## Plot the entire catalog\nplt.scatter(\n\n## Add coastlines\nax.coastlines(resolution='10m',linewidth=1)\n\n## Add Berkeley Campus. Remember to add `label='Berkeley Campus'` to the scatter plot.\nplt.scatter(\n\n## Add x and y labels\nax.set_xlabel('\nax.set_ylabel('\n\n## Add legend\nplt.legend()\n\nplt.show()\n</pre> ## Coordinates for UC Berkeley Berk_lat = 37.8716 Berk_lon = -122.2727  ## Coordinates for the map lat0=36.0 lat1=38.5 lon0=-123.0 lon1=-121.0  ## Make a map using cartopy plt.figure(figsize=(10, 10)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())  ## Plot the entire catalog plt.scatter(  ## Add coastlines ax.coastlines(resolution='10m',linewidth=1)  ## Add Berkeley Campus. Remember to add `label='Berkeley Campus'` to the scatter plot. plt.scatter(  ## Add x and y labels ax.set_xlabel(' ax.set_ylabel('  ## Add legend plt.legend()  plt.show() <ul> <li>Earthquake frequency vs magnitude</li> </ul> <p>Let's first look at the earthquake frequency vs magnitude for the entire catalog.</p> <p>Based on the Gutenberg-Richter Law, we expect a linear relationship between the logarithm of the number of earthquakes and the magnitude.</p> <p>$$ log_{10}N(M) = A - b \\cdot M $$</p> <p>You can use <code>plt.hist()</code> to plot the histogram of <code>LP_catalog['Magnitude']</code>. Use <code>log=True</code> to plot the histogram on a log scale.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure(figsize=(10, 6))\n\n## Plot the histogram of earthquake frequency vs magnitude. \nplt.hist(\n\n## Set the y-axis in log scale. Hint: use plt.yscale(\nplt.\n\n## Add x-axis label and y-axis label\nplt.xlabel('\nplt.ylabel('\n\nplt.show()\n</pre> ## Make a figure plt.figure(figsize=(10, 6))  ## Plot the histogram of earthquake frequency vs magnitude.  plt.hist(  ## Set the y-axis in log scale. Hint: use plt.yscale( plt.  ## Add x-axis label and y-axis label plt.xlabel(' plt.ylabel('  plt.show() <p>Question: Does the histogram follow a linear relationship? Can you approximate the slope and intercept of the linear relationship?</p> <p>Let's have a more accurate estimate of the slope and intercept of the linear relationship.</p> <p>Recall the linear regression model from the lecture, let's use <code>LinearRegression()</code> to fit the data.</p> In\u00a0[\u00a0]: Copied! <pre>## We can use the np.histogram function to get the frequency of each magnitude bin. Hint: define magnitude_bins between M0 and M6 with a bin size of M0.1\nmagnitude_bins = np.arange(0, 6, 0.1)\nfrequency, _ = np.histogram(catalog['Magnitude'], bins=magnitude_bins)\n\n## Redefine the magnitude_bins to be the bin centers, because the histogram function is using the bin edges.\nmagnitude_bins = (magnitude_bins[:-1] + magnitude_bins[1:]) / 2\n\n## Define the input and output variables. Hint: use `np.log10(frequency)` to take the logarithm of the frequency\nX = \ny = \n\n## filter out the inf values\nX = X[~np.isinf(y)]\ny = y[~np.isinf(y)]\n\n## Reshape the input and output variables to be vertical arrays. Hint: use the `reshape(-1, 1)` method\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n## Fit a linear model using LinearRegression() from sklearn\nmodel = \nmodel.fit(\n\n## Get prediction from the model\ny_pred = model.predict(\n\n## Evaluate the model using r2_score\nR_squared = \nprint(f'The model R-squared is {R_squared:.2f}')\n\n## Plot the prediction\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_pred, color='red', label='Linear Regression')\nplt.xlabel('Magnitude')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> ## We can use the np.histogram function to get the frequency of each magnitude bin. Hint: define magnitude_bins between M0 and M6 with a bin size of M0.1 magnitude_bins = np.arange(0, 6, 0.1) frequency, _ = np.histogram(catalog['Magnitude'], bins=magnitude_bins)  ## Redefine the magnitude_bins to be the bin centers, because the histogram function is using the bin edges. magnitude_bins = (magnitude_bins[:-1] + magnitude_bins[1:]) / 2  ## Define the input and output variables. Hint: use `np.log10(frequency)` to take the logarithm of the frequency X =  y =   ## filter out the inf values X = X[~np.isinf(y)] y = y[~np.isinf(y)]  ## Reshape the input and output variables to be vertical arrays. Hint: use the `reshape(-1, 1)` method X = X.reshape(-1, 1) y = y.reshape(-1, 1)  ## Fit a linear model using LinearRegression() from sklearn model =  model.fit(  ## Get prediction from the model y_pred = model.predict(  ## Evaluate the model using r2_score R_squared =  print(f'The model R-squared is {R_squared:.2f}')  ## Plot the prediction plt.figure(figsize=(10, 6)) plt.scatter(X, y, label='Data') plt.plot(X, y_pred, color='red', label='Linear Regression') plt.xlabel('Magnitude') plt.ylabel('Frequency') plt.legend() plt.grid() plt.show() <p>Question: Does the linear model fit the data well? Pay attention to the small magnitude earthquakes.</p> <p>The reason the model does not fit well at the small magnitude earthquakes (M &lt; 1) is because the earthquake catalog is not complete for small magnitude earthquakes meaning that there are many small magnitude earthquakes that are not recorded in the catalog because of many reasons such as the earthquakes are too far away from the seismic stations, the earthquakes are too weak to be recorded by the seismographs, and the background noise level is too high for automatic algorithms to identify the earthquakes.</p> <p>To fix the fit, let's only fit the data for M &gt;= 1.</p> In\u00a0[\u00a0]: Copied! <pre>## We can use the np.histogram function to get the frequency of each magnitude bin. Hint: use magnitude_bins between M0 and M6 with a bin size of M0.1\nmagnitude_bins = np.arange(0, 6, 0.1)\nfrequency, _ = np.histogram(catalog['Magnitude'], bins=magnitude_bins)\n\n## Redefine the magnitude_bins to be the bin centers, because the histogram function returns the bin edges.\nmagnitude_bins = (magnitude_bins[:-1] + magnitude_bins[1:]) / 2 \n\n## Define the input and output variables. Hint: use `np.log10(frequency)` to take the logarithm of the frequency\nX = \ny = \n\n## Filter the input and output variables to only include the data for M &gt;= 1\ny = y[\nX = X[\n\n## filter out the inf values\nX = X[~np.isinf(y)]\ny = y[~np.isinf(y)]\n\n## Reshape the input and output variables to be vertical arrays. Hint: use `reshape(-1, 1)`\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n\n## Fit a linear model using LinearRegression() from sklearn\nmodel = \nmodel.fit(\n\n## Get prediction from the model\ny_pred = model.predict(\n\n## Evaluate the model using r2_score\nR_squared = \nprint(f'The model R-squared is {R_squared:.2f}')\n\n## Plot the prediction\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_pred, color='red', label='Linear Regression')\nplt.xlabel('Magnitude')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> ## We can use the np.histogram function to get the frequency of each magnitude bin. Hint: use magnitude_bins between M0 and M6 with a bin size of M0.1 magnitude_bins = np.arange(0, 6, 0.1) frequency, _ = np.histogram(catalog['Magnitude'], bins=magnitude_bins)  ## Redefine the magnitude_bins to be the bin centers, because the histogram function returns the bin edges. magnitude_bins = (magnitude_bins[:-1] + magnitude_bins[1:]) / 2   ## Define the input and output variables. Hint: use `np.log10(frequency)` to take the logarithm of the frequency X =  y =   ## Filter the input and output variables to only include the data for M &gt;= 1 y = y[ X = X[  ## filter out the inf values X = X[~np.isinf(y)] y = y[~np.isinf(y)]  ## Reshape the input and output variables to be vertical arrays. Hint: use `reshape(-1, 1)` X = X.reshape(-1, 1) y = y.reshape(-1, 1)   ## Fit a linear model using LinearRegression() from sklearn model =  model.fit(  ## Get prediction from the model y_pred = model.predict(  ## Evaluate the model using r2_score R_squared =  print(f'The model R-squared is {R_squared:.2f}')  ## Plot the prediction plt.figure(figsize=(10, 6)) plt.scatter(X, y, label='Data') plt.plot(X, y_pred, color='red', label='Linear Regression') plt.xlabel('Magnitude') plt.ylabel('Frequency') plt.legend() plt.grid() plt.show() <p>We can now extract the slope and intercept of the linear model.</p> In\u00a0[\u00a0]: Copied! <pre>## Get the slope and intercept\nslope = \nintercept = \n\nprint(f'The slope is {slope:.2f} and the intercept is {intercept:.2f}')\n</pre> ## Get the slope and intercept slope =  intercept =   print(f'The slope is {slope:.2f} and the intercept is {intercept:.2f}') <p>Question: How much does the fit improve?</p> <p>Question: What is the slope and intercept of the linear model? Can you estimate the A-value and b-value of the Gutenberg-Richter Law?</p> <p>$$ log_{10}N(M) = A - b \\cdot M $$</p> <p>Question: What does the b-value tell us about the earthquake catalog? If we have one earthquake of magnitude 6, how many earthquakes of magnitude 5 would we expect to have in the catalog? How about magnitude 4, 3, 2, and 1?</p> <p>Let's now look at the aftershocks of the Loma Prieta earthquake.</p> <p>First, we need to extract the period of the Loma Prieta earthquake.</p> In\u00a0[48]: Copied! <pre>## Define the earthquake origin time, latitude, and longitude (https://earthquake.usgs.gov/earthquakes/eventpage/nc216859/executive)\nearthquake_origin_time = pd.to_datetime('1989-10-18 00:04:15')\nearthquake_latitude = 37.036\nearthquake_longitude = -121.885\n\n## Define a new column of days since the earthquake origin time. \ncatalog['days_since_earthquake'] = (catalog['DateTime'] - earthquake_origin_time).dt.days\n\n## Extract the earthquakes within 0.1 degree from the earthquake origin and within 90 days from the earthquake origin time\nLP_catalog = catalog[\n    (catalog['days_since_earthquake'] &gt;= ) &amp; \n    (catalog['days_since_earthquake'] &lt;= ) &amp; \n    (catalog['Latitude'] &gt;= ) &amp; \n    (catalog['Latitude'] &lt;= ) &amp; \n    (catalog['Longitude'] &gt;= ) &amp; \n    (catalog['Longitude'] &lt;= )\n    ]\n</pre> ## Define the earthquake origin time, latitude, and longitude (https://earthquake.usgs.gov/earthquakes/eventpage/nc216859/executive) earthquake_origin_time = pd.to_datetime('1989-10-18 00:04:15') earthquake_latitude = 37.036 earthquake_longitude = -121.885  ## Define a new column of days since the earthquake origin time.  catalog['days_since_earthquake'] = (catalog['DateTime'] - earthquake_origin_time).dt.days  ## Extract the earthquakes within 0.1 degree from the earthquake origin and within 90 days from the earthquake origin time LP_catalog = catalog[     (catalog['days_since_earthquake'] &gt;= ) &amp;      (catalog['days_since_earthquake'] &lt;= ) &amp;      (catalog['Latitude'] &gt;= ) &amp;      (catalog['Latitude'] &lt;= ) &amp;      (catalog['Longitude'] &gt;= ) &amp;      (catalog['Longitude'] &lt;= )     ]  <ul> <li>Plot magnitude vs. time for the aftershock events</li> </ul> <p>You can try to use <code>s=LP_catalog['Magnitude']</code> to scale the marker size by magnitude, or <code>c=LP_catalog['Magnitude']</code> to color the marker by magnitude, cmap='plasma' seems to be a nice color map.</p> In\u00a0[\u00a0]: Copied! <pre>## Make the figure size wide using `figsize=(20, 6)`\nplt.figure(figsize=(20, 6))\n\n## Plot the Loma Prieta aftershock catalog between 'DateTime' and 'Magnitude'. Hint: adjust the color and size of the markers to make the plot more informative\nplt.scatter(\n\n## Add a color bar\nplt.colorbar(label='Magnitude')\n\n## Add x-axis label and y-axis label\nplt.xlabel('\nplt.ylabel('\n\nplt.show()\n</pre> ## Make the figure size wide using `figsize=(20, 6)` plt.figure(figsize=(20, 6))  ## Plot the Loma Prieta aftershock catalog between 'DateTime' and 'Magnitude'. Hint: adjust the color and size of the markers to make the plot more informative plt.scatter(  ## Add a color bar plt.colorbar(label='Magnitude')  ## Add x-axis label and y-axis label plt.xlabel(' plt.ylabel('  plt.show() <p>Question: How would you describe the distribution of large earthquake magnitudes with time?</p> <ul> <li>Plot earthquake frequency vs time</li> </ul> <p>You can use <code>plt.hist()</code> to plot the histogram of <code>days_since_earthquake</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## Make a figure\nplt.figure(figsize=(10, 6))\n\n## Define bins from 0 to 90 days with a bin size of 5 days.\ndays_bins = np.arange(0, 91, 5)\n\n## Plot the histogram of earthquake frequency vs time. Hint: add bins=days_bins\nplt.hist(\n\n## Add x-axis label and y-axis label\nplt.xlabel('\nplt.ylabel('\n\nplt.show()\n</pre> ## Make a figure plt.figure(figsize=(10, 6))  ## Define bins from 0 to 90 days with a bin size of 5 days. days_bins = np.arange(0, 91, 5)  ## Plot the histogram of earthquake frequency vs time. Hint: add bins=days_bins plt.hist(  ## Add x-axis label and y-axis label plt.xlabel(' plt.ylabel('  plt.show() <p>Question: Question: How would you describe the distribution of number of aftershocks with time after the main quake? What is the peak time of aftershock production? How does the aftershock rate decay with time?</p> <p>Let's see if we can do some quantitative analysis of the aftershock rate decay with time.</p> <p>The challenge is that the Omori's law is not a linear relationship, but a power law relationship.</p> <p>$$ N(t) = \\frac{k}{(c + t)^p} $$</p> <p>Let's first convert the equation to a linear form.</p> <p>$$ log_{10}N(t) = log_{10}k - p \\cdot log_{10}(c + t) $$</p> <p>For simplicity let's assume <code>c=0</code> and fit the data to the linear model.</p> <p>$$ log_{10}N(t) = log_{10}k - p \\cdot log_{10}(t) $$</p> <p>So the y value is <code>log10(N(t))</code> and the x value is <code>log10(t)</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## Calcualte frequency of earthquakes using np.histogram. Using using days_bins between 0 and 90 days for every 5 days.\ndays_bins = np.arange(0, 90, 5)\nfrequency, bins = np.histogram(LP_catalog['days_since_earthquake'], bins = days_bins)\n\n## Calculate the days on bin centers becuase the \"bins\" variable is defined as the bin edges\ndays_bins = (bins[:-1] + bins[1:]) / 2\n\n## Define the input and output variables. Hint: use `np.log10(frequency)` to take the logarithm of the frequency and `np.log10(days_bins)` to take the logarithm of the days_bins\nX = \ny = \n\n## filter out the inf values\nX = X[~np.isinf(y)]\ny = y[~np.isinf(y)]\n\n## Reshape the input and output variables to be vertical arrays\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n## Fit a linear model using LinearRegression() from sklearn\nmodel = \nmodel.fit(\n\n## Get prediction from the model\ny_pred = model.predict(X)\n\n## Evaluate the model using r2_score\nR_squared = \n\nprint(f'The model R-squared is {R_squared:.2f}')\n\n## Plot the prediction\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_pred, color='red', label='Linear Regression')\nplt.xlabel('log10(Days after the earthquake)')\nplt.ylabel('log10(Frequency)')\nplt.legend()\nplt.grid()\n\nplt.show()\n</pre> ## Calcualte frequency of earthquakes using np.histogram. Using using days_bins between 0 and 90 days for every 5 days. days_bins = np.arange(0, 90, 5) frequency, bins = np.histogram(LP_catalog['days_since_earthquake'], bins = days_bins)  ## Calculate the days on bin centers becuase the \"bins\" variable is defined as the bin edges days_bins = (bins[:-1] + bins[1:]) / 2  ## Define the input and output variables. Hint: use `np.log10(frequency)` to take the logarithm of the frequency and `np.log10(days_bins)` to take the logarithm of the days_bins X =  y =   ## filter out the inf values X = X[~np.isinf(y)] y = y[~np.isinf(y)]  ## Reshape the input and output variables to be vertical arrays X = X.reshape(-1, 1) y = y.reshape(-1, 1)  ## Fit a linear model using LinearRegression() from sklearn model =  model.fit(  ## Get prediction from the model y_pred = model.predict(X)  ## Evaluate the model using r2_score R_squared =   print(f'The model R-squared is {R_squared:.2f}')  ## Plot the prediction plt.figure(figsize=(10, 6)) plt.scatter(X, y, label='Data') plt.plot(X, y_pred, color='red', label='Linear Regression') plt.xlabel('log10(Days after the earthquake)') plt.ylabel('log10(Frequency)') plt.legend() plt.grid()  plt.show() <p>We can extract the slope and intercept of the linear model.</p> In\u00a0[\u00a0]: Copied! <pre>## Get the slope and intercept\nslope = \nintercept = \n\nprint(f'The slope is {slope:.2f} and the intercept is {intercept:.2f}')\n</pre> ## Get the slope and intercept slope =  intercept =   print(f'The slope is {slope:.2f} and the intercept is {intercept:.2f}') <p>Question: How is the fit of the linear model to the aftershock data? Can you estimate the p-value of the Omori's Law?</p> <p>Question: What does the decay rate $p$ tell us about the rate of aftershock production over time? If we have 1000 aftershocks in the first 5 days, how many aftershocks every 5 day peroid would we expect after 100 days?</p> <ul> <li>Earthquake recurrence in the San Francisco Bay Area</li> </ul> <p>Both the Gutenberg-Richter Law and Omori's Law are very important for understanding the earthquake recurrence in a region.</p> <p>Recall the lecture about probability of earthquake occurrence in 08 probability.</p> <p>Based on the 120 year catalog, we can estimate the average recurrence interval for magnitude 5.0, 6.0, 7.0 and 8.0 events in the San Francisco Bay Area.</p> <p>The bay area is located on the San Andreas Fault, which is the boundary between the Pacific and North American tectonic plates.</p> <p>You can read more information about earthquake hazard of the San Andreas Fault from the USGS website:</p> <p>https://www.usgs.gov/natural-hazards/earthquake-hazards/science/back-future-san-andreas-fault?qt-science_center_objects=0#qt-science_center_objects</p> <p>Although we can not predict when and where the next big earthquake will happen, understanding the earthquake recurrence can help us prepare for the earthquake and mitigate the earthquake risk.</p> <p>Earthquake forecasting and earthquake risk assessment is an important topic in the field of earthquake engineering.</p> <p>You can read more about earthquake forecasting and risk assessment from our BLS website:</p> <p>https://seismo.berkeley.edu/research/eew_basics.html</p> <p>Map of Bay Area faults. Source: https://pubs.er.usgs.gov/publication/fs20163020</p> <p>Question: Why is the San Andreas Fault considered a major hazard for the San Francisco Bay Area? Discuss its role as a tectonic plate boundary.</p> <p>Question: Examine the map of Bay Area faults. Identify at least two other faults besides the San Andreas Fault and discuss their potential contributions to seismic hazards in the region.</p> <p>Question: Refer to the USGS website and summarize key findings about the earthquake hazard associated with the San Andreas Fault. How can these findings be used in urban planning and infrastructure development?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#homework-4-earthquakes-and-plate-tectonics","title":"Homework 4: Earthquakes and Plate Tectonics\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#import-necessary-libraries","title":"Import necessary libraries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#1-plate-tectonics","title":"1. Plate Tectonics\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#11-topography","title":"1.1 Topography\u00b6","text":"<p>Let us use the <code>Orthographic</code> projection (A GLOBE! in the XKCD map projection cartoon) to look at the large underwater mountain range of the Mid-Atlantic ridge.</p> <p>The <code>plt.contourf</code> function is slow such that we will need to have some patience when running the plotting code.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#12-seafloor-age","title":"1.2 Seafloor age\u00b6","text":"<p>Plot seafloor age looking at North Atlantic</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#2-earthquake-catalog","title":"2. Earthquake Catalog\u00b6","text":"<p>Based on the seafloor age data, we know that the tectonic plates are moving away from the ridges and towards the trenches.</p> <p>The movement of the plates will compress and stretch the plates. The accumulated stress will eventually be too much and the plates will break, causing an earthquake.</p> <p>Large earthquakes are often associated with plate boundaries. Let's continue to explore the earthquake data to see if we can find any patterns.</p> <ul> <li>2022's tragic earthquake in Turkey</li> </ul> <p>https://earthquake.usgs.gov/earthquakes/eventpage/us6000jllz/executive</p> <p>https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000jllz&amp;extent=-26.11599,-43.59375&amp;extent=61.39672,100.89844&amp;sort=largest&amp;listOnlyShown=true</p>  &gt; Source: Hasterok et al., 2022"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#21-earthquake-magnitude-distribution","title":"2.1 Earthquake magnitude distribution\u00b6","text":"<p>How often do large earthquakes occur? To start addressing this question, let's plot a histogram of earthquake magnitudes.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#22-earthquake-depth","title":"2.2 Earthquake depth\u00b6","text":"<p>Let's see the range and frequency of depths where earthquakes occur.</p> <p>Make a histogram of earthquake depth. Remember to set <code>log=True</code> within the <code>plt.hist</code> function.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#23-relationship-between-earthquake-location-and-plate-boundaries","title":"2.3 Relationship between earthquake location and plate boundaries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#3-analyze-an-earthquake-seismogram","title":"3. Analyze an Earthquake Seismogram\u00b6","text":"<p>An interesting earthquake in 2020 occured 100km SSE of Perryville, Alaska at 55.0683\u00b0N 158.5543\u00b0W and was a magnitude 7.8 event.</p> <p>You can find more information about this earthquake on the USGS website: https://earthquake.usgs.gov/earthquakes/eventpage/us7000asvb/executive</p> <p>Below is a map of the earthquake location and the location of the Columbia College, Columbia, CA, USA seismic station that recorded a seismograph we will be analyzing.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#31-distance-between-seismograph-and-earthquake","title":"3.1 Distance between seismograph and earthquake\u00b6","text":"<p>We can use the Geopy Python package to calculate the distance between the earthquake and the seismic station.</p> <p>The module <code>distance</code> of the <code>geopy</code> library was  imported at the beginning of this notebook.</p> <p>You can use <code>distance.distance(location1, location2)</code> function where each location is (latitude,longitude).</p> <p>You can read more about this function here: https://geopy.readthedocs.io/en/stable/index.html?highlight=distance#module-geopy.distance</p> <p>In the code cell below, define the locations, use the <code>distance.distance()</code> function to calculate the distance, and then assign the value (in km) to a variable called <code>earthquake_seismograph_distance_km</code>.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#32-load-a-seismogram-of-this-earthquake","title":"3.2 Load a Seismogram of this Earthquake\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/homework04/#4-earthquake-occurrence-statistics-aftershocks","title":"4. Earthquake Occurrence Statistics (Aftershocks)\u00b6","text":"<p>In this section, we will further explore the earthquake catalog data and compute the earthquake occurrence statistics: Omori's Law and Gutenberg-Richter Law.</p> <p>These information will be useful for understanding the earthquake hazard and making earthquake forecasts.</p> <p>There are two primary statistics that describe earthquake catalog data.</p> <p>The first is Gutenberg-Richter, which is used to characterize the rates of primary earthquakes over some period of time to determine rates of occurence, which can then be used to assess hazard and make earthquake forecasts.</p> <p>$$ log_{10}N(M) = A - b \\cdot M $$</p> <p>where $N(M)$ is the number of earthquakes, $M$ is the earthquake magnitude.</p> <p>The second is Omori's Law which describes the distribution of aftershocks following a primary earthquake or mainshock. It can be used to estimate the rate of aftershock production over time after the earthquake.</p> <p>$$ N(t) = \\frac{k}{(c + t)^p} $$</p> <p>where $N(t)$ is the number of aftershocks after $t$ days, $k$ is a constant, $c$ is the time at which the aftershock production rate peaks, and $p$ is the decay rate.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/playground/","title":"Playground","text":"In\u00a0[1]: Copied! <pre>print(\"Hello, Python!\")\n</pre> print(\"Hello, Python!\") <pre>Hello, Python!\n</pre> In\u00a0[3]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.metrics import confusion_matrix import seaborn as sns"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/playground/#welcome-to-the-python-playground","title":"Welcome to the Python Playground!\u00b6","text":"<p>This Jupyter Notebook is your playground for practicing Python exercises in class.</p> <p>Feel free to experiment, test your code, and learn by doing</p> <p>Here are a few tips to get you started</p> <ol> <li>Code Cells: Each cell in this notebook is a code cell. You can write and execute Python code in these cells</li> <li>Execution: To execute a code cell, you can press Shift + Enter or click the Run button in the toolbar above</li> <li>Order of Execution: Cells are executed in the order they appear in the notebook. Make sure to run cells sequentially to avoid errors</li> <li>Variables and Functions: You can define variables and functions in one cell and use them in other cells</li> <li>Documentation: Use comments (#) to add explanations and instructions to your code. This will help you and others understand your code</li> <li>Experiment: Don't be afraid to try different things and experiment with the code. That's how you learn</li> </ol> <p>Now, let's get started with some Python exercises. Have fun exploring and learning Happy coding!</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/exercises/project/","title":"Summary of final project","text":"<p>The final project for the class is to use the tools we have developed regarding data analysis, data visualization and inferential thinking to gain insight from an Earth science data set. The data you choose to investigate is open and the question you choose to ask is open. The goal of your approach should be to analyze data in a way that gives insight into a phenomenon associated with Earth science.</p> <p>Requirements for the project that should all be contained within a Jupyter notebook:</p> <ul> <li>Pose a question and have an introductory markdown text cell that describes the problem, describes the ways you are going to approach the problem and describes the data you are going to use to do so. This text should include references.</li> <li>Use at least three different types of data visualizations plots that are ones we used throughout the course.</li> <li>Use at two different data analysis methods/approaches that you utilized in the course.</li> <li>Conclude with a substantive markdown text cell that contains at least of page of text and describes the results of your analysis.</li> <li>Have a final acknowledgements section and a works cited section. </li> </ul> <p>As your submission, upload a zipped directory that contains:</p> <ul> <li>a polished Jupyter notebook that contains your data analysis, visualizations and explanatory text.</li> <li>the underlying data sets you used in your analysis</li> <li>exported PDF of the Jupyter notebook</li> <li>each person is required to upload one submission. If it is a collaborative project, please note that and inclue names of the group mebers, limted to a maximum of three members.</li> </ul> <p>Choice of problems and data</p> <p>The choice of problems and data that you apply are open --- you get to choose your own adventure. However, it is absolutely essential that you cite the source of the data and discuss/cite any resource, place, or person that you got ideas from. It is perfectly reasonable to get ideas and inspiration from other sources. When you do, you need to be fully transparent about where such ideas came from with acknowledgements and citations as appropriate.</p> <p>Some places to go for Earth Science data:</p> <ul> <li> <p>UNAVCO data that geodetic scientists use for quantifying the motions of rock, ice and water that are monitored by a variety of sensor types at or near the Earth's surface https://www.unavco.org/data/data.html</p> </li> <li> <p>https://www.unavco.org/instrumentation/networks/networks.html Data by network/station</p> </li> <li> <p>https://www.unavco.org/data/gps-gnss/gps-gnss.html GPS data</p> </li> <li> <p>National Center for Environmental Information: https://www.ncdc.noaa.gov/data-access</p> </li> <li> <p>including:</p> <ul> <li>Climate Data Records</li> <li>Climate Monitoring</li> <li>Coastal Indicators</li> <li>Geomagnetism</li> <li>Gulf of Mexico</li> <li>Marine Biology</li> <li>Marine Geology and Geophysics</li> <li>Natural Hazards</li> <li>Ocean Acoustics</li> <li>Ocean Chemistry</li> <li>Ocean Climate Laboratory</li> <li>Ocean Exploration</li> <li>Ocean Physics</li> <li>Paleoclimatology</li> <li>Radar Meteorology</li> <li>Regional Ocean Climatologies</li> <li>Satellite Meteorology</li> <li>Satellite Oceanography</li> <li>Severe Weather</li> <li>Space Weather</li> <li>Surface Weather Observations</li> <li>Upper Air Observations</li> <li>Weather and Climate Models</li> </ul> </li> <li> <p>Earthquake catalog data: https://earthquake.usgs.gov/earthquakes/search/</p> </li> <li> <p>Rock geochemical data: https://www.earthchem.org</p> </li> <li> <p>NASA Earth Data: https://search.earthdata.nasa.gov/search</p> </li> <li> <p>Climate-related datasets from the International Research Institute for Climate and Society at Columbia University IRI/LDEO Climate Data Library</p> </li> <li> <p>United States stream flow data (US Geological Survey): https://waterdata.usgs.gov/nwis/rt</p> </li> <li> <p>Other US Geological Survey data: https://data.usgs.gov/datacatalog</p> </li> <li> <p>https://www.dataone.org</p> </li> <li> <p>http://www.marine-geo.org/index.php</p> </li> <li> <p>National Oceanographic Data Center https://www.nodc.noaa.gov</p> </li> <li> <p>European Space Agency open data portal: https://climate.esa.int/en/odp/#/dashboard</p> </li> <li> <p>Copernicus Climate Change Service (C3S) Climate Data Store: https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset</p> </li> </ul> <p>You can also find data sets in individual papers or other databases. You can search Google Scholar for a topic you are interested in and download the associated data to deal with and analyze.</p> <p>Specific data sets we have used so far:</p> <ul> <li>global elevation data</li> <li>a comprehensive seismic catalog</li> <li>Mauna Loa CO2</li> <li>igneous geochemistry data (additional data can be downloaded from EarthChem.org)</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/","title":"PyEarth: A Python Introduction to Earth Science","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#lecture-1-introduction-python-101","title":"Lecture 1: Introduction &amp;&amp; Python 101","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#course-overview","title":"Course Overview","text":"<ul> <li>Focus on Python programming for Earth Science applications</li> <li>Topics include:</li> <li>Python basics</li> <li>Data analysis with NumPy and Pandas</li> <li>Data visualization with Matplotlib and Cartopy</li> <li>Machine learning with Scikit-learn</li> <li>Deep learning with PyTorch</li> <li>Project-based learning</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#grading","title":"Grading","text":"<ul> <li>Attendance: 10%</li> <li>In-class and homework exercises: 60%</li> <li>Project presentation: 10%</li> <li>Project report: 20%</li> </ul> <p>Bonus points:</p> <ul> <li>Assisting classmates: 10%</li> <li>Contributing course materials: 10%</li> <li>Achieving success in a Kaggle competition (top 10% rank): 10%</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#class-tools","title":"Class Tools","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#github","title":"GitHub","text":"<ul> <li>Version control system</li> <li>Collaboration platform</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#codespaces","title":"Codespaces","text":"<ul> <li>Cloud-based development environment</li> <li>Pre-configured with necessary tools and libraries</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#copilot","title":"Copilot","text":"<ul> <li>AI-powered coding assistant</li> <li>Helps with code completion and generation</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#how-to-use-copilot","title":"How to Use Copilot","text":"<ol> <li>Install Copilot extension</li> <li>Authenticate with your GitHub account</li> <li>Start coding and watch for suggestions</li> <li>Accept suggestions with Tab or continue typing</li> </ol> <p>Tips: - Write clear comments to guide Copilot - Review and understand the suggested code</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#using-chatbots-for-programming-and-earth-science-questions","title":"Using Chatbots for Programming and Earth Science Questions","text":"<ul> <li>AI-powered language models, such as GPT, Claude, Gemini, Grok, LLama, etc.</li> <li>Can assist with:</li> <li>Explaining concepts</li> <li>Debugging code</li> <li>Answering Earth Science questions</li> <li>Providing coding examples</li> </ul> <p>Tips: - Be specific in your questions - Verify information with reliable sources - Use as a learning aid, not a substitute for understanding</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#4-introduction-to-python","title":"4. Introduction to Python","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#what-is-python","title":"What is Python?","text":"<ul> <li>High-level, interpreted programming language</li> <li>Known for its simplicity and readability</li> <li>Widely used in scientific computing and data analysis</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#python-basics-variables-and-data-types","title":"Python Basics: Variables and Data Types","text":"<pre><code># Integer\nage = 25\n\n# Float\ntemperature = 98.6\n\n# String\nname = \"Earth\"\n\n# Boolean\nis_planet = True\n\n# List\nplanets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\"]\n\n# Dictionary\nplanet_info = {\n    \"name\": \"Earth\",\n    \"diameter\": 12742,\n    \"has_atmosphere\": True\n}\n\n# Print variables\nprint(f\"Age: {age}\")\nprint(f\"Temperature: {temperature}\")\nprint(f\"Name: {name}\")\nprint(f\"Is planet: {is_planet}\")\nprint(f\"Planets: {planets}\")\nprint(f\"Planet info: {planet_info}\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#python-basics-arithmetic-operations","title":"Python Basics: Arithmetic Operations","text":"<pre><code># Addition\nsum = 5 + 3\nprint(f\"5 + 3 = {sum}\")\n\n# Subtraction\ndifference = 10 - 4\nprint(f\"10 - 4 = {difference}\")\n\n# Multiplication\nproduct = 6 * 7\nprint(f\"6 * 7 = {product}\")\n\n# Division\nquotient = 20 / 4\nprint(f\"20 / 4 = {quotient}\")\n\n# Integer division\nint_quotient = 20 // 3\nprint(f\"20 // 3 = {int_quotient}\")\n\n# Modulo (remainder)\nremainder = 20 % 3\nprint(f\"20 % 3 = {remainder}\")\n\n# Exponentiation\npower = 2 ** 3\nprint(f\"2 ** 3 = {power}\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#python-basics-control-flow","title":"Python Basics: Control Flow","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#if-else-statements","title":"If-Else Statements","text":"<pre><code>temperature = 25\n\nif temperature &gt; 30:\n    print(\"It's hot outside!\")\nelif temperature &gt; 20:\n    print(\"It's warm outside.\")\nelse:\n    print(\"It's cool outside.\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#for-loops","title":"For Loops","text":"<pre><code>planets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\"]\n\nfor planet in planets:\n    print(f\"{planet} is a planet in our solar system.\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#python-basics-functions","title":"Python Basics: Functions","text":"<pre><code>def celsius_to_fahrenheit(celsius):\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\n# Using the function\ntemp_c = 25\ntemp_f = celsius_to_fahrenheit(temp_c)\nprint(f\"{temp_c}\u00b0C is equal to {temp_f}\u00b0F\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/00_introduction_python101/#conclusion","title":"Conclusion","text":"<ul> <li>We've covered the course overview and tools</li> <li>Introduced basic Python concepts</li> <li>Next class: Numpy &amp; Pandas for data analysis</li> </ul> <p>Questions?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/","title":"PyEarth: A Python Introduction to Earth Science","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#lecture-2-numpy-and-pandas","title":"Lecture 2: NumPy and Pandas","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#review-of-lecture-1","title":"Review of Lecture 1:","text":"<ul> <li>Introduction to Python, Jupyter, and Chatbots</li> <li>Basic data types and structures</li> <li>Control flow, loops, and functions</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#introduction-to-numpy","title":"Introduction to NumPy","text":"<ul> <li>NumPy: Numerical Python</li> <li>Fundamental package for scientific computing in Python</li> <li>Provides support for large, multi-dimensional arrays and matrices</li> <li>Offers a wide range of mathematical functions</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#why-numpy","title":"Why NumPy?","text":"<ul> <li>Efficient: Optimized for performance</li> <li>Versatile: Supports various data types</li> <li>Integrates well with other libraries</li> <li>Essential for data analysis and scientific computing</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#linear-algebra-and-numpy","title":"Linear Algebra and NumPy?","text":"<p>Let's solve the classic \"chickens and rabbits in the same cage\" problem:</p> <ul> <li>There are 35 heads and 94 legs in a cage of chickens and rabbits.</li> <li>How many chickens and rabbits are there?</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#linear-algebra-and-numpy_1","title":"Linear Algebra and NumPy?","text":"<p>Let's solve the classic \"chickens and rabbits in the same cage\" problem:</p> <ul> <li>There are 35 heads and 94 legs in a cage of chickens and rabbits.</li> <li>How many chickens and rabbits are there?</li> </ul> <p>We can use linear algebra to solve this system of equations: 1. x + y = 35 (total heads) 2. 2x + 4y = 94 (total legs)</p> <p>Where x = number of chickens, y = number of rabbits</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#matrix-representation","title":"Matrix Representation","text":"<p>We can represent this system of equations in matrix form:</p> \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 4 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 35 \\\\ 94 \\end{bmatrix} \\] <p>Or more concisely:</p> \\[ A\\vec{x} = \\vec{b} \\] <p>Where: - \\(A\\) is the coefficient matrix - \\(\\vec{x}\\) is the vector of unknowns (chickens and rabbits) - \\(\\vec{b}\\) is the constant vector</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#solving-with-numpy","title":"Solving with NumPy","text":"<pre><code>import numpy as np\n\n# Define the coefficient matrix A and the constant vector b\nA = np.array([[1, 1],   # Coefficients for heads equation\n              [2, 4]])  # Coefficients for legs equation\nb = np.array([35, 94])  # Constants (total heads and legs)\n\n# Solve the system of equations\nsolution = np.linalg.solve(A, b)\n\nprint(f\"Chickens: {int(solution[0])}\")\nprint(f\"Rabbits: {int(solution[1])}\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#creating-numpy-arrays","title":"Creating NumPy Arrays","text":"<pre><code>import numpy as np\n\n# From a list\narr1 = np.array([1, 2, 3, 4, 5])\n\n# Using NumPy functions\narr2 = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\narr3 = np.linspace(0, 1, 5)  # [0, 0.25, 0.5, 0.75, 1]\narr4 = np.zeros((3, 3))  # 3x3 array of zeros\narr5 = np.ones((2, 4))  # 2x4 array of ones\narr6 = np.random.rand(3, 3)  # 3x3 array of random values\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#useful-numpy-functions","title":"Useful NumPy Functions","text":"<ol> <li>Array operations:</li> <li><code>np.reshape()</code>: Reshape an array</li> <li><code>np.concatenate()</code>: Join arrays</li> <li> <p><code>np.split()</code>: Split an array</p> </li> <li> <p>Mathematical operations:</p> </li> <li><code>np.sum()</code>, <code>np.mean()</code>, <code>np.std()</code>: Basic statistics</li> <li><code>np.min()</code>, <code>np.max()</code>: Find minimum and maximum values</li> <li><code>np.argmin()</code>, <code>np.argmax()</code>: Find indices of min/max values</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#useful-numpy-functions-cont","title":"Useful NumPy Functions (cont.)","text":"<ol> <li>Linear algebra:</li> <li><code>np.dot()</code>: Matrix multiplication</li> <li><code>np.linalg.inv()</code>: Matrix inverse</li> <li> <p><code>np.linalg.eig()</code>: Eigenvalues and eigenvectors</p> </li> <li> <p>Array manipulation:</p> </li> <li><code>np.transpose()</code>: Transpose an array</li> <li><code>np.sort()</code>: Sort an array</li> <li><code>np.unique()</code>: Find unique elements</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#how-to-find-numpy-functions","title":"How to Find NumPy Functions","text":"<ol> <li>GPT, Claude, and other AI assistants</li> <li>Use Python's built-in help function:    <code>python    import numpy as np    help(np.array)</code></li> <li>Use IPython/Jupyter Notebook's tab completion and <code>?</code> operator:    <code>python    np.array?</code></li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#numpy-vs-basic-python-speed-comparison","title":"NumPy vs. Basic Python: Speed Comparison","text":"<p>Let's compare the speed of calculating the mean of a large array:</p> <pre><code>import numpy as np\nimport time\n\n# Create large arrays\nsize = 10000000\ndata = list(range(size))\nnp_data = np.array(data)\n\n# Python list comprehension\nstart = time.time()\nresult_py = [x**2 + 2*x + 1 for x in data]\nend = time.time()\nprint(f\"Python time: {end - start:.6f} seconds\")\n\n# NumPy vectorized operation\nstart = time.time()\nresult_np = np_data**2 + 2*np_data + 1\nend = time.time()\nprint(f\"NumPy time: {end - start:.6f} seconds\")\n\n# NumPy is significantly faster due to its optimized C implementation. \n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#real-world-example-analyzing-earthquake-data","title":"Real-world Example: Analyzing Earthquake Data","text":"<p>We'll use NumPy to analyze earthquake data:</p> <pre><code>import numpy as np\n\n# Load earthquake data (magnitude and depth)\n# the first coloumn is utc datetime\nearthquakes = np.loadtxt(\"data/earthquakes.csv\", delimiter=\",\", skiprows=1, usecols=(1, 2, 3, 4), dtype=float)\n\n# Calculate average magnitude and depth\navg_depth = np.mean(earthquakes[:, 2])\navg_magnitude = np.mean(earthquakes[:, 3])\n\n# Find the strongest earthquake\nstrongest_idx = np.argmax(earthquakes[:, 3])\nstrongest_magnitude = earthquakes[strongest_idx, 3]\nstrongest_depth = earthquakes[strongest_idx, 2]\n\nprint(f\"Average magnitude: M{avg_magnitude:.2f}\")\nprint(f\"Average depth: {avg_depth:.2f} km\")\nprint(f\"Strongest earthquake: Magnitude {strongest_magnitude:.2f} at depth {strongest_depth:.2f} km\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#introduction-to-pandas","title":"Introduction to Pandas","text":"<ul> <li>Pandas: Python Data Analysis Library</li> <li>Built on top of NumPy</li> <li>Provides high-performance, easy-to-use data structures and tools</li> <li>Essential for data manipulation and analysis</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#why-pandas","title":"Why Pandas?","text":"<ul> <li>Handles structured data efficiently</li> <li>Powerful data alignment and merging capabilities</li> <li>Integrates well with other libraries</li> <li>Excellent for handling time series data</li> <li>Built-in tools for reading/writing various file formats</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#pandas-data-structures","title":"Pandas Data Structures","text":"<ol> <li>Series: 1D labeled array</li> <li>DataFrame: 2D labeled data structure with columns of potentially different types</li> </ol> <pre><code>import pandas as pd\n\n# Create a Series\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': pd.date_range('20230101', periods=4),\n    'C': pd.Series(1, index=range(4), dtype='float32'),\n    'D': np.array([3] * 4, dtype='int32'),\n    'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n    'F': 'foo'\n})\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#useful-pandas-functions","title":"Useful Pandas Functions","text":"<ol> <li>Data loading and saving:</li> <li><code>pd.read_csv()</code>, <code>pd.read_excel()</code>, <code>pd.read_sql()</code></li> <li> <p><code>df.to_csv()</code>, <code>df.to_excel()</code>, <code>df.to_sql()</code></p> </li> <li> <p>Data inspection:</p> </li> <li><code>df.head()</code>, <code>df.tail()</code>: View first/last rows</li> <li><code>df.info()</code>: Summary of DataFrame</li> <li> <p><code>df.describe()</code>: Statistical summary</p> </li> <li> <p>Data selection:</p> </li> <li><code>df['column']</code>: Select a column</li> <li><code>df.loc[]</code>: Label-based indexing</li> <li><code>df.iloc[]</code>: Integer-based indexing</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#useful-pandas-functions-cont","title":"Useful Pandas Functions (cont.)","text":"<ol> <li>Data manipulation:</li> <li><code>df.groupby()</code>: Group data</li> <li><code>df.merge()</code>: Merge DataFrames</li> <li> <p><code>df.pivot()</code>: Reshape data</p> </li> <li> <p>Data cleaning:</p> </li> <li><code>df.dropna()</code>: Drop missing values</li> <li><code>df.fillna()</code>: Fill missing values</li> <li> <p><code>df.drop_duplicates()</code>: Remove duplicate rows</p> </li> <li> <p>Time series functionality:</p> </li> <li><code>pd.date_range()</code>: Create date ranges</li> <li><code>df.resample()</code>: Resample time series data</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#how-to-find-pandas-functions","title":"How to Find Pandas Functions","text":"<ol> <li>GPT, Claude, and other AI assistants</li> <li>Use Python's built-in help function:    <code>python    import pandas as pd    help(pd.DataFrame)</code></li> <li>Use IPython/Jupyter Notebook's tab completion and <code>?</code> operator:    <code>python    pd.DataFrame?</code></li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#pandas-vs-numpy","title":"Pandas vs. NumPy","text":"<ul> <li>Pandas is built on top of NumPy</li> <li>Pandas adds functionality for handling structured data</li> <li>Pandas excels at:</li> <li>Handling missing data</li> <li>Data alignment</li> <li>Merging and joining datasets</li> <li>Time series functionality</li> <li>NumPy is better for:</li> <li>Large numerical computations</li> <li>Linear algebra operations</li> <li>When you need ultimate performance</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#real-world-example-revisit-the-earthquake-data","title":"Real-world Example: Revisit the Earthquake Data","text":"<p>We'll use Pandas to analyze earthquake data this time:</p> <pre><code>import pandas as pd\n\n# Load earthquake data\ndf = pd.read_csv(\"data/earthquakes.csv\")\n\n# Calculate average magnitude and depth\navg_depth = df['depth'].mean()\navg_magnitude = df['magnitude'].mean()\n\n# Find the strongest earthquake\nstrongest_idx = df['magnitude'].idxmax()\nstrongest_magnitude = df.loc[strongest_idx, 'magnitude']\nstrongest_depth = df.loc[strongest_idx, 'depth']\n\nprint(f\"Average magnitude: M{avg_magnitude:.2f}\")\nprint(f\"Average depth: {avg_depth:.2f} km\")\nprint(f\"Strongest earthquake: Magnitude {strongest_magnitude:.2f} at depth {strongest_depth:.2f} km\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#real-world-example-analyzing-temperature-data","title":"Real-world Example: Analyzing Temperature Data","text":"<p>We'll use Pandas to analyze temperature data:</p> <pre><code>import pandas as pd\n\n# Load temperature data\ndf = pd.read_csv(\"data/global_temperature.csv\")\n\n# Convert date column to datetime\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Set date as index\ndf.set_index(\"date\", inplace=True)\n\n# Find the hottest and coldest days\nhottest_day = df[\"temperature\"].idxmax()\ncoldest_day = df[\"temperature\"].idxmin()\n\nprint(f\"Hottest day: {hottest_day.date()} ({df.loc[hottest_day, 'temperature']:.1f}\u00b0C)\")\nprint(f\"Coldest day: {coldest_day.date()} ({df.loc[coldest_day, 'temperature']:.1f}\u00b0C)\")\n\n# Calculate monthly average temperatures\nyearly_avg = df.resample(\"Y\").mean()\n\n# Plot monthly average temperatures\nyearly_avg[\"temperature\"].plot(figsize=(12, 6))\n\nplt.title(\"Yearly Average Temperatures\")\nplt.ylabel(\"Temperature (\u00b0C)\")\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/01_numpy_pandas/#conclusion","title":"Conclusion","text":"<ul> <li>NumPy and Pandas are essential tools for data analysis in Python</li> <li>NumPy excels at numerical computations and array operations</li> <li>Pandas is great for structured data manipulation and analysis</li> <li>Both libraries integrate well with other scientific Python tools</li> <li>Practice and explore these libraries to become proficient in data analysis!</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/","title":"PyEarth: A Python Introduction to Earth Science","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#lecture-3-matplotlib-cartopy-and-pygmt","title":"Lecture 3: Matplotlib, Cartopy, and PyGMT","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#introduction-to-matplotlib","title":"Introduction to Matplotlib","text":"<ul> <li>Matplotlib is a powerful plotting library for Python</li> <li>It allows you to create a wide variety of static, animated, and interactive visualizations</li> <li>Used extensively in scientific computing and data analysis</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#basic-matplotlib-structure","title":"Basic Matplotlib Structure","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx, y = [0, 1, 2, 3, 4], [0, 1, 4, 9, 16]\n\n# Plot data\nax.plot(x, y, \"o-\", color=\"blue\", label=\"Data\")\n\n# Add labels and title\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.legend()\nax.set_title('My Plot')\n\n# Show the plot\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#common-matplotlib-functions","title":"Common Matplotlib Functions","text":"<ol> <li>Line Plot: <code>plt.plot(x, y)</code></li> <li>Scatter Plot: <code>plt.scatter(x, y)</code></li> <li>Bar Plot: <code>plt.bar(x, y)</code></li> <li>Histogram: <code>plt.hist(data)</code></li> <li>Box Plot: <code>plt.boxplot(data)</code></li> <li>Pie Chart: <code>plt.pie(sizes)</code></li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#line-plot-example","title":"Line Plot Example","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Sine Wave')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#scatter-plot-example","title":"Scatter Plot Example","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.rand(50)\ny = np.random.rand(50)\ncolors = np.random.rand(50)\nsizes = 1000 * np.random.rand(50)\n\nplt.scatter(x, y, c=colors, s=sizes, alpha=0.5)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Scatter Plot')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#bar-plot-example","title":"Bar Plot Example","text":"<pre><code>import matplotlib.pyplot as plt\n\ncategories = ['A', 'B', 'C', 'D']\nvalues = [3, 7, 2, 5]\n\nplt.bar(categories, values)\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Bar Plot')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#histogram-example","title":"Histogram Example","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.randn(1000)\n\nplt.hist(data, bins=30)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#box-plot-example","title":"Box Plot Example","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndata = [np.random.normal(0, std, 100) for std in range(1, 4)]\n\nplt.boxplot(data)\nplt.xlabel('Group')\nplt.ylabel('Value')\nplt.title('Box Plot')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#pie-chart-example","title":"Pie Chart Example","text":"<pre><code>import matplotlib.pyplot as plt\n\nsizes = [30, 20, 25, 15, 10]\nlabels = ['A', 'B', 'C', 'D', 'E']\n\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title('Pie Chart')\nplt.axis('equal')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#global-earthquake-dataset-example","title":"Global Earthquake Dataset Example","text":"<pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Read the earthquake data\ndf = pd.read_csv('data/earthquakes.csv')\n\nplt.figure(figsize=(12, 8))\nplt.scatter(df['longitude'], df['latitude'], \n            s=df['magnitude']*10, alpha=0.5, c=\"red\")\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Global Earthquakes')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#introduction-to-cartopy","title":"Introduction to Cartopy","text":"<ul> <li>Cartopy is a library for cartographic projections and geospatial data visualization</li> <li>It provides object-oriented map projection definitions</li> <li>Allows easy manipulation of geospatial data and creation of map plots</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#cartopy-plotting-earthquakes","title":"Cartopy: Plotting Earthquakes","text":"<pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport pandas as pd\n\ndf = pd.read_csv('data/earthquakes.csv')\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.COASTLINE)\n\nscatter = ax.scatter(df['longitude'], df['latitude'], \n                     s=df['magnitude']*10, alpha=0.5, c=\"red\",\n                     transform=ccrs.PlateCarree())\n\nax.set_global()\nplt.title('Global Earthquakes (Cartopy)')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#common-map-projections","title":"Common Map Projections","text":"<ol> <li>Plate Carr\u00e9e (Equirectangular)</li> <li>Mercator</li> <li>Lambert Conformal Conic</li> <li>Orthographic</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#plate-carree-projection","title":"Plate Carr\u00e9e Projection","text":"<ul> <li>Simplest projection</li> <li>Latitude and longitude lines are equally spaced</li> <li>Distorts shape and size, especially near poles</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.coastlines()\nax.gridlines()\nplt.title(\"PlateCarree Projection\")\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#mercator-projection","title":"Mercator Projection","text":"<ul> <li>Conformal projection (preserves angles)</li> <li>Used for navigation</li> <li>Severely distorts size near poles</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.Mercator())\nax.coastlines()\nax.gridlines()\nplt.title(\"Mercator Projection\")\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#lambert-conformal-conic-projection","title":"Lambert Conformal Conic Projection","text":"<ul> <li>Conformal projection</li> <li>Good for mid-latitude regions</li> <li>Used for aeronautical charts</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.LambertConformal())\nax.coastlines()\nax.gridlines()\nplt.title(\"Lambert Conformal Conic Projection\")\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#orthographic-projection","title":"Orthographic Projection","text":"<ul> <li>Perspective projection (as seen from space)</li> <li>Shows one hemisphere at a time</li> <li>Useful for visualizing global phenomena</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.Orthographic())\nax.coastlines()\nax.gridlines()\nplt.title(\"Orthographic Projection\")\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#when-to-use-each-projection","title":"When to Use Each Projection","text":"<ul> <li>Plate Carr\u00e9e: Simple global views, data in lat/lon coordinates</li> <li>Mercator: Navigation, web maps (e.g., Google Maps)</li> <li>Lambert Conformal Conic: Regional maps, especially mid-latitudes</li> <li>Orthographic: Visualizing Earth as a globe, planetary science</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#example-plate-carree","title":"Example: Plate Carr\u00e9e","text":"<pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport pandas as pd\n\ndf = pd.read_csv('data/earthquakes.csv')\nregion = [-125, -114, 32, 42]\ndf = df[(df['longitude'] &gt;= region[0]) &amp; (df['longitude'] &lt;= region[1]) &amp;\n        (df['latitude'] &gt;= region[2]) &amp; (df['latitude'] &lt;= region[3])]\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n\nax.set_extent(region)\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.COASTLINE)\nax.gridlines(draw_labels=True)\n\nax.scatter(df['longitude'], df['latitude'], \n           s=df['magnitude']*10, alpha=0.5, c=\"red\", \n           transform=ccrs.PlateCarree())\n\nplt.title('California Earthquakes (Plate Carr\u00e9e)')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#adding-topography-with-cartopy","title":"Adding Topography with Cartopy","text":"<pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport pandas as pd\n\ndf = pd.read_csv('data/earthquakes.csv')\nregion = [-125, -114, 32, 42]\ndf = df[(df['longitude'] &gt;= region[0]) &amp; (df['longitude'] &lt;= region[1]) &amp;\n        (df['latitude'] &gt;= region[2]) &amp; (df['latitude'] &lt;= region[3])]\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n\nax.set_extent(region)\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.COASTLINE)\nax.gridlines(draw_labels=True)\n\n# Add topography\nax.stock_img()\n\nax.scatter(df['longitude'], df['latitude'], \n           s=df['magnitude']*10, alpha=0.5, c=\"red\", \n           transform=ccrs.PlateCarree())\n\nplt.title('California Earthquakes (Plate Carr\u00e9e)')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#introduction-to-pygmt","title":"Introduction to PyGMT","text":"<ul> <li>PyGMT is a Python interface for the Generic Mapping Tools (GMT)</li> <li>Provides access to GMT's powerful mapping and data processing capabilities</li> <li>Can handle high-resolution topography data</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#pygmt-downloading-topography-data","title":"PyGMT: Downloading Topography Data","text":"<pre><code>import pygmt\n\n# Download topography data for California\nregion = [-125, -114, 32, 42]\ngrid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=region)\n\n# Plot the topography\nfig = pygmt.Figure()\nfig.grdimage(grid=grid, projection=\"M15c\", frame=True, cmap=\"geo\")\nfig.colorbar(frame=[\"a1000\", \"x+lElevation\", \"y+lm\"])\nfig.coast(shorelines=\"1/0.5p\")\nfig.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#combining-pygmt-and-cartopy","title":"Combining PyGMT and Cartopy","text":"<pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport pygmt\nimport numpy as np\nimport pandas as pd\n\n# Download topography data\nregion = [-125, -114, 32, 42]\ndf = pd.read_csv('data/earthquakes.csv')\ndf = df[(df['longitude'] &gt;= region[0]) &amp; (df['longitude'] &lt;= region[1]) &amp;\n        (df['latitude'] &gt;= region[2]) &amp; (df['latitude'] &lt;= region[3])]\n\ngrid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=region)\n\n# Create a Cartopy plot\nfig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.set_extent(region)\n\n# Plot the topography\nimg_extent = (grid.lon.min(), grid.lon.max(), grid.lat.min(), grid.lat.max())\nax.set_extent(region, crs=ccrs.PlateCarree())\nax.scatter(df['longitude'], df['latitude'], \n           s=df['magnitude']*10, alpha=0.5, c=\"red\", \n           transform=ccrs.PlateCarree())\n\n# Plot the earthquake data\nax.imshow(grid.data, extent=img_extent, transform=ccrs.PlateCarree(), \n          cmap='terrain', origin='lower')\n\n# Add coastlines and borders\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS)\nax.add_feature(cfeature.STATES)\nax.gridlines(draw_labels=True)\n\nplt.colorbar(ax.images[0], label='Elevation (m)')\nplt.title('California Topography (PyGMT + Cartopy)')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/02_matplotlib_cartopy/#summary","title":"Summary","text":"<ul> <li>Matplotlib: Versatile plotting library for various chart types</li> <li>Cartopy: Geospatial data visualization with map projections</li> <li>PyGMT: Access to high-resolution topography data and GMT capabilities</li> <li>Combining these tools allows for powerful Earth science visualizations</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/","title":"PyEarth: A Python Introduction to Earth Science","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#lecture-4-scikit-learn-supervised-learning-regression","title":"Lecture 4: Scikit-learn: Supervised Learning: Regression","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#what-is-linear-regression","title":"What is Linear Regression?","text":"<ul> <li>A statistical method to model the relationship between variables</li> <li>Predicts a dependent variable based on one or more independent variables</li> <li>Assumes a linear relationship between variables</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#examples-co2-concentration","title":"Examples: CO2 Concentration","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#global-temperature-anomalies","title":"Global Temperature Anomalies","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#arctic-sea-ice-extent","title":"Arctic Sea Ice Extent","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#sea-level-rise","title":"Sea Level Rise","text":"<ul> <li>How to quantify the relationship between variables?</li> <li>How to predict future values and interpret the results?</li> <li>How to use Python libraries to model and analyze data?</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#basic-statistics-mean-and-standard-deviation","title":"Basic Statistics: Mean and Standard Deviation","text":"<ul> <li> <p>Mean: Average of a set of numbers  \\(\\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\)\\)</p> </li> <li> <p>Standard Deviation: Measure of spread in a dataset \\(\\(\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\)\\)</p> </li> </ul> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#linear-regression","title":"Linear Regression","text":"\\[y = Ax + b + \\epsilon\\] <ul> <li>\\(y\\): Dependent variable</li> <li>\\(x\\): Independent variable</li> <li>\\(A\\): Slope</li> <li>\\(b\\): Intercept</li> <li>\\(\\epsilon\\): Error term</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#earthquake-example-2019-ridgecrest-earthquakes","title":"Earthquake Example: 2019 Ridgecrest Earthquakes","text":"<ul> <li>July 4, 2019: M6.4 earthquake</li> <li>July 5, 2019: M7.1 earthquake</li> <li>Location: Ridgecrest, California, USA</li> </ul> <p>Link: WIKI</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#ground-motion-data","title":"Ground Motion Data","text":"<p>Earthquake information: USGS ShakeMap: PGA</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#accessing-peak-ground-acceleration-pga-data","title":"Accessing Peak Ground Acceleration (PGA) Data","text":"<pre><code>import json\nimport pandas as pd\nimport numpy as np\nimport fsspec\n\n# M6.4 earthquake\njson_url = \"https://earthquake.usgs.gov/product/shakemap/ci38443183/atlas/1594160017984/download/stationlist.json\"\n# M7.1 earthquake\n# json_url = \"https://earthquake.usgs.gov/product/shakemap/ci38457511/atlas/1594160054783/download/stationlist.json\"\n\nwith fsspec.open(json_url) as f:\n    data = json.load(f)\n\ndef parse_data(data):\n    rows = []\n    for line in data[\"features\"]:\n        rows.append({\n            \"station_id\": line[\"id\"],\n            \"longitude\": line[\"geometry\"][\"coordinates\"][0],\n            \"latitude\": line[\"geometry\"][\"coordinates\"][1],\n            \"pga\": line[\"properties\"][\"pga\"], # unit: %g\n            \"pgv\": line[\"properties\"][\"pgv\"], # unit: cm/s\n            \"distance\": line[\"properties\"][\"distance\"],\n        }\n    )\n    return pd.DataFrame(rows)\n\ndata = parse_data(data)\ndata = data[(data[\"pga\"] != \"null\") &amp; (data[\"pgv\"] != \"null\")]\ndata = data[~data[\"station_id\"].str.startswith(\"DYFI\")]\ndata = data.dropna()\ndata = data.sort_values(by=\"distance\", ascending=True)\ndata[\"logR\"] = data[\"distance\"].apply(lambda x: np.log10(float(x)))\ndata[\"logPGA\"] = data[\"pga\"].apply(lambda x: np.log10(float(x)))\ndata[\"logPGV\"] = data[\"pgv\"].apply(lambda x: np.log10(float(x)))\ndata\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#visualizing-peak-ground-acceleration-pga-data","title":"Visualizing Peak Ground Acceleration (PGA) Data","text":"<pre><code>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\nfig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nscatter = ax.scatter(data['longitude'], data['latitude'], c=data['pga'], \n                     cmap='viridis', transform=ccrs.PlateCarree())\n\nax.add_feature(cfeature.STATES)\nax.add_feature(cfeature.COASTLINE)\nax.set_extent([-120, -116, 34, 37], crs=ccrs.PlateCarree())\n\nplt.colorbar(scatter, label='PGA %g')\nplt.title('PGA Distribution - Ridgecrest Earthquake')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#ground-motion-prediction-equations-gmpes","title":"Ground Motion Prediction Equations (GMPEs)","text":"\\[\\log(PGA) = a \\log(R) + bM + c\\] <ul> <li>PGA: Peak Ground Acceleration</li> <li>R: Distance from earthquake source</li> <li>M: Earthquake magnitude</li> <li>a, b, c: Model parameters</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#visualizing-logpga-vs-logr","title":"Visualizing log(PGA) vs. log(R)","text":"<pre><code>plt.figure(figsize=(10, 6))\nplt.scatter(data['logR'], data['logPGA'])\nplt.xlabel('log(R)')\nplt.ylabel('log(PGA)')\nplt.title('log(PGA) vs. log(R)')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#fitting-a-linear-model-using-scikit-learn","title":"Fitting a Linear Model using Scikit-learn","text":"<pre><code>from sklearn.linear_model import LinearRegression\n\nX = data[['logR']].values\ny = data['logPGA'].values\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(f\"Slope (a): {model.coef_[0]:.4f}\")\nprint(f\"Intercept (b): {model.intercept_:.4f}\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#visualizing-the-linear-regression-model","title":"Visualizing the Linear Regression Model","text":"<pre><code>plt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, model.predict(X), color='red', label='Regression Line')\nplt.xlabel('log(Distance)')\nplt.ylabel('log(PGA)')\nplt.title('Linear Regression Model')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#interpreting-the-results","title":"Interpreting the Results","text":"<ul> <li>Slope (a): Represents the rate of decay of PGA with distance</li> <li>Intercept (b): Related to the earthquake's magnitude and local site conditions</li> </ul> <p>Let's predict the shaking in Los Angeles:</p> <pre><code>la_distance = 177  # km (approximate)\nla_logR = np.log10(la_distance)\nla_prediction = model.predict([[la_logR]])\nprint(f\"Predicted log(PGA) in LA: {la_prediction[0]:.4f}\")\nprint(f\"Predicted PGA in LA: {10**la_prediction[0]:.4f} %g\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#compare-the-prediction-with-the-data","title":"Compare the prediction with the data","text":"<p>USGS PGA data: USGS</p> <ul> <li>Why is the prediction different from the observed PGA in Los Angeles?</li> </ul> <p>M7.08 Ridgecrest Simulation</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#add-the-prediction-to-the-plot","title":"Add the prediction to the plot","text":"<pre><code>plt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, model.predict(X), color='red', label='Regression Line')\nplt.scatter(la_logR, la_prediction, color='green', label='LA Prediction')\nplt.xlabel('log(Distance)')\nplt.ylabel('log(PGA)')\nplt.title('Linear Regression Model with LA Prediction')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#residual-analysis","title":"Residual Analysis","text":"<pre><code>y_pred = model.predict(X)\nresiduals = y - y_pred\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X, residuals)\nplt.xlabel('log(Distance)')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.axhline(y=0, color='r', linestyle='--')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#evaluating-model-quality-r-squared-r2","title":"Evaluating Model Quality: R-squared (R\u00b2)","text":"<ul> <li> <p>R\u00b2 measures the proportion of variance in the dependent variable explained by the independent variable(s) $$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $$</p> </li> <li> <p>Ranges from 0 to 1, with 1 indicating a perfect fit</p> </li> </ul> <pre><code>from sklearn.metrics import r2_score\n\nr2 = r2_score(y, y_pred)\nprint(f\"R-squared: {r2:.4f}\")\n\n# Calculate R-squared manually\nss_res = np.sum((y - y_pred)**2)\nss_tot = np.sum((y - y.mean())**2)\nr2_manual = 1 - ss_res / ss_tot\nprint(f\"R-squared (manual): {r2_manual:.4f}\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#understanding-r2","title":"Understanding R\u00b2","text":"<ul> <li>R\u00b2 = 0: Model explains none of the variance in the dependent variable</li> <li>R\u00b2 = 1: Model explains all the variance in the dependent variable</li> <li>R\u00b2 &lt; 0: Model performs worse than a horizontal line</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#parameter-uncertainty","title":"Parameter Uncertainty","text":"<ul> <li>Standard error of the slope (a):</li> </ul> \\[SE_a = \\sqrt{\\sigma^2 / \\sum (x_i - \\bar{x})^2}\\] <p>where \\(\\sigma^2 = \\sum (y_i - \\hat{y}_i)^2 / (n - p)\\), \\(n\\) is the number of observations, and \\(p\\) is the number of predictors</p> <ul> <li>Standard error of the intercept (b):</li> </ul> \\[SE_b = \\sqrt{\\sigma^2 (1/n + \\bar{x}^2 / \\sum (x_i - \\bar{x})^2)}\\]"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#calculating-parameter-uncertainty","title":"Calculating Parameter Uncertainty","text":"<pre><code>import numpy as np\nfrom scipy import stats\n\nn = len(X)\np = len(model.coef_) + 1\ndof = n - p\n\nsigma = np.sum((y - y_pred)**2) / dof\n\nse_a = np.sqrt(sigma / np.sum((X - X.mean())**2))\nse_b = np.sqrt(sigma * (1/n + (X.mean()**2) / np.sum((X - X.mean())**2)) )\n\nprint(f\"Standard error of slope (a): {se_a:.4f}\")\nprint(f\"Standard error of intercept (b): {se_b:.4f}\")\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/03_regression/#summary","title":"Summary","text":"<ul> <li>Linear regression is a powerful tool for modeling relationships in Earth Science data</li> <li>We applied it to model ground motion attenuation in earthquakes</li> <li>Important considerations:</li> <li>Model assumptions</li> <li>Residual analysis</li> <li>Model evaluation (R\u00b2)</li> <li>Parameter uncertainty</li> <li>Prediction intervals</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/","title":"Regression2","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures In\u00a0[2]: Copied! <pre>Supernova_data = pd.read_csv('data/Freedman2000_Supernova1a.csv')\nSupernova_data.tail()\n</pre> Supernova_data = pd.read_csv('data/Freedman2000_Supernova1a.csv') Supernova_data.tail() Out[2]: Supernova VCMB D(Mpc) HCMB \u03c3 31 SN1994T 10715 149.9 71.5 2.6 32 SN1995ac 14634 185.6 78.8 2.7 33 SN1995ak 6673 82.4 80.9 2.8 34 SN1996C 9024 136.0 66.3 2.5 35 SN1996bl 10446 132.7 78.7 2.7 <p>The <code>VCMB</code> column is velocity relative to the cosmic microwave background in $km \\cdot s^{-1}$ .</p> <p>The <code>D(Mpc)</code> column is the distance in Mpc which is the unit typically used for these measurements. 1 Mpc =  3.09 x $10^{19}$ km</p> <p>Go ahead and double-click on this cell to see how I am getting labels that have the proper superscripts.</p> <p>To create nice labels with superscripts, we can use latex formatting, which can also be done in a markdown cell.  For a superscript, first we need to encase the text in dollar signs and then use the ^ symbol to make the following text a superscript. If there is more than one number in the superscript, you must enclose what you want as the superscript in curly braces. For example, to print $10^3$, we use $10^3$ and for 'per second' ($s^{-1}$)</p> In\u00a0[3]: Copied! <pre>plt.figure()\nplt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],color='red',label='1A Supernovae data (Freedman et al. 2000)')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\nplt.legend()\nplt.show()\n</pre> plt.figure() plt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],color='red',label='1A Supernovae data (Freedman et al. 2000)') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)') plt.legend() plt.show() In\u00a0[4]: Copied! <pre>LinearRegression?\n</pre> LinearRegression? <pre>Init signature:\nLinearRegression(\n    *,\n    fit_intercept=True,\n    copy_X=True,\n    n_jobs=None,\n    positive=False,\n)\nDocstring:     \nOrdinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup in case of sufficiently large problems, that is if firstly\n    `n_targets &gt; 1` and secondly `X` is sparse or if `positive` is set\n    to `True`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary &lt;n_jobs&gt;` for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n&gt;&gt;&gt; # y = 1 * x_0 + 2 * x_1 + 3\n&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3\n&gt;&gt;&gt; reg = LinearRegression().fit(X, y)\n&gt;&gt;&gt; reg.score(X, y)\n1.0\n&gt;&gt;&gt; reg.coef_\narray([1., 2.])\n&gt;&gt;&gt; reg.intercept_\n3.0...\n&gt;&gt;&gt; reg.predict(np.array([[3, 5]]))\narray([16.])\nFile:           ~/.local/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_base.py\nType:           ABCMeta\nSubclasses:     </pre> In\u00a0[5]: Copied! <pre>PolynomialFeatures?\n</pre> PolynomialFeatures? <pre>Init signature:\nPolynomialFeatures(\n    degree=2,\n    *,\n    interaction_only=False,\n    include_bias=True,\n    order='C',\n)\nDocstring:     \nGenerate polynomial and interaction features.\n\nGenerate a new feature matrix consisting of all polynomial combinations\nof the features with degree less than or equal to the specified degree.\nFor example, if an input sample is two dimensional and of the form\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n\nRead more in the :ref:`User Guide &lt;polynomial_features&gt;`.\n\nParameters\n----------\ndegree : int or tuple (min_degree, max_degree), default=2\n    If a single int is given, it specifies the maximal degree of the\n    polynomial features. If a tuple `(min_degree, max_degree)` is passed,\n    then `min_degree` is the minimum and `max_degree` is the maximum\n    polynomial degree of the generated features. Note that `min_degree=0`\n    and `min_degree=1` are equivalent as outputting the degree zero term is\n    determined by `include_bias`.\n\ninteraction_only : bool, default=False\n    If `True`, only interaction features are produced: features that are\n    products of at most `degree` *distinct* input features, i.e. terms with\n    power of 2 or higher of the same input feature are excluded:\n\n        - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc.\n        - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.\n\ninclude_bias : bool, default=True\n    If `True` (default), then include a bias column, the feature in which\n    all polynomial powers are zero (i.e. a column of ones - acts as an\n    intercept term in a linear model).\n\norder : {'C', 'F'}, default='C'\n    Order of output array in the dense case. `'F'` order is faster to\n    compute, but may slow down subsequent estimators.\n\n    .. versionadded:: 0.21\n\nAttributes\n----------\npowers_ : ndarray of shape (`n_output_features_`, `n_features_in_`)\n    `powers_[i, j]` is the exponent of the jth input in the ith output.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_output_features_ : int\n    The total number of polynomial output features. The number of output\n    features is computed by iterating over all suitably sized combinations\n    of input features.\n\nSee Also\n--------\nSplineTransformer : Transformer that generates univariate B-spline bases\n    for features.\n\nNotes\n-----\nBe aware that the number of features in the output array scales\npolynomially in the number of features of the input array, and\nexponentially in the degree. High degrees can cause overfitting.\n\nSee :ref:`examples/linear_model/plot_polynomial_interpolation.py\n&lt;sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py&gt;`\n\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures\n&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)\n&gt;&gt;&gt; X\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n&gt;&gt;&gt; poly = PolynomialFeatures(2)\n&gt;&gt;&gt; poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])\n&gt;&gt;&gt; poly = PolynomialFeatures(interaction_only=True)\n&gt;&gt;&gt; poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.],\n       [ 1.,  2.,  3.,  6.],\n       [ 1.,  4.,  5., 20.]])\nFile:           ~/.local/miniconda3/lib/python3.10/site-packages/sklearn/preprocessing/_polynomial.py\nType:           type\nSubclasses:     </pre> In\u00a0[6]: Copied! <pre>X = Supernova_data[['D(Mpc)']].values\ny = Supernova_data['VCMB'].values\n\nmodel = LinearRegression()\nmodel.fit(X,y)\n\nprint('Slope:',model.coef_)\nprint('Intercept:',model.intercept_)\n</pre> X = Supernova_data[['D(Mpc)']].values y = Supernova_data['VCMB'].values  model = LinearRegression() model.fit(X,y)  print('Slope:',model.coef_) print('Intercept:',model.intercept_) <pre>Slope: [67.5361857]\nIntercept: 711.795667873037\n</pre> <p>So $H_o$, the slope of the best-fit line, is 67.5 (in the odd units of kilometers per second per megaparsec).</p> <p>Let's plot the best fit line on our graph.</p> In\u00a0[7]: Copied! <pre>y_pred = model.predict(X)\n\nplt.figure()\nplt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],color='red',label='1A Supernovae data (Freedman et al. 2000)')\nplt.plot(Supernova_data['D(Mpc)'],y_pred,color='blue',label='Linear fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\nplt.legend()\nplt.show()\n</pre> y_pred = model.predict(X)  plt.figure() plt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],color='red',label='1A Supernovae data (Freedman et al. 2000)') plt.plot(Supernova_data['D(Mpc)'],y_pred,color='blue',label='Linear fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)') plt.legend() plt.show()  In\u00a0[8]: Copied! <pre>y_350 = model.predict([[350]])\nprint('Predicted velocity at 350 Mpc:',y_350)\n</pre> y_350 = model.predict([[350]]) print('Predicted velocity at 350 Mpc:',y_350) <pre>Predicted velocity at 350 Mpc: [24349.46066402]\n</pre> <p>And use it, to get what is normally called the $R^2$ value, which when 1. represents perfect agreement.</p> <p>Pearson correlation coefficient between several example X,Y sets. Source: https://en.wikipedia.org/wiki/Correlation_and_dependence</p> In\u00a0[9]: Copied! <pre>ss_res = np.sum((y - y_pred)**2)\nss_tot = np.sum((y - np.mean(y))**2)\nr2 = 1 - (ss_res / ss_tot)\n\nprint('R^2:',r2)\n</pre> ss_res = np.sum((y - y_pred)**2) ss_tot = np.sum((y - np.mean(y))**2) r2 = 1 - (ss_res / ss_tot)  print('R^2:',r2) <pre>R^2: 0.9782780292778982\n</pre> <p>Not a bad fit!  We can have confidence that there is a strong correlation between distance and velocity. The universe is expanding.</p> In\u00a0[10]: Copied! <pre>res = y - y_pred\n</pre> res = y - y_pred In\u00a0[11]: Copied! <pre>plt.figure()\nplt.scatter(Supernova_data['D(Mpc)'], res, color='red', label='Residuals')\nplt.hlines(0,xmin=0,xmax=500)\nplt.xlim(0,500)\nplt.show()\n</pre> plt.figure() plt.scatter(Supernova_data['D(Mpc)'], res, color='red', label='Residuals') plt.hlines(0,xmin=0,xmax=500) plt.xlim(0,500) plt.show() <p>The residual plot of a good regression shows no pattern. The residuals look about the same, above and below the horizontal line at 0, across the range of the predictor variable.</p> In\u00a0[12]: Copied! <pre>H0 = model.coef_[0]\nt = 1.0/H0\nprint(t)\n</pre> H0 = model.coef_[0] t = 1.0/H0 print(t) <pre>0.014806877077622455\n</pre> <p>But the units are weird (not years, Mpc s/km).  To fix this, we need to know how many kilometers are in a megaparsec.  As it happens,  there are 3.09 x $10^{19}$ km/Mpc.</p> <p>So, we can calculate the age of the universe in seconds (Age_sec) by converting the megaparsecs to km:</p> <p>Age (s) = $t \\frac{s \\cdot Mpc}{km} \\times 3.09 \\times 10^{19} \\frac {km}{Mpc}$</p> In\u00a0[13]: Copied! <pre>age_sec=t*3.09e19\nprint(age_sec)\n</pre> age_sec=t*3.09e19 print(age_sec) <pre>4.575325016985338e+17\n</pre> <p>That's a lot of seconds!  We should convert seconds to years.  Here's another fun fact: there are approximately   $\\pi$ x 10$^7$ seconds in a year.</p> <p>More precisely, there are 60 (s/min) x 60 (min/hr) x 24 (hr/day) x 365.25 (days/yr)</p> In\u00a0[14]: Copied! <pre>sec_yr=60*60*24*365.25\nprint('%e'%(sec_yr))\n</pre> sec_yr=60*60*24*365.25 print('%e'%(sec_yr)) <pre>3.155760e+07\n</pre> <p>Ok.  so not exactly $\\pi \\times 10^7$, but close....</p> In\u00a0[15]: Copied! <pre>age_yr=age_sec/sec_yr\nprint(age_yr)\n</pre> age_yr=age_sec/sec_yr print(age_yr) <pre>14498330091.59549\n</pre> <p>And now in billions of years:</p> In\u00a0[16]: Copied! <pre>age_byr=age_yr*1e-9\nprint(f'Age of the universe (in billions of years): {age_byr:.3f}')\n</pre> age_byr=age_yr*1e-9 print(f'Age of the universe (in billions of years): {age_byr:.3f}') <pre>Age of the universe (in billions of years): 14.498\n</pre> <p>Write a function that takes in a Hubble constant value and calculates the age of the Universe in billions of year</p> In\u00a0[17]: Copied! <pre>def age_of_universe(Hubble_constant):\n\n    age = 1.0/Hubble_constant\n    age_sec = age*3.09e19\n    sec_yr = 60*60*24*365.25\n    age_yrs = age_sec/sec_yr\n    age_byr = age_yrs*1e-9\n    \n    return age_byr\n\nprint(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\")\n</pre> def age_of_universe(Hubble_constant):      age = 1.0/Hubble_constant     age_sec = age*3.09e19     sec_yr = 60*60*24*365.25     age_yrs = age_sec/sec_yr     age_byr = age_yrs*1e-9          return age_byr  print(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\") <pre>Age of the universe (in billions of years): 14.498\n</pre> <p>Exercises:</p> <ul> <li>Import the 'Data/Freedman2000_IBandTullyFisher.csv' file.</li> </ul> In\u00a0[18]: Copied! <pre>data = pd.read_csv('data/Freedman2000_IBandTullyFisher.csv')\n</pre> data = pd.read_csv('data/Freedman2000_IBandTullyFisher.csv') <ul> <li>Make a linear fit to determine the slope between <code>VCMB</code> and <code>D(Mpc)</code>.</li> </ul> In\u00a0[19]: Copied! <pre>model = LinearRegression()\nmodel.fit(data[['D(Mpc)']],data['VCMB'])\n</pre> model = LinearRegression() model.fit(data[['D(Mpc)']],data['VCMB'])  Out[19]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <ul> <li>Calculate the implied age of the universe from these TF galaxy data alone.</li> </ul> In\u00a0[20]: Copied! <pre>H0 = model.coef_[0]\nprint(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\")\n</pre> H0 = model.coef_[0] print(f\"Age of the universe (in billions of years): {age_of_universe(H0):.3f}\") <pre>Age of the universe (in billions of years): 12.679\n</pre> In\u00a0[21]: Copied! <pre>Betoule_data = pd.read_csv('data/mu_z.csv',header=1)\nBetoule_data.head()\n</pre> Betoule_data = pd.read_csv('data/mu_z.csv',header=1) Betoule_data.head() Out[21]: z mu 0 0.010 32.953887 1 0.012 33.879004 2 0.014 33.842141 3 0.016 34.118567 4 0.019 34.593446 <p>Now we can plot it the same way as the cosmologists did in the paper, using $\\mu$ and $\\log z$:</p> In\u00a0[22]: Copied! <pre>plt.figure\nplt.scatter(Betoule_data['z'],Betoule_data['mu'],color='blue')\nplt.xlabel('z')\nplt.ylabel('$\\mu$')\nplt.semilogx()\nplt.show()\n</pre> plt.figure plt.scatter(Betoule_data['z'],Betoule_data['mu'],color='blue') plt.xlabel('z') plt.ylabel('$\\mu$') plt.semilogx() plt.show() <p>To compare these new data with the previous considered data, we must do the following:</p> <ul> <li>Transform $z$  to velocity</li> <li>Transform  $\\mu$ to distance using the equations provided.</li> <li>Truncate the new dataset which goes to much farther distances than the 'old' data set</li> </ul> In\u00a0[23]: Copied! <pre># speed of light in km/s\nc = 2.9979e8 / 1000 \n\n# the formula for v from z (and c)\nBetoule_data['velocity'] = c * (((Betoule_data['z']+1.)**2-1.)/((Betoule_data['z']+1.)**2+1.)) \n\n# convert mu to Gpc\nBetoule_data['distance'] = 10000*(10.**((Betoule_data['mu'])/5.))*1e-9\n</pre> # speed of light in km/s c = 2.9979e8 / 1000   # the formula for v from z (and c) Betoule_data['velocity'] = c * (((Betoule_data['z']+1.)**2-1.)/((Betoule_data['z']+1.)**2+1.))   # convert mu to Gpc Betoule_data['distance'] = 10000*(10.**((Betoule_data['mu'])/5.))*1e-9 In\u00a0[24]: Copied! <pre>plt.figure(figsize=(8,6))\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity'],\n         color='blue',label='1A Supernovae data (Betoule et al. 2014)')\nplt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],\n            color='red',label='1A Supernovae data (Freedman et al. 2000)')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(8,6)) plt.scatter(Betoule_data['distance'],Betoule_data['velocity'],          color='blue',label='1A Supernovae data (Betoule et al. 2014)') plt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],             color='red',label='1A Supernovae data (Freedman et al. 2000)') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)') plt.legend() plt.show() <p>These data sets are similar to one another for the \"close\" objects, but we can see that a linear model doesn't work well for objects that are at greater distances.</p> <p>To visualize this reality, let's plot the fit to the Freedman et al. 2000 data atop this plot.</p> In\u00a0[25]: Copied! <pre>model = LinearRegression()\nmodel.fit(Supernova_data[['D(Mpc)']].values,Supernova_data['VCMB'].values)\ny_pred = model.predict(Betoule_data[['distance']].values)\n\nplt.figure(figsize=(8,6))\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity'],\n         color='blue',label='1A Supernovae data (Betoule et al. 2014)')\nplt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],\n            color='red',label='1A Supernovae data (Freedman et al. 2000)')\nplt.plot(Betoule_data['distance'],y_pred,\n         color='black',label='1A Supernovae fit to Freedman data')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\nplt.legend()\nplt.show()\n</pre> model = LinearRegression() model.fit(Supernova_data[['D(Mpc)']].values,Supernova_data['VCMB'].values) y_pred = model.predict(Betoule_data[['distance']].values)  plt.figure(figsize=(8,6)) plt.scatter(Betoule_data['distance'],Betoule_data['velocity'],          color='blue',label='1A Supernovae data (Betoule et al. 2014)') plt.scatter(Supernova_data['D(Mpc)'],Supernova_data['VCMB'],             color='red',label='1A Supernovae data (Freedman et al. 2000)') plt.plot(Betoule_data['distance'],y_pred,          color='black',label='1A Supernovae fit to Freedman data') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)') plt.legend() plt.show() <p>Clearly this fit is quite poor.</p> <p>Let's make a first-order polynomial fit to all the Betoule data and then plot the residual:</p> In\u00a0[26]: Copied! <pre>model = LinearRegression()\nmodel.fit(Betoule_data[['distance']].values,Betoule_data['velocity'].values)\ny_pred = model.predict(Betoule_data[['distance']].values)\n\nplt.figure(figsize=(8, 10))\nplt.subplot(2,1,1)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity'])\nplt.plot(Betoule_data['distance'],y_pred,color='orange',)\nplt.title('data and a polynomial degree 1 fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.subplot(2,1,2)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred)\nplt.title('residuals of a polynomial degree 1 fit')\nplt.ylabel('Residual velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.tight_layout()\nplt.show()\n</pre> model = LinearRegression() model.fit(Betoule_data[['distance']].values,Betoule_data['velocity'].values) y_pred = model.predict(Betoule_data[['distance']].values)  plt.figure(figsize=(8, 10)) plt.subplot(2,1,1) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']) plt.plot(Betoule_data['distance'],y_pred,color='orange',) plt.title('data and a polynomial degree 1 fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.subplot(2,1,2) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred) plt.title('residuals of a polynomial degree 1 fit') plt.ylabel('Residual velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.tight_layout() plt.show() <p>There is a lot of structure to the residual of this degree 1 fit. Let's try a degree 2 polynomial fit (known as quadratic):</p> <p>$f(x)=ax^2+bx+c$</p> In\u00a0[27]: Copied! <pre>model = LinearRegression()\npoly = PolynomialFeatures(degree=2)\nX = poly.fit_transform(Betoule_data[['distance']].values)\nmodel.fit(X, Betoule_data['velocity'].values)\ny_pred = model.predict(X)\n\nplt.figure(figsize=(8, 10))\nplt.subplot(2,1,1)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity'])\nplt.plot(Betoule_data['distance'],y_pred,color='orange',)\nplt.title('data and a polynomial degree 2 fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.subplot(2,1,2)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred)\nplt.title('residuals of a polynomial degree 2 fit')\nplt.ylabel('Residual velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.tight_layout()\nplt.show()\n</pre> model = LinearRegression() poly = PolynomialFeatures(degree=2) X = poly.fit_transform(Betoule_data[['distance']].values) model.fit(X, Betoule_data['velocity'].values) y_pred = model.predict(X)  plt.figure(figsize=(8, 10)) plt.subplot(2,1,1) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']) plt.plot(Betoule_data['distance'],y_pred,color='orange',) plt.title('data and a polynomial degree 2 fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.subplot(2,1,2) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred) plt.title('residuals of a polynomial degree 2 fit') plt.ylabel('Residual velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.tight_layout() plt.show() <p>There is a lot of structure to the residuals of this degree 2 fit (and the residuals are still high). Let's try a degree 3 polynomial fit (known as cubic):</p> <p>$f(x)=ax^3+bx^2+cx+d$</p> In\u00a0[28]: Copied! <pre>model = LinearRegression()\npoly = PolynomialFeatures(degree=3)\nX = poly.fit_transform(Betoule_data[['distance']].values)\nmodel.fit(X,Betoule_data['velocity'].values)\ny_pred = model.predict(X)\n\nplt.figure(figsize=(8, 10))\nplt.subplot(2,1,1)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity'])\nplt.plot(Betoule_data['distance'],y_pred,color='orange',)\nplt.title('data and a polynomial degree 3 fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.subplot(2,1,2)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred)\nplt.title('residuals of a polynomial degree 3 fit')\nplt.ylabel('Residual velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.tight_layout()\nplt.show()\n</pre> model = LinearRegression() poly = PolynomialFeatures(degree=3) X = poly.fit_transform(Betoule_data[['distance']].values) model.fit(X,Betoule_data['velocity'].values) y_pred = model.predict(X)  plt.figure(figsize=(8, 10)) plt.subplot(2,1,1) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']) plt.plot(Betoule_data['distance'],y_pred,color='orange',) plt.title('data and a polynomial degree 3 fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.subplot(2,1,2) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred) plt.title('residuals of a polynomial degree 3 fit') plt.ylabel('Residual velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.tight_layout() plt.show() <p>Can a degree 4 polynomial fit do better?</p> <p>$f(x)=ax^4+bx^3+cx^2+dx+e$</p> In\u00a0[29]: Copied! <pre>model = LinearRegression()\npoly = PolynomialFeatures(degree=4)\nX = poly.fit_transform(Betoule_data[['distance']].values)\nmodel.fit(X,Betoule_data['velocity'].values)\ny_pred = model.predict(X)\n\nplt.figure(figsize=(8, 10))\nplt.subplot(2,1,1)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity'])\nplt.plot(Betoule_data['distance'],y_pred,color='orange',)\nplt.title('data and a polynomial degree 4 fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.subplot(2,1,2)\nplt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred)\nplt.title('residuals of a polynomial degree 4 fit')\nplt.ylabel('Residual velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.tight_layout()\nplt.show()\n</pre> model = LinearRegression() poly = PolynomialFeatures(degree=4) X = poly.fit_transform(Betoule_data[['distance']].values) model.fit(X,Betoule_data['velocity'].values) y_pred = model.predict(X)  plt.figure(figsize=(8, 10)) plt.subplot(2,1,1) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']) plt.plot(Betoule_data['distance'],y_pred,color='orange',) plt.title('data and a polynomial degree 4 fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.subplot(2,1,2) plt.scatter(Betoule_data['distance'],Betoule_data['velocity']-y_pred) plt.title('residuals of a polynomial degree 4 fit') plt.ylabel('Residual velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.tight_layout() plt.show() <p>That looks about the same as the cubic so might as well stick with that one as a working model.</p> <p>That the velocity-distance relationship is not linear is taken as evidence that the expansion of the universe is accelerating. This acceleration is attributed to dark energy:</p> <p>In a matter-dominated universe, the expansion velocity of the Universe slows down over time owing to the attractive force of gravity. However, a decade ago two independent groups (Perlmutter et al. 1999, Riess et al. 1998) found that supernovae at z \u223c 0.5 appear to be about 10% fainter than those observed locally, consistent instead with models in which the expansion velocity is increasing; that is, a universe that is accelerating in its expansion. Combined with independent estimates of the matter density, these results are consistent with a universe in which one-third of the overall density is in the form of matter (ordinary plus dark), and two-thirds is in a form having a large, negative pressure, termed dark energy. Freedman and Madore (2010)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#regression-and-the-age-of-the-universe","title":"Regression and the age of the universe\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#import-scientific-python-packages","title":"Import scientific python packages\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#bivariate-data","title":"Bivariate data\u00b6","text":"<p>There are many examples in Earth and Planetary Science where we are interested in the dependence of one set of data on another (bivariate data), such as the distance of the last geomagnetic reversal from the ridge crest to get spreading rate and the difference in arrival times of the $P$ and $S$ seismic waves, which is related to distance from the source to the receiver.</p> <p>Today we will be focused on methods that allow us to investigate potential associations and relationships between variables. And using a classic problem from astrophysics to do so. The inspiration for this exercise came from Lecture 16 of Lisa Tauxe's Python for Earth Science Students class and some of the material is modified from those materials (https://github.com/ltauxe/Python-for-Earth-Science-Students).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#age-of-the-universe","title":"Age of the universe\u00b6","text":"<p>Today, we will focus on using the retreat velocity of galaxies and supernova as a function of their distance as our example data set. Such data underlies what has come to be known as \"Hubble's Law\" (same Hubble as for the Hubble telescope). Hubble published these results in 1929 [Hubble, E. P. (1929) Proc. Natl. Acad. Sci., 15, 168\u2013173.]</p> <p>At the time,  it was unclear whether the universe was static, expanding, or collapsing. Hubble hypothesized that if the universe were expanding, then everything in it would be moving away from us. The greater the distance between the Earth and the galaxy, the faster it must be moving.  So all that had to be done was to measure the distance and velocity of distant galaxies.  Easy-peasy - right?</p> <p>To measure velocity, Hubble made use of the doppler shift. To understand how this works, recall that the pitch you hear as an ambulance approaches changes. During doppler shift, the ambulance's pitch changes from high (as it approaches) to low (as it recedes). The pitch changes  because the relative frequency of the sound waves changes. The frequency increases as the ambulance approaches, leading to a higher pitch, and then decreases as it moves away, resulting in a lower pitch.</p> <p>Just in case you haven't had this life experience, let's listen to such a siren here: https://www.youtube.com/watch?v=imoxDcn2Sgo</p> <p>The same principle applies to light, but rather than hear a change in frequency, we observe a shift in the wavelength (the color) emitted by the galaxy. If a star or galaxy is moving away from us, its absorption bands are shifted towards longer wavelengths - the red end of the visible spectrum. The faster the star or galaxy travels away from the observer, the greater the shift will be to the red:</p> <p>So a star (or galaxy) moving away from us will have a red shift with the wavelength being spread out.</p> <p>Figures from http://www.a-levelphysicstutor.com/wav-doppler.php</p> <p>Hubble measured the red shift of different galaxies and converted them to velocities. He then estimated the distance to these objects, which is harder to do (and he was pretty far off).</p> <p>Improving such data was a major motivation of the Hubble Space Telescope. Those data and continued improvement to approaches for estimating these distances and velocities and investigating additional types of celestial objects is a major focus of ongoing research.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#type-1a-supernovae-data","title":"Type 1a supernovae data\u00b6","text":"<p>Let's import data from Freedman et al. (2000) of the distance and retreat velocity of type 1a supernovae. These supernovae are described as follows in a review paper that Freedman wrote in 2010 (https://doi.org/10.1146/annurev-astro-082708-101829):</p> <p>One of the most accurate means of measuring cosmological distances out into the Hubble flow utilizes the peak brightness of SNe Ia. The potential of supernovae for measuring distances was clear to early researchers (e.g., Baade, Minkowski, Zwicky), but it was the Hubble diagram of Kowal (1968) that set the modern course for this field, followed by decades of work by Sandage, Tammann, and collaborators (e.g., Sandage &amp; Tammann 1982, 1990; see also the review by Branch 1998). Analysis by Pskovskii (1984), followed by Phillips (1993), established a correlation between the magnitude of a SN Ia at peak brightness and the rate at which it declines, thus allowing supernova luminosities to be \u201cstandardized.\u201d This method currently probes farthest into the unperturbed Hubble flow, and it possesses very low intrinsic scatter: Freedman and Madore (2010) who then go onto describe how using Cepheid variable stars (a type of pulsating star) has allowed for the distances to be better calibrated.</p> <p>SNe Ia result from the thermonuclear runaway explosions of stars. From observations alone, the presence of SNe Ia in elliptical galaxies suggests that they do not come from massive stars. Many details of the explosion are not yet well understood, but the generally accepted view is that of a carbon-oxygen, electron-degenerate, nearly-Chandrasekharmass white dwarf orbiting in a binary system with a close companion Freedman and Madore (2010)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#determining-the-slope-of-this-line-the-hubble-constant","title":"Determining the slope of this line (the Hubble constant)\u00b6","text":"<p>We have distance on the x-axis in megaparsecs and velocity on the y-axis in km/s. The slope of this line is the Hubble constant:</p> <p>$v = H_o d$</p> <p>where $v$ is velocity, $d$ is distance, and $H_o$ is the Hubble constant.</p> <p>This looks a lot like the equation for a line through the data ($y=Ax + b$) where $A$ is the slope and $b$ is the y-intercept.  In this case, the y-intercept should be 0 or nearly so, and $m$ is $H_o$.</p> <p>So how do we find the slope?</p> <p>Here is where we can use linear regression to find the \"best fit\" line through the data. The approach is to minimize the sum of the squares of the distances (residuals) between the points and a line through them. In this illustration below, the residuals are the vertical distance between each data point and the line:</p> <p>The approach in linear regression is to find the line that minimizes the squared value of these distances all added up.</p> <p>We determine the best-fit line through this least squares approach using scikit-learn.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#fitting-a-line-with-scikit-learn","title":"Fitting a line with scikit-learn\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#using-this-linear-model-for-prediction","title":"Using this linear model for prediction\u00b6","text":"<p>What would we predict that the velocity would be for a supernova that happened to be 350 Mpc?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#evaluating-model-fit","title":"Evaluating model fit\u00b6","text":"<p>We'd also like to know who well this model fits our data (i.e. how correlated the data are). We'll use the $R^{2}$ correlation coefficient for this. $R^{2}$ is zero for uncorrelated data, and 1 for perfectly linear data (so no misfit between the model line and data).</p> <p>Review how to calculate $R^{2}$:</p> <p>$R^{2} = 1 - \\frac{SS_{res}}{SS_{tot}}$</p> <p>where $SS_{res}$ is the sum of the squares of the residuals $SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$,</p> <p>and $SS_{tot}$ is the total sum of the squares of the data $SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#evaluting-the-fit-through-plotting-residuals","title":"Evaluting the fit through plotting residuals\u00b6","text":"<p>To see how well the regression performs, the data scientist must measure how far off the estimates are from the actual values. These differences are called residuals.</p> <p>$$ \\epsilon_i = y_i - \\hat{y}_i $$</p> <p>where $\\epsilon_i$ is the residual for the $i$th data point, $y_i$ is the observed value, and $\\hat{y}_i$ is the regression estimate.</p> <p>A residual is what's left over \u2013 the residue \u2013 after estimation.</p> <p>Residuals are the vertical distances of the points from the regression line. There is one residual for each point in the scatter plot. The residual is the difference between the observed value of $y$ and the fitted value of $y$, so for the point $(x, y)$,</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#estimating-the-age-of-the-universe","title":"Estimating the age of the universe\u00b6","text":"<p>To calculate the age of the universe, we can use Hubble's law:</p> <p>We had $v=H_o d$ as Hubble's law and we know that distance = velocity x time, or,  $d=vt$.  So, if we divide both sides by $v$ and  we get:</p> <p>$1 = H_o t$.</p> <p>Solving for $t$ (the age of the universe), we get</p> <p>$t=1/H_o$ [in some weird units.]</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#using-other-data-sets-to-estimate-the-hubble-constant","title":"Using other data sets to estimate the Hubble constant\u00b6","text":"<p>Determining the Hubble constant continues to be a major avenue of astrophysical research. In fact, Wendy Freedman's group published another study (https://arxiv.org/abs/1907.05922) that is summarized in this short video:</p> <p>https://www.youtube.com/watch?v=awcnVykOKZY</p> <p>From that paper here is a visualization of Hubble constant determinations over the past 18 years:</p> <p>Let's look at another data set from the 2000 study to see how different data sets can lead to different answers.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#tully-fisher-relation-galaxy-data","title":"Tully-Fisher Relation galaxy data\u00b6","text":"<p>The total luminosity of a spiral galaxy (corrected to face-on inclination to account for extinction) is strongly correlated with the galaxy\u2019s maximum (corrected to edge-on inclination) rotation velocity. This relation, calibrated via the Leavitt Law or TRGB, becomes a powerful means of determining extragalactic distances (Tully&amp;Fisher 1977, Aaronson et al. 1986, Pierce&amp;Tully 1988, Giovanelli et al. 1997). The TF relation at present is one of the most widely applied methods for distance measurements Freedman and Madore (2010)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/04_regression/#going-even-further-out-into-the-universe","title":"Going even further out into the universe\u00b6","text":"<p>Let's look at new data sets available for the classic Hubble problem.  I found one published by Betoule et al. in 2014 http://dx.doi.org/10.1051/0004-6361/201423413.</p> <p>In this paper, data are plotted using the parameters $z$ and $\\mu$ which are related to the red shift velocity and distance.  $z$ is the fractional shift in the spectral wavelength and $\\mu$ is related to distance.</p> <p>Here is a plot from the Betoule et al. paper:</p> <p>[Figure from Betoule et al., 2014.]  These data are type Ia supernova from different observation collaborations</p> <p>Notice that they plotted the data on a log scale. (This hides some surprising things.)</p> <p>It turns out that we have been looking at data that are low-z (that is relatively close and low red shift). We  need to convert $z$ and $\\mu$ to distance and velocity to compare to the results we have considered thus far.</p> <p>According to http://hyperphysics.phy-astr.gsu.edu/hbase/Astro/hubble.html</p> <p>velocity $v$ (as fraction of the speed of light, $c$) is given by</p> <p>${v\\over c}= {{(z+1)^2-1}  \\over {(z+1)^2+1}}$</p> <p>where $c=3 \\times 10^8$ $m s^{-1}$.</p> <p>And according to the Betoule et al. (2014) paper, $\\mu$ relates to distance in parsecs $d$ like this:</p> <p>$\\mu = 5 \\log \\frac{d}{10}$</p> <p>Let's read in the data (available from this website:  http://cdsarc.u-strasbg.fr/viz-bin/qcat?J/A+A/568/A22#sRM2.2), which are averages of the data shown in the figure above,and take a peek.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/","title":"Classification1","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import pandas as pd In\u00a0[2]: Copied! <pre>basalt_data = pd.read_csv('data/Vermeesch2006.csv')\nbasalt_data.tail()\n</pre> basalt_data = pd.read_csv('data/Vermeesch2006.csv') basalt_data.tail() Out[2]: affinity SiO2_wt_percent TiO2_wt_percent Al2O3_wt_percent Fe2O3_wt_percent FeO_wt_percent CaO_wt_percent MgO_wt_percent MnO_wt_percent K2O_wt_percent Na2O_wt_percent P2O5(wt%) La_ppm Ce_ppm Pr_ppm Nd_ppm Sm_ppm Eu_ppm Gd_ppm Tb_ppm Dy_ppm Ho_ppm Er_ppm Tm_ppm Yb_ppm Lu_ppm Sc_ppm V_ppm Cr_ppm Co_ppm Ni_ppm Cu_ppm Zn_ppm Ga_ppm Rb_ppm Sr_ppm Y_ppm Zr_ppm Nb_ppm Sn_ppm Cs_ppm Ba_ppm Hf_ppm Ta_ppm Pb_ppm Th_ppm U_ppm 143Nd/144Nd 87Sr/86Sr 206Pb/204Pb 207Pb/204Pb 208Pb/204Pb 751 IAB 50.97 0.78 18.86 NaN NaN 10.85 4.71 0.16 0.60 2.38 0.13 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 36.0 34.0 152.0 NaN NaN 7.5 371.0 19.3 56.0 NaN NaN NaN 130.0 NaN NaN NaN NaN NaN NaN NaN 18.82 15.556 38.389 752 IAB 51.00 1.41 17.06 3.80 7.04 9.97 4.96 0.17 0.73 2.56 0.28 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 13.0 395.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70348 NaN NaN NaN 753 IAB 52.56 1.21 17.74 2.28 7.53 10.48 5.57 0.24 0.29 2.27 0.13 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 163.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70362 NaN NaN NaN 754 IAB 52.59 1.50 16.88 2.41 7.90 10.83 4.91 0.26 0.54 1.63 0.08 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 151.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70363 NaN NaN NaN 755 IAB 52.96 1.27 15.65 2.91 9.32 9.78 4.24 0.23 0.46 2.54 0.15 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 254.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.70352 NaN NaN NaN In\u00a0[3]: Copied! <pre>print(basalt_data.columns)\n</pre> print(basalt_data.columns) <pre>Index(['affinity', 'SiO2_wt_percent', 'TiO2_wt_percent', 'Al2O3_wt_percent',\n       'Fe2O3_wt_percent', 'FeO_wt_percent', 'CaO_wt_percent',\n       'MgO_wt_percent', 'MnO_wt_percent', 'K2O_wt_percent', 'Na2O_wt_percent',\n       'P2O5(wt%)', 'La_ppm', 'Ce_ppm', 'Pr_ppm', 'Nd_ppm', 'Sm_ppm', 'Eu_ppm',\n       'Gd_ppm', 'Tb_ppm', 'Dy_ppm', 'Ho_ppm', 'Er_ppm', 'Tm_ppm', 'Yb_ppm',\n       'Lu_ppm', 'Sc_ppm', 'V_ppm', 'Cr_ppm', 'Co_ppm', 'Ni_ppm', 'Cu_ppm',\n       'Zn_ppm', 'Ga_ppm', 'Rb_ppm', 'Sr_ppm', 'Y_ppm', 'Zr_ppm', 'Nb_ppm',\n       'Sn_ppm', 'Cs_ppm', 'Ba_ppm', 'Hf_ppm', 'Ta_ppm', 'Pb_ppm', 'Th_ppm',\n       'U_ppm', '143Nd/144Nd', '87Sr/86Sr', '206Pb/204Pb', '207Pb/204Pb',\n       '208Pb/204Pb'],\n      dtype='object')\n</pre> In\u00a0[4]: Copied! <pre># Loop through the unique values of the 'affinity' column to plot each group\n\nplt.figure(figsize=(8, 6))\n\nfor affinity in basalt_data['affinity'].unique():\n    subset = basalt_data[basalt_data['affinity'] == affinity]\n    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> # Loop through the unique values of the 'affinity' column to plot each group  plt.figure(figsize=(8, 6))  for affinity in basalt_data['affinity'].unique():     subset = basalt_data[basalt_data['affinity'] == affinity]     plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() In\u00a0[5]: Copied! <pre>basalt_data.groupby('affinity')['TiO2_wt_percent'].describe()\n</pre> basalt_data.groupby('affinity')['TiO2_wt_percent'].describe() Out[5]: count mean std min 25% 50% 75% max affinity IAB 229.0 0.965035 0.356350 0.18 0.78000 0.890 1.0700 2.70 MORB 230.0 1.469783 0.437550 0.45 1.16625 1.425 1.6975 3.53 OIB 210.0 2.907557 0.657444 1.51 2.46500 2.835 3.3275 4.81 <p>Try the other column of 'V_ppm'</p> In\u00a0[6]: Copied! <pre>basalt_data.groupby('affinity')['V_ppm'].describe()\n</pre> basalt_data.groupby('affinity')['V_ppm'].describe() Out[6]: count mean std min 25% 50% 75% max affinity IAB 160.0 292.914500 71.301634 120.0 239.500 285.5 331.000 550.0 MORB 199.0 276.795477 72.389875 27.0 237.500 272.0 309.500 510.0 OIB 166.0 266.404217 70.167589 37.6 222.575 267.5 304.775 500.0 <p>Can you differentiate between the different affinities on titanium or vanadium concentration alone?</p> <p>Let's plot the data on the TiO2 vs V plot and see if we can visually classify the data.</p> In\u00a0[7]: Copied! <pre>sample1 = {'TiO2_wt_percent': 4, 'V_ppm': 300}\nsample2 = {'TiO2_wt_percent': 1, 'V_ppm': 350}\nsample3 = {'TiO2_wt_percent': 1.6, 'V_ppm': 280}\n</pre> sample1 = {'TiO2_wt_percent': 4, 'V_ppm': 300} sample2 = {'TiO2_wt_percent': 1, 'V_ppm': 350} sample3 = {'TiO2_wt_percent': 1.6, 'V_ppm': 280} In\u00a0[8]: Copied! <pre>plt.figure(figsize=(8, 6))\n\nfor affinity in basalt_data['affinity'].unique():\n    subset = basalt_data[basalt_data['affinity'] == affinity]\n    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n\n# ADD the unknow points using the scatter function\nplt.scatter(sample1['TiO2_wt_percent'], sample1['V_ppm'], label='unknown point 1', color='black', edgecolors='white',  marker='d', s=250)\nplt.scatter(sample2['TiO2_wt_percent'], sample2['V_ppm'], label='unknown point 2', color='black', edgecolors='white',  marker='^', s=250)\nplt.scatter(sample3['TiO2_wt_percent'], sample3['V_ppm'], label='unknown point 3', color='black', edgecolors='white', marker='s', s=200)\n\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(8, 6))  for affinity in basalt_data['affinity'].unique():     subset = basalt_data[basalt_data['affinity'] == affinity]     plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)  # ADD the unknow points using the scatter function plt.scatter(sample1['TiO2_wt_percent'], sample1['V_ppm'], label='unknown point 1', color='black', edgecolors='white',  marker='d', s=250) plt.scatter(sample2['TiO2_wt_percent'], sample2['V_ppm'], label='unknown point 2', color='black', edgecolors='white',  marker='^', s=250) plt.scatter(sample3['TiO2_wt_percent'], sample3['V_ppm'], label='unknown point 3', color='black', edgecolors='white', marker='s', s=200)  plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.legend() plt.show() <p>Differences between linear regression and linear classification</p> <p>We covered linear regression last week. In linear regression, the goal is to predict a continuous value. In linear classification, the goal is to predict a discrete value. In the case of the basalt data, we are trying to predict the affinity of the basalt (MORB, OIB, IAB), making this a classification problem.</p> In\u00a0[9]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n</pre> from sklearn.linear_model import LogisticRegression In\u00a0[10]: Copied! <pre>classifier_linear = LogisticRegression(solver='liblinear')\n</pre> classifier_linear = LogisticRegression(solver='liblinear') In\u00a0[11]: Copied! <pre>basalt_data_Ti_V = basalt_data[(basalt_data['TiO2_wt_percent'].notna()) &amp; (basalt_data['V_ppm'].notna())]\n</pre> basalt_data_Ti_V = basalt_data[(basalt_data['TiO2_wt_percent'].notna()) &amp; (basalt_data['V_ppm'].notna())] In\u00a0[12]: Copied! <pre>print(f\"The number of basalt data is {len(basalt_data)}\")\n\nprint(f\"The number of basalt data with both Ti and V is {len(basalt_data_Ti_V)}\")\n</pre> print(f\"The number of basalt data is {len(basalt_data)}\")  print(f\"The number of basalt data with both Ti and V is {len(basalt_data_Ti_V)}\") <pre>The number of basalt data is 756\nThe number of basalt data with both Ti and V is 514\n</pre> <p>In machine learning literature and code conventions, uppercase \"X\" is often used to represent the matrix of feature variables (predictors), while lowercase \"y\" is used to represent the target variable (response).</p> <p>This notation is used to visually distinguish between the two variables and indicate that \"X\" is a matrix (usually with multiple columns for features), while \"y\" is a vector (usually with a single column representing the target variable).</p> <p>The uppercase \"X\" signifies a multi-dimensional data structure, and the lowercase \"y\" signifies a one-dimensional data structure.</p> In\u00a0[13]: Copied! <pre># Let's define X as the features of ['TiO2_wt_percent', 'V_ppm'] and y as the target variable 'affinity'\nX = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values\ny = basalt_data_Ti_V['affinity'].values\n</pre> # Let's define X as the features of ['TiO2_wt_percent', 'V_ppm'] and y as the target variable 'affinity' X = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values y = basalt_data_Ti_V['affinity'].values <p>The categorical variables that represent different categories that we have here are: 'MORB', 'IAB', and 'OIB'. However, most machine learning algorithms require numerical inputs.</p> <p>Label encoding is a technique that transforms the categorical variables into numerical labels. We can use the <code>sklearn.preprocessing</code> <code>LabelEncoder</code> function to do this task for us.</p> In\u00a0[14]: Copied! <pre>from sklearn.preprocessing import LabelEncoder\n\n# Encode the target variable\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n</pre> from sklearn.preprocessing import LabelEncoder  # Encode the target variable le = LabelEncoder() y_encoded = le.fit_transform(y) <p>Let's take a look at the original 'affinity' categories and the encoded categories.</p> In\u00a0[15]: Copied! <pre>print(f\"The original affinity values are {np.unique(y)}\")\nprint(f\"The encoded affinity values are {np.unique(y_encoded)}\")\n</pre> print(f\"The original affinity values are {np.unique(y)}\") print(f\"The encoded affinity values are {np.unique(y_encoded)}\") <pre>The original affinity values are ['IAB' 'MORB' 'OIB']\nThe encoded affinity values are [0 1 2]\n</pre> In\u00a0[16]: Copied! <pre>## Define the classifier\nclassifier_linear.fit(X, y_encoded)\n</pre> ## Define the classifier classifier_linear.fit(X, y_encoded) Out[16]: <pre>LogisticRegression(solver='liblinear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression(solver='liblinear')</pre> In\u00a0[17]: Copied! <pre># Generate a grid of points over the feature space\nxx, yy = np.meshgrid(np.linspace(0, max(basalt_data_Ti_V['TiO2_wt_percent']), 101),\n                     np.linspace(0, max(basalt_data_Ti_V['V_ppm']), 101))\ngrid = np.array([xx.ravel(), yy.ravel()]).T\n\nplt.figure(figsize=(4, 3))\nplt.scatter(xx, yy, s=1)\nplt.tight_layout()\n</pre> # Generate a grid of points over the feature space xx, yy = np.meshgrid(np.linspace(0, max(basalt_data_Ti_V['TiO2_wt_percent']), 101),                      np.linspace(0, max(basalt_data_Ti_V['V_ppm']), 101)) grid = np.array([xx.ravel(), yy.ravel()]).T  plt.figure(figsize=(4, 3)) plt.scatter(xx, yy, s=1) plt.tight_layout() In\u00a0[18]: Copied! <pre>grid_classes = classifier_linear.predict(grid)\n</pre> grid_classes = classifier_linear.predict(grid) <p>We can now plot up those grid with the actual data</p> In\u00a0[19]: Copied! <pre>grid_classes = grid_classes.reshape(xx.shape)\ncmap = ListedColormap(['C2', 'C0', 'C1']) # Define the colors to make the decision boundary and the data points with the same colors\n\nplt.figure(figsize=(8, 6))\n\n# Plot the decision boundary\nplt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n\n# Add real data points\nplt.scatter(basalt_data_Ti_V['TiO2_wt_percent'], basalt_data_Ti_V['V_ppm'], c=y_encoded, cmap=cmap, edgecolor='k', s=50)\n\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> grid_classes = grid_classes.reshape(xx.shape) cmap = ListedColormap(['C2', 'C0', 'C1']) # Define the colors to make the decision boundary and the data points with the same colors  plt.figure(figsize=(8, 6))  # Plot the decision boundary plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)  # Add real data points plt.scatter(basalt_data_Ti_V['TiO2_wt_percent'], basalt_data_Ti_V['V_ppm'], c=y_encoded, cmap=cmap, edgecolor='k', s=50)  plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <p>We can now plot the unknown points onto this classified grid and see what their assignment would be.</p> In\u00a0[20]: Copied! <pre>cmap = ListedColormap(['C2', 'C0', 'C1'])\ngrid_classes = grid_classes.reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\n\n# Plot the decision boundary \nplt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n\n# Add real data points\nplt.scatter(basalt_data_Ti_V['TiO2_wt_percent'], basalt_data_Ti_V['V_ppm'], c=y_encoded, cmap=cmap, edgecolor='k', s=50)\n\n# ADD the unknow points\nplt.scatter(sample1['TiO2_wt_percent'], sample1['V_ppm'], label='Unknown point 1', color='black', edgecolors='white',  marker='d', s=250)\nplt.scatter(sample2['TiO2_wt_percent'], sample2['V_ppm'], label='Unknown point 2', color='black', edgecolors='white',  marker='^', s=250)\nplt.scatter(sample3['TiO2_wt_percent'], sample3['V_ppm'], label='Unknown point 3', color='black', edgecolors='white', marker='s', s=200)\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> cmap = ListedColormap(['C2', 'C0', 'C1']) grid_classes = grid_classes.reshape(xx.shape)  plt.figure(figsize=(8, 6))  # Plot the decision boundary  plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)  # Add real data points plt.scatter(basalt_data_Ti_V['TiO2_wt_percent'], basalt_data_Ti_V['V_ppm'], c=y_encoded, cmap=cmap, edgecolor='k', s=50)  # ADD the unknow points plt.scatter(sample1['TiO2_wt_percent'], sample1['V_ppm'], label='Unknown point 1', color='black', edgecolors='white',  marker='d', s=250) plt.scatter(sample2['TiO2_wt_percent'], sample2['V_ppm'], label='Unknown point 2', color='black', edgecolors='white',  marker='^', s=250) plt.scatter(sample3['TiO2_wt_percent'], sample3['V_ppm'], label='Unknown point 3', color='black', edgecolors='white', marker='s', s=200)  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <p>While we can visually see where the points fall, we can also ask the classifier to predict the values of these unknown points using <code>classifier_svc_linear.predict()</code>.</p> <p>We can return the actual labels of the data (rather than the encoded numbers) by using <code>le.inverse_transform()</code></p> In\u00a0[21]: Copied! <pre># Predict the class of the unknown points\nprediction_point1_encoded = classifier_linear.predict([[sample1['TiO2_wt_percent'], sample1['V_ppm']]])\nprediction_point2_encoded = classifier_linear.predict([[sample2['TiO2_wt_percent'], sample2['V_ppm']]])\nprediction_point3_encoded = classifier_linear.predict([[sample3['TiO2_wt_percent'], sample3['V_ppm']]])\n\n# Decode the predicted classes into the original labels\nprediction_point1 = le.inverse_transform(prediction_point1_encoded)\nprediction_point2 = le.inverse_transform(prediction_point2_encoded)\nprediction_point3 = le.inverse_transform(prediction_point3_encoded)\n\n# Print the results\nprint(f\"Sample 1 {sample1} is classified as class {prediction_point1_encoded}, which is {prediction_point1}\")\nprint(f\"Sample 2 {sample2} is classified as class {prediction_point2_encoded}, which is {prediction_point2}\")\nprint(f\"Sample 3 {sample3} is classified as class {prediction_point3_encoded}, which is {prediction_point3}\")\n</pre> # Predict the class of the unknown points prediction_point1_encoded = classifier_linear.predict([[sample1['TiO2_wt_percent'], sample1['V_ppm']]]) prediction_point2_encoded = classifier_linear.predict([[sample2['TiO2_wt_percent'], sample2['V_ppm']]]) prediction_point3_encoded = classifier_linear.predict([[sample3['TiO2_wt_percent'], sample3['V_ppm']]])  # Decode the predicted classes into the original labels prediction_point1 = le.inverse_transform(prediction_point1_encoded) prediction_point2 = le.inverse_transform(prediction_point2_encoded) prediction_point3 = le.inverse_transform(prediction_point3_encoded)  # Print the results print(f\"Sample 1 {sample1} is classified as class {prediction_point1_encoded}, which is {prediction_point1}\") print(f\"Sample 2 {sample2} is classified as class {prediction_point2_encoded}, which is {prediction_point2}\") print(f\"Sample 3 {sample3} is classified as class {prediction_point3_encoded}, which is {prediction_point3}\") <pre>Sample 1 {'TiO2_wt_percent': 4, 'V_ppm': 300} is classified as class [2], which is ['OIB']\nSample 2 {'TiO2_wt_percent': 1, 'V_ppm': 350} is classified as class [0], which is ['IAB']\nSample 3 {'TiO2_wt_percent': 1.6, 'V_ppm': 280} is classified as class [1], which is ['MORB']\n</pre> In\u00a0[22]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n</pre> from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score In\u00a0[23]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42) In\u00a0[24]: Copied! <pre>print(f\"Number of training samples: {len(X_train)}\")\nprint(f\"Number of testing samples: {len(X_test)}\")\n</pre> print(f\"Number of training samples: {len(X_train)}\") print(f\"Number of testing samples: {len(X_test)}\") <pre>Number of training samples: 359\nNumber of testing samples: 155\n</pre> In\u00a0[25]: Copied! <pre>classifier_linear.fit(X_train, y_train)\n</pre> classifier_linear.fit(X_train, y_train) Out[25]: <pre>LogisticRegression(solver='liblinear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression(solver='liblinear')</pre> In\u00a0[26]: Copied! <pre># Make predictions on the test set\ny_pred = classifier_linear.predict(X_test)\n\n# Let's count the number of correct predictions\ncorrect_predictions = np.sum(y_pred == y_test)\ntotal_predictions = len(y_test)\nprint(f\"The accuracy of the classifier is {correct_predictions / total_predictions:.2f}\")\n\n# Evaluate the classifier using the accuracy_score function from scikit-learn\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy of the classifier is {accuracy:.2f}\")\n</pre> # Make predictions on the test set y_pred = classifier_linear.predict(X_test)  # Let's count the number of correct predictions correct_predictions = np.sum(y_pred == y_test) total_predictions = len(y_test) print(f\"The accuracy of the classifier is {correct_predictions / total_predictions:.2f}\")  # Evaluate the classifier using the accuracy_score function from scikit-learn accuracy = accuracy_score(y_test, y_pred) print(f\"The accuracy of the classifier is {accuracy:.2f}\") <pre>The accuracy of the classifier is 0.85\nThe accuracy of the classifier is 0.85\n</pre> <p>The above <code>accuracy_score</code> computes the proportion of correct predictions out of the total number of predictions. The accuracy score ranges from 0 to 1, where a higher score indicates better classification performance.</p> <p>We can make a plot that is the classification based on the training data and can then plot the test data. The fraction of points that are plotting within the correct classification corresponds to the accuracy score.</p> In\u00a0[27]: Copied! <pre>grid_classes = classifier_linear.predict(grid)\ngrid_classes = grid_classes.reshape(xx.shape)\n\ncmap = ListedColormap(['C2', 'C0', 'C1'])\n\nplt.figure()\n\n# Plot the decision boundary\nplt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n\n# Plot the test data points\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolor='k', s=50)\n\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> grid_classes = classifier_linear.predict(grid) grid_classes = grid_classes.reshape(xx.shape)  cmap = ListedColormap(['C2', 'C0', 'C1'])  plt.figure()  # Plot the decision boundary plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)  # Plot the test data points plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolor='k', s=50)  plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() <p>How well did the linear classifier do?</p> In\u00a0[28]: Copied! <pre>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix using ConfusionMatrixDisplay\nfig, ax = plt.subplots(figsize=(4, 4))\nclass_names = le.inverse_transform(np.unique(y_test)).astype(str)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\ndisp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d', colorbar=False)\n\nax.set_title('Confusion Matrix')\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nplt.show()\n</pre> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Calculate the confusion matrix cm = confusion_matrix(y_test, y_pred)  # Plot the confusion matrix using ConfusionMatrixDisplay fig, ax = plt.subplots(figsize=(4, 4)) class_names = le.inverse_transform(np.unique(y_test)).astype(str) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names) disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d', colorbar=False)  ax.set_title('Confusion Matrix') ax.set_xlabel('Predicted') ax.set_ylabel('True') plt.show()  <p>Can you interpret the confusion matrix? What does it tell you about the performance of the classifier?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#introduction-to-machine-learning-classification-of-basalt-source","title":"Introduction to machine learning: classification of basalt source\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#import-scientific-python-libraries","title":"Import scientific python libraries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#machine-learning","title":"Machine learning\u00b6","text":"<p>Text from: https://scikit-learn.org/stable/tutorial/basic/tutorial.html</p> <p>In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.</p> <p>Learning problems fall into a few categories:</p> <ul> <li><p>supervised learning, in which the data comes with additional attributes that we want to predict (https://scikit-learn.org/stable/supervised_learning.html). This problem can be either:</p> <ul> <li>regression: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.</li> <li>classification: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of a classification problem would be handwritten digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.</li> </ul> </li> <li><p>unsupervised learning, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization (https://scikit-learn.org/stable/unsupervised_learning.html).</p> </li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#training-set-and-testing-set","title":"Training set and testing set\u00b6","text":"<p>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the training set, on which we learn some properties; we call the other set the testing set, on which we test the learned properties.</p> <p>Today we will focus on classification through a supervised learning approach</p> <p>Systems doing this type of analysis are all around us. Consider a spam filter for example</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#classifying-volcanic-rocks","title":"Classifying volcanic rocks\u00b6","text":"<p>Today we are going to deal with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a week ago. During class today, we will focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).</p> <p>Igneous rocks form in a wide variety of tectonic settings, including mid-ocean ridges, ocean islands, and volcanic arcs. It is a problem of great interest to igneous petrologists to recover the original tectonic setting of mafic rocks of the past. When the geological setting alone cannot unambiguously resolve this question, the chemical composition of these rocks might contain the answer. The major, minor, and trace elemental composition of basalts shows large variations, for example as a function of formation depth (e.g., Kushiro and Kuno, 1963) --- Vermeesch (2006)</p> <p>For this analysis, we are going to use a dataset that was compiled in</p> <p>Vermeesch (2006) Tectonic discrimination of basalts with classification trees, Geochimica et Cosmochimica Acta https://doi.org/10.1016/j.gca.2005.12.016</p> <p>These data were grouped into 3 categories:</p> <ul> <li>256 Island arc basalts (IAB) from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.</li> <li>241 Mid-ocean ridge (MORB) samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.</li> <li>259 Ocean-island (OIB) samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.</li> </ul> <p>Let's look at the illustration above and determine where each of these settings are within a plate tectonic context</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#import-data","title":"Import data\u00b6","text":"<p>The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#can-geochemical-data-be-used-to-classify-the-tectonic-setting","title":"Can geochemical data be used to classify the tectonic setting?\u00b6","text":"<p>These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?</p> <p>A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. For the moment, you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.</p> <p>Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas Earth and Planetary Science Letters https://doi.org/10.1016/0012-821X(82)90120-0</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#plot-tio2-wt-vs-v-ppm","title":"Plot TiO2 (wt%) vs V (ppm)\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#use-the-pandas-groupby-function-to-group-by-affinity-and-describe-the-values-of-column-tio2_wt_percent","title":"Use the pandas groupby function to group by affinity and describe the values of column 'TiO2_wt_percent'\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#eye-test-classification-method","title":"Eye test classification method\u00b6","text":"<p>In order to classify the basalt into their affinity based on titanium and vanadium concentrations, we can use a classification method.</p> <p>The goal here is to be able to make an inference of what environment an unknown basalt formed in based on comparison to these data.</p> <p>Let's say that we have three points where their affinity is unknown.</p> <ul> <li>point 1 has TiO2 of 4% and V concentration of 300 ppm</li> <li>point 2 has TiO2 of 1% and V concentration of 350 ppm</li> <li>point 3 has TiO2 of 1.9% and V concentration of 200 ppm</li> </ul> <p>What do you think the classification of these three points should be?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#a-linear-classification","title":"A linear classification\u00b6","text":"<p>An approach that has been taken in volcanic geochemistry is to draw lines to use for classification.</p> <p>We are using the package scikit-learn in order to implement such a classification. Scikit-learn is a widely-used Python library for machine learning and data analysis. It provides a wide range of tools for data preprocessing, model selection, and evaluation. Its user-friendly interface and extensive documentation make it a popular choice for researchers, data analysts, and machine learning practitioners.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#import-sci-kit-learn","title":"Import sci-kit learn\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#define-our-classifier","title":"Define our classifier\u00b6","text":"<p>We can use <code>LogisticRegression</code> from scikit-learn as a classifier. The algorithm finds the best straight line, also called a hyperplane, to separate different groups of data.</p> <p>Once the lines have been found they can be used predict the group of new data points based on which side of the line they fall on.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#preparing-the-data-for-classification","title":"Preparing the data for classification\u00b6","text":"<p>We need to do a bit of prep work on the data first as not all of the data have Ti (wt %) and V (ppm) data.</p> <p>Let's define a new dataframe <code>basalt_data_Ti_V</code> that has the rows that contain both values. This will result in us using fewer data than is in the total dataset.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#fittrain-the-classifier","title":"Fit/train the classifier\u00b6","text":"<p>Now that we have <code>X</code> as DataFrame of <code>['TiO2_wt_percent', 'V_ppm']</code> and <code>y_encoded</code> as numerical representation of the categories we can fit the classifier to the data.</p> <p>To do this, we feed the DataFrame of the data and the array of the classification into a <code>.fit</code> function preformed on the classifier object.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#visualizing-the-decision-boundaries","title":"Visualizing the decision boundaries\u00b6","text":"<p>Let's make a 101 x 101 grid of x and y values between 0 and the maximum values.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#classify-the-grid","title":"Classify the grid\u00b6","text":"<p>We can then predict the class labels for each point in the grid.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#training-and-testing","title":"Training and testing\u00b6","text":"<p>How good is our linear classifier? To answer this we'll need to find out how frequently our classifications are correct.</p> <p>Discussion question</p> <p>How should we determine the accuracy of this classification scheme using the data that we already have?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#why-do-we-need-to-set-training-validation-and-test-datasets","title":"Why Do We Need to Set Training, Validation, and Test Datasets?\u00b6","text":"<p>When building a machine learning model, it's crucial to evaluate its performance accurately and ensure it generalizes well to new, unseen data. To achieve this, we typically split our dataset into three parts: training, validation, and test sets.</p> <ul> <li>The training set is used to train the model. The model learns the patterns and relationships within this data. The goal is to minimize the error on the training set by adjusting the model's parameters.</li> <li>The validation set is used to tune the model's hyperparameters. It helps in assessing the model's performance during the training phase and prevents overfitting. By evaluating the model on the validation set, we can choose the best model configuration.</li> <li>The test set is used to evaluate the final model's performance. It provides an unbiased estimate of how well the model will perform on new, unseen data. The test set should only be used once the model is fully trained and all hyperparameters are tuned.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#variance-and-bias-trade-off","title":"Variance and Bias Trade-Off\u00b6","text":"<p>In machine learning, the variance and bias trade-off is a fundamental concept that affects model performance.</p> <ul> <li>Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).</li> <li>Variance refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).</li> </ul> <p>Trade-Off:</p> <ul> <li>High Bias, Low Variance: The model is too simple and cannot capture the underlying patterns of the data (underfitting).</li> <li>Low Bias, High Variance: The model is too complex and captures noise in the training data (overfitting).</li> <li>Optimal Balance: The goal is to find a balance where the model has low bias and low variance, meaning it generalizes well to new data.</li> </ul> <p>By using training, validation, and test sets, we can monitor and adjust our model to achieve this balance, ensuring it performs well on unseen data.</p> <p>Let's try it in our classification problem.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#split-the-data-into-training-and-testing-sets","title":"Split the data into training and testing sets\u00b6","text":"<p>Because we are using a linear classifier, there are not many hyperparameters to tune. For simplicity, we will only divide the data into training and testing sets, using 70% of the data for training and 30% for testing.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#fit-the-model-to-the-training-data","title":"Fit the model to the training data\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#make-predictions-on-the-testing-data","title":"Make predictions on the testing data\u00b6","text":"<p>The test set was held back from training. We can use it to evaluate the model. How often are the categorizations correction? To do this evaluation, we can predict the categories using <code>classifier_svc_linear.predict()</code> and the compare those to the actual labels using <code>accuracy_score()</code></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/05_classification/#visualizing-the-classification-using-a-confusion-matrix","title":"Visualizing the classification using a \"confusion matrix\"\u00b6","text":"<p>A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It visually displays the accuracy of a classifier by comparing its predicted labels against the true labels. The matrix consists of rows and columns that represent the true and predicted classes, respectively. Each cell in the matrix corresponds to the number of samples for a specific combination of true and predicted class labels.</p> <p>The main diagonal of the matrix represents the correctly classified instances, while the off-diagonal elements represent the misclassified instances. By analyzing the confusion matrix, you can gain insights into the performance of the classifier and identify where it gets \"confused.\"</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/","title":"Classification2","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import pandas as pd  In\u00a0[2]: Copied! <pre>basalt_data = pd.read_csv('data/Vermeesch2006.csv')\nbasalt_data.tail()\n</pre> basalt_data = pd.read_csv('data/Vermeesch2006.csv') basalt_data.tail() Out[2]: affinity SiO2_wt_percent TiO2_wt_percent Al2O3_wt_percent Fe2O3_wt_percent FeO_wt_percent CaO_wt_percent MgO_wt_percent MnO_wt_percent K2O_wt_percent ... Hf_ppm Ta_ppm Pb_ppm Th_ppm U_ppm 143Nd/144Nd 87Sr/86Sr 206Pb/204Pb 207Pb/204Pb 208Pb/204Pb 751 IAB 50.97 0.78 18.86 NaN NaN 10.85 4.71 0.16 0.60 ... NaN NaN NaN NaN NaN NaN NaN 18.82 15.556 38.389 752 IAB 51.00 1.41 17.06 3.80 7.04 9.97 4.96 0.17 0.73 ... NaN NaN NaN NaN NaN NaN 0.70348 NaN NaN NaN 753 IAB 52.56 1.21 17.74 2.28 7.53 10.48 5.57 0.24 0.29 ... NaN NaN NaN NaN NaN NaN 0.70362 NaN NaN NaN 754 IAB 52.59 1.50 16.88 2.41 7.90 10.83 4.91 0.26 0.54 ... NaN NaN NaN NaN NaN NaN 0.70363 NaN NaN NaN 755 IAB 52.96 1.27 15.65 2.91 9.32 9.78 4.24 0.23 0.46 ... NaN NaN NaN NaN NaN NaN 0.70352 NaN NaN NaN <p>5 rows \u00d7 52 columns</p> In\u00a0[3]: Copied! <pre>print(basalt_data.columns)\n</pre> print(basalt_data.columns) <pre>Index(['affinity', 'SiO2_wt_percent', 'TiO2_wt_percent', 'Al2O3_wt_percent',\n       'Fe2O3_wt_percent', 'FeO_wt_percent', 'CaO_wt_percent',\n       'MgO_wt_percent', 'MnO_wt_percent', 'K2O_wt_percent', 'Na2O_wt_percent',\n       'P2O5(wt%)', 'La_ppm', 'Ce_ppm', 'Pr_ppm', 'Nd_ppm', 'Sm_ppm', 'Eu_ppm',\n       'Gd_ppm', 'Tb_ppm', 'Dy_ppm', 'Ho_ppm', 'Er_ppm', 'Tm_ppm', 'Yb_ppm',\n       'Lu_ppm', 'Sc_ppm', 'V_ppm', 'Cr_ppm', 'Co_ppm', 'Ni_ppm', 'Cu_ppm',\n       'Zn_ppm', 'Ga_ppm', 'Rb_ppm', 'Sr_ppm', 'Y_ppm', 'Zr_ppm', 'Nb_ppm',\n       'Sn_ppm', 'Cs_ppm', 'Ba_ppm', 'Hf_ppm', 'Ta_ppm', 'Pb_ppm', 'Th_ppm',\n       'U_ppm', '143Nd/144Nd', '87Sr/86Sr', '206Pb/204Pb', '207Pb/204Pb',\n       '208Pb/204Pb'],\n      dtype='object')\n</pre> In\u00a0[4]: Copied! <pre>plt.figure(figsize=(8, 6))\n\nfor affinity in basalt_data['affinity'].unique():\n    subset = basalt_data[basalt_data['affinity'] == affinity]\n    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> plt.figure(figsize=(8, 6))  for affinity in basalt_data['affinity'].unique():     subset = basalt_data[basalt_data['affinity'] == affinity]     plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show() In\u00a0[5]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.impute import SimpleImputer\n</pre> from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay from sklearn.impute import SimpleImputer  In\u00a0[6]: Copied! <pre>## Prepare the data to define the features X and the target y\nbasalt_data_Ti_V = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['V_ppm'].isna())]\nX = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values\ny = basalt_data_Ti_V['affinity'].values\n\n## Encode the target variable from string to integer\nle = LabelEncoder()\ny = le.fit_transform(y)\n</pre> ## Prepare the data to define the features X and the target y basalt_data_Ti_V = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['V_ppm'].isna())] X = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values y = basalt_data_Ti_V['affinity'].values  ## Encode the target variable from string to integer le = LabelEncoder() y = le.fit_transform(y) In\u00a0[7]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n\n## Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the model\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> from sklearn.linear_model import LogisticRegression  ## Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Train the model model = LogisticRegression(solver=\"liblinear\") model.fit(X_train, y_train)  # Predict the test set y_pred = model.predict(X_test)  # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.2f}')  # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.85\n</pre> <p>While there is a nice simplicity to the linear classifier approach comparing the $TiO_2$ vs V data, we have a lot more information from other aspects of the geochemistry. We might as well use that information as well.</p> <p>Let's use all the data we can.</p> In\u00a0[8]: Copied! <pre>basalt_data.head(1)\n</pre> basalt_data.head(1) Out[8]: affinity SiO2_wt_percent TiO2_wt_percent Al2O3_wt_percent Fe2O3_wt_percent FeO_wt_percent CaO_wt_percent MgO_wt_percent MnO_wt_percent K2O_wt_percent ... Hf_ppm Ta_ppm Pb_ppm Th_ppm U_ppm 143Nd/144Nd 87Sr/86Sr 206Pb/204Pb 207Pb/204Pb 208Pb/204Pb 0 MORB 48.2 2.52 15.2 2.31 8.56 9.69 7.15 0.17 0.9 ... 5.2 NaN 2.15 1.4881 0.4941 NaN 0.703 NaN NaN NaN <p>1 rows \u00d7 52 columns</p> In\u00a0[9]: Copied! <pre>## Prepare the data into features (X) and target (y)\nX = basalt_data.drop('affinity', axis=1)\ny = basalt_data['affinity']\n\n## We will keep the column names for later use\ncolumns = X.columns\n\n## Encode the target variable\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n## Impute missing values using median imputation\nimputer = SimpleImputer(strategy='median')\nX = imputer.fit_transform(X)\n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n</pre> ## Prepare the data into features (X) and target (y) X = basalt_data.drop('affinity', axis=1) y = basalt_data['affinity']  ## We will keep the column names for later use columns = X.columns  ## Encode the target variable le = LabelEncoder() y = le.fit_transform(y)  ## Impute missing values using median imputation imputer = SimpleImputer(strategy='median') X = imputer.fit_transform(X)  ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41) <p>Let's try to apply the same classification methods to the data using all the features.</p> In\u00a0[10]: Copied! <pre># Train the model\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> # Train the model model = LogisticRegression(solver=\"liblinear\") model.fit(X_train, y_train)  # Predict the test set y_pred = model.predict(X_test)  # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.2f}')  # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.90\n</pre> <p>How do the results compare to the linear logistic regression using only TiO2 and V?</p> In\u00a0[11]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n## Define the classifier using make_pipeline\nclassifier = make_pipeline(StandardScaler(), SVC(kernel=\"linear\"))\n\n## Train the model\nclassifier.fit(X_train, y_train)\n\n# Predict the test set\ny_pred = classifier.predict(X_test)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> from sklearn.svm import SVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler  ## Define the classifier using make_pipeline classifier = make_pipeline(StandardScaler(), SVC(kernel=\"linear\"))  ## Train the model classifier.fit(X_train, y_train)  # Predict the test set y_pred = classifier.predict(X_test)  # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.2f}')  # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.90\n</pre> <p>Let's try to classify the data using a non-linear kernel. We will use the radial basis function (RBF) kernel, which is the default kernel for the <code>SVC</code> class. The RBF kernel is defined as:</p> <p>$$K(x, x') = \\exp(-\\gamma ||x - x'||^2)$$</p> <p>where $\\gamma$ is a hyperparameter that controls the smoothness of the decision boundary.</p> In\u00a0[12]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n## Define the classifier using make_pipeline and the RBF kernel\ngamma = 0.03 ## gamma is a hyperparameter of the RBF kernel\nclassifier = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=gamma))\n\n## Train the model\nclassifier.fit(X_train, y_train)\n\n## Predict the test set\ny_pred = classifier.predict(X_test)\n\n## Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n## Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> from sklearn.svm import SVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler  ## Define the classifier using make_pipeline and the RBF kernel gamma = 0.03 ## gamma is a hyperparameter of the RBF kernel classifier = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=gamma))  ## Train the model classifier.fit(X_train, y_train)  ## Predict the test set y_pred = classifier.predict(X_test)  ## Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.2f}')  ## Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.94\n</pre> <p>The gamma parameter of the RBF kernel controls the smoothness of the decision boundary.</p> <ul> <li>A larger value of gamma will result in a more complex decision boundary, which can lead to overfitting.</li> <li>A smaller value of gamma will result in a smoother decision boundary, which can lead to underfitting.</li> </ul> <p>How can we choose the best value of gamma? Let's try a few different values and see how they affect the accuracy of the classifier on the training and test data.</p> <p>Question to consider:</p> <ul> <li>How will the training accuracy change as we increase the value of gamma?</li> <li>How will the test accuracy change as we increase the value of gamma?</li> </ul> In\u00a0[13]: Copied! <pre># Function to train SVM and return accuracies\ndef train_svm_and_get_accuracies(gamma, X_train, y_train, X_test, y_test):\n\n    classifier = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=gamma))\n    classifier.fit(X_train, y_train)\n    \n    # Calculate accuracies\n    train_accuracy = accuracy_score(y_train, classifier.predict(X_train))\n    test_accuracy = accuracy_score(y_test, classifier.predict(X_test))\n    \n    return train_accuracy, test_accuracy\n\n# Generate a range of gamma values\ngamma_values = np.logspace(-4, 1, 20)\n\n# Lists to store accuracies\ntrain_accuracies = []\ntest_accuracies = []\n\n# Calculate accuracies for each gamma value\nfor gamma in gamma_values:\n    train_acc, test_acc = train_svm_and_get_accuracies(gamma, X_train, y_train, X_test, y_test)\n    train_accuracies.append(train_acc)\n    test_accuracies.append(test_acc)\n\n# Plot accuracy vs gamma\nplt.figure(figsize=(12, 8))\nplt.semilogx(gamma_values, train_accuracies, label='Training Accuracy', marker='o')\nplt.semilogx(gamma_values, test_accuracies, label='Test Accuracy', marker='s')\nplt.xlabel('Gamma')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs Gamma (SVM with RBF Kernel)')\nplt.legend()\nplt.grid(True)\n\nidx = np.argmax(test_accuracies)\nbest_gamma = gamma_values[idx]\nplt.axvline(x=best_gamma, color='r', linestyle='--', label=f'Best Gamma: {best_gamma:.2f}\\nTraining Accuracy: {train_accuracies[idx]:.2f}\\nTest Accuracy: {test_accuracies[idx]:.2f}')\nplt.text(10**((-3 + np.log10(best_gamma)) / 2), 0.7, 'High Bias\\nUnderfitting', ha='center')\nplt.text(10**((1 + np.log10(best_gamma)) / 2), 0.7, 'High Variance\\nOverfitting', ha='center')\n\nplt.legend()\nplt.show()\n</pre> # Function to train SVM and return accuracies def train_svm_and_get_accuracies(gamma, X_train, y_train, X_test, y_test):      classifier = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=gamma))     classifier.fit(X_train, y_train)          # Calculate accuracies     train_accuracy = accuracy_score(y_train, classifier.predict(X_train))     test_accuracy = accuracy_score(y_test, classifier.predict(X_test))          return train_accuracy, test_accuracy  # Generate a range of gamma values gamma_values = np.logspace(-4, 1, 20)  # Lists to store accuracies train_accuracies = [] test_accuracies = []  # Calculate accuracies for each gamma value for gamma in gamma_values:     train_acc, test_acc = train_svm_and_get_accuracies(gamma, X_train, y_train, X_test, y_test)     train_accuracies.append(train_acc)     test_accuracies.append(test_acc)  # Plot accuracy vs gamma plt.figure(figsize=(12, 8)) plt.semilogx(gamma_values, train_accuracies, label='Training Accuracy', marker='o') plt.semilogx(gamma_values, test_accuracies, label='Test Accuracy', marker='s') plt.xlabel('Gamma') plt.ylabel('Accuracy') plt.title('Accuracy vs Gamma (SVM with RBF Kernel)') plt.legend() plt.grid(True)  idx = np.argmax(test_accuracies) best_gamma = gamma_values[idx] plt.axvline(x=best_gamma, color='r', linestyle='--', label=f'Best Gamma: {best_gamma:.2f}\\nTraining Accuracy: {train_accuracies[idx]:.2f}\\nTest Accuracy: {test_accuracies[idx]:.2f}') plt.text(10**((-3 + np.log10(best_gamma)) / 2), 0.7, 'High Bias\\nUnderfitting', ha='center') plt.text(10**((1 + np.log10(best_gamma)) / 2), 0.7, 'High Variance\\nOverfitting', ha='center')  plt.legend() plt.show() <p>Let's try a Decision Trees approach which is another supervised machine learning algorithm for classification</p> <p>Decision Trees are a type of flowchart-like structure where internal nodes represent decisions based on the input features, branches represent the outcome of these decisions, and leaf nodes represent the final output or class label. The primary goal of a decision tree is to recursively split the data into subsets based on feature values that maximize the separation between the classes.</p> <p>Why use Decision Trees?</p> <ul> <li>Easy to understand and interpret: Decision Trees are human-readable and can be visualized, making them easy to understand and interpret even for those with limited machine learning experience.</li> <li>Minimal data preprocessing: Decision Trees do not require extensive data preprocessing, such as scaling or normalization, as they can handle both numerical and categorical features.</li> <li>Non-linear relationships: Decision Trees can model complex, non-linear relationships between features and target variables.</li> <li>Feature importance: Decision Trees can provide insights into feature importance, helping to identify the most relevant features for the problem at hand.</li> </ul> In\u00a0[14]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\n\n## Define the decision tree classifier\nclassifier = DecisionTreeClassifier()\n\n## Train the model\nclassifier.fit(X_train, y_train)\n\n## Make predictions on the test set\ny_pred = classifier.predict(X_test)\n\n## Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.tree import DecisionTreeClassifier  ## Define the decision tree classifier classifier = DecisionTreeClassifier()  ## Train the model classifier.fit(X_train, y_train)  ## Make predictions on the test set y_pred = classifier.predict(X_test)  ## Evaluate the classifier accuracy = accuracy_score(y_test, y_pred) print(f\"Accuracy: {accuracy:.2f}\") <pre>Accuracy: 0.86\n</pre> In\u00a0[15]: Copied! <pre>from sklearn.tree import plot_tree\n\nfig, ax = plt.subplots(figsize=(30, 20))\n\n## Get the original class names\nclass_names = le.inverse_transform(np.unique(y)).astype(str)\n\n## Visualize the decision tree\nplot_tree(classifier, filled=True, feature_names=columns, class_names=class_names, ax=ax);\n</pre> from sklearn.tree import plot_tree  fig, ax = plt.subplots(figsize=(30, 20))  ## Get the original class names class_names = le.inverse_transform(np.unique(y)).astype(str)  ## Visualize the decision tree plot_tree(classifier, filled=True, feature_names=columns, class_names=class_names, ax=ax); In\u00a0[16]: Copied! <pre>## Get the feature importances from the classifier\nimportances = classifier.feature_importances_\n\n## Pair the feature names with their corresponding importances\nfeature_importances = list(zip(columns, importances))\n\n## Create a DataFrame from the feature importances\ndf_feature_importances = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])\n\n## Sort the feature importances in descending order\ndf_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)\n\n## Reset the index and drop the old index column\ndf_feature_importances.reset_index(drop=True, inplace=True)\n\n## Display the sorted feature importances\ndisplay(df_feature_importances)\n</pre> ## Get the feature importances from the classifier importances = classifier.feature_importances_  ## Pair the feature names with their corresponding importances feature_importances = list(zip(columns, importances))  ## Create a DataFrame from the feature importances df_feature_importances = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])  ## Sort the feature importances in descending order df_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)  ## Reset the index and drop the old index column df_feature_importances.reset_index(drop=True, inplace=True)  ## Display the sorted feature importances display(df_feature_importances)  Feature Importance 0 Sr_ppm 0.287424 1 TiO2_wt_percent 0.269707 2 Y_ppm 0.128004 3 Ni_ppm 0.060744 4 K2O_wt_percent 0.048790 5 MgO_wt_percent 0.036844 6 Sm_ppm 0.036048 7 SiO2_wt_percent 0.031882 8 Ce_ppm 0.010861 9 Zr_ppm 0.008846 10 Eu_ppm 0.008293 11 87Sr/86Sr 0.008082 12 Al2O3_wt_percent 0.007582 13 MnO_wt_percent 0.005618 14 Cr_ppm 0.005308 15 Ba_ppm 0.005213 16 Zn_ppm 0.005178 17 Tb_ppm 0.005119 18 Hf_ppm 0.005061 19 Lu_ppm 0.003895 20 P2O5(wt%) 0.003791 21 V_ppm 0.003791 22 Cs_ppm 0.003323 23 Co_ppm 0.002843 24 U_ppm 0.002810 25 Fe2O3_wt_percent 0.002704 26 206Pb/204Pb 0.002238 27 Gd_ppm 0.000000 28 La_ppm 0.000000 29 Pr_ppm 0.000000 30 Nd_ppm 0.000000 31 CaO_wt_percent 0.000000 32 FeO_wt_percent 0.000000 33 Na2O_wt_percent 0.000000 34 Dy_ppm 0.000000 35 Rb_ppm 0.000000 36 Ga_ppm 0.000000 37 Cu_ppm 0.000000 38 Ho_ppm 0.000000 39 Tm_ppm 0.000000 40 Er_ppm 0.000000 41 Yb_ppm 0.000000 42 Sc_ppm 0.000000 43 Ta_ppm 0.000000 44 Sn_ppm 0.000000 45 Nb_ppm 0.000000 46 Pb_ppm 0.000000 47 143Nd/144Nd 0.000000 48 Th_ppm 0.000000 49 207Pb/204Pb 0.000000 50 208Pb/204Pb 0.000000 <p>Question</p> <p>If we were going to build a classifier on just two variables, which should be pick?</p> In\u00a0[17]: Copied! <pre># Code source: Ga\u00ebl Varoquaux\n#              Andreas M\u00fcller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=41)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()\n</pre> # Code source: Ga\u00ebl Varoquaux #              Andreas M\u00fcller # Modified for documentation by Jaques Grobler # License: BSD 3 clause  import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_moons, make_circles, make_classification from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis  h = .02  # step size in the mesh  names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",          \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",          \"Naive Bayes\", \"QDA\"]  classifiers = [     KNeighborsClassifier(3),     SVC(kernel=\"linear\", C=0.025),     SVC(gamma=2, C=1),     GaussianProcessClassifier(1.0 * RBF(1.0)),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),     MLPClassifier(alpha=1, max_iter=1000),     AdaBoostClassifier(),     GaussianNB(),     QuadraticDiscriminantAnalysis()]  X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]  figure = plt.figure(figsize=(27, 9)) i = 1 # iterate over datasets for ds_cnt, ds in enumerate(datasets):     # preprocess dataset, split into training and test part     X, y = ds     X = StandardScaler().fit_transform(X)     X_train, X_test, y_train, y_test = \\         train_test_split(X, y, test_size=.4, random_state=41)      x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))      # just plot the dataset first     cm = plt.cm.RdBu     cm_bright = ListedColormap(['#FF0000', '#0000FF'])     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)     if ds_cnt == 0:         ax.set_title(\"Input data\")     # Plot the training points     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                edgecolors='k')     # Plot the testing points     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,                edgecolors='k')     ax.set_xlim(xx.min(), xx.max())     ax.set_ylim(yy.min(), yy.max())     ax.set_xticks(())     ax.set_yticks(())     i += 1      # iterate over classifiers     for name, clf in zip(names, classifiers):         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)         clf.fit(X_train, y_train)         score = clf.score(X_test, y_test)          # Plot the decision boundary. For that, we will assign a color to each         # point in the mesh [x_min, x_max]x[y_min, y_max].         if hasattr(clf, \"decision_function\"):             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])         else:             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]          # Put the result into a color plot         Z = Z.reshape(xx.shape)         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)          # Plot the training points         ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                    edgecolors='k')         # Plot the testing points         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,                    edgecolors='k', alpha=0.6)          ax.set_xlim(xx.min(), xx.max())         ax.set_ylim(yy.min(), yy.max())         ax.set_xticks(())         ax.set_yticks(())         if ds_cnt == 0:             ax.set_title(name)         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),                 size=15, horizontalalignment='right')         i += 1  plt.tight_layout() plt.show() <pre>/home/vscode/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n/home/vscode/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n/home/vscode/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n</pre> In\u00a0[18]: Copied! <pre>basalt_data_Ti_Sr = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['Sr_ppm'].isna())]\n</pre> basalt_data_Ti_Sr = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['Sr_ppm'].isna())] In\u00a0[19]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\n# Extract the necessary features and target variable\nX = basalt_data_Ti_Sr[['TiO2_wt_percent', 'Sr_ppm']].values\ny = basalt_data_Ti_Sr['affinity'].values\n\n# Encode the target variable 'affinity' using LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n\n# Instantiate the scaler\nscaler = StandardScaler()\n\n# Fit the scaler to the training data and transform it\nX_train_normalized = scaler.fit_transform(X_train)\n\n# Transform the testing data using the fitted scaler\nX_test_normalized = scaler.transform(X_test)\n</pre> from sklearn.preprocessing import StandardScaler  # Extract the necessary features and target variable X = basalt_data_Ti_Sr[['TiO2_wt_percent', 'Sr_ppm']].values y = basalt_data_Ti_Sr['affinity'].values  # Encode the target variable 'affinity' using LabelEncoder le = LabelEncoder() y = le.fit_transform(y)  # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)  # Instantiate the scaler scaler = StandardScaler()  # Fit the scaler to the training data and transform it X_train_normalized = scaler.fit_transform(X_train)  # Transform the testing data using the fitted scaler X_test_normalized = scaler.transform(X_test) In\u00a0[20]: Copied! <pre>from sklearn.neighbors import KNeighborsClassifier\n\n## Extract the necessary features and target variable; Filter out rows with nan values\nbasalt_data = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['Sr_ppm'].isna())]\n\n## Prepare the data into features (X) and target (y); Encode the target variable\nX = basalt_data[['TiO2_wt_percent', 'Sr_ppm']].values\ny = le.fit_transform(basalt_data['affinity'])\n\n## Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n\n## Define the KNN classifier with 3 neighbors using make_pipeline\nclassifier = make_pipeline(StandardScaler(), KNeighborsClassifier(3))\n\n## Train the model\nclassifier.fit(X_train, y_train)\n\n## Make predictions on the test set\ny_pred = classifier.predict(X_test)\n\n## Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.neighbors import KNeighborsClassifier  ## Extract the necessary features and target variable; Filter out rows with nan values basalt_data = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) &amp; (~basalt_data['Sr_ppm'].isna())]  ## Prepare the data into features (X) and target (y); Encode the target variable X = basalt_data[['TiO2_wt_percent', 'Sr_ppm']].values y = le.fit_transform(basalt_data['affinity'])  ## Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)  ## Define the KNN classifier with 3 neighbors using make_pipeline classifier = make_pipeline(StandardScaler(), KNeighborsClassifier(3))  ## Train the model classifier.fit(X_train, y_train)  ## Make predictions on the test set y_pred = classifier.predict(X_test)  ## Evaluate the classifier accuracy = accuracy_score(y_test, y_pred) print(f\"Accuracy: {accuracy:.2f}\") <pre>Accuracy: 0.94\n</pre> <p>Let's plot the decision boundary for the KNN classifier. The decision boundary is the line that separates the different classes in the feature space.</p> In\u00a0[21]: Copied! <pre># Create a meshgrid \nx_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\ny_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 20), np.linspace(y_min, y_max, 20))\n\n# Classify the grid points using the classifier\ngrid = np.c_[xx.ravel(), yy.ravel()]\ngrid_classes = classifier.predict(grid)\n\n# Reshape the predicted class labels to match the shape of the input grid\ngrid_classes = grid_classes.reshape(xx.shape)\n\n# Plot the decision boundary and the test data points\ncmap = ListedColormap(['C2', 'C0', 'C1'])\nplt.figure()\nplt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolors='k', marker='o', s=50)\n\n# Add a legend\ncbar = plt.colorbar(ticks=[0.375, 1., 1.625])\ncbar.set_ticklabels(le.inverse_transform([0, 1, 2]))\n\nplt.xlabel('Normalized TiO2_wt_percent')\nplt.ylabel('Normalized Sr_ppm')\nplt.title('Classifier (Test Data with Decision Boundary)')\nplt.show()\n</pre> # Create a meshgrid  x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1 y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 20), np.linspace(y_min, y_max, 20))  # Classify the grid points using the classifier grid = np.c_[xx.ravel(), yy.ravel()] grid_classes = classifier.predict(grid)  # Reshape the predicted class labels to match the shape of the input grid grid_classes = grid_classes.reshape(xx.shape)  # Plot the decision boundary and the test data points cmap = ListedColormap(['C2', 'C0', 'C1']) plt.figure() plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6) plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolors='k', marker='o', s=50)  # Add a legend cbar = plt.colorbar(ticks=[0.375, 1., 1.625]) cbar.set_ticklabels(le.inverse_transform([0, 1, 2]))  plt.xlabel('Normalized TiO2_wt_percent') plt.ylabel('Normalized Sr_ppm') plt.title('Classifier (Test Data with Decision Boundary)') plt.show()"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#introduction-to-machine-learning-classification-of-basalt-source","title":"Introduction to machine learning: classification of basalt source\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#import-scientific-python-libraries","title":"Import scientific python libraries\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#classifying-volcanic-rocks","title":"Classifying volcanic rocks\u00b6","text":"<p>Today we will continue to deal with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a week ago. During class today, we will focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).</p> <p>Igneous rocks form in a wide variety of tectonic settings, including mid-ocean ridges, ocean islands, and volcanic arcs. It is a problem of great interest to igneous petrologists to recover the original tectonic setting of mafic rocks of the past. When the geological setting alone cannot unambiguously resolve this question, the chemical composition of these rocks might contain the answer. The major, minor, and trace elemental composition of basalts shows large variations, for example as a function of formation depth (e.g., Kushiro and Kuno, 1963) --- Vermeesch (2006)</p> <p>For this analysis, we are going to use a dataset that was compiled in</p> <p>Vermeesch (2006) Tectonic discrimination of basalts with classification trees, Geochimica et Cosmochimica Acta https://doi.org/10.1016/j.gca.2005.12.016</p> <p>These data were grouped into 3 categories:</p> <ul> <li>256 Island arc basalts (IAB) from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.</li> <li>241 Mid-ocean ridge (MORB) samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.</li> <li>259 Ocean-island (OIB) samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.</li> </ul> <p>Let's look at the illustration above and determine where each of these settings are within a plate tectonic context</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#import-data","title":"Import data\u00b6","text":"<p>The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#can-geochemical-data-be-used-to-classify-the-tectonic-setting","title":"Can geochemical data be used to classify the tectonic setting?\u00b6","text":"<p>These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?</p> <p>A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. For the moment, you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.</p> <p>Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas Earth and Planetary Science Letters https://doi.org/10.1016/0012-821X(82)90120-0</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#plot-tio2-wt-vs-v-ppm","title":"Plot TiO2 (wt%) vs V (ppm)\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#training-and-testing","title":"Training and testing\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#import-more-sklearn-tools","title":"Import more <code>sklearn</code> tools\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#review-linear-logistic-regression","title":"Review: Linear logistic regression\u00b6","text":"<ul> <li>Using only $TiO_2$ and V, we can use logistic regression to classify the data into MORB, OIB, and IAB.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#more-classification-methods","title":"More classification methods\u00b6","text":"<p>Today we will explore a few more classification methods:</p> <ul> <li>Support vector machines</li> <li>Decision trees (Random forests)</li> <li>Nearest neighbors</li> <li>Neural networks</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#preparing-the-data-to-use-features-other-than-tio2-and-v","title":"Preparing the data to use features other than TiO2 and V\u00b6","text":"<ol> <li><p>Encode the target variable 'affinity' using LabelEncoder: The target variable 'affinity' contains categorical data, which needs to be encoded as numerical values for the decision tree classifier. The LabelEncoder from scikit-learn is used to transform the 'affinity' column into numerical labels.</p> </li> <li><p>Split the data into features (X) and target (y): The dataset is split into two parts, features (X) and the target variable (y). Features are the input variables that the classifier will use to make predictions, and the target variable is the output we want the classifier to predict.</p> </li> <li><p>Impute missing values using median imputation: most classifiers cannot handle missing values in the input data. Therefore, missing values in the dataset need to be imputed (filled in) before training the classifier. Let's use median imputation which replaces the missing values with the median of the non-missing values in the same column. We can import and use the <code>SimpleImputer</code> function.</p> </li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#support-vector-machines-svm","title":"Support Vector Machines (SVM)\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#what-is-a-support-vector-machine","title":"What is a support vector machine?\u00b6","text":"<p>Support vector machines are a type of supervised learning model that can be used for both classification and regression. The basic idea is to find the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the support vectors, which are the points that are closest to the hyperplane. The distance between the support vectors and the hyperplane is called the margin. The goal of the SVM is to maximize the margin, which is done by minimizing the norm of the weight vector. The SVM can be used for both linear and non-linear classification by using different kernel functions.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#how-does-the-svm-work","title":"How does the SVM work?\u00b6","text":"<p>Support vector machines work by finding the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the support vectors, which are the points that are closest to the hyperplane. The distance between the support vectors and the hyperplane is called the margin. The goal of the SVM is to maximize the margin, which is done by minimizing the norm of the weight vector. The SVM can be used for both linear and non-linear classification by using different kernel functions.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#what-is-a-kernel-function","title":"What is a kernel function?\u00b6","text":"<p>A kernel function is a function that takes two input vectors and returns a scalar value. The kernel function is used to map the input vectors into a higher-dimensional space where the classes are linearly separable. The most common kernel functions are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#lets-try-a-support-vector-machine-classifier-for-our-data","title":"Let's try a support vector machine classifier for our data\u00b6","text":"<p>First, we will use a linear kernel to classify the data. Same as before, we will use the <code>SVC</code> class from the <code>sklearn.svm</code> module. We will use the <code>fit</code> method to train the model on the training data and the <code>predict</code> method to make predictions on the test data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#decision-tree","title":"Decision tree\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#implement-the-decisiontreeclassifier","title":"Implement the <code>DecisionTreeClassifier</code>\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#plot-the-decision-tree","title":"Plot the decision tree\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#what-aspects-of-the-data-are-important-for-the-classification","title":"What aspects of the data are important for the classification?\u00b6","text":"<p>We want to be able to readily determine what data fields are the most important for the decision tree. We can do that by determining \"feature importance.\" The code below extracts and displays the importance score, also known as Gini importance or Mean Decrease Impurity, which is a measure of how much a feature contributes to the decision-making process of the decision tree model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#exploring-other-classification-algorithms","title":"Exploring other classification algorithms\u00b6","text":"<p>If you go to the scikit-learn homepage you will find many available classifiers: https://scikit-learn.org/stable/index.html. They are nicely illustrated in this code from the scikit-learn documentation.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#normalize-the-data","title":"Normalize the data\u00b6","text":"<p>The decision tree suggested that Ti and Sr were the biggest differentiators. Let's have a look at Ti (wt %) vs Sr (ppm) and apply different classifying algorithms. For many of these algorithms, it is essential to normalize the data. For example, the nearest neighbor is a distance. Consider that in the TiO2\u00a0(wt%) vs. Sr (ppm) or V (ppm) the y-axis and x-axis are so different (in part because of different units). So we need to normalize the data.</p> <p>We can use the <code>sklearn.preprocessing</code> function <code>StandardScaler</code> to help us here.</p> <p>Let's make a <code>basalt_data_Ti_Sr</code> dataframe and then apply the <code>StandardScaler</code> approach.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#k-nearest-neighbors","title":"K-nearest neighbors\u00b6","text":"<p>Let's try another classification method called K-nearest neighbors (KNN) which is a simple and intuitive algorithm for classification. The basic idea behind KNN is to classify a new data point based on the majority class of its K nearest neighbors in the feature space.</p> <p>Apply the <code>KNeighborsClassifier</code> to the two most important features from the decision tree classifier.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/06_classification/#a-word-of-warning","title":"A word of warning\u00b6","text":"<p>As a word of warning, we shouldn't get too carried away. Clearly, there are complexities related to this approach (our accuracy scores aren't that high). There are other types of contextual data that can give insight. For example, Shervais (1982) notes that:</p> <p>\"More specific evaluation of the tectonic setting of these and other ophiolites requires application of detailed geologic and petrologic data as well as geochemistry. The Ti/V discrimination diagram, however, is a potentially powerful adjunct to these techniques.\"</p> <p>Vermeesch adds to this point by writing:</p> <p>\"no classification method based solely on geochemical data will ever be able to perfectly determine the tectonic affinity of basaltic rocks (or other rocks for that matter) simply because there is a lot of actual overlap between the geochemistry of the different tectonic settings. Notably IABs have a much wider range of compositions than either MORBs or OIBs. Therefore, geochemical classification should never be the only basis for determining tectonic affinity. This is especially the case for rocks that have undergone alteration. In such cases, mobile elements such as Sr, which have great discriminative power, cannot be used.\"</p> <p>Additionally, we would like to be able to assign physical processes to any classification given that we are seeking insight into how the Earth works.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/","title":"PyEarth: A Python Introduction to Earth Science","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#unsupervised-learning-clustering","title":"Unsupervised Learning: Clustering","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#major-rock-types-on-earth","title":"Major Rock Types on Earth","text":"<p>Earth's crust is composed of three main rock types:</p> <ol> <li>Igneous Rocks</li> <li>Sedimentary Rocks</li> <li>Metamorphic Rocks</li> </ol> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#igneous-rocks","title":"Igneous Rocks","text":"<ul> <li>Formed from cooled and solidified magma or lava</li> <li>Examples: Granite, Basalt, Obsidian</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#sedimentary-rocks","title":"Sedimentary Rocks","text":"<ul> <li>Formed from the deposition and consolidation of sediments</li> <li>Examples: Sandstone, Limestone, Shale</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#metamorphic-rocks","title":"Metamorphic Rocks","text":"<ul> <li>Formed from pre-existing rocks under high heat and pressure</li> <li>Examples: Marble, Gneiss, Slate</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#an-imaginary-scenario-as-a-geologist","title":"An imaginary scenario as a geologist","text":"<p>Imagine you're a geologist who has collected rock samples from around the world. You've analyzed their chemical composition, but now you face a challenge:</p> <p>How do you organize these samples into different groups based on their chemical composition?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#exploring-the-data","title":"Exploring the Data","text":"<p>Let's look at our rock samples data:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display # pretty display of dataframes\n\n# Load the data\ndata = pd.read_csv('data/rock_samples.csv')\n\n# Display the first few rows\ndisplay(data.head())\n\n# Randomly select two components to plot\ncomponents = np.random.choice(data.columns, 2, replace=False)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(data[components[0]], data[components[1]])\nplt.xlabel(components[0])\nplt.ylabel(components[1])\nplt.title(f'{components[0]} vs {components[1]}')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#the-need-for-advanced-techniques","title":"The Need for Advanced Techniques","text":"<ul> <li>Simple 2D plots don't reveal clear groupings</li> <li>We have multiple chemical components (high-dimensional data)</li> <li>We need techniques to:</li> <li>Reduce the dimensionality of our data</li> <li>Find natural groupings in our data</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#dimension-reduction","title":"Dimension Reduction","text":"<p>Why do we need dimension reduction?</p> <ol> <li>Visualization of high-dimensional data</li> <li>Noise reduction</li> <li>Feature extraction</li> <li>Computational efficiency</li> </ol> <p>Common methods: - Principal Component Analysis (PCA) - t-SNE - UMAP</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>PCA finds the directions (principal components) along which our data has the most variance.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#applying-pca-to-our-rock-samples","title":"Applying PCA to Our Rock Samples","text":"<pre><code>from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Apply PCA\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(data_scaled)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(data_pca[:, 0], data_pca[:, 1])\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA of Rock Samples')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#interpreting-pca-results","title":"Interpreting PCA Results","text":"<ul> <li>The first principal component captures the most variance in the data</li> <li>The second principal component captures the second most variance</li> <li>Each data point is represented by its coordinates along these components</li> </ul> <p>Can you see any groupings in the PCA plot? How many clusters do you think there are?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#clustering","title":"Clustering","text":"<p>Why do we need clustering? - Discover patterns in data - Identify natural groups - Simplify data representation</p> <p>Common clustering methods: - K-Means - DBSCAN - Hierarchical Clustering</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#k-means-clustering","title":"K-Means Clustering","text":"<p>K-Means divides data into K clusters, each represented by its centroid.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#applying-k-means-to-rock-samples","title":"Applying K-Means to Rock Samples","text":"<pre><code>from sklearn.cluster import KMeans\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Apply K-Means\nkmeans = KMeans(n_clusters=3)\nkmeans_labels = kmeans.fit_predict(data_scaled)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(data_pca[:, 0], data_pca[:, 1], c=kmeans_labels, cmap='viridis')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('K-Means Clustering of Rock Samples')\nplt.colorbar(label='Cluster')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#dbscan-clustering","title":"DBSCAN Clustering","text":"<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds clusters based on the density of data points.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#applying-dbscan-to-rock-samples","title":"Applying DBSCAN to Rock Samples","text":"<pre><code>from sklearn.cluster import DBSCAN\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=1.5, min_samples=50)\ndbscan_labels = dbscan.fit_predict(data_scaled)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(data_pca[:, 0], data_pca[:, 1], c=dbscan_labels, cmap='viridis')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('DBSCAN Clustering of Rock Samples')\nplt.colorbar(label='Cluster')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Hierarchical clustering builds a tree of clusters.</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#applying-hierarchical-clustering-to-rock-samples","title":"Applying Hierarchical Clustering to Rock Samples","text":"<pre><code>from sklearn.cluster import AgglomerativeClustering\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Apply Hierarchical Clustering\nhierarchical = AgglomerativeClustering(n_clusters=3)\nhierarchical_labels = hierarchical.fit_predict(data_scaled)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(data_pca[:, 0], data_pca[:, 1], c=hierarchical_labels, cmap='viridis')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('Hierarchical Clustering of Rock Samples')\nplt.colorbar(label='Cluster')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#comparing-with-true-rock-types","title":"Comparing with True Rock Types","text":"<pre><code># Load true rock types\nrock_types = pd.read_csv('data/rock_types.csv')\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nfor rock_type in rock_types['rock_type'].unique():\n    data_pca_type = data_pca[rock_types['rock_type'] == rock_type]\n    plt.scatter(data_pca_type[:, 0], data_pca_type[:, 1], label=rock_type)\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('True Rock Types')\nplt.legend()\nplt.show()\n</code></pre> <p>Does your clustering method match the true rock types?</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#extra-exercise","title":"Extra Exercise","text":"<ol> <li>Try different numbers of clusters for K-Means and Hierarchical Clustering.</li> <li>Experiment with different <code>eps</code> and <code>min_samples</code> values for DBSCAN.</li> <li>Apply clustering before and after PCA. How does PCA affect clustering results?</li> <li>Apply PCA with different numbers of components and observe the effect on clustering.</li> <li>For each distribution in the previous slide, which clustering method do you think works best? Why?</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#comparing-clustering-results-with-and-without-pca","title":"Comparing Clustering Results With and Without PCA","text":"<pre><code>from sklearn.cluster import DBSCAN\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Apply PCA\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(data_scaled)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=20)\ndbscan_labels = dbscan.fit_predict(data_pca)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(data_pca[:, 0], data_pca[:, 1], c=dbscan_labels, cmap='viridis')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('DBSCAN Clustering of Rock Samples')\nplt.colorbar(label='Cluster')\nplt.show()\n</code></pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/07_clustering/#conclusion","title":"Conclusion","text":"<ul> <li>Clustering is a powerful tool for grouping similar data points</li> <li>Different clustering methods work better for different data distributions</li> <li>Dimension reduction techniques like PCA can help visualize and sometimes improve clustering results</li> <li>Always consider the nature of your data when choosing a clustering method</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/","title":"Statistics","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom math import factorial\n</pre> import matplotlib.pyplot as plt import pandas as pd import numpy as np from math import factorial In\u00a0[2]: Copied! <pre># np.random.seed(0)\nfor flip in range(0,10):\n    flip_result = np.random.choice([0, 1])\n    print(f\"Flip {flip} result: {flip_result}\")\n</pre> # np.random.seed(0) for flip in range(0,10):     flip_result = np.random.choice([0, 1])     print(f\"Flip {flip} result: {flip_result}\") <pre>Flip 0 result: 1\nFlip 1 result: 0\nFlip 2 result: 0\nFlip 3 result: 1\nFlip 4 result: 0\nFlip 5 result: 0\nFlip 6 result: 1\nFlip 7 result: 1\nFlip 8 result: 0\nFlip 9 result: 0\n</pre> <p>What does <code>np.random.seed(0)</code> do? Try running the code block above without it and with it. What do you notice?</p> <p>Now let's record how many times the result was heads. We will make a list called <code>flip_results</code> and have it be blank to start. Each time we go through the code we will append the result to the list:</p> In\u00a0[3]: Copied! <pre>flip_results = []\n\nfor flip in range(0,10):\n    flip_result = np.random.choice([0, 1])\n    flip_results.append(flip_result)\n    \nprint(f\"{flip_results = }\")\n</pre> flip_results = []  for flip in range(0,10):     flip_result = np.random.choice([0, 1])     flip_results.append(flip_result)      print(f\"{flip_results = }\") <pre>flip_results = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n</pre> <p>We can calculate how many times were heads by taking the sum of the list:</p> In\u00a0[4]: Copied! <pre>## use sum() to calculate the number of heads\nnumber_heads = sum(flip_results)\nprint(f\"{number_heads = }\")\n</pre> ## use sum() to calculate the number of heads number_heads = sum(flip_results) print(f\"{number_heads = }\") <pre>number_heads = 5\n</pre> <p>Now let's flip the coin 10 times and do that 10 times. Each time we flip it, let's record how many heads resulted from the flip.</p> In\u00a0[5]: Copied! <pre>number_heads = []\n\n## do 100 experiments\nnumber_experiments = 100\nnumber_flips = 10\nfor flip_experiment in range(number_experiments):\n\n    flip_results = []\n    \n    ## each experiment, do 10 flips\n    for flip in range(0,number_flips):\n        flip_result = np.random.choice([0, 1])\n        flip_results.append(flip_result)\n\n    ## count the number of heads in the current experiment\n    current_experiment = sum(flip_results)\n    \n    ## save the result\n    number_heads.append(current_experiment)\n\n## print the number of heads\nprint(f\"{number_heads[:10] = }\")\n</pre> number_heads = []  ## do 100 experiments number_experiments = 100 number_flips = 10 for flip_experiment in range(number_experiments):      flip_results = []          ## each experiment, do 10 flips     for flip in range(0,number_flips):         flip_result = np.random.choice([0, 1])         flip_results.append(flip_result)      ## count the number of heads in the current experiment     current_experiment = sum(flip_results)          ## save the result     number_heads.append(current_experiment)  ## print the number of heads print(f\"{number_heads[:10] = }\") <pre>number_heads[:10] = [6, 7, 5, 6, 5, 4, 8, 8, 3, 5]\n</pre> <p>Let's visualize the distribution of the number of heads.</p> In\u00a0[6]: Copied! <pre>plt.figure()\nbins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered\nplt.hist(number_heads, bins=bins, density=True, edgecolor='white')\nplt.show()\n</pre> plt.figure() bins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered plt.hist(number_heads, bins=bins, density=True, edgecolor='white') plt.show() <p>Instead of 10 experiments, let's do 1000 experiments. Plot the histogram of the result and check how the distribution changes.</p> In\u00a0[7]: Copied! <pre>number_heads = []\n\n## do 10 experiments\nnumber_experiments = 1000\nnumber_flips = 10\nfor flip_experiment in range(number_experiments):\n\n    flip_results = []\n    \n    ## each experiment, do 10 flips\n    for flip in range(0,number_flips):\n        flip_result = np.random.choice([0, 1])\n        flip_results.append(flip_result)\n\n    ## count the number of heads in the current experiment\n    current_experiment = sum(flip_results)\n    \n    ## save the result\n    number_heads.append(current_experiment)\n\n## print the number of heads\nprint(f\"{number_heads[:10] = }\")\n</pre> number_heads = []  ## do 10 experiments number_experiments = 1000 number_flips = 10 for flip_experiment in range(number_experiments):      flip_results = []          ## each experiment, do 10 flips     for flip in range(0,number_flips):         flip_result = np.random.choice([0, 1])         flip_results.append(flip_result)      ## count the number of heads in the current experiment     current_experiment = sum(flip_results)          ## save the result     number_heads.append(current_experiment)  ## print the number of heads print(f\"{number_heads[:10] = }\") <pre>number_heads[:10] = [6, 6, 3, 6, 6, 5, 8, 5, 8, 4]\n</pre> In\u00a0[8]: Copied! <pre>plt.figure()\nbins = np.arange(0, number_flips+1, 1) - 0.5 # make the bins centered\nplt.hist(number_heads, bins=bins, density=True, edgecolor='white')\nplt.show()\n</pre> plt.figure() bins = np.arange(0, number_flips+1, 1) - 0.5 # make the bins centered plt.hist(number_heads, bins=bins, density=True, edgecolor='white') plt.show()  <p>How does the distribution change as the number of experiments increases? Is the center of the distribution changing? Is the width of the distribution changing?</p> In\u00a0[9]: Copied! <pre>def binomial_probability(number_positive,positive_rate,number_attempts):\n    \"\"\"\n    This function computes the probability of getting x particular outcomes (heads) in n attempts, where p is the \n    probability of a particular outcome (head) for any given attempt (coin toss).\n    \n    Parameters\n    ----------\n    number_positive : number of a particular outcome\n    positive_rate : probability of that outcome in a given attempt\n    number_attempts : number of attempts\n    \n    Returns\n    ---------\n    prob : probability of that number of the given outcome occuring in that number of attempts\n    \"\"\"\n    x = number_positive\n    p = positive_rate\n    n = number_attempts\n\n    ## compute the binomial probability following the equation above\n    prob = factorial(n)/(factorial(x)*factorial(n-x))*(p**x)*((1-p)**(n-x))\n\n    return prob\n</pre> def binomial_probability(number_positive,positive_rate,number_attempts):     \"\"\"     This function computes the probability of getting x particular outcomes (heads) in n attempts, where p is the      probability of a particular outcome (head) for any given attempt (coin toss).          Parameters     ----------     number_positive : number of a particular outcome     positive_rate : probability of that outcome in a given attempt     number_attempts : number of attempts          Returns     ---------     prob : probability of that number of the given outcome occuring in that number of attempts     \"\"\"     x = number_positive     p = positive_rate     n = number_attempts      ## compute the binomial probability following the equation above     prob = factorial(n)/(factorial(x)*factorial(n-x))*(p**x)*((1-p)**(n-x))      return prob <p>We can use this function to calculate the probability of getting 10 heads ($x=10$) when there are 10 coin tosses ($n=10$) given with the $p$ (probability) of 0.5.</p> In\u00a0[10]: Copied! <pre>binomial_probability(10,0.5,10)\n</pre> binomial_probability(10,0.5,10) Out[10]: <pre>0.0009765625</pre> <p>Let's calculate the probability of getting [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10] heads.</p> In\u00a0[11]: Copied! <pre>## create an array of the number of heads we want to calculate the probability for\nx = np.arange(11)\nprint(f\"{x = }\")\n</pre> ## create an array of the number of heads we want to calculate the probability for x = np.arange(11) print(f\"{x = }\") <pre>x = array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n</pre> In\u00a0[12]: Copied! <pre>prob_heads = 0.5\nnumber_flips = 10\n\nprobabilities = []\nfor number_head in x:\n    prob = binomial_probability(number_head,prob_heads,number_flips)\n    probabilities.append(prob)\n    \n    print(f\"Number of heads: {number_head}, probability: {prob:.5f}\")\n</pre> prob_heads = 0.5 number_flips = 10  probabilities = [] for number_head in x:     prob = binomial_probability(number_head,prob_heads,number_flips)     probabilities.append(prob)          print(f\"Number of heads: {number_head}, probability: {prob:.5f}\") <pre>Number of heads: 0, probability: 0.00098\nNumber of heads: 1, probability: 0.00977\nNumber of heads: 2, probability: 0.04395\nNumber of heads: 3, probability: 0.11719\nNumber of heads: 4, probability: 0.20508\nNumber of heads: 5, probability: 0.24609\nNumber of heads: 6, probability: 0.20508\nNumber of heads: 7, probability: 0.11719\nNumber of heads: 8, probability: 0.04395\nNumber of heads: 9, probability: 0.00977\nNumber of heads: 10, probability: 0.00098\n</pre> <p>These probabilities are the theoretical probabilities of getting a particular number of heads in 10 coin flips. Let's verify that the histogram of the number of heads in 1000 coin flips matches the theoretical probabilities.</p> <p>Make a plot where you both plot the histogram from 1000 coin flips (using <code>plt.hist()</code> with <code>density=True</code>) and you plot the results head_numbers probabilities (using <code>plt.plot()</code>).</p> In\u00a0[13]: Copied! <pre>plt.figure(figsize=(10,5))\n\n## plot the histogram from 1000 coin flips\nbins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered\nplt.hist(number_heads, bins=bins, density=True, edgecolor='white', label='Simulated')\n\n## plot the theoretical probabilities calculated based on the binomial distribution\nplt.plot(x, probabilities, 'o-', color='red', lw=2, label='Theoretical')\n\nplt.xlabel(f'Number of heads out of {number_flips} attempts')\nplt.ylabel('Fraction of times with this number of heads') \n\nplt.title(f'Coin flip results (n={number_flips})')\n\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(10,5))  ## plot the histogram from 1000 coin flips bins = np.arange(0, number_flips+1, 1)-0.5 # make the bins centered plt.hist(number_heads, bins=bins, density=True, edgecolor='white', label='Simulated')  ## plot the theoretical probabilities calculated based on the binomial distribution plt.plot(x, probabilities, 'o-', color='red', lw=2, label='Theoretical')  plt.xlabel(f'Number of heads out of {number_flips} attempts') plt.ylabel('Fraction of times with this number of heads')   plt.title(f'Coin flip results (n={number_flips})')  plt.legend() plt.show() <p>How does the histogram of the number of heads in 1000 coin flips match the theoretical probabilities? Does this imply that the theoretical distribution i.e., the binomial distribution, is a good model for the coin flip experiment?</p> In\u00a0[14]: Copied! <pre>help(np.random.binomial)\n</pre> help(np.random.binomial) <pre>Help on built-in function binomial:\n\nbinomial(...) method of numpy.random.mtrand.RandomState instance\n    binomial(n, p, size=None)\n    \n    Draw samples from a binomial distribution.\n    \n    Samples are drawn from a binomial distribution with specified\n    parameters, n trials and p probability of success where\n    n an integer &gt;= 0 and p is in the interval [0,1]. (n may be\n    input as a float, but it is truncated to an integer in use)\n    \n    .. note::\n        New code should use the `~numpy.random.Generator.binomial`\n        method of a `~numpy.random.Generator` instance instead;\n        please see the :ref:`random-quick-start`.\n    \n    Parameters\n    ----------\n    n : int or array_like of ints\n        Parameter of the distribution, &gt;= 0. Floats are also accepted,\n        but they will be truncated to integers.\n    p : float or array_like of floats\n        Parameter of the distribution, &gt;= 0 and &lt;=1.\n    size : int or tuple of ints, optional\n        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n        ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n        a single value is returned if ``n`` and ``p`` are both scalars.\n        Otherwise, ``np.broadcast(n, p).size`` samples are drawn.\n    \n    Returns\n    -------\n    out : ndarray or scalar\n        Drawn samples from the parameterized binomial distribution, where\n        each sample is equal to the number of successes over the n trials.\n    \n    See Also\n    --------\n    scipy.stats.binom : probability density function, distribution or\n        cumulative density function, etc.\n    random.Generator.binomial: which should be used for new code.\n    \n    Notes\n    -----\n    The probability density for the binomial distribution is\n    \n    .. math:: P(N) = \\binom{n}{N}p^N(1-p)^{n-N},\n    \n    where :math:`n` is the number of trials, :math:`p` is the probability\n    of success, and :math:`N` is the number of successes.\n    \n    When estimating the standard error of a proportion in a population by\n    using a random sample, the normal distribution works well unless the\n    product p*n &lt;=5, where p = population proportion estimate, and n =\n    number of samples, in which case the binomial distribution is used\n    instead. For example, a sample of 15 people shows 4 who are left\n    handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27*15 = 4,\n    so the binomial distribution should be used in this case.\n    \n    References\n    ----------\n    .. [1] Dalgaard, Peter, \"Introductory Statistics with R\",\n           Springer-Verlag, 2002.\n    .. [2] Glantz, Stanton A. \"Primer of Biostatistics.\", McGraw-Hill,\n           Fifth Edition, 2002.\n    .. [3] Lentner, Marvin, \"Elementary Applied Statistics\", Bogden\n           and Quigley, 1972.\n    .. [4] Weisstein, Eric W. \"Binomial Distribution.\" From MathWorld--A\n           Wolfram Web Resource.\n           http://mathworld.wolfram.com/BinomialDistribution.html\n    .. [5] Wikipedia, \"Binomial distribution\",\n           https://en.wikipedia.org/wiki/Binomial_distribution\n    \n    Examples\n    --------\n    Draw samples from the distribution:\n    \n    &gt;&gt;&gt; n, p = 10, .5  # number of trials, probability of each trial\n    &gt;&gt;&gt; s = np.random.binomial(n, p, 1000)\n    # result of flipping a coin 10 times, tested 1000 times.\n    \n    A real world example. A company drills 9 wild-cat oil exploration\n    wells, each with an estimated probability of success of 0.1. All nine\n    wells fail. What is the probability of that happening?\n    \n    Let's do 20,000 trials of the model, and count the number that\n    generate zero positive results.\n    \n    &gt;&gt;&gt; sum(np.random.binomial(9, 0.1, 20000) == 0)/20000.\n    # answer = 0.38885, or 38%.\n\n</pre> <p><code>np.random.binomial( )</code> requires 2 parameters, $n$ and $p$, with an optional keyword argument <code>size</code> (if <code>size</code> is not specified, it returns a single trial). We could have used this function earlier to get the number of heads that were flipped, but the way we did it also worked.</p> <p>Let's follow the example the is given in the <code>np.random.binomial( )</code> docstring.</p> <p>A company drills 9 wild-cat oil exploration wells (high risk drilling in unproven areas), each with an estimated probability of success of 0.1. All nine wells fail. What is the probability of that happening?</p> <p>Note that success in this context means that liquid hydocarbons came out of the well. In reality, you may not consider this a success given that the result is that more hydrocarbons will be combusted as a result, leading to higher atmospheric carbon dioxide levels and associated global warming.</p> <p>If we do <code>np.random.binomial(9, 0.1, 100)</code> we will get a list of 100 values that represent the number of wells that yielded oil when there is a 10% (p = 0.1) chance of each individual well yielding oil.</p> In\u00a0[15]: Copied! <pre>success_rate = 0.1\nnumber_wells = 9\nnumber_simulations = 100\n\nnumber_success = np.random.binomial(number_wells, success_rate, number_simulations)\n\nprint(f\"{number_success[:10] = }\")\n</pre> success_rate = 0.1 number_wells = 9 number_simulations = 100  number_success = np.random.binomial(number_wells, success_rate, number_simulations)  print(f\"{number_success[:10] = }\")  <pre>number_success[:10] = array([3, 0, 2, 0, 0, 2, 0, 3, 1, 0])\n</pre> In\u00a0[16]: Copied! <pre>## Count the number of failures\nnumber_failures = np.sum(number_success == 0)\n\n## Calculate the failure rate\nfailure_rate = number_failures/number_simulations\n\nprint(f\"The number of failures is {number_failures} out of {number_simulations} simulations\")\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> ## Count the number of failures number_failures = np.sum(number_success == 0)  ## Calculate the failure rate failure_rate = number_failures/number_simulations  print(f\"The number of failures is {number_failures} out of {number_simulations} simulations\") print(f\"The failure rate is {failure_rate:.2f}\") <pre>The number of failures is 36 out of 100 simulations\nThe failure rate is 0.36\n</pre> <p>We can write a function that uses this process to simulate fraction of times that there no successful wells for a given number of wells, a given probability and a given number of simulations;</p> In\u00a0[17]: Copied! <pre>def wildcat_failure_rate(number_wells,success_rate,number_simulations):\n    '''\n    Simulate the number of times that there are no successful wells for a given number of wells and a given probability for each well.\n    \n    Parameters\n    ----------\n    number_wells : number of wells drilled in each simulation\n    success_rate : probability that each well will be successful\n    number_simulations : number of times that drilling number_wells is simulated\n    '''\n    \n    simulations = np.random.binomial(number_wells, success_rate, number_simulations)\n    number_failures = np.sum(simulations == 0)\n    failure_rate = number_failures/number_simulations\n\n    return failure_rate\n</pre> def wildcat_failure_rate(number_wells,success_rate,number_simulations):     '''     Simulate the number of times that there are no successful wells for a given number of wells and a given probability for each well.          Parameters     ----------     number_wells : number of wells drilled in each simulation     success_rate : probability that each well will be successful     number_simulations : number of times that drilling number_wells is simulated     '''          simulations = np.random.binomial(number_wells, success_rate, number_simulations)     number_failures = np.sum(simulations == 0)     failure_rate = number_failures/number_simulations      return failure_rate <p>Put the <code>wildcat_failure_rate</code> function to use</p> <p>Use the function to simulate the failure rate for the above scenario (9 wells drilled, 0.1 probability of success for each well) and do it for 10 simulations</p> In\u00a0[18]: Copied! <pre>number_wells = 9\nsuccess_rate = 0.1\nnumber_simulations = 10\n\nfailure_rate = wildcat_failure_rate(number_wells,success_rate,number_simulations)\n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> number_wells = 9 success_rate = 0.1 number_simulations = 10  failure_rate = wildcat_failure_rate(number_wells,success_rate,number_simulations)  print(f\"The failure rate is {failure_rate:.2f}\") <pre>The failure rate is 0.40\n</pre> <p>Use the function to simulate the failure rate for the same scenario for 1000 simulations</p> In\u00a0[19]: Copied! <pre>number_wells = 9\nsuccess_rate = 0.1\nnumber_simulations = 1000\n\nfailure_rate = wildcat_failure_rate(number_wells,success_rate,number_simulations)\n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> number_wells = 9 success_rate = 0.1 number_simulations = 1000  failure_rate = wildcat_failure_rate(number_wells,success_rate,number_simulations)  print(f\"The failure rate is {failure_rate:.2f}\") <pre>The failure rate is 0.39\n</pre> <p>Use the function to simulate the failure rate for 100,000 simulations</p> In\u00a0[20]: Copied! <pre>number_wells = 9\nsuccess_rate = 0.1\nnumber_simulations = 100000\n\nfailure_rate = wildcat_failure_rate(number_wells,success_rate,number_simulations)\n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> number_wells = 9 success_rate = 0.1 number_simulations = 100000  failure_rate = wildcat_failure_rate(number_wells,success_rate,number_simulations)  print(f\"The failure rate is {failure_rate:.2f}\") <pre>The failure rate is 0.39\n</pre> <p>Put the <code>binomial_probability</code> function to use</p> <p>Instead of the examples above we are simulating the result, we could directly use the theoretical binomial_probability distribution to calculate the probability without doing many simulations.</p> In\u00a0[21]: Copied! <pre>## calculate the probability of 0 successful wells, which is the failure rate\nfailure_rate = binomial_probability(number_positive=0,positive_rate=0.1,number_attempts=9)\n\nprint(f\"The failure rate is {failure_rate:.2f}\")\n</pre> ## calculate the probability of 0 successful wells, which is the failure rate failure_rate = binomial_probability(number_positive=0,positive_rate=0.1,number_attempts=9)  print(f\"The failure rate is {failure_rate:.2f}\") <pre>The failure rate is 0.39\n</pre> <p>How well does the calculated binomial_probability match the simulated wildcat_failure rates? How many times do you need to simulate the problem to get a number that matches the theoretical probability?</p> In\u00a0[22]: Copied! <pre>def poisson_probability(k,lamb):\n    \"\"\"\n    This function computes the probability of getting k particular outcomes when the expected rate is lambda.\n    \"\"\"\n    \n    ## compute the poisson probability of getting k outcomes when the expected rate is lambda\n    prob = (np.exp(-1*lamb))*(lamb**k)/factorial(k)\n    \n    return prob\n</pre> def poisson_probability(k,lamb):     \"\"\"     This function computes the probability of getting k particular outcomes when the expected rate is lambda.     \"\"\"          ## compute the poisson probability of getting k outcomes when the expected rate is lambda     prob = (np.exp(-1*lamb))*(lamb**k)/factorial(k)          return prob In\u00a0[23]: Copied! <pre>lamb = 5\nk = 6\nprob = poisson_probability(k,lamb)\nprint(f\"The probability of getting {k} outcomes when the expected rate is {lamb} is {prob*100:.3f}%\")\n</pre> lamb = 5 k = 6 prob = poisson_probability(k,lamb) print(f\"The probability of getting {k} outcomes when the expected rate is {lamb} is {prob*100:.3f}%\") <pre>The probability of getting 6 outcomes when the expected rate is 5 is 14.622%\n</pre> <p>So that result tells us that there is a 14.6% chance of observing exactly 6, but it would be much more helpful to be able to visualize the probability distribution. So let's go through and calculate the probability of seeing any number between 0 and 10. First, we can make an array between 0 and 11:</p> In\u00a0[24]: Copied! <pre>## Let's calcualte the theoretical probability of seeing different numbers of meteors when the expected rate is 5 (Southern Taurids)\ntaurid_meteor_rate = 5\nnumber_meteors_seen = np.arange(0,11)\ntaurid_meteor_sighting_probability = []\n\nfor n in number_meteors_seen:\n    prob = poisson_probability(n,taurid_meteor_rate)\n    taurid_meteor_sighting_probability.append(prob)\n    print(f\"The probability to see {n} meteors is {prob:.3f}\")\n</pre> ## Let's calcualte the theoretical probability of seeing different numbers of meteors when the expected rate is 5 (Southern Taurids) taurid_meteor_rate = 5 number_meteors_seen = np.arange(0,11) taurid_meteor_sighting_probability = []  for n in number_meteors_seen:     prob = poisson_probability(n,taurid_meteor_rate)     taurid_meteor_sighting_probability.append(prob)     print(f\"The probability to see {n} meteors is {prob:.3f}\")  <pre>The probability to see 0 meteors is 0.007\nThe probability to see 1 meteors is 0.034\nThe probability to see 2 meteors is 0.084\nThe probability to see 3 meteors is 0.140\nThe probability to see 4 meteors is 0.175\nThe probability to see 5 meteors is 0.175\nThe probability to see 6 meteors is 0.146\nThe probability to see 7 meteors is 0.104\nThe probability to see 8 meteors is 0.065\nThe probability to see 9 meteors is 0.036\nThe probability to see 10 meteors is 0.018\n</pre> <p>Based on the probability of seeing 0 meteors, what is the theoretical probability of seeing at least 1 meteor?</p> In\u00a0[25]: Copied! <pre>prob_w_meteor_sighting = 1 - taurid_meteor_sighting_probability[0]\nprint(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the Southern Taurids rate of {taurid_meteor_rate} meteors per hour\")\n</pre> prob_w_meteor_sighting = 1 - taurid_meteor_sighting_probability[0] print(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the Southern Taurids rate of {taurid_meteor_rate} meteors per hour\") <pre>The probability of seeing at least 1 meteor is 99.326% at the Southern Taurids rate of 5 meteors per hour\n</pre> In\u00a0[26]: Copied! <pre>## Plot the probability distribution\nplt.figure()\nplt.plot(number_meteors_seen,taurid_meteor_sighting_probability,label='Southern Taurids ($\\lambda = 5$)')\nplt.xlabel('Number of meteors seen')\nplt.ylabel('Probability')\nplt.legend()\nplt.show()\n</pre> ## Plot the probability distribution plt.figure() plt.plot(number_meteors_seen,taurid_meteor_sighting_probability,label='Southern Taurids ($\\lambda = 5$)') plt.xlabel('Number of meteors seen') plt.ylabel('Probability') plt.legend() plt.show() <p>When there is not an active shower the background meteor rate is about 2 an hour (although it is variable depending on time of night and season; see more here: https://www.amsmeteors.org/meteor-showers/meteor-faq/).</p> <p>Let's update our code to calculate the probability of seeing different numbers of meteors when the background rate is 2 an hour (lambda = 2).</p> <ul> <li>Calculate the probability of seeing different numbers of meteors when the background rate is 2 an hour (lambda = 2).</li> <li>Plot these probabilities alongside the probability of seeing those same numbers during the Southern Taurids shower.</li> </ul> In\u00a0[27]: Copied! <pre>## Let's calcualte the theoretical probability of seeing different numbers of meteors when the background rate is 2 an hour\n\nbackground_meteor_rate = 2\nnumber_meteors_seen = np.arange(0,11)\nbackground_sighting_probability = []\n\nfor n in number_meteors_seen:\n    \n    prob = poisson_probability(n, background_meteor_rate)\n    background_sighting_probability.append(prob)\n\n    print(f\"The probability to see {n} meteors is {prob:.3f}\")\n</pre> ## Let's calcualte the theoretical probability of seeing different numbers of meteors when the background rate is 2 an hour  background_meteor_rate = 2 number_meteors_seen = np.arange(0,11) background_sighting_probability = []  for n in number_meteors_seen:          prob = poisson_probability(n, background_meteor_rate)     background_sighting_probability.append(prob)      print(f\"The probability to see {n} meteors is {prob:.3f}\") <pre>The probability to see 0 meteors is 0.135\nThe probability to see 1 meteors is 0.271\nThe probability to see 2 meteors is 0.271\nThe probability to see 3 meteors is 0.180\nThe probability to see 4 meteors is 0.090\nThe probability to see 5 meteors is 0.036\nThe probability to see 6 meteors is 0.012\nThe probability to see 7 meteors is 0.003\nThe probability to see 8 meteors is 0.001\nThe probability to see 9 meteors is 0.000\nThe probability to see 10 meteors is 0.000\n</pre> <p>Based on the probability of seeing 0 meteors, what is the theoretical probability of seeing at least 1 meteor?</p> In\u00a0[28]: Copied! <pre>prob_w_meteor_sighting = 1 - background_sighting_probability[0]\nprint(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the background rate of {background_meteor_rate} meteors per hour\")\n</pre> prob_w_meteor_sighting = 1 - background_sighting_probability[0] print(f\"The probability of seeing at least 1 meteor is {prob_w_meteor_sighting*100:.3f}% at the background rate of {background_meteor_rate} meteors per hour\") <pre>The probability of seeing at least 1 meteor is 86.466% at the background rate of 2 meteors per hour\n</pre> In\u00a0[29]: Copied! <pre>## Plot the probability distribution\nplt.figure()\nplt.plot(number_meteors_seen,taurid_meteor_sighting_probability,label='Southern Taurids ($\\lambda = 5$)')\nplt.plot(number_meteors_seen,background_sighting_probability,label='Background meteors ($\\lambda = 2$)')\nplt.ylabel('Probability')\nplt.xlabel('Number of meteors seen')\nplt.legend()\nplt.show()\n</pre> ## Plot the probability distribution plt.figure() plt.plot(number_meteors_seen,taurid_meteor_sighting_probability,label='Southern Taurids ($\\lambda = 5$)') plt.plot(number_meteors_seen,background_sighting_probability,label='Background meteors ($\\lambda = 2$)') plt.ylabel('Probability') plt.xlabel('Number of meteors seen') plt.legend() plt.show() In\u00a0[30]: Copied! <pre>lamb = 2\nnumber_hours = 1000\nnumber_meteors = []\n\nfor hour in np.arange(number_hours):\n    number_meteor_per_hour = np.random.poisson(lamb)\n    number_meteors.append(number_meteor_per_hour)\n\n## Calculate the fraction of hours watched with at least 1 meteor sighting\nnumber_meteors = np.array(number_meteors)\nnumber_hours_w_meteor_sighting = np.sum(number_meteors &gt;= 1) # at least 1 meteor sighting\nprob_w_meteor_sighting = number_hours_w_meteor_sighting/number_hours\n\nprint(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\")\n</pre> lamb = 2 number_hours = 1000 number_meteors = []  for hour in np.arange(number_hours):     number_meteor_per_hour = np.random.poisson(lamb)     number_meteors.append(number_meteor_per_hour)  ## Calculate the fraction of hours watched with at least 1 meteor sighting number_meteors = np.array(number_meteors) number_hours_w_meteor_sighting = np.sum(number_meteors &gt;= 1) # at least 1 meteor sighting prob_w_meteor_sighting = number_hours_w_meteor_sighting/number_hours  print(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\") <pre>The probability of seeing at least 1 meteor per hour is 85.300% from 1000 hours of watching\n</pre> <ul> <li>Do the same meteor watching simulation with $\\lambda = 5$ (the Southern Taurids rate).</li> </ul> In\u00a0[31]: Copied! <pre>lamb = 5\nnumber_hours = 1000\nnumber_meteors = []\n\nfor hour in np.arange(number_hours):\n    number_meteor_per_hour = np.random.poisson(lamb)\n    number_meteors.append(number_meteor_per_hour)\n\n## Calculate the fraction of hours watched with at least 1 meteor sighting\nnumber_meteors = np.array(number_meteors)\nnumber_hours_w_meteor_sighting = np.sum(number_meteors &gt;= 1) # at least 1 meteor sighting        \nprob_w_meteor_sighting = number_hours_w_meteor_sighting/number_hours\n\nprint(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\")\n</pre> lamb = 5 number_hours = 1000 number_meteors = []  for hour in np.arange(number_hours):     number_meteor_per_hour = np.random.poisson(lamb)     number_meteors.append(number_meteor_per_hour)  ## Calculate the fraction of hours watched with at least 1 meteor sighting number_meteors = np.array(number_meteors) number_hours_w_meteor_sighting = np.sum(number_meteors &gt;= 1) # at least 1 meteor sighting         prob_w_meteor_sighting = number_hours_w_meteor_sighting/number_hours  print(f\"The probability of seeing at least 1 meteor per hour is {prob_w_meteor_sighting*100:.3f}% from {number_hours} hours of watching\") <pre>The probability of seeing at least 1 meteor per hour is 99.300% from 1000 hours of watching\n</pre> <p>The above estimate is based on Monte Carlo simulation of hundreds and thousands of hours. How does it compare to the theoretical probability?</p> <p>Hint: If your results are not similar, try increasing the number of hours simulated.</p> In\u00a0[32]: Copied! <pre>## Calculate the expected number of M5 earthquakes in a year\nM = 5\nlogN = 3.266 - 0.797 * M\nN = 10**logN\nprint(f\"The expected number of M5 earthquakes in 1 year is {N:.2f}\")\n\n## Calculate the expected number of M5 earthquakes in 30 years\nlamb = N * 30\nprint(f\"The expected number of M5 earthquakes in 30 years is {lamb:.2f}\")\n\n## Calculate the probability of observing 0-9 M5 earthquake in a year\nnumber_earthquakes = np.arange(0,10)\nearthquake_probability = []\nfor n in number_earthquakes:\n    prob = poisson_probability(n,lamb)\n    earthquake_probability.append(prob)\n    print(f\"The probability of observing {n} M5 earthquakes in a year is {prob*100:.3f}%\")\n</pre> ## Calculate the expected number of M5 earthquakes in a year M = 5 logN = 3.266 - 0.797 * M N = 10**logN print(f\"The expected number of M5 earthquakes in 1 year is {N:.2f}\")  ## Calculate the expected number of M5 earthquakes in 30 years lamb = N * 30 print(f\"The expected number of M5 earthquakes in 30 years is {lamb:.2f}\")  ## Calculate the probability of observing 0-9 M5 earthquake in a year number_earthquakes = np.arange(0,10) earthquake_probability = [] for n in number_earthquakes:     prob = poisson_probability(n,lamb)     earthquake_probability.append(prob)     print(f\"The probability of observing {n} M5 earthquakes in a year is {prob*100:.3f}%\") <pre>The expected number of M5 earthquakes in 1 year is 0.19\nThe expected number of M5 earthquakes in 30 years is 5.73\nThe probability of observing 0 M5 earthquakes in a year is 0.325%\nThe probability of observing 1 M5 earthquakes in a year is 1.861%\nThe probability of observing 2 M5 earthquakes in a year is 5.332%\nThe probability of observing 3 M5 earthquakes in a year is 10.183%\nThe probability of observing 4 M5 earthquakes in a year is 14.587%\nThe probability of observing 5 M5 earthquakes in a year is 16.715%\nThe probability of observing 6 M5 earthquakes in a year is 15.962%\nThe probability of observing 7 M5 earthquakes in a year is 13.065%\nThe probability of observing 8 M5 earthquakes in a year is 9.357%\nThe probability of observing 9 M5 earthquakes in a year is 5.957%\n</pre> In\u00a0[33]: Copied! <pre>## Plot the probability distribution\nplt.figure()\nplt.plot(number_earthquakes,earthquake_probability,label='M5 earthquakes ($\\lambda = 10^{3.266-0.797M}$)')\nplt.ylabel('Probability')\nplt.xlabel('Number of M5 earthquakes in 30 years')\nplt.legend()\nplt.show()\n</pre> ## Plot the probability distribution plt.figure() plt.plot(number_earthquakes,earthquake_probability,label='M5 earthquakes ($\\lambda = 10^{3.266-0.797M}$)') plt.ylabel('Probability') plt.xlabel('Number of M5 earthquakes in 30 years') plt.legend() plt.show()  <p>How does the probability change with the number of events? How does that reconcile with the rate of M5 earthquakes in 30 years?</p> In\u00a0[34]: Copied! <pre>## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 30 years\nmagnitudes = [4, 5, 6, 7]\nnumber_years = 30\nfor M in magnitudes:\n    logN = 3.266 - 0.797 * M\n    N = 10**logN\n    lamb = N * number_years\n    prob0 = poisson_probability(0,lamb)\n\n    prob = 1 - prob0\n    print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\") \n</pre> ## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 30 years magnitudes = [4, 5, 6, 7] number_years = 30 for M in magnitudes:     logN = 3.266 - 0.797 * M     N = 10**logN     lamb = N * number_years     prob0 = poisson_probability(0,lamb)      prob = 1 - prob0     print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\")  <pre>The probability of observing at least one M4 earthquake in 30 years is 100.000%\nThe probability of observing at least one M5 earthquake in 30 years is 99.675%\nThe probability of observing at least one M6 earthquake in 30 years is 59.923%\nThe probability of observing at least one M7 earthquake in 30 years is 13.578%\n</pre> <p>How do the probabilities change if a 10 year period is considered?</p> In\u00a0[35]: Copied! <pre>## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 10 years\nmagnitudes = [4, 5, 6, 7]\nnumber_years = 10\nfor M in magnitudes:\n    logN = 3.266 - 0.797 * M\n    N = 10**logN\n    lamb = N * number_years\n    prob0 = poisson_probability(0,lamb)\n    prob = 1 - prob0\n    print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\") \n</pre> ## Calculate the probability of observing at least one M4, M5, M6 and M7 earthquake in 10 years magnitudes = [4, 5, 6, 7] number_years = 10 for M in magnitudes:     logN = 3.266 - 0.797 * M     N = 10**logN     lamb = N * number_years     prob0 = poisson_probability(0,lamb)     prob = 1 - prob0     print(f\"The probability of observing at least one M{M} earthquake in {number_years} years is {prob*100:.3f}%\")   <pre>The probability of observing at least one M4 earthquake in 10 years is 99.999%\nThe probability of observing at least one M5 earthquake in 10 years is 85.190%\nThe probability of observing at least one M6 earthquake in 10 years is 26.272%\nThe probability of observing at least one M7 earthquake in 10 years is 4.748%\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#probability-distributions-meteor-shower-gazing","title":"Probability distributions &amp; meteor shower gazing\u00b6","text":"<p>Our goals for today:</p> <ul> <li>Discuss some key statistics topics: samples versus populations and empirical versus theorectical distributions</li> <li>Simulate a head/tail coin toss and well drilling i.e. Binomial distribution</li> <li>Simulate meteors entering Earth's atmosphere i.e. Poisson distribution</li> <li>Simulate geomagnetic polarity reversals i.e. Gamma distribution</li> <li>Use Gutenberg-Richter to assess earthquake probability</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#setup","title":"Setup\u00b6","text":"<p>Run this cell as it is to setup your environment.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#case-1-flipping-a-coin","title":"Case 1: Flipping a coin\u00b6","text":"<p>Let's pretend we are flipping a coin 10 times using <code>np.random.choice([0, 1])</code>. How many times will be get heads? 1 is heads, 0 is tails. Let's use a for loop and get Python to simulate such a coin flip scenario for us.</p> <p>What for loops do is take a chunk of code (in Python the chunk that is indented) being run multiple times. In this case, the code will get looped through 10 times -- specified by <code>range(0,10)</code>.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#binomial-distribution","title":"Binomial distribution:\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#theoretical","title":"Theoretical\u00b6","text":"<p>A relatively straight-forward distribution is the binomial distribution which describes the probability of a particular outcome when there are only two possibilities (yes or no, heads or tails, 1 or 0).</p> <p>For example, in a coin toss experiment (heads or tails), if we flip the coin  $n$ times, what is the probability of getting $x$ 'heads'?  We assume that the probability $p$ of a head for any given coin toss is 50%; put another way $p$ = 0.5.</p> <p>The binomial distribution can be described by an equation:</p> <p>$$P=f(x,p,n)= \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$$</p> <p>We can look at this kind of distribution by evaluating the probability for getting $x$ 'heads' out of $n$ attempts. We'll code the equation as a function, and calculate the probability $P$ of a particular outcome (e.g., $x$ heads in $n$ attempts).</p> <p>Note that for a coin toss, $p$ is 0.5, but other yes/no questions can be investigated as well (e.g., chance of finding a fossil in a sedimentary layer; whether or not a landslide occurs following an earthquake).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#empirical","title":"Empirical\u00b6","text":"<p>The type of sampling we were doing above where we were flipping coins is called a Monte Carlo simulation. We can use simulate data from all sorts of distributions. Let's keep focusing on the binomial distribution and look at using the <code>np.random.binomial</code> function.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#poisson-distribution","title":"Poisson distribution:\u00b6","text":"<p>A Poisson Distribution gives the probability of a number of events in an interval generated by a Poisson process: the average time between events is known, but the exact timing of events is random. The events must be independent and may occur only one at a time.</p> <p>Within Earth and Planetary Science there are many processes that approximately meet this criteria.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#theoretical","title":"Theoretical\u00b6","text":"<p>The Poisson distribution gives the probability that an event (with two possible outcomes) occurs $k$ number of times in an interval of time where $\\lambda$ is the expected rate of occurance. The Poisson distribution is the limit of the binomial distribution for large $n$. So if you take the limit of the binomial distribution as $n \\rightarrow \\infty$ you'll get the Poisson distribution:</p> <p>$$P(k) = e^{-\\lambda}\\frac{\\lambda^{k}}{k!}$$</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#observing-meteors","title":"Observing meteors\u00b6","text":"<p>From https://www.amsmeteors.org/meteor-showers/meteor-faq/:</p> <p>How big are most meteoroids? How fast do they travel? The majority of visible meteors are caused by particles ranging in size from about that of a small pebble down to a grain of sand, and generally weigh less than 1-2 grams. Those of asteroid origin can be composed of dense stony or metallic material (the minority) while those of cometary origin (the majority) have low densities and are composed of a \u201cfluffy\u201d conglomerate of material, frequently called a \u201cdustball.\u201d The brilliant flash of light from a meteor is not caused so much by the meteoroid\u2019s mass, but by its high level of kinetic energy as it collides with the atmosphere.</p> <p>Meteors enter the atmosphere at speeds ranging from 11 km/sec (25,000 mph), to 72 km/sec (160,000 mph!). When the meteoroid collides with air molecules, its high level of kinetic energy rapidly ionizes and excites a long, thin column of atmospheric atoms along the meteoroid\u2019s path, creating a flash of light visible from the ground below. This column, or meteor trail, is usually less than 1 meter in diameter, but will be tens of kilometers long.</p> <p>The wide range in meteoroid speeds is caused partly by the fact that the Earth itself is traveling at about 30 km/sec (67,000 mph) as it revolves around the sun. On the evening side, or trailing edge of the Earth, meteoroids must catch up to the earth\u2019s atmosphere to cause a meteor, and tend to be slow. On the morning side, or leading edge of the earth, meteoroids can collide head-on with the atmosphere and tend to be fast.</p> <p>What is a meteor shower? Does a shower occur \u201call at once\u201d or over a period of time? Most meteor showers have their origins with comets. Each time a comet swings by the sun, it produces copious amounts of meteoroid sized particles which will eventually spread out along the entire orbit of the comet to form a meteoroid \u201cstream.\u201d If the Earth\u2019s orbit and the comet\u2019s orbit intersect at some point, then the Earth will pass through this stream for a few days at roughly the same time each year, encountering a meteor shower. The only major shower clearly shown to be non-cometary is the Geminid shower, which share an orbit with the asteroid (3200 Phaethon): one that comes unusually close to the sun as well as passing through the earth\u2019s orbit. Most shower meteoroids appear to be \u201cfluffy\u201d, but the Geminids are much more durable as might be expected from asteroid fragments.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#observing-the-southern-taurids-meteor-shower","title":"Observing the Southern Taurids meteor shower\u00b6","text":"<p>Let's say you are planning a camping trip to go out try to see shooting stars next Fall in a rural location. You are looking at a date in October and that there is an active shower:</p> <p>Southern Taurids</p> <p>Active from September 28th to December 2, 2021. The peak is November 4-5, 2021</p> <p>The Southern Taurids are a long-lasting shower that reaches a barely noticeable maximum on October 9 or 10. The shower is active for more than two months but rarely produces more than five shower members per hour, even at maximum activity. The Taurids (both branches) are rich in fireballs and are often responsible for increased number of fireball reports from September through November. https://www.amsmeteors.org/meteor-showers/meteor-shower-calendar/</p> <p>At a rate of 5 observed meteors per hour, what is the probability of observing 6 meteors in an hour?</p> <p>We can use the Poisson probability function to answer this question:</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#simulate-meteor-observing","title":"Simulate meteor observing\u00b6","text":"<p>There are many cases where it can be useful to simulate data sets. In this case, one could simulate what your experience could be in terms of the number of hours you could spend looking at the night sky and seeing 1 meteor or more on a normal night vs. a night with the Southern Taurids shower ongoing.</p> <p>We can use the <code>np.random.poisson</code> function to simulate 'realistic' data.</p> <p>Recall the Poisson distribution represents the probability of observing $k$ events when the expected rate is $\\lambda$.</p> <p>The Poisson distribution assumes that the events are independent and occur at a constant rate. Each call to <code>np.random.poisson( )</code> is doing an experiment following the Poisson distribution to see how many events occur.</p> <p>Let's try it with $\\lambda = 2$ (the background rate) and watch the sky for 1000 hours.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#extension-earthquake-probability","title":"Extension: Earthquake Probability\u00b6","text":"<p>The occurrence of earthquakes is also a Poisson process, events occur randomly in time, and the average recurrence can be determined from Gutenberg-Richter.</p> <p>The Gutenberg-Richter statistic which gives the annual rate of earthquakes above a given magnitude.</p> <p>Assumeing the Gutenberg-Richter relationshipfor the San Franciso Bay Area: log10(N)= 3.266 - 0.797M, for each year.</p> <p>Let's apply the Poisson distribution to this problem.</p> <p>So $\\lambda = N * {\\Delta}time$, where N is the annual rate. It is common to consider ${\\Delta}time=30 yrs$.</p> <p>Use the Poisson's distribution to find the probability of 0 to 9 M5 events in a 30 year period.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/08_probabilities/#poisson-probability-of-1-or-more-earthquakes","title":"Poisson Probability of 1 or more earthquakes\u00b6","text":"<p>The Poisson probability of zero events has an interesting use in characterizing earthquake hazard.</p> <p>$P(k=0)=e^{-\\lambda}$</p> <p>The complement of the zero event probability is the probability of 1 or more earthquakes occuring in the period of time. It is this probability that is used in earthquake forecast reports. The probability of one or more events is written as;</p> <p>$P(k &gt;= 1) = 1 - e^{-\\lambda}$</p> <p>Determine the probability of 1 or more M4, M5, M6 and M7 in a 30 year period.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/09_neural_networks1/","title":"Neural Networks 1","text":"<p>Created by Minh-Chien Trinh, Jeonbuk National University,</p> <p>If you want to learn more about Deep Learning (Deep Nerual Networks), you can check the Deep Learning Specialization on Coursera or watch videos on YouTube.</p> <p>In this lecture, we will use PyTorch to build and train neural networks. The pytorch library is a powerful tool for building and training neural networks. It provides a flexible and efficient library for deep learning. It is also currently the most popular library for deep learning.</p> <p> </p> <p>Let's import the necessary libraries of PyTorch and other libraries.</p> In\u00a0[1]: Copied! <pre>## First part of this semester\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n## Second part of this semester\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n## Last part of this semester\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n</pre> ## First part of this semester import matplotlib.pyplot as plt import numpy as np import pandas as pd  ## Second part of this semester from sklearn.impute import SimpleImputer from sklearn.metrics import r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder  ## Last part of this semester import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim <pre>Matplotlib is building the font cache; this may take a moment.\n</pre> In\u00a0[2]: Copied! <pre>## Set random seed\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n</pre> ## Set random seed torch.manual_seed(0) torch.cuda.manual_seed(0) np.random.seed(0) <ul> <li>Load the Betoule data</li> </ul> In\u00a0[3]: Copied! <pre>## Load the Betoule data\n# betoule_data = pd.read_csv('data/mu_z.csv',header=1) ## reading from local file\nbetoule_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/mu_z.csv',header=1) ## reading from github for running on colab\nbetoule_data.head()\n\n## Apply processing to convert to distance and velocity\n# speed of light in km/s\nc = 2.9979e8 / 1000 \n\n# the formula for v from z (and c)\nbetoule_data['velocity'] = c * (((betoule_data['z']+1.)**2-1.)/((betoule_data['z']+1.)**2+1.)) \n\n# convert mu to Gpc\nbetoule_data['distance'] = 10000*(10.**((betoule_data['mu'])/5.))*1e-9\n</pre> ## Load the Betoule data # betoule_data = pd.read_csv('data/mu_z.csv',header=1) ## reading from local file betoule_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/mu_z.csv',header=1) ## reading from github for running on colab betoule_data.head()  ## Apply processing to convert to distance and velocity # speed of light in km/s c = 2.9979e8 / 1000   # the formula for v from z (and c) betoule_data['velocity'] = c * (((betoule_data['z']+1.)**2-1.)/((betoule_data['z']+1.)**2+1.))   # convert mu to Gpc betoule_data['distance'] = 10000*(10.**((betoule_data['mu'])/5.))*1e-9 In\u00a0[4]: Copied! <pre>## Review the data\nplt.figure()\nplt.scatter(betoule_data['distance'],betoule_data['velocity'])\nplt.xlabel('Distance (Mpc)')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.show()\n</pre> ## Review the data plt.figure() plt.scatter(betoule_data['distance'],betoule_data['velocity']) plt.xlabel('Distance (Mpc)') plt.ylabel('Velocity (km s$^{-1}$)') plt.show()  <ul> <li>Prepare the data into features (X) and target (y). This is same as the previous lecture.</li> </ul> In\u00a0[5]: Copied! <pre>## Define features (X) and target (y)\nX = betoule_data[['distance']].values\ny = betoule_data[['velocity']].values\n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n</pre> ## Define features (X) and target (y) X = betoule_data[['distance']].values y = betoule_data[['velocity']].values  ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41) <ul> <li>Let's start to build the first neural network model to fit the Betoule data.</li> </ul> In\u00a0[6]: Copied! <pre>## Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\n## Normalize the data to make the training process more efficient\nmagnitude_X = 10**int(np.log10(X.max()))\nmagnitude_y = 10**int(np.log10(y.max()))\nX_train_tensor = X_train_tensor / magnitude_X\ny_train_tensor = y_train_tensor / magnitude_y\nX_test_tensor = X_test_tensor / magnitude_X\ny_test_tensor = y_test_tensor / magnitude_y\n\n## Define the neural network model\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size):\n        super(SimpleNN, self).__init__()\n        ## Define the neural network layers\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)  \n    \n    def forward(self, x):\n        ## Apply the neural network layers\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x\n\n## Initialize the model, loss function, and optimizer\ninput_size = X.shape[-1]\noutput_size = 1 # Output layer for regression (1 output neuron)\nhidden_size = 16\n\nmodel = SimpleNN(input_size, output_size, hidden_size)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)\n\n## Define fit function\ndef fit(model, X, y, epochs=100):\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = model(X)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if (epoch+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n    return losses\n\n## Define predict function\ndef predict(model, X):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(X)\n    return outputs\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=100)\n\n## Plot the loss during the training process\nplt.figure()\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Convert data to PyTorch tensors X_train_tensor = torch.tensor(X_train, dtype=torch.float32) y_train_tensor = torch.tensor(y_train, dtype=torch.float32) X_test_tensor = torch.tensor(X_test, dtype=torch.float32) y_test_tensor = torch.tensor(y_test, dtype=torch.float32)  ## Normalize the data to make the training process more efficient magnitude_X = 10**int(np.log10(X.max())) magnitude_y = 10**int(np.log10(y.max())) X_train_tensor = X_train_tensor / magnitude_X y_train_tensor = y_train_tensor / magnitude_y X_test_tensor = X_test_tensor / magnitude_X y_test_tensor = y_test_tensor / magnitude_y  ## Define the neural network model class SimpleNN(nn.Module):     def __init__(self, input_size, output_size, hidden_size):         super(SimpleNN, self).__init__()         ## Define the neural network layers         self.fc1 = nn.Linear(input_size, hidden_size)         self.fc2 = nn.Linear(hidden_size, output_size)            def forward(self, x):         ## Apply the neural network layers         x = self.fc1(x)         x = F.relu(x)         x = self.fc2(x)         return x  ## Initialize the model, loss function, and optimizer input_size = X.shape[-1] output_size = 1 # Output layer for regression (1 output neuron) hidden_size = 16  model = SimpleNN(input_size, output_size, hidden_size) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.1)  ## Define fit function def fit(model, X, y, epochs=100):     model.train()     losses = []     for epoch in range(epochs):         optimizer.zero_grad()         outputs = model(X)         loss = criterion(outputs, y)         loss.backward()         optimizer.step()         losses.append(loss.item())         if (epoch+1) % 10 == 0:             print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')     return losses  ## Define predict function def predict(model, X):     model.eval()     with torch.no_grad():         outputs = model(X)     return outputs  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=100)  ## Plot the loss during the training process plt.figure() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('Loss') plt.show()  <pre>Epoch [10/100], Loss: 0.0607\nEpoch [20/100], Loss: 0.0178\nEpoch [30/100], Loss: 0.0206\nEpoch [40/100], Loss: 0.0087\nEpoch [50/100], Loss: 0.0040\nEpoch [60/100], Loss: 0.0028\nEpoch [70/100], Loss: 0.0019\nEpoch [80/100], Loss: 0.0012\nEpoch [90/100], Loss: 0.0009\nEpoch [100/100], Loss: 0.0007\n</pre> <ul> <li>Evaluate the model on the test set. This is same as the previous lecture.</li> </ul> In\u00a0[7]: Copied! <pre>## Predict on the test set\ny_pred_tensor = predict(model, X_test_tensor)\ny_pred = y_pred_tensor.numpy() * magnitude_y\n\n## Calculate R-squared metric\nr2 = r2_score(y_test, y_pred)\nprint(f'R-squared: {r2:.4f}')\n</pre> ## Predict on the test set y_pred_tensor = predict(model, X_test_tensor) y_pred = y_pred_tensor.numpy() * magnitude_y  ## Calculate R-squared metric r2 = r2_score(y_test, y_pred) print(f'R-squared: {r2:.4f}') <pre>R-squared: 0.9950\n</pre> In\u00a0[8]: Copied! <pre>## Plot the results\n\n## Predict on the whole dataset for plotting\nX_tensor = torch.tensor(X, dtype=torch.float32)\nX_tensor = X_tensor / magnitude_X\ny_pred_tensor = predict(model, X_tensor)\ny_pred = y_pred_tensor.numpy() * magnitude_y\ny_pred = y_pred.squeeze() # remove the extra dimension\n\nplt.figure(figsize=(6, 6))\nplt.subplot(2,1,1)\nplt.scatter(betoule_data['distance'],betoule_data['velocity'])\nplt.plot(betoule_data['distance'],y_pred,color='orange',)\nplt.title('data and a nerual network fit')\nplt.ylabel('Velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.subplot(2,1,2)\nplt.scatter(betoule_data['distance'],betoule_data['velocity']-y_pred)\nplt.title('residuals of nerual network fit')\nplt.ylabel('Residual velocity (km s$^{-1}$)')\nplt.xlabel('Distance (Mpc)')\n\nplt.tight_layout()\nplt.show()\n</pre> ## Plot the results  ## Predict on the whole dataset for plotting X_tensor = torch.tensor(X, dtype=torch.float32) X_tensor = X_tensor / magnitude_X y_pred_tensor = predict(model, X_tensor) y_pred = y_pred_tensor.numpy() * magnitude_y y_pred = y_pred.squeeze() # remove the extra dimension  plt.figure(figsize=(6, 6)) plt.subplot(2,1,1) plt.scatter(betoule_data['distance'],betoule_data['velocity']) plt.plot(betoule_data['distance'],y_pred,color='orange',) plt.title('data and a nerual network fit') plt.ylabel('Velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.subplot(2,1,2) plt.scatter(betoule_data['distance'],betoule_data['velocity']-y_pred) plt.title('residuals of nerual network fit') plt.ylabel('Residual velocity (km s$^{-1}$)') plt.xlabel('Distance (Mpc)')  plt.tight_layout() plt.show()  <ul> <li>Compare the results with previous polynomial regression. How does the neural network perform?</li> </ul> <ul> <li>Load the basalt affinity data</li> </ul> In\u00a0[9]: Copied! <pre>## Load the basalt affinity data\n# basalt_data = pd.read_csv('data/Vermeesch2006.csv')\nbasalt_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/Vermeesch2006.csv') # for running on colab\nbasalt_data.tail()\n</pre> ## Load the basalt affinity data # basalt_data = pd.read_csv('data/Vermeesch2006.csv') basalt_data = pd.read_csv('https://raw.githubusercontent.com/AI4EPS/EPS88_PyEarth/refs/heads/main/docs/scripts/data/Vermeesch2006.csv') # for running on colab basalt_data.tail() Out[9]: affinity SiO2_wt_percent TiO2_wt_percent Al2O3_wt_percent Fe2O3_wt_percent FeO_wt_percent CaO_wt_percent MgO_wt_percent MnO_wt_percent K2O_wt_percent ... Hf_ppm Ta_ppm Pb_ppm Th_ppm U_ppm 143Nd/144Nd 87Sr/86Sr 206Pb/204Pb 207Pb/204Pb 208Pb/204Pb 751 IAB 50.97 0.78 18.86 NaN NaN 10.85 4.71 0.16 0.60 ... NaN NaN NaN NaN NaN NaN NaN 18.82 15.556 38.389 752 IAB 51.00 1.41 17.06 3.80 7.04 9.97 4.96 0.17 0.73 ... NaN NaN NaN NaN NaN NaN 0.70348 NaN NaN NaN 753 IAB 52.56 1.21 17.74 2.28 7.53 10.48 5.57 0.24 0.29 ... NaN NaN NaN NaN NaN NaN 0.70362 NaN NaN NaN 754 IAB 52.59 1.50 16.88 2.41 7.90 10.83 4.91 0.26 0.54 ... NaN NaN NaN NaN NaN NaN 0.70363 NaN NaN NaN 755 IAB 52.96 1.27 15.65 2.91 9.32 9.78 4.24 0.23 0.46 ... NaN NaN NaN NaN NaN NaN 0.70352 NaN NaN NaN <p>5 rows \u00d7 52 columns</p> In\u00a0[10]: Copied! <pre>## Review the data\nplt.figure(figsize=(8, 6))\n\nfor affinity in basalt_data['affinity'].unique():\n    subset = basalt_data[basalt_data['affinity'] == affinity]\n    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n\nplt.legend()\nplt.xlabel('TiO2 (wt%)')\nplt.ylabel('V (ppm)')\nplt.show()\n</pre> ## Review the data plt.figure(figsize=(8, 6))  for affinity in basalt_data['affinity'].unique():     subset = basalt_data[basalt_data['affinity'] == affinity]     plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)  plt.legend() plt.xlabel('TiO2 (wt%)') plt.ylabel('V (ppm)') plt.show()  <ul> <li>Prepare the data into features (X) and target (y). This is same as the previous lecture.</li> </ul> In\u00a0[11]: Copied! <pre>## Prepare the data into features (X) and target (y)\nX = basalt_data.drop('affinity', axis=1)\ny = basalt_data['affinity']\n\n## Encode the target variable\nle = LabelEncoder()\nle.fit(y)\ny = le.transform(y)\n\n## Impute missing values using median imputation\nimputer = SimpleImputer(strategy='median')\nX = imputer.fit_transform(X)\n\n## Split the data into training and test sets using 30% of the data for testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n</pre> ## Prepare the data into features (X) and target (y) X = basalt_data.drop('affinity', axis=1) y = basalt_data['affinity']  ## Encode the target variable le = LabelEncoder() le.fit(y) y = le.transform(y)  ## Impute missing values using median imputation imputer = SimpleImputer(strategy='median') X = imputer.fit_transform(X)  ## Split the data into training and test sets using 30% of the data for testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41) In\u00a0[12]: Copied! <pre>X_train_tensor.shape\n</pre> X_train_tensor.shape Out[12]: <pre>torch.Size([21, 1])</pre> <ul> <li>Let's start to build the second neural network model to fit the basalt affinity data.</li> </ul> In\u00a0[13]: Copied! <pre>## Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n## Normalize the data to make the training process more efficient\nmu = X_train_tensor.mean(dim=0, keepdim=True)\nstd = X_train_tensor.std(dim=0, keepdim=True)\nX_train_tensor = (X_train_tensor - mu) / std\nX_test_tensor = (X_test_tensor - mu) / std\n\n## Define the neural network model\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size,  output_size, hidden_size):\n        super(SimpleNN, self).__init__()\n        ## Define the neural network layers\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        ## Apply the neural network layers\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x\n\n## Initialize the model, loss function, and optimizer\ninput_size = X_train.shape[-1]\noutput_size = len(le.classes_) # Output layer for classification (number of classes)\nhidden_size = 16\n\nmodel = SimpleNN(input_size, output_size, hidden_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)\n\n## Define fit function\ndef fit(model, X_train, y_train, epochs=100):\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if (epoch+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n    return losses\n\n## Define predict function\ndef predict(model, X):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(X)\n        _, predicted = torch.max(outputs, 1)\n    return predicted\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=100)\n\n## Plot the loss\nplt.figure()\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Convert data to PyTorch tensors X_train_tensor = torch.tensor(X_train, dtype=torch.float32) y_train_tensor = torch.tensor(y_train, dtype=torch.long) X_test_tensor = torch.tensor(X_test, dtype=torch.float32) y_test_tensor = torch.tensor(y_test, dtype=torch.long)  ## Normalize the data to make the training process more efficient mu = X_train_tensor.mean(dim=0, keepdim=True) std = X_train_tensor.std(dim=0, keepdim=True) X_train_tensor = (X_train_tensor - mu) / std X_test_tensor = (X_test_tensor - mu) / std  ## Define the neural network model class SimpleNN(nn.Module):     def __init__(self, input_size,  output_size, hidden_size):         super(SimpleNN, self).__init__()         ## Define the neural network layers         self.fc1 = nn.Linear(input_size, hidden_size)         self.fc2 = nn.Linear(hidden_size, output_size)          def forward(self, x):         ## Apply the neural network layers         x = self.fc1(x)         x = F.relu(x)         x = self.fc2(x)         return x  ## Initialize the model, loss function, and optimizer input_size = X_train.shape[-1] output_size = len(le.classes_) # Output layer for classification (number of classes) hidden_size = 16  model = SimpleNN(input_size, output_size, hidden_size) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.1)  ## Define fit function def fit(model, X_train, y_train, epochs=100):     model.train()     losses = []     for epoch in range(epochs):         optimizer.zero_grad()         outputs = model(X_train)         loss = criterion(outputs, y_train)         loss.backward()         optimizer.step()         losses.append(loss.item())         if (epoch+1) % 10 == 0:             print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')     return losses  ## Define predict function def predict(model, X):     model.eval()     with torch.no_grad():         outputs = model(X)         _, predicted = torch.max(outputs, 1)     return predicted  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=100)  ## Plot the loss plt.figure() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('Loss') plt.show()  <pre>Epoch [10/100], Loss: 0.1224\nEpoch [20/100], Loss: 0.0299\nEpoch [30/100], Loss: 0.0067\nEpoch [40/100], Loss: 0.0016\nEpoch [50/100], Loss: 0.0008\nEpoch [60/100], Loss: 0.0005\nEpoch [70/100], Loss: 0.0003\nEpoch [80/100], Loss: 0.0003\nEpoch [90/100], Loss: 0.0002\nEpoch [100/100], Loss: 0.0002\n</pre> <ul> <li>Evaluate the model on the test set. This is same as the previous lecture.</li> </ul> In\u00a0[14]: Copied! <pre>## Predict on the test set\ny_pred_tensor = predict(model, X_test_tensor)\ny_pred = y_pred_tensor.numpy()\n\n## Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\n\n## Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Predict on the test set y_pred_tensor = predict(model, X_test_tensor) y_pred = y_pred_tensor.numpy()  ## Calculate accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.4f}')  ## Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Accuracy: 0.9075\n</pre> <ul> <li>Compare the results with previous classification methods. How does the neural network perform?</li> </ul> <ul> <li>Compare the two neural networks built for the regression and classification tasks. Please list the similarities and differences.</li> </ul> <ul> <li>The neural networks we built are very simple with only one hidden layer. Do you know which variable controls the complexity of the neural networks?</li> </ul> <ul> <li>If we want to build a more complex neural network, how can we do it? Think about the number of layers and neurons in each layer.</li> </ul> <p>If you are interested to build a more complex neural network, you can try the following website.</p> <p>The more layers and neurons you add, the more complex the neural network becomes, it can fit more complex data, while in the meantime, it is also more challenging to train.</p> <p>There are many hyperparameters you can tune in the online playgroud. Explore if we can find the parameters that can fit all the data distributions.</p> <p>Train a neural network online</p> <p></p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/09_neural_networks1/#regression-and-classification-with-neural-networks","title":"Regression and Classification with Neural Networks\u00b6","text":"Run in Google Colab"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/09_neural_networks1/#11-a-brief-history","title":"1.1. A Brief History\u00b6","text":"<p>In the 1940s, NNs were conceived.</p> <p>In the 1960s, the concept of backpropagation came, then people know how to train them.</p> <p>In 2010, NNs started winning competitions and get much attention than before.</p> <p>Since 2010, NNs have been on a meteoric rise as their magical ability to solve problems previously deemed unsolvable (i.e., image captioning, language translation, audio and video synthesis, and more).</p> <p>One important milestone is the AlexNet architecture in 2012, which won the ImageNet competition.</p> <p></p> <p></p> <p>The ImageNet competition is a benchmark for image classification, where the goal is to classify images into one of 1,000 categories.</p> <p></p> <p>You can find more information about the AlexNet model on Wikipedia. We will use the AlexNet model in the next lecture to classify images of rocks.</p> <p>Currently, NNs are the primary solution to most competitions and technological challenges like self-driving cars, calculating risk, detecting fraud, early cancer detection,\u2026</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/09_neural_networks1/#12-what-is-a-neural-network","title":"1.2. What is a Neural Network?\u00b6","text":"<p>ANNs are inspired by the organic brain, translated to the computer.</p> <p>ANNs have neurons, activations, and interconnectivities.</p> <p>NNs are considered \u201cblack boxes\u201d between inputs and outputs.</p> <p>Each connection between neurons has a weight associated with it. Weights are multiplied by corresponding input values. These multiplications flow into the neuron and are summed before being added with a bias. Weights and biases are trainable or tunable.</p> <p>$$ \\begin{aligned} output &amp; = weight \\cdot input + bias \\\\ y &amp; = a \\cdot x + b \\end{aligned} $$</p> <p>The formula should look very familiar to you. It is similar to the previous linear regression and classification models.</p> <p>Then, an activation function is applied to the output.</p> <p>$$ \\begin{aligned} output &amp; = \\sum (weight \\cdot input) + bias \\\\ output &amp; = activation (output) \\end{aligned} $$</p> <p>When a step function that mimics a neuron in the brain (i.e., \u201cfiring\u201d or not, on-off switch) is used as an activation function:</p> <ul> <li>If its output is greater than 0, the neuron fires (it would output 1).</li> <li>If its output is less than 0, the neuron does not fire and would pass along a 0.</li> </ul> <p>The input layer represents the actual input data (i.e., pixel values from an image, temperature, \u2026)</p> <ul> <li>The data can be \u201craw\u201d, should be preprocessed like normalization and scaling.</li> <li>The input needs to be in numeric form.</li> </ul> <p>The output layer is whatever the NN returns.</p> <ul> <li>In regression, the predicted value is a scalar value, the output layer has a single neuron.</li> <li>In classification, the class of the input is predicted, the output layer has as many neurons as the training dataset has classes. But can also have a single output neuron for binary (two classes) classification.</li> </ul> <p>A typical NN has thousands or even up to millions of adjustable parameters (weights and biases).</p> <p>NNs act as enormous functions with vast numbers of parameters.</p> <p>Finding the combination of parameter (weight and bias) values is the challenging part.</p> <p>The end goal for NNs is to adjust their weights and biases (the parameters), so they produce the desired output for unseen data.</p> <p>A major issue in supervised learning is overfitting, where the algorithm doesn\u2019t understand underlying input-output dependencies, just basically \u201cmemorizes\u201d the training data.</p> <p>The goal of NN is generalization, that can be obtained when separating the data into training data and validation data.</p> <p>Weights and biases are adjusted based on the error/loss presenting how \u201cwrong\u201d the algorithm in NN predicting the output.</p> <p>NNs can be used for regression (predict a scalar, singular, value), clustering (assigned unstructured data into groups), and many other tasks.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/09_neural_networks1/#applying-neural-networks-for-regression","title":"Applying Neural Networks for Regression\u00b6","text":"<p>In today's lecture, we will revisit the Betoule data and apply neural networks for regression.</p> <p>If you already forgot the background of this data, please review the lecture 04 regression.</p> <p>Remember the challenge of the Betoule data is that the velocity is non-linear with respect to the distance.</p> <p>In the previous lecture, we used sklearn to fit the linear regression model with high polynomial degrees.</p> <p>Here we will use PyTorch to fit the Betoule data and compare the results with the linear regression model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/09_neural_networks1/#applying-neural-networks-for-classification","title":"Applying Neural Networks for Classification\u00b6","text":"<p>Neural networks work well for the regression tasks, how about the classification tasks?</p> <p>Let's continue to apply neural networks for the binary classification task.</p> <p>Again, we will re-use the basalt affinity dataset that we covered in the previous lecture.</p> <p>If you already forgot the background of this data, please review the lecture 05 classification.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/","title":"Neural Networks 2","text":"<p>Note: If you are running this in a colab notebook, we recommend you enable a free GPU by going:</p> <p>Runtime \u2006\u2006\u2192\u2006\u2006 Change runtime type \u2006\u2006\u2192\u2006\u2006 Hardware Accelerator: GPU</p> <p>GPUs are commonly used to accelerate training of deep learning models. If you want to learn more about GPUs or even TPUs for training nerual networks, check out this short video.</p> <p>This notebooks shows how to define and train a simple modern Neural Network with PyTorch.</p> <p>We will use the MNIST dataset, which is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.</p> <p>The dateaset is very popular and is often used as a \"Hello World\" example in the field of Machine Learning. It is a good dataset to start with, as it is small and easy to work with.</p> <p>For this small dataset, we will use the socalled \"LeNet\" architecture is used here. It is a simple convolutional neural network, which was introduced by Yann LeCun in 1998. It is a simple and effective architecture for small image datasets. You can read more about the model on Wikipedia.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay import torch from torch import nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset from tqdm import tqdm In\u00a0[2]: Copied! <pre>## Set random seed\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n</pre> ## Set random seed torch.manual_seed(0) torch.cuda.manual_seed(0) np.random.seed(0) <p>Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image.</p> <p>Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.</p> In\u00a0[3]: Copied! <pre>mnist = fetch_openml('mnist_784', as_frame=False)\n</pre> mnist = fetch_openml('mnist_784', as_frame=False) In\u00a0[4]: Copied! <pre>print(f\"MNIST data shape: {mnist.data.shape}, data type: {mnist.data.dtype}\")\nprint(f\"MNIST target shape: {mnist.target.shape}, target type: {mnist.target.dtype}\")\n</pre> print(f\"MNIST data shape: {mnist.data.shape}, data type: {mnist.data.dtype}\") print(f\"MNIST target shape: {mnist.target.shape}, target type: {mnist.target.dtype}\") <pre>MNIST data shape: (70000, 784), data type: int64\nMNIST target shape: (70000,), target type: object\n</pre> In\u00a0[5]: Copied! <pre>## Make sure the data is float32 and the labels are int64\nX = mnist.data.astype('float32')\ny = mnist.target.astype('int64')\n\n## Normalize the data to [0, 1]. Hint: the raw pixel values are in [0, 255].\nX /= 255.0\nprint(f\"{X.min() = }, {X.max() = }\")\n</pre> ## Make sure the data is float32 and the labels are int64 X = mnist.data.astype('float32') y = mnist.target.astype('int64')  ## Normalize the data to [0, 1]. Hint: the raw pixel values are in [0, 255]. X /= 255.0 print(f\"{X.min() = }, {X.max() = }\") <pre>X.min() = 0.0, X.max() = 1.0\n</pre> <p>Same as prevoious lectures, let split the data into training and testing sets.</p> In\u00a0[6]: Copied! <pre>## Split data into training and testing sets using 30% of the data for testing and random seed 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n</pre> ## Split data into training and testing sets using 30% of the data for testing and random seed 42 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) In\u00a0[7]: Copied! <pre>## Define a function to plot a selection of images and their labels\ndef plot_example(X, y, n_samples=10):\n    \"\"\"Plot the first n_samples images and their labels in a row.\"\"\"\n    fig, axes = plt.subplots(1, n_samples, figsize=(2*n_samples, 4))\n    for i, ax in enumerate(axes):\n        ax.imshow(X[i].reshape(28, 28), cmap='gray')\n        ax.set_title(y[i], fontsize=32)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n</pre> ## Define a function to plot a selection of images and their labels def plot_example(X, y, n_samples=10):     \"\"\"Plot the first n_samples images and their labels in a row.\"\"\"     fig, axes = plt.subplots(1, n_samples, figsize=(2*n_samples, 4))     for i, ax in enumerate(axes):         ax.imshow(X[i].reshape(28, 28), cmap='gray')         ax.set_title(y[i], fontsize=32)         ax.axis('off')     plt.tight_layout()     plt.show() In\u00a0[8]: Copied! <pre>## Plot the first 10 images from the training set\nplot_example(X_train, y_train, n_samples=10)\n</pre> ## Plot the first 10 images from the training set plot_example(X_train, y_train, n_samples=10) <ul> <li>Prepare the data for training and testing</li> </ul> In\u00a0[9]: Copied! <pre>## Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.int64)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n</pre> ## Convert data to PyTorch tensors X_train_tensor = torch.tensor(X_train, dtype=torch.float32) y_train_tensor = torch.tensor(y_train, dtype=torch.int64) X_test_tensor = torch.tensor(X_test, dtype=torch.float32) y_test_tensor = torch.tensor(y_test, dtype=torch.int64) <ul> <li>Build a simple fully connected neural network in PyTorch's framework.</li> </ul> In\u00a0[10]: Copied! <pre>## Define the network architecture: A simple fully connected neural network\nclass FCN(nn.Module):\n    def __init__(\n            self,\n            input_dim=28*28,\n            hidden_dim=28*4,\n            output_dim=10,\n            dropout=0.5,\n    ):\n        super(FCN, self).__init__()\n        ## Define the neural network layers\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, **kwargs):\n        ## Apply the neural network layers\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n</pre> ## Define the network architecture: A simple fully connected neural network class FCN(nn.Module):     def __init__(             self,             input_dim=28*28,             hidden_dim=28*4,             output_dim=10,             dropout=0.5,     ):         super(FCN, self).__init__()         ## Define the neural network layers         self.fc1 = nn.Linear(input_dim, hidden_dim)         self.fc2 = nn.Linear(hidden_dim, output_dim)         self.dropout = nn.Dropout(dropout)      def forward(self, x, **kwargs):         ## Apply the neural network layers         x = self.fc1(x)         x = F.relu(x)         x = self.dropout(x)         x = self.fc2(x)         return x In\u00a0[11]: Copied! <pre>## Define the model dimensions\nmnist_dim = X.shape[1]\nhidden_dim = int(mnist_dim/8)\noutput_dim = len(np.unique(mnist.target))\n\n## Define the model, loss function, and optimizer\nmodel = FCN(input_dim=mnist_dim, hidden_dim=hidden_dim, output_dim=output_dim)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n## Define the device to choose the fastest for training\n## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise\ndevice = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\n## Define fit function\ndef fit(model, X_train, y_train, epochs=100):\n    dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)\n    model.to(device)\n\n    ## set the model to training mode\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        loss = 0\n        for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):\n            X_train = X_train.to(device)\n            y_train = y_train.to(device)\n            ## zero the gradients\n            optimizer.zero_grad()\n\n            ## get the model predictions\n            outputs = model(X_train)\n            ## calculate the loss\n            batch_loss = criterion(outputs, y_train)\n            batch_loss.backward()\n            ## update the weights\n            optimizer.step()\n            \n            loss += batch_loss.item()\n\n        # average loss per batch\n        loss = loss / len(dataloader)\n        losses.append(loss)\n\n        print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')\n\n    return losses\n\n## Define predict function\ndef predict(model, X):\n    dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)\n    ## set the model to evaluation mode\n    model.eval()\n    device = next(model.parameters()).device\n    with torch.no_grad():\n        predicted = []\n        for X, in tqdm(dataloader, desc='Predicting'):\n            X = X.to(device)\n            \n            ## get the model predictions\n            outputs = model(X)\n\n            _, predicted_batch = torch.max(outputs, 1)\n            predicted.append(predicted_batch.cpu())\n    return torch.cat(predicted)\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=15)\n\n## Plot the loss\nplt.figure()\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Define the model dimensions mnist_dim = X.shape[1] hidden_dim = int(mnist_dim/8) output_dim = len(np.unique(mnist.target))  ## Define the model, loss function, and optimizer model = FCN(input_dim=mnist_dim, hidden_dim=hidden_dim, output_dim=output_dim) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001)  ## Define the device to choose the fastest for training ## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'  ## Define fit function def fit(model, X_train, y_train, epochs=100):     dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)     model.to(device)      ## set the model to training mode     model.train()     losses = []     for epoch in range(epochs):         loss = 0         for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):             X_train = X_train.to(device)             y_train = y_train.to(device)             ## zero the gradients             optimizer.zero_grad()              ## get the model predictions             outputs = model(X_train)             ## calculate the loss             batch_loss = criterion(outputs, y_train)             batch_loss.backward()             ## update the weights             optimizer.step()                          loss += batch_loss.item()          # average loss per batch         loss = loss / len(dataloader)         losses.append(loss)          print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')      return losses  ## Define predict function def predict(model, X):     dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)     ## set the model to evaluation mode     model.eval()     device = next(model.parameters()).device     with torch.no_grad():         predicted = []         for X, in tqdm(dataloader, desc='Predicting'):             X = X.to(device)                          ## get the model predictions             outputs = model(X)              _, predicted_batch = torch.max(outputs, 1)             predicted.append(predicted_batch.cpu())     return torch.cat(predicted)  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=15)  ## Plot the loss plt.figure() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('Loss') plt.show() <pre>Training Epoch 1/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 84.52it/s] \n</pre> <pre>Epoch 1/15: Loss: 0.6224\n</pre> <pre>Training Epoch 2/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 97.92it/s] \n</pre> <pre>Epoch 2/15: Loss: 0.3276\n</pre> <pre>Training Epoch 3/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 104.05it/s]\n</pre> <pre>Epoch 3/15: Loss: 0.2684\n</pre> <pre>Training Epoch 4/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 93.66it/s] \n</pre> <pre>Epoch 4/15: Loss: 0.2408\n</pre> <pre>Training Epoch 5/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 99.05it/s] \n</pre> <pre>Epoch 5/15: Loss: 0.2120\n</pre> <pre>Training Epoch 6/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 102.76it/s]\n</pre> <pre>Epoch 6/15: Loss: 0.1991\n</pre> <pre>Training Epoch 7/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 100.87it/s]\n</pre> <pre>Epoch 7/15: Loss: 0.1863\n</pre> <pre>Training Epoch 8/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 103.79it/s]\n</pre> <pre>Epoch 8/15: Loss: 0.1715\n</pre> <pre>Training Epoch 9/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 82.94it/s] \n</pre> <pre>Epoch 9/15: Loss: 0.1644\n</pre> <pre>Training Epoch 10/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 126.21it/s]\n</pre> <pre>Epoch 10/15: Loss: 0.1596\n</pre> <pre>Training Epoch 11/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 77.86it/s] \n</pre> <pre>Epoch 11/15: Loss: 0.1561\n</pre> <pre>Training Epoch 12/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 82.26it/s] \n</pre> <pre>Epoch 12/15: Loss: 0.1450\n</pre> <pre>Training Epoch 13/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 97.77it/s] \n</pre> <pre>Epoch 13/15: Loss: 0.1407\n</pre> <pre>Training Epoch 14/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 89.12it/s] \n</pre> <pre>Epoch 14/15: Loss: 0.1392\n</pre> <pre>Training Epoch 15/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:03&lt;00:00, 97.44it/s] \n</pre> <pre>Epoch 15/15: Loss: 0.1344\n</pre> <ul> <li>Evaluate the model on the test set. This is same as the previous lecture.</li> </ul> In\u00a0[12]: Copied! <pre>## Predict on the test set\ny_pred_tensor = predict(model, X_test_tensor)\ny_pred = y_pred_tensor.numpy()\n\n## Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\n\n## Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y))))\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Predict on the test set y_pred_tensor = predict(model, X_test_tensor) y_pred = y_pred_tensor.numpy()  ## Calculate accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.4f}')  ## Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y)))) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 165/165 [00:00&lt;00:00, 224.32it/s]\n</pre> <pre>Accuracy: 0.9686\n</pre> <p>What accuracy did you get? Is it above 95%?</p> <p>An accuracy of above 95% for a network with only one hidden layer is not too bad.</p> <p>Let's take a look at some predictions that went wrong:</p> In\u00a0[13]: Copied! <pre>error_mask_fcn = y_pred != y_test\nplot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn], n_samples=10)\n</pre> error_mask_fcn = y_pred != y_test plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn], n_samples=10) <p>Are these errors reasonable?</p> <ul> <li>Prepare the data for training and testing</li> </ul> In\u00a0[14]: Copied! <pre>## Convert data to PyTorch tensors and reshape to 4D tensor (batch_size, channel, height, width)\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1, 28, 28)\ny_train_tensor = torch.tensor(y_train, dtype=torch.int64)\ny_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n</pre> ## Convert data to PyTorch tensors and reshape to 4D tensor (batch_size, channel, height, width) X_train_tensor = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28) X_test_tensor = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1, 28, 28) y_train_tensor = torch.tensor(y_train, dtype=torch.int64) y_test_tensor = torch.tensor(y_test, dtype=torch.int64) <ul> <li>Build a simple convolutional neural network in PyTorch's framework.</li> </ul> In\u00a0[15]: Copied! <pre>## Define the network architecture: A simple convolutional neural network\nclass CNN(nn.Module):\n    def __init__(self, input_dim=28*28, hidden_dim=112, output_dim=10, dropout=0.5):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.conv1_drop = nn.Dropout2d(p=dropout)\n        self.conv2_drop = nn.Dropout2d(p=dropout)\n        num_features = 64 * int((input_dim**0.5 // 4 - 2)**2)\n        self.fc1 = nn.Linear(num_features, hidden_dim) # 1600 = number channels * width * height\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.fc1_drop = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n    \n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.conv1_drop(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.conv2_drop(x)\n        \n        # flatten over channel, height and width        \n        x = x.view(x.size(0), -1)\n        \n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc1_drop(x)\n        x = self.fc2(x)\n\n        return x\n</pre> ## Define the network architecture: A simple convolutional neural network class CNN(nn.Module):     def __init__(self, input_dim=28*28, hidden_dim=112, output_dim=10, dropout=0.5):         super(CNN, self).__init__()         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)         self.conv1_drop = nn.Dropout2d(p=dropout)         self.conv2_drop = nn.Dropout2d(p=dropout)         num_features = 64 * int((input_dim**0.5 // 4 - 2)**2)         self.fc1 = nn.Linear(num_features, hidden_dim) # 1600 = number channels * width * height         self.fc2 = nn.Linear(hidden_dim, output_dim)         self.fc1_drop = nn.Dropout(p=dropout)      def forward(self, x):              x = self.conv1(x)         x = F.relu(x)         x = F.max_pool2d(x, 2)         x = self.conv1_drop(x)          x = self.conv2(x)         x = F.relu(x)         x = F.max_pool2d(x, 2)         x = self.conv2_drop(x)                  # flatten over channel, height and width                 x = x.view(x.size(0), -1)                  x = self.fc1(x)         x = F.relu(x)         x = self.fc1_drop(x)         x = self.fc2(x)          return x In\u00a0[16]: Copied! <pre>## Initialize the network parameters, loss function, optimizer, and device\nmnist_dim = X.shape[1]\nhidden_dim = int(mnist_dim/8)\noutput_dim = len(np.unique(mnist.target))\n\n## Define the model, loss function, and optimizer\nmodel = CNN(input_dim=mnist_dim, hidden_dim=hidden_dim, output_dim=output_dim)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n## Define the device to choose the fastest for training\n## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise\ndevice = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\n## Define fit function\ndef fit(model, X_train, y_train, epochs=100):\n\n    dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)\n    model.to(device)\n\n    ## set the model to training mode\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        loss = 0\n        for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):\n            X_train = X_train.to(device)\n            y_train = y_train.to(device)\n            ## zero the gradients\n            optimizer.zero_grad()\n\n            ## get the model predictions\n            outputs = model(X_train)\n            ## calculate the loss\n            batch_loss = criterion(outputs, y_train)\n            ## update the weights\n            batch_loss.backward()\n            optimizer.step()\n\n            loss += batch_loss.item()\n\n        # average loss per batch\n        loss = loss / len(dataloader)\n        losses.append(loss)\n\n        print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')\n\n    return losses\n\n## Define predict function\ndef predict(model, X):\n\n    dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)\n\n    ## set the model to evaluation mode\n    model.eval()\n    device = next(model.parameters()).device\n    with torch.no_grad():\n        predicted = []\n        for X, in tqdm(dataloader, desc='Predicting'):\n            X = X.to(device)\n\n            ## get the model predictions\n            outputs = model(X)\n            \n            _, predicted_batch = torch.max(outputs, 1)\n            predicted.append(predicted_batch.cpu())\n    return torch.cat(predicted)\n\n## Train the model\nlosses = fit(model, X_train_tensor, y_train_tensor, epochs=15)\n\n## Plot the loss\nplt.figure()\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n</pre> ## Initialize the network parameters, loss function, optimizer, and device mnist_dim = X.shape[1] hidden_dim = int(mnist_dim/8) output_dim = len(np.unique(mnist.target))  ## Define the model, loss function, and optimizer model = CNN(input_dim=mnist_dim, hidden_dim=hidden_dim, output_dim=output_dim) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001)  ## Define the device to choose the fastest for training ## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'  ## Define fit function def fit(model, X_train, y_train, epochs=100):      dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)     model.to(device)      ## set the model to training mode     model.train()     losses = []     for epoch in range(epochs):         loss = 0         for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):             X_train = X_train.to(device)             y_train = y_train.to(device)             ## zero the gradients             optimizer.zero_grad()              ## get the model predictions             outputs = model(X_train)             ## calculate the loss             batch_loss = criterion(outputs, y_train)             ## update the weights             batch_loss.backward()             optimizer.step()              loss += batch_loss.item()          # average loss per batch         loss = loss / len(dataloader)         losses.append(loss)          print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')      return losses  ## Define predict function def predict(model, X):      dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)      ## set the model to evaluation mode     model.eval()     device = next(model.parameters()).device     with torch.no_grad():         predicted = []         for X, in tqdm(dataloader, desc='Predicting'):             X = X.to(device)              ## get the model predictions             outputs = model(X)                          _, predicted_batch = torch.max(outputs, 1)             predicted.append(predicted_batch.cpu())     return torch.cat(predicted)  ## Train the model losses = fit(model, X_train_tensor, y_train_tensor, epochs=15)  ## Plot the loss plt.figure() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('Loss') plt.show() <pre>Training Epoch 1/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:08&lt;00:00, 47.01it/s]\n</pre> <pre>Epoch 1/15: Loss: 0.6035\n</pre> <pre>Training Epoch 2/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:09&lt;00:00, 38.57it/s]\n</pre> <pre>Epoch 2/15: Loss: 0.2225\n</pre> <pre>Training Epoch 3/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:07&lt;00:00, 50.48it/s]\n</pre> <pre>Epoch 3/15: Loss: 0.1759\n</pre> <pre>Training Epoch 4/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:07&lt;00:00, 54.05it/s]\n</pre> <pre>Epoch 4/15: Loss: 0.1474\n</pre> <pre>Training Epoch 5/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:08&lt;00:00, 46.59it/s]\n</pre> <pre>Epoch 5/15: Loss: 0.1295\n</pre> <pre>Training Epoch 6/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:06&lt;00:00, 57.71it/s]\n</pre> <pre>Epoch 6/15: Loss: 0.1206\n</pre> <pre>Training Epoch 7/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:05&lt;00:00, 70.02it/s]\n</pre> <pre>Epoch 7/15: Loss: 0.1133\n</pre> <pre>Training Epoch 8/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:05&lt;00:00, 71.75it/s]\n</pre> <pre>Epoch 8/15: Loss: 0.1065\n</pre> <pre>Training Epoch 9/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 88.28it/s]\n</pre> <pre>Epoch 9/15: Loss: 0.1030\n</pre> <pre>Training Epoch 10/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 86.71it/s]\n</pre> <pre>Epoch 10/15: Loss: 0.0927\n</pre> <pre>Training Epoch 11/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 90.95it/s] \n</pre> <pre>Epoch 11/15: Loss: 0.0897\n</pre> <pre>Training Epoch 12/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:05&lt;00:00, 76.21it/s]\n</pre> <pre>Epoch 12/15: Loss: 0.0867\n</pre> <pre>Training Epoch 13/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 82.52it/s]\n</pre> <pre>Epoch 13/15: Loss: 0.0863\n</pre> <pre>Training Epoch 14/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:05&lt;00:00, 75.57it/s]\n</pre> <pre>Epoch 14/15: Loss: 0.0816\n</pre> <pre>Training Epoch 15/15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:04&lt;00:00, 91.97it/s] \n</pre> <pre>Epoch 15/15: Loss: 0.0769\n</pre> In\u00a0[17]: Copied! <pre>## Predict on the test set\ny_pred_tensor = predict(model, X_test_tensor)\ny_pred = y_pred_tensor.numpy()\n\n## Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\n\n## Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y))))\ndisp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);\n</pre> ## Predict on the test set y_pred_tensor = predict(model, X_test_tensor) y_pred = y_pred_tensor.numpy()  ## Calculate accuracy accuracy = accuracy_score(y_test, y_pred) print(f'Accuracy: {accuracy:.4f}')  ## Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y)))) disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False); <pre>Predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 165/165 [00:00&lt;00:00, 179.11it/s]\n</pre> <pre>Accuracy: 0.9883\n</pre> <ul> <li>What accuracy did you get? Is it better than the fully connected network?</li> </ul> <p>An accuracy of &gt;98% should suffice for this example!</p> <p>Let's take a look at some predictions that went wrong:</p> In\u00a0[18]: Copied! <pre>error_mask_cnn = y_pred != y_test\nplot_example(X_test[error_mask_cnn], y_pred[error_mask_cnn], n_samples=10)\n</pre> error_mask_cnn = y_pred != y_test plot_example(X_test[error_mask_cnn], y_pred[error_mask_cnn], n_samples=10) <ul> <li>Let's further look at the accuracy of the convolutional network model (CNN) on the misclassified examples previously by the fully connected network (FCN).</li> </ul> In\u00a0[19]: Copied! <pre>accuracy = accuracy_score(y_test[error_mask_fcn], y_pred[error_mask_fcn])\nprint(f\"Accuracy: {accuracy:.4f}\")\n</pre> accuracy = accuracy_score(y_test[error_mask_fcn], y_pred[error_mask_fcn]) print(f\"Accuracy: {accuracy:.4f}\") <pre>Accuracy: 0.7318\n</pre> <p>About 70% of the previously misclassified images are now correctly identified.</p> <p>Let's take a look at some of the misclassified examples before:</p> In\u00a0[20]: Copied! <pre>plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn])\n</pre> plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn]) <ul> <li>Last questions. Please take a look at the training loops of the fully connected network and the convolutional network. Comment on the similarities and differences.</li> </ul> <p>You can see although the network architecture is different, the training loops are very similar.</p> <p>This is one feature of neural networks models. You can build significantly larger models and train them efficiently with a similar training loop, as long as you have enough computational power.</p> <p>Bonus points: One challenge in deep learning training is to find the proper hyperparameters. There are many hyperparameters to tune, such as the learning rate, the number of hidden layers, the number of neurons in each layer, the batch size, the number of epochs, etc.</p> <p>Try to tune the hyperparameters of the convolutional network model to achieve a higher accuracy. For the MNIST dataset, an accuracy of &gt;99% is possible with the LeNet architecture. Could you achieve this?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/#neural-networks-with-pytorch","title":"Neural Networks with PyTorch\u00b6","text":"Run in Google Colab"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/#loading-data","title":"Loading Data\u00b6","text":"<p>Using SciKit-Learns <code>fetch_openml</code> to load MNIST data.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/#preprocessing-data","title":"Preprocessing Data\u00b6","text":"<p>The above <code>featch_mldata</code> method to load MNIST returns <code>data</code> and <code>target</code> as <code>uint8</code> which we convert to <code>float32</code> and <code>int64</code> respectively.</p> <p>To avoid big weights that deal with the pixel values from between [0, 255], we scale <code>X</code> down. A commonly used range is [0, 1].</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/#visualize-a-selection-of-training-images-and-their-labels","title":"Visualize a selection of training images and their labels\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/#build-neural-network-with-pytorch","title":"Build Neural Network with PyTorch\u00b6","text":"<p>In the previous lecture, we have built a simple fully connected neural network with one hidden layer for both linear regression and classification tasks. Let's first try a similar network for the classification task of MNIST.</p> <p>Note the dataset is much larger than our previous examples, so we need to adjust the network size accordingly. (It is still tiny compared to modern standards)</p> <p>Let's think about the network architecture:</p> <ul> <li>Input layer: 784 dimensions (28x28). This is defined by the MNIST data shape.</li> <li>Hidden layer: 98 (= 784 / 8). This is a free parameter that we can choose.</li> <li>Output layer: 10 neurons, representing digits 0 - 9. This is defined by the number of classes in the dataset.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/10_neural_networks2/#convolutional-network","title":"Convolutional Network\u00b6","text":"<p>To further improve the performance, let's try a convolutional neural network (CNN) for MNIST.</p> <p>The 2D convolutional layer expects a 4 dimensional tensor as input. The dimensions represent:</p> <ul> <li>Batch size</li> <li>Number of channel</li> <li>Height</li> <li>Width</li> </ul> <p>So we need to reshape the MNIST data to have the right shape. MNIST data has only one channel. As stated above, each MNIST vector represents a 28x28 pixel image. Hence, the resulting shape for PyTorch tensor needs to be (x, 1, 28, 28).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/","title":"11 neural networks3","text":"In\u00a0[1]: Copied! <pre>%%capture\nimport os\nif not os.path.exists('LiCS'):\n    if not os.path.exists('LiCS.zip'):\n        !wget https://github.com/AI4EPS/EPS88_PyEarth/releases/download/LiCS/LiCS.zip\n    !unzip LiCS.zip\nelse:\n    print(\"File already exists.\")\n</pre> %%capture import os if not os.path.exists('LiCS'):     if not os.path.exists('LiCS.zip'):         !wget https://github.com/AI4EPS/EPS88_PyEarth/releases/download/LiCS/LiCS.zip     !unzip LiCS.zip else:     print(\"File already exists.\") In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\nimport random\n</pre> import torch import torch.nn as nn import torch.optim as optim import numpy as np from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from tqdm import tqdm from collections import Counter from torch.utils.data import Subset from sklearn.model_selection import train_test_split import random <pre>Matplotlib is building the font cache; this may take a moment.\n</pre> In\u00a0[3]: Copied! <pre>## Set random seed\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n</pre> ## Set random seed torch.manual_seed(0) torch.cuda.manual_seed(0) np.random.seed(0) In\u00a0[4]: Copied! <pre># We run the model on the GPU if possible\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\n# If you get errors about CUDA running out of memory, you can set it to run on the CPU by uncommenting the next line.\n# device = torch.device(\"cpu\")\nprint(device)\n</pre> # We run the model on the GPU if possible if torch.cuda.is_available():     device = torch.device(\"cuda\") elif torch.backends.mps.is_available():     device = torch.device(\"mps\") else:     device = torch.device(\"cpu\")  # If you get errors about CUDA running out of memory, you can set it to run on the CPU by uncommenting the next line. # device = torch.device(\"cpu\") print(device) <pre>mps\n</pre> In\u00a0[5]: Copied! <pre># Path to the dataset\ndataset_dir = 'LiCS'\n\n# Transformers applied to the dataset\ndata_transforms = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ndataset = datasets.ImageFolder(root=dataset_dir, transform=data_transforms)\nclass_names = dataset.classes\n\n# Show information about the dataset\nprint(f'Dataset size: {len(dataset)}')\nprint(f'Mapping from class names to indexes: {dataset.class_to_idx}')\nprint(f'Number of samples per class: {dict(Counter(dataset.targets))}')\n</pre> # Path to the dataset dataset_dir = 'LiCS'  # Transformers applied to the dataset data_transforms = transforms.Compose([     transforms.ToTensor(), ])  dataset = datasets.ImageFolder(root=dataset_dir, transform=data_transforms) class_names = dataset.classes  # Show information about the dataset print(f'Dataset size: {len(dataset)}') print(f'Mapping from class names to indexes: {dataset.class_to_idx}') print(f'Number of samples per class: {dict(Counter(dataset.targets))}') <pre>Dataset size: 6808\nMapping from class names to indexes: {'background_noise': 0, 'volcano_deformation': 1}\nNumber of samples per class: {0: 3203, 1: 3605}\n</pre> In\u00a0[6]: Copied! <pre>def plot_example(images, labels, show_labels=False):\n    num_cols = 4\n    num_rows = (len(images) - 1) // num_cols + 1\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), squeeze=False)\n    for i in range(num_rows):\n        for j in range(num_cols):\n            idx = i * num_cols + j\n            if idx &lt; len(images):\n                image = images[idx][0]\n                image = image * 2 * np.pi - np.pi\n                axs[i, j].imshow(image, cmap='jet', vmin=-np.pi, vmax=np.pi)\n                if show_labels:\n                    axs[i, j].set_title(class_names[labels[idx]])\n                axs[i, j].axis('off')\n            else:\n                axs[i, j].axis('off')\n    plt.show()\n</pre> def plot_example(images, labels, show_labels=False):     num_cols = 4     num_rows = (len(images) - 1) // num_cols + 1     fig, axs = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows), squeeze=False)     for i in range(num_rows):         for j in range(num_cols):             idx = i * num_cols + j             if idx &lt; len(images):                 image = images[idx][0]                 image = image * 2 * np.pi - np.pi                 axs[i, j].imshow(image, cmap='jet', vmin=-np.pi, vmax=np.pi)                 if show_labels:                     axs[i, j].set_title(class_names[labels[idx]])                 axs[i, j].axis('off')             else:                 axs[i, j].axis('off')     plt.show() In\u00a0[7]: Copied! <pre>num_samples_per_class = 10\nnoise_indices = [i for i, label in enumerate(dataset.targets) if label == 0][:num_samples_per_class]\nvolcano_indices = [i for i, label in enumerate(dataset.targets) if label == 1][:num_samples_per_class]\nindices = noise_indices + volcano_indices\nrandom.shuffle(indices)\nimages, labels = zip(*[dataset[i] for i in indices])\nplot_example(images, labels, show_labels=False)\n</pre> num_samples_per_class = 10 noise_indices = [i for i, label in enumerate(dataset.targets) if label == 0][:num_samples_per_class] volcano_indices = [i for i, label in enumerate(dataset.targets) if label == 1][:num_samples_per_class] indices = noise_indices + volcano_indices random.shuffle(indices) images, labels = zip(*[dataset[i] for i in indices]) plot_example(images, labels, show_labels=False) <p>Question: Based on the images, can you tell which images are from the class 'volcano_deformation' and which are from the class 'background_noise'? How can you tell the difference?</p> <p>Question: Pass <code>show_labels=True</code> to the function and run the code again to show the ground truth labels of the images. Do the labels match your prediction?</p> <p>Now let's see if we can also train a neural network to differentiate between the two classes. If we can, we can use this neural network to automatically detect volcanic deformation signals in InSAR images. As you can imagine, the InSAR satellites are taking thousands of images every day, and it is impossible for a human to look at all of them. This is where the neural network and machine learning come in handy.</p> <p>Recall what we learned in the previous lectures about neural networks. We will folow a similar approach to build a neural network that can differentiate between the two classes of images for automatic detection of volcanic deformation signals.</p> In\u00a0[8]: Copied! <pre>train_dataset_size = 2000\ntest_dataset_size = 200\n\ntargets = np.array(dataset.targets)\n# The two classes are unbalanced with 'volcano_deformation' having more images. Thus we remove some of them. \n# All the 3203 'background_noise' images are in front, followed by the 3605 'volcano_deformation' images. \n# So from the array with all the images, we can select the first 6406 images, which is all the 'background_noise' ones followed by 3203 'volcano_deformation' ones.\n# Thus, we select the first 6406 images from the dataset.\ntargets = targets[:6406]\ntrain_indices, test_indices = train_test_split(\n    np.arange(targets.shape[0]),\n    train_size=train_dataset_size,\n    test_size=test_dataset_size,\n    stratify=targets,\n)\n\n# We use the splits we obtained to create subsets of the main dataset, one for train and one for test\ntrain_dataset = Subset(dataset, indices=train_indices)\ntest_dataset = Subset(dataset, indices=test_indices)\n\n# Show information about our subsets\ntrain_classes = [dataset.targets[i] for i in train_dataset.indices]\nprint(f'Numner of samples per class for the train dataset: {dict(Counter(train_classes))}')\n\ntest_classes = [dataset.targets[i] for i in test_dataset.indices]\nprint(f'Numner of samples per class for the test dataset: {dict(Counter(test_classes))}')\n</pre> train_dataset_size = 2000 test_dataset_size = 200  targets = np.array(dataset.targets) # The two classes are unbalanced with 'volcano_deformation' having more images. Thus we remove some of them.  # All the 3203 'background_noise' images are in front, followed by the 3605 'volcano_deformation' images.  # So from the array with all the images, we can select the first 6406 images, which is all the 'background_noise' ones followed by 3203 'volcano_deformation' ones. # Thus, we select the first 6406 images from the dataset. targets = targets[:6406] train_indices, test_indices = train_test_split(     np.arange(targets.shape[0]),     train_size=train_dataset_size,     test_size=test_dataset_size,     stratify=targets, )  # We use the splits we obtained to create subsets of the main dataset, one for train and one for test train_dataset = Subset(dataset, indices=train_indices) test_dataset = Subset(dataset, indices=test_indices)  # Show information about our subsets train_classes = [dataset.targets[i] for i in train_dataset.indices] print(f'Numner of samples per class for the train dataset: {dict(Counter(train_classes))}')  test_classes = [dataset.targets[i] for i in test_dataset.indices] print(f'Numner of samples per class for the test dataset: {dict(Counter(test_classes))}') <pre>Numner of samples per class for the train dataset: {0: 1000, 1: 1000}\nNumner of samples per class for the test dataset: {1: 100, 0: 100}\n</pre> In\u00a0[9]: Copied! <pre>train_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=2,\n    shuffle=True\n)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=2,\n    # shuffle=False\n    shuffle=True ## just for visualization purposes\n)\n</pre> train_dataloader = torch.utils.data.DataLoader(     train_dataset,     batch_size=2,     shuffle=True )  test_dataloader = torch.utils.data.DataLoader(     test_dataset,     batch_size=2,     # shuffle=False     shuffle=True ## just for visualization purposes ) In\u00a0[10]: Copied! <pre>models.list_models()\n</pre> models.list_models() Out[10]: <pre>['alexnet',\n 'convnext_base',\n 'convnext_large',\n 'convnext_small',\n 'convnext_tiny',\n 'deeplabv3_mobilenet_v3_large',\n 'deeplabv3_resnet101',\n 'deeplabv3_resnet50',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'efficientnet_b0',\n 'efficientnet_b1',\n 'efficientnet_b2',\n 'efficientnet_b3',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_v2_l',\n 'efficientnet_v2_m',\n 'efficientnet_v2_s',\n 'fasterrcnn_mobilenet_v3_large_320_fpn',\n 'fasterrcnn_mobilenet_v3_large_fpn',\n 'fasterrcnn_resnet50_fpn',\n 'fasterrcnn_resnet50_fpn_v2',\n 'fcn_resnet101',\n 'fcn_resnet50',\n 'fcos_resnet50_fpn',\n 'googlenet',\n 'inception_v3',\n 'keypointrcnn_resnet50_fpn',\n 'lraspp_mobilenet_v3_large',\n 'maskrcnn_resnet50_fpn',\n 'maskrcnn_resnet50_fpn_v2',\n 'maxvit_t',\n 'mc3_18',\n 'mnasnet0_5',\n 'mnasnet0_75',\n 'mnasnet1_0',\n 'mnasnet1_3',\n 'mobilenet_v2',\n 'mobilenet_v3_large',\n 'mobilenet_v3_small',\n 'mvit_v1_b',\n 'mvit_v2_s',\n 'quantized_googlenet',\n 'quantized_inception_v3',\n 'quantized_mobilenet_v2',\n 'quantized_mobilenet_v3_large',\n 'quantized_resnet18',\n 'quantized_resnet50',\n 'quantized_resnext101_32x8d',\n 'quantized_resnext101_64x4d',\n 'quantized_shufflenet_v2_x0_5',\n 'quantized_shufflenet_v2_x1_0',\n 'quantized_shufflenet_v2_x1_5',\n 'quantized_shufflenet_v2_x2_0',\n 'r2plus1d_18',\n 'r3d_18',\n 'raft_large',\n 'raft_small',\n 'regnet_x_16gf',\n 'regnet_x_1_6gf',\n 'regnet_x_32gf',\n 'regnet_x_3_2gf',\n 'regnet_x_400mf',\n 'regnet_x_800mf',\n 'regnet_x_8gf',\n 'regnet_y_128gf',\n 'regnet_y_16gf',\n 'regnet_y_1_6gf',\n 'regnet_y_32gf',\n 'regnet_y_3_2gf',\n 'regnet_y_400mf',\n 'regnet_y_800mf',\n 'regnet_y_8gf',\n 'resnet101',\n 'resnet152',\n 'resnet18',\n 'resnet34',\n 'resnet50',\n 'resnext101_32x8d',\n 'resnext101_64x4d',\n 'resnext50_32x4d',\n 'retinanet_resnet50_fpn',\n 'retinanet_resnet50_fpn_v2',\n 's3d',\n 'shufflenet_v2_x0_5',\n 'shufflenet_v2_x1_0',\n 'shufflenet_v2_x1_5',\n 'shufflenet_v2_x2_0',\n 'squeezenet1_0',\n 'squeezenet1_1',\n 'ssd300_vgg16',\n 'ssdlite320_mobilenet_v3_large',\n 'swin3d_b',\n 'swin3d_s',\n 'swin3d_t',\n 'swin_b',\n 'swin_s',\n 'swin_t',\n 'swin_v2_b',\n 'swin_v2_s',\n 'swin_v2_t',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'vit_b_16',\n 'vit_b_32',\n 'vit_h_14',\n 'vit_l_16',\n 'vit_l_32',\n 'wide_resnet101_2',\n 'wide_resnet50_2']</pre> In\u00a0[11]: Copied! <pre>model = models.alexnet(weights='DEFAULT') # equivalent to ``models.alexnet(weights='IMAGENET1K_V1')``\nprint(model.children)\n</pre> model = models.alexnet(weights='DEFAULT') # equivalent to ``models.alexnet(weights='IMAGENET1K_V1')`` print(model.children) <pre>&lt;bound method Module.children of AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)&gt;\n</pre> In\u00a0[12]: Copied! <pre># We take the last layer in the network, which is the output layer. In this case, it is the 6-th layer inside `classifier`\nprint(f'Initian output layer of the network: {model.classifier[6]}')\nin_features = model.classifier[6].in_features\nprint(f'Input features of the output layer: {in_features}')\n\n# We replace the last layer and set the output size to 2 since we have 2 classes\nnew_output_layer = nn.Linear(in_features=in_features, out_features=2)\nmodel.classifier[6] = new_output_layer\nprint(f'New output layer of the network: {model.classifier[6]}')\n</pre> # We take the last layer in the network, which is the output layer. In this case, it is the 6-th layer inside `classifier` print(f'Initian output layer of the network: {model.classifier[6]}') in_features = model.classifier[6].in_features print(f'Input features of the output layer: {in_features}')  # We replace the last layer and set the output size to 2 since we have 2 classes new_output_layer = nn.Linear(in_features=in_features, out_features=2) model.classifier[6] = new_output_layer print(f'New output layer of the network: {model.classifier[6]}') <pre>Initian output layer of the network: Linear(in_features=4096, out_features=1000, bias=True)\nInput features of the output layer: 4096\nNew output layer of the network: Linear(in_features=4096, out_features=2, bias=True)\n</pre> In\u00a0[13]: Copied! <pre># We move the model to the selected device (either GPU or CPU)\nmodel = model.to(device)\n\n# We define a loss function\ncriterion = nn.CrossEntropyLoss()\n\n# We define an optimizer. All parameters are being optimized\noptimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=25 * 1e-5)\n</pre> # We move the model to the selected device (either GPU or CPU) model = model.to(device)  # We define a loss function criterion = nn.CrossEntropyLoss()  # We define an optimizer. All parameters are being optimized optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=25 * 1e-5) In\u00a0[14]: Copied! <pre>def fit(model, criterion, optimizer, epochs=25, save_path='results/', save_name='best_model_params.pt', eval_interval=1):\n    start_time = time.time()\n    \n    if not os.path.isdir(save_path):\n        os.mkdir(save_path) \n    model_params_path = os.path.join(save_path, save_name)\n\n    torch.save(model.state_dict(), model_params_path)\n    best_acc = 0.0\n\n    for epoch in range(1, epochs + 1):\n        print(f'Epoch {epoch} / {epochs}')\n        print('*' * 20)\n\n        epoch_start = time.time()\n        # Training phase\n        # We need to set the model to training mode\n        model.train()\n        \n        total_loss = 0.0\n        total_correct_preds = 0\n\n        for inputs, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch} / {epochs}\"):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # get output of model\n            outputs = model(inputs)\n\n            # calculate the loss\n            loss = criterion(outputs, labels)\n\n            # backpropagation + optimize the weights\n            loss.backward()\n            optimizer.step()\n\n            # the prediction is the class with the highest probability\n            _, preds = torch.max(outputs, 1)\n\n            # Batch stats\n            total_loss += loss.item() * inputs.size(0)\n            total_correct_preds += torch.sum(preds == labels.data)\n\n\n        # Epoch stats\n        epoch_loss = total_loss / len(train_dataloader.dataset)\n        epoch_acc = total_correct_preds / len(train_dataloader.dataset)\n\n        print(f'Training Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}')\n        \n\n        if epoch % eval_interval == 0:\n            test_loss, test_acc = predict(model, criterion)\n\n            # save the model\n            if test_acc &gt; best_acc:\n                best_acc = test_acc\n                torch.save(model.state_dict(), model_params_path)\n\n        print()\n        time_elapsed = time.time() - epoch_start\n        print(f'Epoch complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n        print()\n    \n    time_elapsed = time.time() - start_time\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best evaluation Accuracy: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(torch.load(model_params_path, weights_only=True))\n    return model\n</pre> def fit(model, criterion, optimizer, epochs=25, save_path='results/', save_name='best_model_params.pt', eval_interval=1):     start_time = time.time()          if not os.path.isdir(save_path):         os.mkdir(save_path)      model_params_path = os.path.join(save_path, save_name)      torch.save(model.state_dict(), model_params_path)     best_acc = 0.0      for epoch in range(1, epochs + 1):         print(f'Epoch {epoch} / {epochs}')         print('*' * 20)          epoch_start = time.time()         # Training phase         # We need to set the model to training mode         model.train()                  total_loss = 0.0         total_correct_preds = 0          for inputs, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch} / {epochs}\"):             inputs = inputs.to(device)             labels = labels.to(device)              # zero the parameter gradients             optimizer.zero_grad()              # get output of model             outputs = model(inputs)              # calculate the loss             loss = criterion(outputs, labels)              # backpropagation + optimize the weights             loss.backward()             optimizer.step()              # the prediction is the class with the highest probability             _, preds = torch.max(outputs, 1)              # Batch stats             total_loss += loss.item() * inputs.size(0)             total_correct_preds += torch.sum(preds == labels.data)           # Epoch stats         epoch_loss = total_loss / len(train_dataloader.dataset)         epoch_acc = total_correct_preds / len(train_dataloader.dataset)          print(f'Training Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}')                   if epoch % eval_interval == 0:             test_loss, test_acc = predict(model, criterion)              # save the model             if test_acc &gt; best_acc:                 best_acc = test_acc                 torch.save(model.state_dict(), model_params_path)          print()         time_elapsed = time.time() - epoch_start         print(f'Epoch complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')         print()          time_elapsed = time.time() - start_time     print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')     print(f'Best evaluation Accuracy: {best_acc:4f}')      # load best model weights     model.load_state_dict(torch.load(model_params_path, weights_only=True))     return model <p>For testing, we don't need to calculate the gradient. In order to calculate the gradient, PyTorch saves all the operations performed on the data. This uses memory and is unnecessary during testing. Thus, we need to call torch.no_grad(). This tells the PyTorch library not to save the following operations.</p> In\u00a0[15]: Copied! <pre>def predict(model, criterion):\n    # Testing phase\n    # We need to set the model to evaluation mode\n    model.eval()\n    \n    total_loss = 0.0\n    total_correct_preds = 0\n\n    with torch.no_grad():  \n        for inputs, labels in test_dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n    \n            # get output of model\n            outputs = model(inputs)\n\n            # calculate the loss\n            loss = criterion(outputs, labels)\n        \n            # the prediction is the class with the highest probability\n            _, preds = torch.max(outputs, 1)\n    \n            # stats\n            total_loss += loss.item() * inputs.size(0)\n            total_correct_preds += torch.sum(preds == labels.data)\n            \n    test_loss = total_loss / len(test_dataloader.dataset)\n    test_acc = total_correct_preds / len(test_dataloader.dataset)\n\n    print(f'Test Loss: {test_loss:.4f} Accuracy: {test_acc:.4f}')\n\n    return test_loss, test_acc\n</pre> def predict(model, criterion):     # Testing phase     # We need to set the model to evaluation mode     model.eval()          total_loss = 0.0     total_correct_preds = 0      with torch.no_grad():           for inputs, labels in test_dataloader:             inputs = inputs.to(device)             labels = labels.to(device)                  # get output of model             outputs = model(inputs)              # calculate the loss             loss = criterion(outputs, labels)                      # the prediction is the class with the highest probability             _, preds = torch.max(outputs, 1)                  # stats             total_loss += loss.item() * inputs.size(0)             total_correct_preds += torch.sum(preds == labels.data)                  test_loss = total_loss / len(test_dataloader.dataset)     test_acc = total_correct_preds / len(test_dataloader.dataset)      print(f'Test Loss: {test_loss:.4f} Accuracy: {test_acc:.4f}')      return test_loss, test_acc In\u00a0[16]: Copied! <pre>model_conv = fit(model, criterion, optimizer, epochs=5, \n                save_path='results/', save_name='best_model_params.pt', eval_interval=1)\n\n# Evaluate the model again after training is complete\nprint()\nprint('Final results:')\ntest_loss, test_acc = predict(model, criterion)\n</pre> model_conv = fit(model, criterion, optimizer, epochs=5,                  save_path='results/', save_name='best_model_params.pt', eval_interval=1)  # Evaluate the model again after training is complete print() print('Final results:') test_loss, test_acc = predict(model, criterion) <pre>Epoch 1 / 5\n********************\n</pre> <pre>Epoch 1 / 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:55&lt;00:00, 17.87it/s]\n</pre> <pre>Training Loss: 0.3155 Accuracy: 0.8865\nTest Loss: 0.1509 Accuracy: 0.9600\n\nEpoch complete in 0m 58s\n\nEpoch 2 / 5\n********************\n</pre> <pre>Epoch 2 / 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:52&lt;00:00, 18.91it/s]\n</pre> <pre>Training Loss: 0.2096 Accuracy: 0.9250\nTest Loss: 0.2099 Accuracy: 0.9200\n\nEpoch complete in 0m 54s\n\nEpoch 3 / 5\n********************\n</pre> <pre>Epoch 3 / 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:51&lt;00:00, 19.40it/s]\n</pre> <pre>Training Loss: 0.1484 Accuracy: 0.9340\nTest Loss: 0.1556 Accuracy: 0.9300\n\nEpoch complete in 0m 53s\n\nEpoch 4 / 5\n********************\n</pre> <pre>Epoch 4 / 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:49&lt;00:00, 20.18it/s]\n</pre> <pre>Training Loss: 0.1309 Accuracy: 0.9495\nTest Loss: 0.0947 Accuracy: 0.9500\n\nEpoch complete in 0m 51s\n\nEpoch 5 / 5\n********************\n</pre> <pre>Epoch 5 / 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:49&lt;00:00, 20.22it/s]\n</pre> <pre>Training Loss: 0.0908 Accuracy: 0.9680\nTest Loss: 0.0738 Accuracy: 0.9800\n\nEpoch complete in 0m 51s\n\nTraining complete in 4m 27s\nBest evaluation Accuracy: 0.980000\n\nFinal results:\nTest Loss: 0.0738 Accuracy: 0.9800\n</pre> In\u00a0[17]: Copied! <pre>## Lists to save the results for visualization\ninputs = []\nlabels = []\npreds = []\n\n## Predict on the test dataset and \nmodel.eval()\nwith torch.no_grad():\n    for i, (input, label) in enumerate(test_dataloader):\n        input = input.to(device)\n        label = label.to(device)\n        output = model(input)\n        pred = output.argmax(-1)\n\n        if not torch.equal(label, pred):\n            inputs.append(input)\n            labels.append(label)\n            preds.append(pred)\n\n## Concatenate the results for visualization\ninputs = torch.cat(inputs)\nlabels = torch.cat(labels)\npreds = torch.cat(preds)\n\n## Plot the results\nplot_example(inputs.cpu(), preds.cpu(), show_labels=True)\n</pre> ## Lists to save the results for visualization inputs = [] labels = [] preds = []  ## Predict on the test dataset and  model.eval() with torch.no_grad():     for i, (input, label) in enumerate(test_dataloader):         input = input.to(device)         label = label.to(device)         output = model(input)         pred = output.argmax(-1)          if not torch.equal(label, pred):             inputs.append(input)             labels.append(label)             preds.append(pred)  ## Concatenate the results for visualization inputs = torch.cat(inputs) labels = torch.cat(labels) preds = torch.cat(preds)  ## Plot the results plot_example(inputs.cpu(), preds.cpu(), show_labels=True) <p>Let's compare the predictions of the model with the ground truth labels.</p> In\u00a0[18]: Copied! <pre>plot_example(inputs.cpu(), labels.cpu(), show_labels=True)\n</pre> plot_example(inputs.cpu(), labels.cpu(), show_labels=True) <p>Question: Do you think the model is doing a good job at classifying the images? For the examples where the model is wrong, can you tell why it is wrong?</p> <p>Question: Can you think of ways to improve the model?</p>  CONGRATULATIONS  <p>You finished the tutorial. If you want to see this model being run in real time, you can visit the COMET portal: https://comet.nerc.ac.uk/comet-volcano-portal/.</p> <p>Select a volcano you want and scroll to to bottom of the page. On top of identifying deformation in images, the model on the portal can also pinpoint the location of the deformation by showing which parts of the image have a higher probability.</p> <p>If you want to try your own image, you can use the code below:</p> In\u00a0[19]: Copied! <pre># If you stopped the notebook, you need to redefine the model. You can either run the code cells above that define the model\n# or uncomment the next lines\n# model = models.alexnet(weights='DEFAULT')\n# in_features = model.classifier[6].in_features\n# new_output_layer = nn.Linear(in_features=in_features, out_features=2)\n# model.classifier[6] = new_output_layer\n# model.to(device)\n\n# Uncomment the next line to load the model checkpoint\n# This only needs to be done if you closed the notebook or if you want to load a different checkpoint\n# model.load_state_dict(torch.load('results/best_model_params.pt', weights_only=True))\n\n\n# Replace this line with an image loaded by you\nimage = np.random.rand(500, 500, 3)\n\n# If your image has only one channel, uncomment the next line\n# image = torch.cat([image, image, image], dim=1)\n\n# We added a resize in case your image is not the correct size\nimage_transforms = transforms.Compose([\n    # Changes the input to a tensor, also brings the channels to the front, so image.shape = (3, 227, 227) after this operation\n    transforms.ToTensor(),\n    # In case the image is in another format\n    transforms.ConvertImageDtype(torch.float),\n    # Resize the image to the correct size\n    transforms.Resize((227, 227)),\n])\n\n# We apply the transforms to the image\nimage = image_transforms(image)\n# We change the shape of the image to 1x3x227x227 because the model needs a batch of images, so we shape our image as a batch of size 1\n# Here, image.shape = (3, 227, 227)\nimage = image.view(1, *image.shape)\n\nwith torch.no_grad():\n    image = image.to(device)\n    output = model(image)\n    preds = output.argmax(dim=-1)\n    probs = output.softmax(dim=-1)\n\n# We use image[0] to select the first image in our batch of size 1 (the only image in the batch)\nplot_example(image.cpu(), preds.cpu(), show_labels=True)\n</pre> # If you stopped the notebook, you need to redefine the model. You can either run the code cells above that define the model # or uncomment the next lines # model = models.alexnet(weights='DEFAULT') # in_features = model.classifier[6].in_features # new_output_layer = nn.Linear(in_features=in_features, out_features=2) # model.classifier[6] = new_output_layer # model.to(device)  # Uncomment the next line to load the model checkpoint # This only needs to be done if you closed the notebook or if you want to load a different checkpoint # model.load_state_dict(torch.load('results/best_model_params.pt', weights_only=True))   # Replace this line with an image loaded by you image = np.random.rand(500, 500, 3)  # If your image has only one channel, uncomment the next line # image = torch.cat([image, image, image], dim=1)  # We added a resize in case your image is not the correct size image_transforms = transforms.Compose([     # Changes the input to a tensor, also brings the channels to the front, so image.shape = (3, 227, 227) after this operation     transforms.ToTensor(),     # In case the image is in another format     transforms.ConvertImageDtype(torch.float),     # Resize the image to the correct size     transforms.Resize((227, 227)), ])  # We apply the transforms to the image image = image_transforms(image) # We change the shape of the image to 1x3x227x227 because the model needs a batch of images, so we shape our image as a batch of size 1 # Here, image.shape = (3, 227, 227) image = image.view(1, *image.shape)  with torch.no_grad():     image = image.to(device)     output = model(image)     preds = output.argmax(dim=-1)     probs = output.softmax(dim=-1)  # We use image[0] to select the first image in our batch of size 1 (the only image in the batch) plot_example(image.cpu(), preds.cpu(), show_labels=True) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#finetuning-alexnet-for-volcanic-deformation-classification","title":"Finetuning AlexNet for Volcanic Deformation Classification\u00b6","text":"Run in Google Colab <p>Note: If you are running this in a colab notebook, we recommend you enable a free GPU by going:</p> <p>Runtime \u2006\u2006\u2192\u2006\u2006 Change runtime type \u2006\u2006\u2192\u2006\u2006 Hardware Accelerator: GPU</p> <p>This tutorial demonstrates how to fine-tune a pre-trained AlexNet model on the LICS dataset, which consists of volcanic InSAR (Interferometric Synthetic Aperture Radar) images.</p> <p>As you recall, AlexNet is a convolutional neural network that won the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). You can watch this video to learn more about the AlexNet model: link.</p> <p>The dataset contains 6,808 images, divided into two categories: 3,605 images displaying deformation signals (class 1 'volcano_deformation') and 3,203 images without deformation signals (class 0 'background_noise'). Originally, the pre-trained AlexNet was designed to classify 1,000 types of natural objects, such as tables, cars, dogs, etc.</p> <p>In this tutorial, we will modify the final layer of the neural network to classify images into just two classes instead of the original 1,000.</p> <p></p> <p>Created by Robert Gabriel Popescu and Juliet Biggs, University of Bristol, UK.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#lics-dataset","title":"LiCS Dataset\u00b6","text":"<p>Download and Unzip the dataset and make sure the folder LiCS has two subfolders: background_noise and volcano_deformation.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#imports","title":"Imports\u00b6","text":"<p>We will import the necessary libraries:</p> <ul> <li>torch is the main library that will run the Artificial neural network, called PyTorch</li> <li>matplotlib is for creating graphs</li> <li>numpy is for storying arrays of data. A few other libraries work only with this (such as matplotlib)</li> <li>sklearn is a machine learning library that we will use to separate the dataset into train and test</li> <li>torchvision is a library for loading images as datasets to be used by PyTorch</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#running-the-models-on-the-gpu","title":"Running the models on the GPU\u00b6","text":"<p>We want to run the code on the GPU if possible. Artifial neural networks work faster on a GPU so we will select the device cuda if it is available. Alternatively, if you are using a Mac, you want to select mps.</p> <p>If you encounter problems with the memory, manually setting this to the CPU will solve the problem, but it will also slow down the training process.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#loading-the-dataset","title":"Loading the dataset\u00b6","text":"<p>We will load the LICS dataset that contains two folders, one with images of volcano deformation and one with images of background noise. If the dataset is not in the same folder as this notebook, change dataset_dir to the correct path of the dataset.</p> <p>We need to define the transformations that will be applied to the images. In this case, we will only transform the images into tensors, which is how PyTorch stores and uses data. This will also bring the images from the interval [0,255] (the interval used to store RGB images) to [0,1]. Neural networks work better with small numbers, ideally between the interval [-1,1] or [0,1].</p> <p>Tensors are a specialized data structure that is similar to arrays and matrices. Tensors are used to encode the inputs and outputs of a model, as well as the parameters of the model. Tensors are similar to NumPy\u2019s ndarrays, except that tensors can run on GPUs or other hardware accelerators. They are also optimized for automatic differentiation.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#visualizing-the-dataset","title":"Visualizing the dataset\u00b6","text":"<p>We will create a function to visualize some of the images inside the dataset. Then we will load some images using the train dataloader and visualize them.</p> <p>The images are wrapped InSAR interferograms. Each fringe represents 2.8 cm of displacement. The images are automatically processed from satellite data. You can read more about InSAR images here.</p> <p>Watch this video to understand how InSAR works: link</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#splitting-the-dataset-into-train-and-test","title":"Splitting the dataset into train and test\u00b6","text":"<p>PyTorch allows us to split a dataset into two parts only at random, so if we want to have the same number of images from each class into both the train and the test dataset, we need to use a function from sklearn: train_test_split.</p> <p>In general, the size of the test dataset is much smaller than the training dataset. Here we use a training dataset that is ten times bigger than the test. The optimum split depends on the use case, the model used, the size of the dataset, etc., but a generally good split is 80% train and 20% test.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#creating-a-dataloader","title":"Creating a DataLoader\u00b6","text":"<p>A DataLoader is responsible for loading the data, in our case images, from the disk into memory during training or testing.</p> <p>We need to decide how many images are loaded at once by setting batch_size. The batch_size option influences how fast and efficient the model trains, but a bigger batch size also means more memory is required. A bigger batch size can also yield higher accuracy.</p> <p>The argument shuffle decides if the order in which the images are loaded is random or not. We will make the order random for training but not for testing. Making the order random when training can boost the final accuracy of the model.</p> <p>Here, the batch size is set to 2. You can increase this number if you run the program on a GPU or a strong CPU.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#if-you-get-an-error-like-this-torchcudaoutofmemoryerror-cuda-out-of-memory-one-way-of-solving-it-is-to-choose-a-smaller-number-for-the-batch_size-option-if-this-doesnt-work-as-a-last-resort-you-can-choose-to-run-the-model-on-the-cpu-instead-of-the-gpu-from-the-second-code-cell","title":"If you get an error like this: torch.cuda.OutOfMemoryError: CUDA out of memory, one way of solving it is to choose a smaller number for the batch_size option. If this doesn't work, as a last resort, you can choose to run the model on the CPU instead of the GPU from the second code cell.\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#training-an-artificial-neural-network","title":"Training an Artificial Neural Network\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#introduction","title":"Introduction\u00b6","text":"<p>Artificial Neural Networks, or ANNs for short, are Machine Learning models inspired by biological neural networks that are found in animal brains.</p> <p>An ANN is a collection of nodes called artificial neurons that abstract the neurons found in a brain. An artificial neuron, like its biological counterpart, receives signals from other neurons, processes them and sends its own signal to other neurons connected to it. The signal, or the input, received from a connection with another neuron is a real number, and each connection has a weight associated with it that is adjusted during training. The output of the neuron is computed by applying a non-linear function to the weighted sum of the inputs. Neurons are generally grouped into layers and each layer may perform a different transformation to its inputs. The first layer is called the input layer and it has the same number of neurons as the number of dimensions of the input. The last layer is called the output layer and the layers in-between are known as hidden layers. An example of a network is presented in the figure below.</p> <p>In our case, the input is an RGB image of size 227x227. Since RBG images have 3 channels for colours, each pixel has 3 values, so our input layer has 3x227x227=154587 neurons. For the network that we will use, the hidden layers are fixed. We will modify the output layer to have 2 neurons.</p> <p>The model for one neuron is: \\begin{align*} y = \\varphi(\\sum_{i} w_i x_i - b) \\end{align*} where $x_i$ is the i<sup>th</sup> input received, $w_i$ is the weight adjusting that input, $\\varphi$ is a non-linear function, $b$ is the bias and $y$ is the output.</p> <p>The input of the network is a feature vector $\\mathbf{x} = \\mathbf{x}^0$. Denoting all the weights of a neuron as $\\mathbf{w} = [w_0, w_1, \\dots]$ and the weights of all neurons in a layer as $W = [\\mathbf{w_0}, \\mathbf{w_1}, \\dots]$, the output of a layer $l$ can be written as: \\begin{align*} \\mathbf{y}^l = \\varphi(W^l \\mathbf{x}^l - \\mathbf{b}^l) \\end{align*} The output of the network will be a vector $\\mathbf{y}^N$, with $N$ being the number of layers in the network.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#the-backpropagation-algorithm","title":"The Backpropagation Algorithm\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#intuition","title":"Intuition\u00b6","text":"<p>In order for the network to learn, an algorithm is needed to update the weights of the neurons such that the output of the network gets closer to the desired one.</p> <p>In supervised learning, each input $\\mathbf{x}^0$ is paired with an output $\\mathbf{y}^*$. In a classification problem, $\\mathbf{y}^*$ will be a one-hot vector with the bit that represents the class of the input set to $1$. You can read more about one-hot vectors here.</p> <p>The loss function or cost function $\\mathcal{L}(\\mathbf{y}^N, \\mathbf{y}^*)$ is a function that computes the distance between the current output of the network and the desired one. The gradient descent method relies on calculating the derivative of the loss function with respect to the weights of the network.</p> <p>\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}^l} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}^{l+1}} \\frac{\\partial \\mathbf{y}^{l+1}}{\\partial \\mathbf{y}^{l}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W^l} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}^l} \\frac{\\partial \\mathbf{y}^l}{\\partial W^l} \\end{align*}</p> <p>The weights can then be updated: \\begin{equation*} W^l \\leftarrow W^l - \\eta\\frac{\\partial \\mathcal{L}}{\\partial W^l} \\end{equation*} where $\\eta &gt; 0$ is a given learning rate. You can read more about the learning rate here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#loss-function","title":"Loss function\u00b6","text":"<p>The loss function needs to fulfil the following conditions:</p> <ul> <li>it has a derivative</li> <li>it is written as a function of the outputs of the network</li> <li>it can be written as an average $\\mathcal{L}(Y^N, Y^*) = \\frac{1}{n} \\sum_\\mathbf{y} \\mathcal{L}(\\mathbf{y}^N, \\mathbf{y}^*) $</li> </ul> <p>The last condition exists because a network is trained, in general, with a set of samples at once, known as a batch. The aim of training the network is to achieve a loss equal to $0$ on the entire dataset, as such using the whole dataset at once to calculate the gradient would be ideal. Unfortunately, this is unfeasible on big datasets. For this reason, small batches that fit into memory are used instead. They provide a more accurate update for the weights than using one sample at a time, while not being too computationally and memory intensive. Splitting the dataset into multiple batches, and then iterating over all of them is known as an epoch.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#the-algorithm","title":"The algorithm\u00b6","text":"<p>Since calculating the gradient of the weights depends only on the output of the current and next layer, the Backpropagation algorithm calculates the updates for the entire network starting from the last layer. The current result is then used to calculate the gradients for the previous layer until the entire network is updated. The stopping criteria for the algorithm could be completing a number of epochs, achieving a certain accuracy or not producing any significant changes. The algorithm can be seen below.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#learning-methods","title":"Learning methods\u00b6","text":"<p>Online gradient descent uses one sample per iteration, which only roughly approximates aspects of the cost function. This results in a noisy gradient descent that may not find the local minimum. Deterministic gradient descent uses the entire dataset in every iteration and, given enough time, will find the true local minimum. As discussed previously, this has a high computational cost and is not usable in practice. Stochastic gradient descent (SGD) uses a batch of samples in every iteration, thus achieving a good approximation of the real gradient. This especially reduces the computational cost for high-dimensional optimization problems, in trade for a lower convergence rate.</p> <p>Many improvements have been proposed to SGD. In machine learning, in particular, the result of gradient descent relies too much on setting a good learning rate. Setting this parameter too high will result in the algorithm diverging, meanwhile setting it too low will make learning slow. Denoting $\\Delta\\mathcal{J}(X, W_t) = \\frac{\\partial \\mathcal{L}}{\\partial W_t}$ as the steepest gradient for input $X$ and weights $W_t$ at iteration $t$, the SGD learning rule can be written as:</p> <p>\\begin{equation*} W_{t+1} = W_t - \\eta\\Delta\\mathcal{J}(X, W_t) \\end{equation*}</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#choosing-a-pre-trained-network","title":"Choosing a pre-trained network\u00b6","text":"<p>We will use a pre-trained network to speed up our training time. We can use the command below to check all the available pre-trained networks in PyTorch.</p> <p>Not all networks have the same input size, so you need to check what input a network expects before using it.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#alexnet","title":"AlexNet\u00b6","text":"<p>We will choose AlexNet as our pre-trained network. AlexNet expects an RGB image of size 227x227.</p> <p>We also check the name of all the layers, as we need to know the name of the last layer. We will replace this layer since it doesn't fit our case. AlexNet was trained to classify images into 1000 classes, so the output of the network is an array of size 1000. We only have two classes, so we need to modify the output layer.</p> <p>We take a look at the layers of the AlexNet and identify the last layer. The last layer, or the output layer, is the layer numbered as 6 in the classifier part of the network. It is a Linear layer, also known as a fully connected layer, and has an input of 4096 numbers and an output of 1000 numbers, corresponding to the 1000 classes. The input of this layer comes from the output of the previous layer, so it cannot be changed, but we will change the output to just 2 numbers, to correspond to the 2 classes we have.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#replace-the-last-layer-of-the-network","title":"Replace the last layer of the network\u00b6","text":"<p>Since we have two classes, volcano_deformation and background_noise, we need a layer with 2 neurons. We create a new Linear layer with the same input size of 4096, but with an output of 2. We then set our newly created layer in position 6 in the classifier.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#loss-function-and-optimizer","title":"Loss function and Optimizer\u00b6","text":"<p>We define a loss function and an optimizer. The loss function will be Cross Entropy Loss and the optimizer will be Adam.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#cross-entropy-loss","title":"Cross Entropy Loss\u00b6","text":"<p>Cross-entropy loss measures the performance of a classification model whose output should represent a set of probabilities. Since the output of a network is not guaranteed to be a set of numbers from the interval [0,1] whose sum is 1, the output is also normalized. You can read more about Cross Entropy Loss here and here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#adam","title":"Adam\u00b6","text":"<p>Adaptive Moment Estimation (Adam) is an optimizer, where running averages of both the gradients and the second moments of the gradients are used. You can read more about Adam here and here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>For training, we get a batch of images and we run the batch through the model and get an output. We use the loss function to get a loss value.</p> <p>By calling loss.backwards(), we calculate the gradient of the loss value by using the Backpropagation algorithm. We can then call optimizer.step() to update the weights of the network and perform the gradient descent. Since the function step() is called on the optimizer, the selected optimizing algorithm is used to update the weights.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#running-the-model","title":"Running the model\u00b6","text":"<p>Now we can just call the training function to train the model on our dataset.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/11_neural_networks3/#visualizing-the-predictions-of-the-model","title":"Visualizing the predictions of the model\u00b6","text":"<p>Finally, we can visualize the predictions of the model.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_assignment_glacial_cycles/","title":"W10 assignment glacial cycles","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <p>After you have done your analysis, you will find that there are different periods for the more recent glacial-interglacial cycles than the more recent ones.</p> <p>When did this transition happen? (2 points)</p> <p>write answer here</p> <p>What is the period of the more recent glacial-interglacial cycles? What orbital forcing might it correspond to? (2 points)</p> <p>write answer here</p> <p>What is the period of the more ancient glacial-interglacial cycles? What orbital forcing might it correspond to? (2 points)</p> <p>write answer here</p> <p>This change in glacial-interglacial cycle period is often called the mid-Pleistocene transition. Using Google Scholar, find a paper that presents a hypothesis for what caused the transition. Provide the reference here and write a paragraph summarizing the hypothesis (6 points)</p> <p>write answer here</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_assignment_glacial_cycles/#glacial-cycle-assignment","title":"Glacial Cycle Assignment\u00b6","text":"<p>Your assignment is conduct spectral analysis on different portions of the Lisecki and Raymo (2005) benthic oxygen isotope data to determine the period of glacial-interglacial cycles. Choose at least 4 different intervals to look at with each analyzed interval being &gt;500,000 years. A good place to start will be to look at data from the 800,000 years and compare it to data from the 800,000 to 1,600,000 year (remember 1000 years ago is 1 ka).</p> <p>Go ahead and filter the data between for these different times, plot the data and determine what frequencies have significant spectral power. (12 points)</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/","title":"W10 inclass glacial cycles","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd In\u00a0[\u00a0]: Copied! <pre>orbital_cycles = pd.read_csv('./data/INSOLN.LA2004.BTL.100.csv')\norbital_cycles.head()\n</pre> orbital_cycles = pd.read_csv('./data/INSOLN.LA2004.BTL.100.csv') orbital_cycles.head() <p>Code for you to write</p> <p>Define a variable called <code>orbital_cycles_1Myr</code> that is filtered to only have data for the past 1 million years (1000 ka).</p> In\u00a0[\u00a0]: Copied! <pre>orbital_cycles_1Myr = \n</pre> orbital_cycles_1Myr =  In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8,8))\n\nplt.subplot(3,1,1)\nplt.plot(orbital_cycles_1Myr['Age (ka)'],orbital_cycles_1Myr['Eccentricity'])\nplt.ylabel('Eccentricity')\n\nplt.subplot(3,1,2)\nplt.plot(orbital_cycles_1Myr['Age (ka)'],orbital_cycles_1Myr['Obliquity'])\nplt.ylabel('Obliquity')\n\nplt.subplot(3,1,3)\nplt.plot(orbital_cycles_1Myr['Age (ka)'],orbital_cycles_1Myr['Precession'])\nplt.ylabel('Precession')\nplt.xlabel('Age (ka)')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(8,8))  plt.subplot(3,1,1) plt.plot(orbital_cycles_1Myr['Age (ka)'],orbital_cycles_1Myr['Eccentricity']) plt.ylabel('Eccentricity')  plt.subplot(3,1,2) plt.plot(orbital_cycles_1Myr['Age (ka)'],orbital_cycles_1Myr['Obliquity']) plt.ylabel('Obliquity')  plt.subplot(3,1,3) plt.plot(orbital_cycles_1Myr['Age (ka)'],orbital_cycles_1Myr['Precession']) plt.ylabel('Precession') plt.xlabel('Age (ka)') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>insolation = pd.read_csv('./data/j_65north.csv')\ninsolation.head()\n</pre> insolation = pd.read_csv('./data/j_65north.csv') insolation.head() <p>Code for you to write</p> <p>Make a plot using <code>plt.plot</code> of the insolation curve. Use <code>plt.xlim()</code> to make it so that time is limited and goes from 1000 ka to 0 ka (1,000 thousand years ago [same as 1 million] to present-day).</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>ice_core_co2 = pd.read_csv('./data/antarctica2015co2composite.txt',header=137,sep='\\t')\nice_core_co2['age_ka'] = ice_core_co2['age_gas_calBP']/1000\nice_core_co2.tail()\n</pre> ice_core_co2 = pd.read_csv('./data/antarctica2015co2composite.txt',header=137,sep='\\t') ice_core_co2['age_ka'] = ice_core_co2['age_gas_calBP']/1000 ice_core_co2.tail() <p>To analyze these data, we need to have an evenly spaced timeseries. We can use the scipy interpolation capabilities to do so. Here is an simple example from the scipy documentation:</p> In\u00a0[\u00a0]: Copied! <pre>from scipy.interpolate import interp1d\n\nx = np.linspace(0, 10, num=11, endpoint=True)\ny = np.cos(-x**2/9.0)\nf = interp1d(x, y)\nf2 = interp1d(x, y, kind='cubic')\n\nxnew = np.linspace(0, 10, num=41, endpoint=True)\nplt.plot(x, y, 'o')\nplt.plot(xnew, f(xnew), '-')\nplt.plot(xnew, f2(xnew), '--')\nplt.legend(['data', 'linear', 'cubic'], loc='best')\nplt.show()\n</pre> from scipy.interpolate import interp1d  x = np.linspace(0, 10, num=11, endpoint=True) y = np.cos(-x**2/9.0) f = interp1d(x, y) f2 = interp1d(x, y, kind='cubic')  xnew = np.linspace(0, 10, num=41, endpoint=True) plt.plot(x, y, 'o') plt.plot(xnew, f(xnew), '-') plt.plot(xnew, f2(xnew), '--') plt.legend(['data', 'linear', 'cubic'], loc='best') plt.show() <p>We can apply such an interpolation to the ice core data and resample it a evenly spaced 1 kyr intervals:</p> In\u00a0[\u00a0]: Copied! <pre>ice_interp = interp1d(ice_core_co2['age_ka'], ice_core_co2['co2_ppm'], kind='linear')\n\nxnew = np.linspace(0, 800,num=801,endpoint=True)\nco2_interp = ice_interp(xnew)\nplt.plot(ice_core_co2['age_ka'],ice_core_co2['co2_ppm'], 'o')\nplt.plot(xnew, co2_interp, '-')\nplt.legend(['data', 'interpolation'], loc='best')\nplt.xlim(800,0)\nplt.xlabel('Age (ka)')\nplt.ylabel('CO$_2$ (ppm)')\nplt.show()\n</pre> ice_interp = interp1d(ice_core_co2['age_ka'], ice_core_co2['co2_ppm'], kind='linear')  xnew = np.linspace(0, 800,num=801,endpoint=True) co2_interp = ice_interp(xnew) plt.plot(ice_core_co2['age_ka'],ice_core_co2['co2_ppm'], 'o') plt.plot(xnew, co2_interp, '-') plt.legend(['data', 'interpolation'], loc='best') plt.xlim(800,0) plt.xlabel('Age (ka)') plt.ylabel('CO$_2$ (ppm)') plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8,6))\n\nplt.subplot(2,1,1)\nplt.plot(xnew, co2_interp, '-')\nplt.xlim(800,0)\nplt.xlabel('Age (ka)')\nplt.ylabel('CO$_2$ (ppm)')\n\nplt.subplot(2,1,2)\nplt.plot(insolation['Age (ka)'],insolation['Insolation (W/m^2)'])\nplt.xlim(1000,0)\nplt.ylabel('Insolation (W m$^{-2}$)')\nplt.xlabel('Age (ka)')\nplt.show()\n</pre> plt.figure(figsize=(8,6))  plt.subplot(2,1,1) plt.plot(xnew, co2_interp, '-') plt.xlim(800,0) plt.xlabel('Age (ka)') plt.ylabel('CO$_2$ (ppm)')  plt.subplot(2,1,2) plt.plot(insolation['Age (ka)'],insolation['Insolation (W/m^2)']) plt.xlim(1000,0) plt.ylabel('Insolation (W m$^{-2}$)') plt.xlabel('Age (ka)') plt.show() In\u00a0[\u00a0]: Copied! <pre>from scipy import signal\n</pre> from scipy import signal In\u00a0[\u00a0]: Copied! <pre>co2_freqs,co2_power = signal.periodogram(co2_interp,window='hann')\n\nplt.plot(co2_freqs,co2_power,label='CO$_2$ power spectra',linewidth=2)\n\nplt.xlim(.001,.1)\n\nplt.xlabel('Frequency')\nplt.ylabel('Power') \nplt.legend()\nplt.show()\n</pre> co2_freqs,co2_power = signal.periodogram(co2_interp,window='hann')  plt.plot(co2_freqs,co2_power,label='CO$_2$ power spectra',linewidth=2)  plt.xlim(.001,.1)  plt.xlabel('Frequency') plt.ylabel('Power')  plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>eccentricity_frequency = 1.0/100.0\nobliquity_frequency = 1.0/41.0\n</pre> eccentricity_frequency = 1.0/100.0 obliquity_frequency = 1.0/41.0 In\u00a0[\u00a0]: Copied! <pre>plt.plot(co2_freqs,co2_power,label='CO$_2$ power spectra',linewidth=2)\nplt.axvline(x=eccentricity_frequency,color='red',label='Eccentricity',linewidth=2,linestyle='--') \nplt.axvline(x=obliquity_frequency,color='orange',label='Obliquity',linewidth=2,linestyle='--') \nplt.xlim(.001,.06)\n\nplt.xlabel('Frequency')\nplt.ylabel('Power') \nplt.legend()\nplt.show()\n</pre> plt.plot(co2_freqs,co2_power,label='CO$_2$ power spectra',linewidth=2) plt.axvline(x=eccentricity_frequency,color='red',label='Eccentricity',linewidth=2,linestyle='--')  plt.axvline(x=obliquity_frequency,color='orange',label='Obliquity',linewidth=2,linestyle='--')  plt.xlim(.001,.06)  plt.xlabel('Frequency') plt.ylabel('Power')  plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Code for you to write</p> <p>Plot the periodogram and plot the eccentricity and obliquity lines as well.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>(800000/4.6e9)*100\n</pre> (800000/4.6e9)*100 In\u00a0[\u00a0]: Copied! <pre>LR04_d18O = pd.read_csv('./data/LR04stack.csv')\nLR04_d18O.head()\n</pre> LR04_d18O = pd.read_csv('./data/LR04stack.csv') LR04_d18O.head() In\u00a0[\u00a0]: Copied! <pre>d18O_1Ma=LR04_d18O[LR04_d18O['Age (ka)']&lt;1000] # filter data for last 1Ma\n\nplt.plot(d18O_1Ma['Age (ka)'],d18O_1Ma['d18O'])\nplt.xlabel('Age (ka)')\nplt.ylabel('$\\delta ^{18}$O')\nplt.ylim(5,3)\nplt.xlim(1000,0)\nplt.show()\n</pre> d18O_1Ma=LR04_d18O[LR04_d18O['Age (ka)']&lt;1000] # filter data for last 1Ma  plt.plot(d18O_1Ma['Age (ka)'],d18O_1Ma['d18O']) plt.xlabel('Age (ka)') plt.ylabel('$\\delta ^{18}$O') plt.ylim(5,3) plt.xlim(1000,0) plt.show() <p>Let's do the same signal analysis in order to analyze the power spectra:</p> In\u00a0[\u00a0]: Copied! <pre>d18O_freqs,d18O_power = signal.periodogram(d18O_1Ma['d18O'],window='hann')\n\nplt.plot(d18O_freqs,d18O_power,label='insolation',linewidth=2)\nplt.axvline(x=eccentricity_frequency,color='red',label='Eccentricity',linewidth=2,linestyle='--') \nplt.axvline(x=obliquity_frequency,color='orange',label='Obliquity',linewidth=2,linestyle='--') \nplt.xlim(.001,.06)\n\nplt.xlabel('Frequency')\nplt.ylabel('Power') \nplt.show()\n</pre> d18O_freqs,d18O_power = signal.periodogram(d18O_1Ma['d18O'],window='hann')  plt.plot(d18O_freqs,d18O_power,label='insolation',linewidth=2) plt.axvline(x=eccentricity_frequency,color='red',label='Eccentricity',linewidth=2,linestyle='--')  plt.axvline(x=obliquity_frequency,color='orange',label='Obliquity',linewidth=2,linestyle='--')  plt.xlim(.001,.06)  plt.xlabel('Frequency') plt.ylabel('Power')  plt.show() In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom scipy.signal import periodogram\n\n# Fit an AR1 model to the co2_interp data\nar1_model = ARIMA(co2_interp, order=(1, 0, 0)).fit()\n\n# Generate a large number of synthetic red noise time series\nn_synthetic = 1000\nn_samples = len(co2_interp)\nphi = ar1_model.params[1]\nresiduals = ar1_model.resid\nresidual_variance = np.var(residuals)\nwhite_noise_std = np.sqrt(residual_variance)\n\nsynthetic_power_spectra = np.zeros((n_synthetic, n_samples // 2))\nfor i in range(n_synthetic):\n    red_noise = np.zeros(n_samples)\n    red_noise[0] = np.random.normal(loc=0, scale=white_noise_std)\n    for t in range(1, n_samples):\n        red_noise[t] = phi * red_noise[t - 1] + np.random.normal(loc=0, scale=white_noise_std)\n    \n    red_noise_freqs, synthetic_power_spectrum = periodogram(red_noise, window='hann')\n    synthetic_power_spectra[i, :] = synthetic_power_spectrum[:n_samples // 2]\n\n# Compute the 95th percentile of the synthetic power spectra\nthreshold_power_spectrum = np.percentile(synthetic_power_spectra, 95, axis=0)\n\n# Compute the periodogram for co2_interp data\nco2_freqs, co2_power = periodogram(co2_interp, window='hann')\n\n# Make sure the lengths of co2_freqs and threshold_power_spectrum are the same\nco2_freqs = co2_freqs[:len(threshold_power_spectrum)]\n\n# Plot the power spectra and the significant peaks\nplt.plot(co2_freqs, co2_power[:len(threshold_power_spectrum)], label='CO$_2$ power spectra', linewidth=2)\nplt.plot(co2_freqs, threshold_power_spectrum, label='95% confidence level', linestyle='--', linewidth=1)\n\nplt.xlim(.001, .1)\nplt.xlabel('Frequency')\nplt.ylabel('Power')\nplt.legend()\nplt.show()\n</pre> import numpy as np from scipy.stats import chi2 from statsmodels.tsa.arima.model import ARIMA from scipy.signal import periodogram  # Fit an AR1 model to the co2_interp data ar1_model = ARIMA(co2_interp, order=(1, 0, 0)).fit()  # Generate a large number of synthetic red noise time series n_synthetic = 1000 n_samples = len(co2_interp) phi = ar1_model.params[1] residuals = ar1_model.resid residual_variance = np.var(residuals) white_noise_std = np.sqrt(residual_variance)  synthetic_power_spectra = np.zeros((n_synthetic, n_samples // 2)) for i in range(n_synthetic):     red_noise = np.zeros(n_samples)     red_noise[0] = np.random.normal(loc=0, scale=white_noise_std)     for t in range(1, n_samples):         red_noise[t] = phi * red_noise[t - 1] + np.random.normal(loc=0, scale=white_noise_std)          red_noise_freqs, synthetic_power_spectrum = periodogram(red_noise, window='hann')     synthetic_power_spectra[i, :] = synthetic_power_spectrum[:n_samples // 2]  # Compute the 95th percentile of the synthetic power spectra threshold_power_spectrum = np.percentile(synthetic_power_spectra, 95, axis=0)  # Compute the periodogram for co2_interp data co2_freqs, co2_power = periodogram(co2_interp, window='hann')  # Make sure the lengths of co2_freqs and threshold_power_spectrum are the same co2_freqs = co2_freqs[:len(threshold_power_spectrum)]  # Plot the power spectra and the significant peaks plt.plot(co2_freqs, co2_power[:len(threshold_power_spectrum)], label='CO$_2$ power spectra', linewidth=2) plt.plot(co2_freqs, threshold_power_spectrum, label='95% confidence level', linestyle='--', linewidth=1)  plt.xlim(.001, .1) plt.xlabel('Frequency') plt.ylabel('Power') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#drivers-of-glacial-interglacial-cycles","title":"Drivers of glacial-interglacial cycles\u00b6","text":"<p>We have talked about how 20,000 years ago during the Last Glacial Maximum, much of North America has covered by a thick ice sheet:</p> <p>We have looked at ice core data that shows that CO$_2$ has varied through time:</p> <p>What sets the timing of these glacial-interglacial cycles?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#earths-orbit-pacemaker-of-the-ice-ages","title":"Earth's orbit - pacemaker of the Ice Ages\u00b6","text":"<p>material in this section of the notebook is modified from materials of Lisa Tauxe's Python for Earth Science Students course: https://github.com/ltauxe/Python-for-Earth-Science-Students</p> <p>In 1920, Milutin Milankovitz  explained the coming and going of ice ages as a response to changes in the Earth's insolation (the amount of energy recieved from the sun). He argued that insolation is controlled by changes in the Earth's orbit around the sun.  This idea has now been widely embraced by the paleoclimate community, largely because of the very strong coherence between  cycles in Earth's orbit and evidence for changes in ice volume using geochemical proxies like oxygen isotopes \u2014\u2014 data we will look at today.</p> <p>The orbital cycles are influenced by gravity of the Moon, Sun, Jupiter, and Saturn and can be calculated knowing their orbital parameters.  Milankovitch famously took the first stab at it from his prison cell during WWI. Nowadays it is calculated with supercomputers.  The key parameters: eccentricity, obliquity, and precession.</p> <p>Eccentricity refers to the shape of Earth's orbit around the Sun. It varies between nearly circular (low eccentricity) and more elliptical (high eccentricity) over a period of approximately 100,000 years. Although eccentricity itself does not cause significant changes in the total amount of solar radiation (insolation) received by Earth, it modulates the influence of the other two orbital parameters, obliquity and precession.</p> <p>Obliquity is the tilt of Earth's axis relative to its orbital plane. It changes between 22.1\u00b0 and 24.5\u00b0 over a period of about 41,000 years. Obliquity has a direct impact on insolation, as it determines the seasonal distribution of solar radiation at different latitudes. Higher obliquity generally leads to more pronounced seasons, with warmer summers and colder winters.</p> <p>Precession is the wobble of Earth's axis, which causes the timing of the solstices and equinoxes to change relative to Earth's position in its orbit. It has a period of approximately 19,000 to 23,000 years. Precession affects the seasonal contrast between hemispheres, making one hemisphere's summer more intense while the other hemisphere experiences a milder summer.</p> <p>[Figure from http://www.jamstec.go.jp/e/about/press_release/20141027/; see also http://www.sciencecourseware.org/eec/GlobalWarming/Tutorials/Milankovitch/].</p> <p>The Earth's orbital parameters of ellipticity, obliquity and precession vary in predictable ways.  One commonly used model for variations in them over the last few hundred million years was published by Laskar et al. (2004; http://dx.doi.org/10.1051/0004-6361:20041335).</p> <p>This solution has been improved...by using a direct integration of the gravitational equations for the orbital motion, and by improving the dissipative contributions, in particular in the evolution of the Earth\u2013Moon System. The orbital solution has been used for the calibration of the Neogene period (Lourens et al.\u2002 [CITE]), and is expected to be used for age calibrations of paleoclimatic data over 40 to 50 Myr, eventually over the full Palaeogene period (65 Myr) with caution. Beyond this time span, the chaotic evolution of the orbits prevents a precise determination of the Earth's motion. However, the most regular components of the orbital solution could still be used over a much longer time span. - Laskar et al. (2004)</p> <p>Let's take a look for the behavior of the last few million years using the data file from the Laskar et al. (2004) paper.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#import-the-scientific-python-packages-we-will-need","title":"Import the scientific python packages we will need\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#import-the-lasker-orbital-solution","title":"Import the Lasker orbital solution\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#relating-these-cycles-to-incoming-insolation","title":"Relating these cycles to incoming insolation\u00b6","text":"<p>You can see a lot of cycles on different time scales. The question is how this relates to the amount of insolation.  In the literature, there are many attempts to convert the orbital parameters, like those in the plot above, to the amount of insolation received by the Earth's atmosphere as a function of latitude and age. You can get such estimates here: https://www.ncdc.noaa.gov/paleo-search/study/5792 associated with this paper (Huybers, P. 2006, http://www.sciencemag.org/cgi/content/full/313/5786/508).</p> <p>It is traditional to consider the amount of insolation received at 65$^{\\circ}$N.  So let's take a look.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#how-big-are-these-insolation-changes","title":"How big are these insolation changes?\u00b6","text":"<p>These changes in insolation are interpretted to be important in driving glacial-interglacial cycles. How big are the changes in insolation?</p> <p>Code for you to write</p> <p>Calculate the mean insolation from data (<code>np.mean()</code> could help) and the maximum insolation from the data (<code>np.max()</code> could help. Then use these values to calculate the percentage difference between the maximum insolation and the mean. Do the same between the maximum and the minimum.</p> <p>Does this change seem like a big change sufficient to drive these cycles?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#lets-have-a-look-at-the-ice-core-co_2-data-we-have-been-dealing-with-does-the-signal-match-with-the-insolation-signal","title":"Let's have a look at the ice core CO$_2$ data we have been dealing with. Does the signal match with the insolation signal?\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#spectral-analysis","title":"Spectral analysis\u00b6","text":"<p>Both insolation and $\\delta ^{18}$O have a lot of wiggles over the last million years, but are the wiggles related to each other?  One way to look at this is using time series analysis.   There are entire courses devoted to this subject and a complete treatment is beyond the scope of this class, but we can begin to answer the basic question: Do the two data sets have wiggles with the same frequencies?</p> <p>The analysis boils down to this:</p> <ul> <li>According to Fourier, any periodic function $f(t)$ can be represented as the sum of a series of sines and cosines:</li> </ul> <p>$$f(t)=a_0+ \\sum_{r=1}^\\infty \\bigr[a_r    \\cos  \\bigr( { {2\\pi r}\\over{T}}   t\\bigr)  + b_r    \\sin  \\bigr( { {2\\pi r}\\over{T}}   t\\bigl) \\bigl]  $$</p> <ul> <li>You can represent data as a series in time (in the time-domain) as we have been doing OR you can represent the data in terms of frequency, looking for the power in the data as a function of frequency.  This is known as the power spectrum.</li> </ul> <p>Phone audio demo</p> <p>Let us take advantage of a signal.periodogram function in the scipy package.  That module has functions that allow us to  calculate the power spectral density for a time series.  As a result we will be able to generate a periodogram, which is a plot of power versus frequency.</p> <p>We will also use a window in the periodogram calculation.  What a window does is multiply the time series by a function (called a taper) that weights information, suppressing data at the edges of the window and focussing on the center of the window. This step is necessary as otherwise there are artifacts that are introduced particularly given the finite nature of the data (i.e. that the signal starts and ends).  The simplest window is a box car which gives equal weight to everything inside the window.  In the following, we will use a Hann window_ which looks more like a bell curve. The Hann window is a popular choice for general-purpose spectral analysis as it provides a good balance between frequency resolution and spectral leakage reduction. You can check it out here: https://en.wikipedia.org/wiki/Window_function</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#what-is-the-dominant-signal-in-the-ice-sheet-co_2-data","title":"What is the dominant signal in the ice sheet CO$_2$ data?\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#what-does-this-dominant-peak-correspond-to","title":"What does this dominant peak correspond to?\u00b6","text":"<p>We know that eccentricity is supposed to have a dominant period at 100 kyr, obliquity at 41 kyr and precession at 23 and 19 kyr. Remember that these numbers are expressed in terms of the period, which is the inverse of the frequency so the frequency of them with be the inverse of the period</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#what-is-the-dominant-periodic-signal-in-the-insolation-curve","title":"What is the dominant periodic signal in the insolation curve?\u00b6","text":"<p>Code for you to write</p> <p>Take the same approach above and use the <code>signal.periodogram()</code> function to calculate the power spectral density of the 65$^{\\circ}$N insolation curve.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#what-is-the-dominant-signal-in-the-65on-insolation-curve-and-how-does-it-compare-to-dominant-period-in-the-ice-sheet-co_2-data","title":"What is the dominant signal in the 65\u00baN insolation curve and how does it compare to dominant period in the ice sheet CO$_2$ data?\u00b6","text":"<p>Write your answer here</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#longer-timescale-paleoclimate-records","title":"Longer timescale paleoclimate records\u00b6","text":"<p>The ice core record is a very impressive one, but unfortunately, it only goes back ~800,000 years. While these seems like a long time, it is only 0.02% of the history of the Earth.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#oxygen-isotopes","title":"Oxygen isotopes\u00b6","text":"<p>One way that we can go further back with a paleoclimate record is using marine fossils such as foraminifera. Foraminifera are made of calcium carbonate: CaCO$_3$</p> <p>The oxygen isotopes ($^{16}$O vs $^{18}$O) in the CaCO$_3$ forms a record of both temperature and ice volume. $\\delta$^{18}$O$ is a way to express the ratio between $^{16}$O and $^{18}$O.</p> <p>$$\\delta ^{{18}}O={\\Biggl (}{\\frac  {{\\bigl (}{\\frac  {^{{18}}O}{^{{16}}O}}{\\bigr )}_{{sample}}}{{\\bigl (}{\\frac  {^{{18}}O}{^{{16}}O}}{\\bigr )}_{{standard}}}}-1{\\Biggr )}\\times 1000\\ ^{{o}}\\!/\\!_{{oo}}$$</p> <p>When water is warm, the $\\delta ^{{18}}O$ of the fossils is low and when water is cold $\\delta ^{{18}}O$ is higher. There is a similar relationship with ice volume: when there is less ice the $\\delta ^{{18}}O$ of water is lower and there is more ice is cold $\\delta ^{{18}}O$ of water is higher which ends up contributing to the value of the fossil foraminifera.</p> <p>So higher $\\delta$^{18}$O$ values are associated with a colder planet and lower $\\delta$^{18}$O$ values with a warmer planet.</p> <p>A compilation of these data was published by Lisecki and Raymo (2005, http://dx.doi.org/10.1029/2004PA001071) called the LR04 stack.  This is a stack of 58 records of oxygen isotopic variations, several of which were independently dated using magnetostratigraphy, from all over the world's oceans.</p> <p>Let's import the data and take a look at the record:</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/glacial_cycles/W10_inclass_glacial_cycles/#bonus","title":"Bonus\u00b6","text":"<p>How can we determine that a spectral peak is significant and not just the result of noise in the data?</p> <p>Let's take this approach:</p> <ol> <li><p>Fit an AR1 (Autoregressive order 1) model to the CO2 data (<code>co2_interp</code>): An AR1 model is a simple time series model that assumes that the value at time t depends linearly on the value at time t-1 and some noise. It is often used to model \"red noise\" or \"autocorrelated noise\" that is common in many natural time series like climate data.</p> </li> <li><p>Generate synthetic red noise time series: We generate a large number (<code>n_synthetic = 1000</code>) of synthetic time series using the AR1 model parameters estimated from the CO2 data. Each synthetic time series has the same length (n_samples) as the CO2 data. By generating these synthetic time series, we create a set of surrogate data with similar statistical properties to the original CO2 data but without any real-world signal.</p> </li> <li><p>Compute the power spectra of the synthetic time series: For each synthetic time series, we calculate the power spectrum using the periodogram method with a Hann window. This gives us an estimate of the power at different frequencies in each synthetic time series. We store these power spectra in a 2D array, synthetic_power_spectra.</p> </li> <li><p>Calculate the 95th percentile of the synthetic power spectra: Compute the 95th percentile power spectrum across all the synthetic time series. This spectrum represents the threshold above which we can consider a peak in the CO2 power spectrum as significant (with a 95% confidence level).</p> </li> <li><p>Compute the periodogram of the CO2 data (<code>co2_interp</code>): Calculate the power spectrum of the CO2 data using the periodogram method with a Hann window. This gives us an estimate of the power at different frequencies in the CO2 data.</p> </li> <li><p>Plot the power spectra and the significant peaks: Plot the power spectra of the CO2 data along with the 95th percentile threshold calculated from the synthetic power spectra. Any peaks in the CO2 power spectrum that rise above the 95th percentile threshold can be considered significant, meaning that they are unlikely to have occurred due to random fluctuations (red noise) alone.</p> </li> </ol> <p>Following this approach, we can evaluate our confidence in interpreting the power spectra in the CO2 data. We can identify significant peaks in the power spectrum that are likely to be genuine features of the data rather than artifacts of random fluctuations or noise.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/","title":"W11 NonLinear Regression InClass","text":"In\u00a0[\u00a0]: include Copied! <pre>import math\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport otter\n</pre> import math import numpy as np import pandas as pd from scipy import stats from scipy.optimize import curve_fit import matplotlib.pyplot as plt import otter In\u00a0[\u00a0]: include Copied! <pre>#Read the Peak Ground Acceleration data\npark_pga=pd.read_csv('parkfieldeq_pga.csv')\nnapa_pga=pd.read_csv('napaeq_pga.csv')\npark_pga.head()\n</pre> #Read the Peak Ground Acceleration data park_pga=pd.read_csv('parkfieldeq_pga.csv') napa_pga=pd.read_csv('napaeq_pga.csv') park_pga.head() In\u00a0[\u00a0]: include Copied! <pre>#Plot the two data sets\n\nfig, ax = plt.subplots()\nplt.plot(park_pga['Dist(km)'],park_pga['PGA(g)'],'.',color='blue',alpha=0.2)\nplt.plot(napa_pga['Dist(km)'],napa_pga['PGA(g)'],'.',color='green')\nax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',\n       title='Peak Acceleration Data Linear Plot')\nplt.legend(['Napa','Parkfield'],fontsize=12)\nplt.show()\n\nfig, ax = plt.subplots()\nplt.loglog(park_pga['Dist(km)'],park_pga['PGA(g)'],'.',color='blue',alpha=0.2)\nplt.loglog(napa_pga['Dist(km)'],napa_pga['PGA(g)'],'.',color='green')\nax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',\n       title='Peak Acceleration Data Log Plot')\nplt.legend(['Napa','Parkfield'],fontsize=12)\nplt.show()\n</pre> #Plot the two data sets  fig, ax = plt.subplots() plt.plot(park_pga['Dist(km)'],park_pga['PGA(g)'],'.',color='blue',alpha=0.2) plt.plot(napa_pga['Dist(km)'],napa_pga['PGA(g)'],'.',color='green') ax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',        title='Peak Acceleration Data Linear Plot') plt.legend(['Napa','Parkfield'],fontsize=12) plt.show()  fig, ax = plt.subplots() plt.loglog(park_pga['Dist(km)'],park_pga['PGA(g)'],'.',color='blue',alpha=0.2) plt.loglog(napa_pga['Dist(km)'],napa_pga['PGA(g)'],'.',color='green') ax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',        title='Peak Acceleration Data Log Plot') plt.legend(['Napa','Parkfield'],fontsize=12) plt.show() In\u00a0[\u00a0]: include Copied! <pre>#Combine the two similar magnitude earthquake data\ndist=np.concatenate((np.array(napa_pga['Dist(km)']),np.array(park_pga['Dist(km)'])))\npga=np.concatenate((np.array(napa_pga['PGA(g)']),np.array(park_pga['PGA(g)'])))\n\n#Examine individual earthquake data\n#dist=np.array(park['Dist(km)'])\n#pga=np.array(park['PGA(g)'])\n</pre> #Combine the two similar magnitude earthquake data dist=np.concatenate((np.array(napa_pga['Dist(km)']),np.array(park_pga['Dist(km)']))) pga=np.concatenate((np.array(napa_pga['PGA(g)']),np.array(park_pga['PGA(g)'])))  #Examine individual earthquake data #dist=np.array(park['Dist(km)']) #pga=np.array(park['PGA(g)']) In\u00a0[\u00a0]: include Copied! <pre>#Try fitting data with np.polyfit()\np=np.polyfit(...)\nx=np.arange(0.1,np.max(dist),0.1)\ny=np.polyval(p,x)\n\nplt.plot(dist,pga,'.',color='blue')\nplt.plot(x,y,'-',color='red')\nplt.xlabel('Distance(km)')\nplt.ylabel('Peak Ground Acceleration (g)')\nplt.show()\n</pre> #Try fitting data with np.polyfit() p=np.polyfit(...) x=np.arange(0.1,np.max(dist),0.1) y=np.polyval(p,x)  plt.plot(dist,pga,'.',color='blue') plt.plot(x,y,'-',color='red') plt.xlabel('Distance(km)') plt.ylabel('Peak Ground Acceleration (g)') plt.show()  In\u00a0[\u00a0]: include Copied! <pre>#dist=dist+1    #add a small number to avoid singularity (dist=0)\np=np.polyfit(...)\nprint(p)\nx=np.arange(np.min(dist),np.max(dist),0.1)\ny=np.polyval(p,np.log(x))\n\n#dist=dist-1\nplt.plot(dist,pga,'.',color='blue')\nplt.plot(x,np.exp(y),'-',color='red')\nplt.xlabel('Distance(km)')\nplt.ylabel('Peak Ground Acceleration (g)')\nplt.show()\n</pre> #dist=dist+1    #add a small number to avoid singularity (dist=0) p=np.polyfit(...) print(p) x=np.arange(np.min(dist),np.max(dist),0.1) y=np.polyval(p,np.log(x))  #dist=dist-1 plt.plot(dist,pga,'.',color='blue') plt.plot(x,np.exp(y),'-',color='red') plt.xlabel('Distance(km)') plt.ylabel('Peak Ground Acceleration (g)') plt.show() In\u00a0[\u00a0]: include Copied! <pre>x=np.array((1, 2.2, 4.3, 7.7))\ndata=-1.5 + 3*x                   #construct data with an intercept of -1.5 and slope of 3.\n\n#random number array\n#rand=np.random.uniform(low=-2., high=2.0, size=4)    #apply random numbers\n#data=data + rand\n\nm=np.polyfit(x,data,1)\n\nplt.plot(x,data,'o',color='blue')\n#syn=np.polyval(m,x)\n#plt.plot(x,syn,'-',color='red')\nplt.show()\n\nprint(f'From polyfit(): a={m[1]:.2f}  b={m[0]:.2f}')\n\n#Solve via least squares\nA=np.vstack((...,...)).transpose()\n#AtA=np.dot(...)\n#AtD=np.dot(...)\n#a, b=np.linalg.solve(...)\n#print(f'From manual least squares: a={a:.2f}  b={b:.2f}')\n\n#Now lets use the scipy non-linear least-squares curve_fit() method\n#def linmod(x,a,b):\n#    return ...\n\n#m=curve_fit(linmod,x,data)[0]\n#print(f'From curve_fit(): a={m[0]:.2f}  b={m[1]:.2f}')\n</pre> x=np.array((1, 2.2, 4.3, 7.7)) data=-1.5 + 3*x                   #construct data with an intercept of -1.5 and slope of 3.  #random number array #rand=np.random.uniform(low=-2., high=2.0, size=4)    #apply random numbers #data=data + rand  m=np.polyfit(x,data,1)  plt.plot(x,data,'o',color='blue') #syn=np.polyval(m,x) #plt.plot(x,syn,'-',color='red') plt.show()  print(f'From polyfit(): a={m[1]:.2f}  b={m[0]:.2f}')  #Solve via least squares A=np.vstack((...,...)).transpose() #AtA=np.dot(...) #AtD=np.dot(...) #a, b=np.linalg.solve(...) #print(f'From manual least squares: a={a:.2f}  b={b:.2f}')  #Now lets use the scipy non-linear least-squares curve_fit() method #def linmod(x,a,b): #    return ...  #m=curve_fit(linmod,x,data)[0] #print(f'From curve_fit(): a={m[0]:.2f}  b={m[1]:.2f}')     In\u00a0[\u00a0]: include Copied! <pre>#Setup a linearized inverse problem for Parkfield\nh=4.0  #Assume a depth (km)\nr=np.sqrt(dist**2 + h**2)\n\n#Setup G matrix\nintercept_term=\nln_term=\nexp_term=\nG=\n\n#Setup Data Matrix\nd=\n\n#Setup of least squares\ngtg=np.dot(...)\ngtd=np.dot(...)\n\n#Solve for a, b, c\na, b, c=np.linalg.solve(gtg,gtd)\n\n#Measure fit\nm=np.array((a,b,c))\nsyn=np.exp(a + b*np.log(r) + c*r)\nrms_fit=np.sqrt(np.mean((pga - syn)**2))\nprint(f'(a,b,c)={a:.3f}/{b:.3f}/{c:.3f}  RMS={rms_fit:.3f}')\n\n#Plot results\nx=np.arange(0.0,np.max(dist),0.1)\nxr=np.sqrt(x**2 + h**2)\ny=np.exp(a + b*np.log(xr) + c*xr)\nplt.loglog(dist,pga,'.',color='blue')\nplt.loglog(x,y,'-',color='red')\nplt.show()\n</pre> #Setup a linearized inverse problem for Parkfield h=4.0  #Assume a depth (km) r=np.sqrt(dist**2 + h**2)  #Setup G matrix intercept_term= ln_term= exp_term= G=  #Setup Data Matrix d=  #Setup of least squares gtg=np.dot(...) gtd=np.dot(...)  #Solve for a, b, c a, b, c=np.linalg.solve(gtg,gtd)  #Measure fit m=np.array((a,b,c)) syn=np.exp(a + b*np.log(r) + c*r) rms_fit=np.sqrt(np.mean((pga - syn)**2)) print(f'(a,b,c)={a:.3f}/{b:.3f}/{c:.3f}  RMS={rms_fit:.3f}')  #Plot results x=np.arange(0.0,np.max(dist),0.1) xr=np.sqrt(x**2 + h**2) y=np.exp(a + b*np.log(xr) + c*xr) plt.loglog(dist,pga,'.',color='blue') plt.loglog(x,y,'-',color='red') plt.show()   In\u00a0[\u00a0]: include Copied! <pre>#Test the scipy curve_fit method\n\n#Define the non-linear function\ndef gm_model(x,a,b,c):\n    #This function returns ln(pga)\n    return ...\n\nh=4.0\nr=np.sqrt(dist**2 + h**2)\nm=curve_fit(...,..., ...,bounds=([...,...,...],...))[0]\n\n#Measure fit\nsyn=np.exp(gm_model(r,m[0],m[1],m[2]))\nrms_fit=np.sqrt(np.mean((pga - syn)**2))\nprint(f'(a,b,c,h)={m[0]:.3f}/{m[1]:.3f}/{m[2]:.3f}  RMS={rms_fit:.3f}')\n\nplt.loglog(dist,pga,'.')\nx=np.arange(0.1,200,0.1)\nxr=np.sqrt(x**2 + h**2)\ny=np.exp(gm_model(xr,m[0],m[1],m[2]))\nplt.loglog(x,y,'-',color='red')\nplt.show()\n</pre> #Test the scipy curve_fit method  #Define the non-linear function def gm_model(x,a,b,c):     #This function returns ln(pga)     return ...  h=4.0 r=np.sqrt(dist**2 + h**2) m=curve_fit(...,..., ...,bounds=([...,...,...],...))[0]  #Measure fit syn=np.exp(gm_model(r,m[0],m[1],m[2])) rms_fit=np.sqrt(np.mean((pga - syn)**2)) print(f'(a,b,c,h)={m[0]:.3f}/{m[1]:.3f}/{m[2]:.3f}  RMS={rms_fit:.3f}')  plt.loglog(dist,pga,'.') x=np.arange(0.1,200,0.1) xr=np.sqrt(x**2 + h**2) y=np.exp(gm_model(xr,m[0],m[1],m[2])) plt.loglog(x,y,'-',color='red') plt.show() In\u00a0[\u00a0]: include Copied! <pre>#Compute 95% confidence levels\ndegfree=len(r)-3                           #degrees of freedom (num data - num model params)\ne=np.log(pga)-np.log(syn)                  #residuals between data and model\nvar=np.sum(e**2)/degfree                   #variance\nse_y=np.sqrt(var)                          #standard error of the estimate\nsdev=np.sqrt(var)                          #standard deviation\n#Calculate 95% confidence bounds\nt=stats.t.ppf(1-0.05/2,degfree)             #division by 2 to map from single-tail to dual-tail t-distribution\nlower95=np.exp(np.log(y)-t*se_y)\nupper95=np.exp(np.log(y)+t*se_y)\n</pre> #Compute 95% confidence levels degfree=len(r)-3                           #degrees of freedom (num data - num model params) e=np.log(pga)-np.log(syn)                  #residuals between data and model var=np.sum(e**2)/degfree                   #variance se_y=np.sqrt(var)                          #standard error of the estimate sdev=np.sqrt(var)                          #standard deviation #Calculate 95% confidence bounds t=stats.t.ppf(1-0.05/2,degfree)             #division by 2 to map from single-tail to dual-tail t-distribution lower95=np.exp(np.log(y)-t*se_y) upper95=np.exp(np.log(y)+t*se_y) In\u00a0[\u00a0]: include Copied! <pre>#Plot Results\nfig, ax = plt.subplots()\nax.loglog(dist,pga,'b.',x,y,'k-',linewidth=2)\nax.loglog(x,lower95,'r-',x,upper95,'r-',linewidth=1)\nax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',\n       title='Peak Acceleration Data and Weighted Least Squares Inversion')\n#plt.legend(['Napa','Parkfield'],fontsize=12,loc=3)\nplt.show()\n</pre> #Plot Results fig, ax = plt.subplots() ax.loglog(dist,pga,'b.',x,y,'k-',linewidth=2) ax.loglog(x,lower95,'r-',x,upper95,'r-',linewidth=1) ax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',        title='Peak Acceleration Data and Weighted Least Squares Inversion') #plt.legend(['Napa','Parkfield'],fontsize=12,loc=3) plt.show()  <p>What depth produces the best fitting model (minimum variance)? How sensitive is the model to depth? Consider depths ranging from say 1 to 20 km.</p> <p>Write your answer here.</p> In\u00a0[\u00a0]: include Copied! <pre>grader = otter.Notebook()\nfrom otter.export import export_notebook\nfrom IPython.display import display, HTML\nexport_notebook(\"W09_NonLinear_Regression_InClass.ipynb\", filtering=True, pagebreaks=False)\ndisplay(HTML(\"&lt;p style='font-size:20px'&gt; &lt;br&gt;Save this notebook, then click &lt;a href='W09_NonLinear_Regression_InClass.pdf' download&gt;here&lt;/a&gt; to open the pdf.&lt;br&gt;&lt;/p&gt;\"))\n</pre> grader = otter.Notebook() from otter.export import export_notebook from IPython.display import display, HTML export_notebook(\"W09_NonLinear_Regression_InClass.ipynb\", filtering=True, pagebreaks=False) display(HTML(\"<p> Save this notebook, then click here to open the pdf.</p>\"))"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#non-linear-regression-in-class-exercise","title":"Non-Linear Regression In Class Exercise\u00b6","text":"<p>Our goals for today:</p> <ul> <li>Load peak ground acceleration observations from two notable M6 quakes in California</li> <li>Attempt to fit data using <code>polyfit()</code></li> <li>Develop a physics-based model and fit to data</li> <li>Vary assumed mean event depth to find better fitting model</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#setup","title":"Setup\u00b6","text":"<p>Run this cell as it is to setup your environment.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#analysis-of-strong-ground-motion-data","title":"Analysis of Strong Ground Motion Data\u00b6","text":"<p>Earthquakes are the sudden dislocation of rock on opposite sides of a fault due to applied stress. Seismic waves are generated by this process and propagate away from the fault affecting nearby communities. It is the strong shaking from earthquakes that we recognize as the earthquake. These motions can lead to landslides, liquefaction of the ground, and of course impact anything built within or on the ground. The motions generated by fault dislocation affect many aspects of modern society. Earthquake Engineering is a field that studies the ground motions generated by earthquakes and how they affect the built environment. To utilize ground motions for engineering applications requires studying the physics of seismic wave propagation, and the development of models that effectively describe it. Of particular importance is the need to accurately model and predict seismic wave amplitudes. Such studies generally focus on examining the peak acceleration and velocity as a function of distance from the source. The physics indicates that the ground motions generally decrease in amplitude with increasing distance.</p> <p>On August 24, 2014 a M6 earthquake occurred in south Napa. The following figure shows the observed strong ground acceleration. There is a lot of complexity in the distribution that seismologists and earthquake engineers need to consider, but the general trend is that the ground motions decrease with distance from the earthquake.</p> <p>In this module we will combine acceleration ground motion observations from two M6 events (2014 Napa, and 2004 Parkfield) to have a more complete distance distribution of observations. We will analyze the data first by attempting to fit curves as we have done for other datasets in the class (sea floor age, sea floor magnetism, distance and velocity of supernovae). We will then examine a physics-based model and a variety of methods to fit data. A model that describes the decrease (attenuation) of strong ground motion data over the years has been called 'attenuation relationships', 'ground motion prediction equations (GMPE)' and most recently 'ground motion models (GMM)'. Whatever it is called it is a fundamental to being able to characterized strong ground motion of future earthquakes and is used by the USGS and collaborators to develop earthquake forecast maps. GMM information coupled with the statistics of earthquake occurrence rates, notably Gutenberg-Richter statistics, provides the frame work for characterizing future ground motion hazard, as illustrated in the following map (red is high shaking hazard).</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#part-1-load-plot-and-fit-models-to-peak-ground-acceleration-data","title":"Part 1, Load, Plot and Fit Models to Peak Ground Acceleration Data\u00b6","text":"<p>We will make use of peak ground acceleration data from the 2014 Napa and 2004 Parkfield earthquakes. The acceleration is given in units of 'g', where 1g is 981 $\\frac{cm}{s^2}$. Earthquake Engineers commonly use the peak ground acceleration in such units in their geotechnical materials and structural engineering analyses. 0.1%g is the level people generally can perceive shaking, at 2%g some people may be disoriented, at 50% the shaking is very violent and unengineered structures can suffer damage and collapse, while well engineered buildings can survive if the duration is short.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#first-try-fitting-the-data-with-standard-curves-as-we-did-before-using-nppolyfit","title":"First. try fitting the data with standard curves as we did before using np.polyfit()\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#how-well-can-the-data-be-fit-with-polynomials","title":"How well can the data be fit with polynomials?\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#try-fitting-the-data-with-a-power-law-pga-fracadistb","title":"Try fitting the data with a power law ($pga = \\frac{a}{dist^b}$)\u00b6","text":"<ul> <li>To do this we linearize the equation to use polyfit() for a line</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#how-well-does-a-power-law-fit","title":"How well does a power law fit?\u00b6","text":"<p>What is wrong with this function?</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#part-2-fitting-strong-motion-data","title":"Part 2, Fitting Strong Motion Data\u00b6","text":"<p>In order to use the observations of peak ground acceleration to characterize seismic ground motion hazard it is necessary to develop a model that accurately describes the behavior seismic wave propagation, for example how the waves travel through the earth and dissipate. From physics seismic ground motions decay as a power law with distance (referred to as geometrical spreading), but we saw earlier that a power law alone does not work well, it is linear in log-space, where it does not explain the plateauing of ground motions close to the earthquake.</p> <p>To fix this we also need to consider that waves travel upward as well as away from an earthquake where $r=\\sqrt{(dist^2 + h^2)}$ is the total distance comprised of the horizontal distance and the depth (h) of the earthquake.</p> <p>Finally, in addition to geometrical spreading, there is an inelastic attenuation term that accounts for dissipative energy loss due to material imperfections. Based on this theory the following is a simple relationship that describes the dissipation or attenuation of seismic wave energy with distance from the earthquake,</p> <p>$pga=a*{\\frac{1}{r^b}}*e^{cr}$,</p> <p>where $a$ is a coeffient that depends on magnitude and scales the overall motions, $b$ is the exponent for the power-law geometrical spreading term, and $c$ is the coefficient for the in-elastic term (important only at large distances), and r is the total distance that considers the depth of the earthquake (h). Note that in the far-field the theoretical geometrical spreading decay of ground motions is ~1/r (in the near-field it is ~$1/r^2$). This is a non-linear equation, but it can be linearized by taking the natural logarithm.</p> <p>$\\mathrm{ln}(pga)=a + b*\\mathrm{ln}(r) + c*r$</p> <ul> <li>How do we setup this inverse problem? Let's first consider a simple linear example.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#how-to-setup-a-linear-linearized-inverse-problem","title":"How to setup a linear (linearized) inverse problem\u00b6","text":"<ul> <li><p>Until now we have been using \"canned\" functions to fit lines, or polynomials to data, but this doesn't always work because 1) sometimes more complicated functions are needed, 2) functions are non-linear, 3) we need to fit a physics-based model to the data.</p> </li> <li><p>We can construct our own inverse problem to fit more complex functions, as illustrated below.</p> </li> <li><p>When fitting a model such as a line to data, each data point can be considered a separate equation of two variables (a, b). That is for each x value there is a corresponding y value related to x through the equation for a line, where a is the intercept and b is the slope of the line.</p> </li> </ul> <ul> <li>The system of equations can be constructed in matrix form, and least squares (or other methods may be used to solve the matrix equation for the model parameters. Some of the functions we have been using are doing this \"under the hood\".</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#lets-try-it-for-a-simple-linear-case","title":"Let's try it for a simple linear case\u00b6","text":"<ol> <li>Consider data from a line with some random noise added</li> <li>Fit data using polyfit()</li> <li>Construct the linear inverse problem from basic principles</li> <li>Apply non-linear least-squares scipy.optimize.curve_fit()</li> </ol>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#now-setup-a-lineared-inverse-problem-for-the-pga-data","title":"Now Setup a lineared inverse problem for the PGA data\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#how-well-does-this-inversion-perform-are-the-model-parameters-consistent-with-the-theory-for-geometrical-spreading-and-anelastic-attenuation","title":"How well does this inversion perform? Are the model parameters consistent with the theory for geometrical spreading and anelastic attenuation?\u00b6","text":"<ul> <li>write answer here</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#part-3-apply-non-linear-least-squares","title":"Part 3, Apply Non-linear least-squares\u00b6","text":"<p>The model that we are trying to fit is non-linear in distance so it makes sense to try the non-linear least-squares method. We will also discover that with this optimization method we can find solution with a assumed range of parameters that can be constraint by our understanding of the physics or by some other observations.</p> <p>Non-linear optimization is a topic that requires an entire semester by itself, and would include non-linear least-squares, grid-search (though slow for large data sets), Montecarlo sampling, Bayesian inference, genetic algorithm, etc.</p> <p>We will use the scipy.optimization.curve_fit() which utilizes non-linear least squares. So that this is not entirely a black box, briefly non-linear least-squares involves using a starting model to estimate a prediction error, differentiating the prediction error with respect to model parameters, and then updating the model and repeating until convergence is achieved. This wiki describes it in some detail. https://en.wikipedia.org/wiki/Non-linear_least_squares</p> <p>If $y$ is the data and $f(x, m)$ is the prediction as a function of (m) model parameters then the initial prediction error is $e_i=(y_i - f(x_i, m_0))$. Given an initial model $m_0$, $f$ can be represented as a Taylor series where $f(x_i, m_1)=f(x_i, m_0) + \\frac{\\partial f}{\\partial m}(m_1 - m_0)$=$f(x_i, $m_0$) + \\frac{\\partial f}{\\partial m}(\\Delta m)$=$y_i$. Combining the prediction error and Taylor series equations gives:</p> <p>$e_i=[\\frac{\\partial f}{\\partial m}](\\Delta m)$, which as the form of the previous matrix equation we used. Suppose m=(a,b), and f(m)=a+bx then this results in a system of equations:</p> <p>$e_1=\\frac{\\partial f}{\\partial a}\\rvert_{x_1}\\Delta a + \\frac{\\partial f}{\\partial b}\\rvert_{x_1}\\Delta b$</p> <p>$e_2=\\frac{\\partial f}{\\partial a}\\rvert_{x_2}\\Delta a + \\frac{\\partial f}{\\partial b}\\rvert_{x_2}\\Delta b$</p> <p>$e_N=\\frac{\\partial f}{\\partial a}\\rvert_{x_N}\\Delta a + \\frac{\\partial f}{\\partial b}\\rvert_{x_N}\\Delta b$</p> <p>If $m_0$=(0,0) then the system of equations becomes what we found for the linear least-squares problem, where:</p> <p>$y_1=a + bx_1$</p> <p>$y_2=a + bx_2$</p> <p>$y_N=a + bx_N$</p> <p>The following is the general non-linear least-squares equation: \\begin{equation*} Y= \\begin{bmatrix} \\frac{\\partial f}{\\partial m_1}\\rvert_{x_1} &amp; \\frac{\\partial f}{\\partial m_1}\\rvert_{x_1} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial m_M}\\rvert_{x_1} \\\\ \\frac{\\partial f}{\\partial m_1}\\rvert_{x_2} &amp; \\frac{\\partial f}{\\partial m_1}\\rvert_{x_2} &amp; \\cdots &amp;\\frac{\\partial f}{\\partial m_M}\\rvert_{x_2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f}{\\partial m_1}\\rvert_{x_N} &amp; \\frac{\\partial f}{\\partial m_1}\\rvert_{x_N} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial m_M}\\rvert_{x_N} \\end{bmatrix} \\quad \\begin{bmatrix} \\Delta m_1 \\\\ \\Delta m_2 \\\\ \\vdots \\\\ \\Delta m_M \\end{bmatrix} \\end{equation*}</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#compute-95-confidence-intervals","title":"Compute 95% confidence intervals\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#test-our-assumption-that-the-mean-depth-of-the-earthquakes-is-40km","title":"Test our assumption that the mean depth of the earthquakes is 4.0km.\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#compare-solutions-using-the-napa-and-parkfield-data-separately-and-discuss-how-the-results-compare","title":"Compare solutions using the Napa and Parkfield data separately and discuss how the results compare.\u00b6","text":"<p>Write you answer here.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W11_NonLinear_Regression_InClass/#turn-in-this-notebook","title":"Turn in this Notebook\u00b6","text":"<p>Run the cell below to export this notebook as a pdf and upload to bCourses. Make sure to run all cells and save before doing so for changes to be reflected in the pdf.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/","title":"W12 Nonlinear Regression InClass","text":"In\u00a0[\u00a0]: include Copied! <pre>import math\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport otter\n</pre> import math import numpy as np import pandas as pd from scipy import stats from scipy.optimize import curve_fit import matplotlib.pyplot as plt import otter In\u00a0[\u00a0]: include Copied! <pre>#Load the data\npark_pga=pd.read_csv('parkfieldeq_pga.csv')\nnapa_pga=pd.read_csv('napaeq_pga.csv')\ndist=np.concatenate((np.array(napa_pga['Dist(km)']),np.array(park_pga['Dist(km)'])))\npga=np.concatenate((np.array(napa_pga['PGA(g)']),np.array(park_pga['PGA(g)'])))\n</pre> #Load the data park_pga=pd.read_csv('parkfieldeq_pga.csv') napa_pga=pd.read_csv('napaeq_pga.csv') dist=np.concatenate((np.array(napa_pga['Dist(km)']),np.array(park_pga['Dist(km)']))) pga=np.concatenate((np.array(napa_pga['PGA(g)']),np.array(park_pga['PGA(g)']))) In\u00a0[\u00a0]: include Copied! <pre>#GMM model inversion and ground motion analysis\n#Write code here\n</pre> #GMM model inversion and ground motion analysis #Write code here In\u00a0[\u00a0]: include Copied! <pre>\n</pre> <ul> <li><p>How does your model's parameters including depth (h) compare to what was found in class?</p> </li> <li><p>Does the data require a intrinsic attenuation term (c parameter)? You can test this by allowing the bounds for c to have an upper limit of zero (theoretically is should not be larger than zero).</p> </li> <li><p>How do the model parameters compare for the two events individually? Are there differences in how well the model is constrained by the two individual data sets?</p> </li> <li><p>Write answers here</p> </li> </ul> In\u00a0[\u00a0]: include Copied! <pre>def as2008(dist,M,Ztor):\n    \"\"\"\n    This function takes an array of distances, a magnitude and the depth\n    and returns the natural logarithm of median peak ground acceleration from the AS2008 GMPE, and the\n    standard deviation (sigma) in natural log units.\n    \n    The function is not the complete AS2008 formulation. It is limited to the hard rock case Vs30=865, and only\n    computes pga.\n    \n    \"\"\"\n    #Defined by A&amp;S2008 DO NOT CHANGE\n    c1=6.75;\n    c4=4.5;\n    a1=0.804;   #for PGA only this parameter is period dependent\n    a2=-0.9679; #for PGA only this parameter is period dependent\n    a3=0.265;\n    a4=-0.231;\n    a5=-0.398;\n    a8=-0.0372; #for PGA only this parameter is period dependent\n    a16=0.9000; #for PGA only\n    VLIN=865.1; #for PGA only note for vs30=vlin f5==0\n    #Defined by A&amp;S2008 DO NOT CHANGE\n    \n    R=np.sqrt(dist*dist + c4*c4)      #compute total distance\n\n    #Standard Deviation varies from 0.8 for M5 to 0.6 for M7 assume linear in ln\n    if M &lt;= 7 and M &gt;= 5:\n        sigma=0.8+(0.8-0.6)/(5-7)*(M-5)\n\n    if M &gt; 7:\n        sigma=0.6;\n\n    #Base model\n    if M &lt;= c1:\n        f1=a1+a4*(M-c1)+a8*(8.5-M)**2+(a2+a3*(M-c1))*np.log(R)\n\n    if M &gt; c1:\n        f1=a1+a5*(M-c1)+a8*(8.5-M)**2+(a2+a3*(M-c1))*np.log(R)\n\n    #Depth of fault\n    if Ztor &lt;= 10:\n        f6=Ztor/10*a16\n\n    if Ztor &gt; 10:\n        f6=a16\n\n\n    lnpga=f1 + f6\n    \n    return lnpga, sigma\n</pre> def as2008(dist,M,Ztor):     \"\"\"     This function takes an array of distances, a magnitude and the depth     and returns the natural logarithm of median peak ground acceleration from the AS2008 GMPE, and the     standard deviation (sigma) in natural log units.          The function is not the complete AS2008 formulation. It is limited to the hard rock case Vs30=865, and only     computes pga.          \"\"\"     #Defined by A&amp;S2008 DO NOT CHANGE     c1=6.75;     c4=4.5;     a1=0.804;   #for PGA only this parameter is period dependent     a2=-0.9679; #for PGA only this parameter is period dependent     a3=0.265;     a4=-0.231;     a5=-0.398;     a8=-0.0372; #for PGA only this parameter is period dependent     a16=0.9000; #for PGA only     VLIN=865.1; #for PGA only note for vs30=vlin f5==0     #Defined by A&amp;S2008 DO NOT CHANGE          R=np.sqrt(dist*dist + c4*c4)      #compute total distance      #Standard Deviation varies from 0.8 for M5 to 0.6 for M7 assume linear in ln     if M &lt;= 7 and M &gt;= 5:         sigma=0.8+(0.8-0.6)/(5-7)*(M-5)      if M &gt; 7:         sigma=0.6;      #Base model     if M &lt;= c1:         f1=a1+a4*(M-c1)+a8*(8.5-M)**2+(a2+a3*(M-c1))*np.log(R)      if M &gt; c1:         f1=a1+a5*(M-c1)+a8*(8.5-M)**2+(a2+a3*(M-c1))*np.log(R)      #Depth of fault     if Ztor &lt;= 10:         f6=Ztor/10*a16      if Ztor &gt; 10:         f6=a16       lnpga=f1 + f6          return lnpga, sigma <p>Use the <code>AS2008</code> function defined above to compute the GMPE for a M6 earthquake with a top fault depth of 1.0km</p> <p>Compare the AS2008 result with the result you obtained by fitting the data</p> In\u00a0[\u00a0]: include Copied! <pre>#M6 AS2008 GMPE result compared with the non-linear least-squares inversion results\n</pre> #M6 AS2008 GMPE result compared with the non-linear least-squares inversion results <p>How does AS2008 fit the data, and how does it compare to the GMPE model you developed by inverting the data? Are your models within the 95% confidence bounds of AS2008?</p> <ul> <li>Write answers here</li> </ul> <p>Next use the RMS measure of misfit to find a best magnitude and fault depth for the combined data set. You can do this different ways. One would be to first find magnitude holding depth fixed, and then for the best magnitude find the best fault depth. A better way would be to write nested loops over both parameters to find the best combination.</p> In\u00a0[\u00a0]: include Copied! <pre>#Magnitude and fault depth analysis here\n</pre> #Magnitude and fault depth analysis here <p>Write your answer here.</p> <p>Finally, explore the effect of earthquake magnitude on ground acceleration. Use the <code>as2008</code> function defined above to compute the GMPE for M5, M6, M7, and M8 earthquakes (all with a top fault depth of 1.0km). Plot them together.</p> In\u00a0[\u00a0]: include Copied! <pre>#Analysis of AS2008 GMPE for different magnitude\n</pre> #Analysis of AS2008 GMPE for different magnitude <p>How does the ground acceleration curve change with increasing earthquake magnitude?</p> <p>If the events are on the San Andreas fault what are the ground motions for the different earthquakes on the UC Berkeley campus?</p> <p>Write your answers here.</p> In\u00a0[\u00a0]: include Copied! <pre>grader = otter.Notebook()\nfrom otter.export import export_notebook\nfrom IPython.display import display, HTML\nexport_notebook(\"W09_Nonlinear_Regression_Assignment.ipynb\", filtering=True, pagebreaks=False)\ndisplay(HTML(\"&lt;p style='font-size:20px'&gt; &lt;br&gt;Save this notebook, then click &lt;a href='W09_Nonlinear_Regression_Assignment.pdf' download&gt;here&lt;/a&gt; to open the pdf.&lt;br&gt;&lt;/p&gt;\"))\n</pre> grader = otter.Notebook() from otter.export import export_notebook from IPython.display import display, HTML export_notebook(\"W09_Nonlinear_Regression_Assignment.ipynb\", filtering=True, pagebreaks=False) display(HTML(\"<p> Save this notebook, then click here to open the pdf.</p>\"))"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#non-linear-regression-assignment","title":"Non-linear Regression Assignment\u00b6","text":"<p>Non-linear model fitting</p> <p>In-class we:</p> <ul> <li>Tested data fitting using <code>np.polyfit()</code>, manual least-squares, and non-linear least-squares <code>curve_fit()</code> for two notable M6 earthquakes</li> </ul> <p>Our goals for this assignment:</p> <ul> <li>Write code using the scipy non-linear least-squares curve_fit() function to invert for the a, b, c <code>and h (depth)</code> model parameters. Recall the total distance is a non-linear function of depth</li> <li>Compare the derived model with a prediction using the Abrahamson and Silva (2008) relationship (included as a defined function) which was constrained by data from 140 earthquakes of various magnitudes.</li> <li>Examine the behavior of strong ground motion with respect to depth of the fault, and magnitude.</li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#setup","title":"Setup\u00b6","text":"<p>Run this cell to setup your environment.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#exercise-1-write-code-using-curve_fit","title":"Exercise 1, Write Code using curve_fit()\u00b6","text":"<ul> <li><p>Load and combine the Napa and Parkfield earthquake PGA data as was done in class</p> </li> <li><p>Write code using the scipy curve_fit() function to estimate the a, b, c and h (depth) parameters. Note that you will need to place bounds on the parameter search.</p> </li> <li><p>Compute the 95% confidence curves for the best fitting model</p> </li> </ul>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#write-code-to-perform-a-non-linear-least-squares-inversion-for-the-a-b-c-and-h-depth-model-parameters-plot-your-results-with-the-95-confidence-bounds","title":"Write code to perform a non-linear least-squares inversion for the a, b, c and h (depth) model parameters. Plot your results with the 95% confidence bounds\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#exercise-2-abrahamson-and-silva-2008-gmpe","title":"Exercise 2, Abrahamson and Silva (2008) GMPE\u00b6","text":"<p>The GMPE that you developed for the combined Napa and Parkfield earthquake data set is actually quite good, but it is limited to only the two M6 earthquakes. Abrahamson and Silva (2008, AS2008) developed a GMPE considering 2750 recordings from 140 earthquakes ranging in magnitude from 4.27 to 7.62. They report that the derived GMPE is applicable to M 5.0 to 8.5 earthquakes. The following shows the AS2008 relationship for a M7.5 earthquake (black is the median motion, and red is 95% confidence).</p> <p>In the following cell the definition AS2008 GMPE for hard rock, considering, distance, magnitude, and the depth to the top of the fault is given. The function takes three input arguments, an array of distances, a magnitude and the depth to the top of the fault. The output is the natural logarithm of median peak ground acceleration from the AS2008 GMPE (lnpga), and the 95% confidence level (sigma, also in natural log units). For M6.5+ events in California we can consider the top of the fault to be at zero depth. A M5 may be at 8 km in comparison.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#note-that-an-interesting-final-class-project-might-be-combining-the-gutenberg-richter-statistic-with-the-ground-motion-esimation-and-uncertainty-to-arrive-at-a-hazard-estimate-for-the-uc-berkeley-campus-for-earthquakes-on-the-san-andreas-hayward-and-concord-faults","title":"Note that an interesting final class project might be combining the Gutenberg-Richter statistic with the ground motion esimation and uncertainty to arrive at a hazard estimate for the UC Berkeley campus for earthquakes on the San Andreas, Hayward and Concord faults.\u00b6","text":""},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/lectures/others/ground_motion/W12_Nonlinear_Regression_InClass/#turn-in-this-notebook","title":"Turn in this Notebook\u00b6","text":"<p>Run the cell below to export this notebook as a pdf and upload to bCourses. Make sure to run all cells and save before doing so for changes to be reflected in the pdf.</p>"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/scripts/01_numpy_pandas/","title":"01 numpy pandas","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport kaggle\nimport numpy as np\nimport pandas as pd\n\n# Set up Kaggle API credentials\n# Make sure you have a kaggle.json file in ~/.kaggle/ with your API token\nkaggle.api.authenticate()\n</pre> import os  import kaggle import numpy as np import pandas as pd  # Set up Kaggle API credentials # Make sure you have a kaggle.json file in ~/.kaggle/ with your API token kaggle.api.authenticate() In\u00a0[\u00a0]: Copied! <pre># Download datasets\nkaggle.api.dataset_download_files(\"usgs/earthquake-database\", path=\"./data\", unzip=True)\n</pre> # Download datasets kaggle.api.dataset_download_files(\"usgs/earthquake-database\", path=\"./data\", unzip=True) In\u00a0[\u00a0]: Copied! <pre>earthquakes = pd.read_csv(\"./data/database.csv\")\nearthquakes.head()\n# renmae columns Date to date, Time to time, Latitude to latitude, Longitude to longitude, Depth to depth, Magnitude to magnitude, Type to type\nearthquakes.rename(\n    columns={\n        \"Date\": \"date\",\n        \"Time\": \"time\",\n        \"Latitude\": \"latitude\",\n        \"Longitude\": \"longitude\",\n        \"Depth\": \"depth\",\n        \"Magnitude\": \"magnitude\",\n        \"Type\": \"type\",\n        \"ID\": \"id\",\n        \"Source\": \"source\",\n        \"Status\": \"status\",\n    },\n    inplace=True,\n)\nearthquakes[\"time\"] = earthquakes[\"time\"].str.replace(\"Z\", \"\")\nearthquakes[\"time\"] = pd.to_datetime(\n    earthquakes[\"date\"] + \" \" + earthquakes[\"time\"], format=\"%m/%d/%Y %H:%M:%S\", errors=\"coerce\"\n)\nearthquakes = earthquakes[[\"time\", \"latitude\", \"longitude\", \"depth\", \"magnitude\", \"type\", \"id\", \"source\", \"status\"]]\nearthquakes.to_csv(\"./data/earthquakes.csv\", index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n</pre> earthquakes = pd.read_csv(\"./data/database.csv\") earthquakes.head() # renmae columns Date to date, Time to time, Latitude to latitude, Longitude to longitude, Depth to depth, Magnitude to magnitude, Type to type earthquakes.rename(     columns={         \"Date\": \"date\",         \"Time\": \"time\",         \"Latitude\": \"latitude\",         \"Longitude\": \"longitude\",         \"Depth\": \"depth\",         \"Magnitude\": \"magnitude\",         \"Type\": \"type\",         \"ID\": \"id\",         \"Source\": \"source\",         \"Status\": \"status\",     },     inplace=True, ) earthquakes[\"time\"] = earthquakes[\"time\"].str.replace(\"Z\", \"\") earthquakes[\"time\"] = pd.to_datetime(     earthquakes[\"date\"] + \" \" + earthquakes[\"time\"], format=\"%m/%d/%Y %H:%M:%S\", errors=\"coerce\" ) earthquakes = earthquakes[[\"time\", \"latitude\", \"longitude\", \"depth\", \"magnitude\", \"type\", \"id\", \"source\", \"status\"]] earthquakes.to_csv(\"./data/earthquakes.csv\", index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\") In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\n# Load earthquake data (magnitude and depth)\n# the first coloumn is utc datetime\nearthquakes = np.loadtxt(\"data/earthquakes.csv\", delimiter=\",\", skiprows=1, usecols=(1, 2, 3, 4), dtype=float)\n\n# Calculate average magnitude and depth\navg_depth = np.mean(earthquakes[:, 2])\navg_magnitude = np.mean(earthquakes[:, 3])\nprint(f\"Average magnitude: M{avg_magnitude:.2f}\")\nprint(f\"Average depth: {avg_depth:.2f} km\")\n\n# Find the strongest earthquake\nstrongest_idx = np.argmax(earthquakes[:, 3])\nstrongest_magnitude = earthquakes[strongest_idx, 3]\nstrongest_depth = earthquakes[strongest_idx, 2]\n\nprint(f\"Average magnitude: M{avg_magnitude:.2f}\")\nprint(f\"Average depth: {avg_depth:.2f} km\")\nprint(f\"Strongest earthquake: Magnitude {strongest_magnitude:.2f} at depth {strongest_depth:.2f} km\")\n</pre> import numpy as np  # Load earthquake data (magnitude and depth) # the first coloumn is utc datetime earthquakes = np.loadtxt(\"data/earthquakes.csv\", delimiter=\",\", skiprows=1, usecols=(1, 2, 3, 4), dtype=float)  # Calculate average magnitude and depth avg_depth = np.mean(earthquakes[:, 2]) avg_magnitude = np.mean(earthquakes[:, 3]) print(f\"Average magnitude: M{avg_magnitude:.2f}\") print(f\"Average depth: {avg_depth:.2f} km\")  # Find the strongest earthquake strongest_idx = np.argmax(earthquakes[:, 3]) strongest_magnitude = earthquakes[strongest_idx, 3] strongest_depth = earthquakes[strongest_idx, 2]  print(f\"Average magnitude: M{avg_magnitude:.2f}\") print(f\"Average depth: {avg_depth:.2f} km\") print(f\"Strongest earthquake: Magnitude {strongest_magnitude:.2f} at depth {strongest_depth:.2f} km\") In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# Load earthquake data\ndf = pd.read_csv(\"data/earthquakes.csv\")\n\n# Calculate average magnitude and depth\navg_depth = df[\"depth\"].mean()\navg_magnitude = df[\"magnitude\"].mean()\n\n# Find the strongest earthquake\nstrongest_idx = df[\"magnitude\"].idxmax()\nstrongest_magnitude = df.loc[strongest_idx, \"magnitude\"]\nstrongest_depth = df.loc[strongest_idx, \"depth\"]\n\nprint(f\"Average magnitude: M{avg_magnitude:.2f}\")\nprint(f\"Average depth: {avg_depth:.2f} km\")\nprint(f\"Strongest earthquake: Magnitude {strongest_magnitude:.2f} at depth {strongest_depth:.2f} km\")\n</pre> import pandas as pd  # Load earthquake data df = pd.read_csv(\"data/earthquakes.csv\")  # Calculate average magnitude and depth avg_depth = df[\"depth\"].mean() avg_magnitude = df[\"magnitude\"].mean()  # Find the strongest earthquake strongest_idx = df[\"magnitude\"].idxmax() strongest_magnitude = df.loc[strongest_idx, \"magnitude\"] strongest_depth = df.loc[strongest_idx, \"depth\"]  print(f\"Average magnitude: M{avg_magnitude:.2f}\") print(f\"Average depth: {avg_depth:.2f} km\") print(f\"Strongest earthquake: Magnitude {strongest_magnitude:.2f} at depth {strongest_depth:.2f} km\") In\u00a0[\u00a0]: Copied! <pre>kaggle.api.dataset_download_files(\n    \"berkeleyearth/climate-change-earth-surface-temperature-data\", path=\"./data\", unzip=True\n)\n</pre> kaggle.api.dataset_download_files(     \"berkeleyearth/climate-change-earth-surface-temperature-data\", path=\"./data\", unzip=True ) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"data/GlobalTemperatures.csv\")\ndf.head()\n\ndf.rename(\n    columns={\n        \"dt\": \"date\",\n        \"LandAverageTemperature\": \"temperature\",\n    },\n    inplace=True,\n)\ndf = df[[\"date\", \"temperature\"]]\n# filter out rows with missing values\ndf = df.dropna()\ndf = df[df[\"date\"] &gt;= \"1900-01-01\"]\ndf.to_csv(\"data/global_temperature.csv\", index=False, date_format=\"%Y-%m-%d\")\n\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd  df = pd.read_csv(\"data/GlobalTemperatures.csv\") df.head()  df.rename(     columns={         \"dt\": \"date\",         \"LandAverageTemperature\": \"temperature\",     },     inplace=True, ) df = df[[\"date\", \"temperature\"]] # filter out rows with missing values df = df.dropna() df = df[df[\"date\"] &gt;= \"1900-01-01\"] df.to_csv(\"data/global_temperature.csv\", index=False, date_format=\"%Y-%m-%d\")  import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# Load temperature data\ndf = pd.read_csv(\"data/global_temperature.csv\")\n\n# Convert date column to datetime\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Set date as index\ndf.set_index(\"date\", inplace=True)\n\n# Find the hottest and coldest days\nhottest_day = df[\"temperature\"].idxmax()\ncoldest_day = df[\"temperature\"].idxmin()\n\nprint(f\"Hottest day: {hottest_day.date()} ({df.loc[hottest_day, 'temperature']:.1f}\u00b0C)\")\nprint(f\"Coldest day: {coldest_day.date()} ({df.loc[coldest_day, 'temperature']:.1f}\u00b0C)\")\n\n# Calculate monthly average temperatures\nyearly_avg = df.resample(\"Y\").mean()\n\n# Plot monthly average temperatures\nyearly_avg[\"temperature\"].plot(figsize=(12, 6))\n\nplt.title(\"Yearly Average Temperatures\")\nplt.ylabel(\"Temperature (\u00b0C)\")\nplt.show()\n</pre> import pandas as pd  # Load temperature data df = pd.read_csv(\"data/global_temperature.csv\")  # Convert date column to datetime df[\"date\"] = pd.to_datetime(df[\"date\"])  # Set date as index df.set_index(\"date\", inplace=True)  # Find the hottest and coldest days hottest_day = df[\"temperature\"].idxmax() coldest_day = df[\"temperature\"].idxmin()  print(f\"Hottest day: {hottest_day.date()} ({df.loc[hottest_day, 'temperature']:.1f}\u00b0C)\") print(f\"Coldest day: {coldest_day.date()} ({df.loc[coldest_day, 'temperature']:.1f}\u00b0C)\")  # Calculate monthly average temperatures yearly_avg = df.resample(\"Y\").mean()  # Plot monthly average temperatures yearly_avg[\"temperature\"].plot(figsize=(12, 6))  plt.title(\"Yearly Average Temperatures\") plt.ylabel(\"Temperature (\u00b0C)\") plt.show()"},{"location":"eps88-pyearth-a-python-introduction-to-earth-science/scripts/synthetic_rocks/","title":"Synthetic rocks","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler In\u00a0[\u00a0]: Copied! <pre># Set random seed for reproducibility\nnp.random.seed(42)\n</pre> # Set random seed for reproducibility np.random.seed(42) In\u00a0[\u00a0]: Copied! <pre># Define the number of samples for each rock type\nn_samples = 300\n</pre> # Define the number of samples for each rock type n_samples = 300 In\u00a0[\u00a0]: Copied! <pre># Function to generate correlated data\ndef generate_correlated_data(mean, cov, n):\n    return np.random.multivariate_normal(mean, cov, n)\n</pre> # Function to generate correlated data def generate_correlated_data(mean, cov, n):     return np.random.multivariate_normal(mean, cov, n) In\u00a0[\u00a0]: Copied! <pre># Generate synthetic data for igneous rocks\nigneous_mean = [70, 14, 3, 2, 1, 3, 5, 0.3]\nigneous_cov = np.array(\n    [\n        [25, -5, -2, -1, -0.5, 1, 2, -0.1],\n        [-5, 4, 0.5, 0.2, 0.1, -0.2, -0.4, 0.02],\n        [-2, 0.5, 1, 0.1, 0.05, -0.1, -0.2, 0.01],\n        [-1, 0.2, 0.1, 0.5, 0.02, -0.05, -0.1, 0.005],\n        [-0.5, 0.1, 0.05, 0.02, 0.2, -0.02, -0.05, 0.002],\n        [1, -0.2, -0.1, -0.05, -0.02, 0.5, 0.1, -0.005],\n        [2, -0.4, -0.2, -0.1, -0.05, 0.1, 1, -0.01],\n        [-0.1, 0.02, 0.01, 0.005, 0.002, -0.005, -0.01, 0.05],\n    ]\n)\nigneous_data = generate_correlated_data(igneous_mean, igneous_cov, n_samples)\n</pre> # Generate synthetic data for igneous rocks igneous_mean = [70, 14, 3, 2, 1, 3, 5, 0.3] igneous_cov = np.array(     [         [25, -5, -2, -1, -0.5, 1, 2, -0.1],         [-5, 4, 0.5, 0.2, 0.1, -0.2, -0.4, 0.02],         [-2, 0.5, 1, 0.1, 0.05, -0.1, -0.2, 0.01],         [-1, 0.2, 0.1, 0.5, 0.02, -0.05, -0.1, 0.005],         [-0.5, 0.1, 0.05, 0.02, 0.2, -0.02, -0.05, 0.002],         [1, -0.2, -0.1, -0.05, -0.02, 0.5, 0.1, -0.005],         [2, -0.4, -0.2, -0.1, -0.05, 0.1, 1, -0.01],         [-0.1, 0.02, 0.01, 0.005, 0.002, -0.005, -0.01, 0.05],     ] ) igneous_data = generate_correlated_data(igneous_mean, igneous_cov, n_samples) In\u00a0[\u00a0]: Copied! <pre># Generate synthetic data for sedimentary rocks\nsedimentary_mean = [50, 10, 5, 20, 5, 1, 2, 0.5]\nsedimentary_cov = np.array(\n    [\n        [100, -10, -5, -20, -5, -1, -2, -0.5],\n        [-10, 9, 1, 2, 0.5, 0.1, 0.2, 0.05],\n        [-5, 1, 4, 1, 0.25, 0.05, 0.1, 0.025],\n        [-20, 2, 1, 36, 2, 0.2, 0.4, 0.1],\n        [-5, 0.5, 0.25, 2, 4, 0.05, 0.1, 0.025],\n        [-1, 0.1, 0.05, 0.2, 0.05, 0.1, 0.02, 0.005],\n        [-2, 0.2, 0.1, 0.4, 0.1, 0.02, 0.4, 0.01],\n        [-0.5, 0.05, 0.025, 0.1, 0.025, 0.005, 0.01, 0.04],\n    ]\n)\nsedimentary_data = generate_correlated_data(sedimentary_mean, sedimentary_cov, n_samples)\n</pre> # Generate synthetic data for sedimentary rocks sedimentary_mean = [50, 10, 5, 20, 5, 1, 2, 0.5] sedimentary_cov = np.array(     [         [100, -10, -5, -20, -5, -1, -2, -0.5],         [-10, 9, 1, 2, 0.5, 0.1, 0.2, 0.05],         [-5, 1, 4, 1, 0.25, 0.05, 0.1, 0.025],         [-20, 2, 1, 36, 2, 0.2, 0.4, 0.1],         [-5, 0.5, 0.25, 2, 4, 0.05, 0.1, 0.025],         [-1, 0.1, 0.05, 0.2, 0.05, 0.1, 0.02, 0.005],         [-2, 0.2, 0.1, 0.4, 0.1, 0.02, 0.4, 0.01],         [-0.5, 0.05, 0.025, 0.1, 0.025, 0.005, 0.01, 0.04],     ] ) sedimentary_data = generate_correlated_data(sedimentary_mean, sedimentary_cov, n_samples) In\u00a0[\u00a0]: Copied! <pre># Generate synthetic data for metamorphic rocks\nmetamorphic_mean = [60, 18, 8, 8, 4, 2, 2, 0.8]\nmetamorphic_cov = np.array(\n    [\n        [64, -12, -6, -4, -2, -1, -1, -0.4],\n        [-12, 7, 1, 0.5, 0.25, 0.125, 0.125, 0.05],\n        [-6, 1, 4, 0.25, 0.125, 0.0625, 0.0625, 0.025],\n        [-4, 0.5, 0.25, 7, 0.125, 0.0625, 0.0625, 0.025],\n        [-2, 0.25, 0.125, 0.125, 2, 0.03125, 0.03125, 0.0125],\n        [-1, 0.125, 0.0625, 0.0625, 0.03125, 0.5, 0.015625, 0.00625],\n        [-1, 0.125, 0.0625, 0.0625, 0.03125, 0.015625, 0.5, 0.00625],\n        [-0.4, 0.05, 0.025, 0.025, 0.0125, 0.00625, 0.00625, 0.1],\n    ]\n)\nmetamorphic_data = generate_correlated_data(metamorphic_mean, metamorphic_cov, n_samples)\n</pre> # Generate synthetic data for metamorphic rocks metamorphic_mean = [60, 18, 8, 8, 4, 2, 2, 0.8] metamorphic_cov = np.array(     [         [64, -12, -6, -4, -2, -1, -1, -0.4],         [-12, 7, 1, 0.5, 0.25, 0.125, 0.125, 0.05],         [-6, 1, 4, 0.25, 0.125, 0.0625, 0.0625, 0.025],         [-4, 0.5, 0.25, 7, 0.125, 0.0625, 0.0625, 0.025],         [-2, 0.25, 0.125, 0.125, 2, 0.03125, 0.03125, 0.0125],         [-1, 0.125, 0.0625, 0.0625, 0.03125, 0.5, 0.015625, 0.00625],         [-1, 0.125, 0.0625, 0.0625, 0.03125, 0.015625, 0.5, 0.00625],         [-0.4, 0.05, 0.025, 0.025, 0.0125, 0.00625, 0.00625, 0.1],     ] ) metamorphic_data = generate_correlated_data(metamorphic_mean, metamorphic_cov, n_samples) In\u00a0[\u00a0]: Copied! <pre># Combine all rock types\ncomponents = [\"SiO2\", \"Al2O3\", \"Fe2O3\", \"CaO\", \"MgO\", \"Na2O\", \"K2O\", \"TiO2\"]\nigneous = pd.DataFrame(igneous_data, columns=components)\nigneous[\"rock_type\"] = \"igneous\"\nsedimentary = pd.DataFrame(sedimentary_data, columns=components)\nsedimentary[\"rock_type\"] = \"sedimentary\"\nmetamorphic = pd.DataFrame(metamorphic_data, columns=components)\nmetamorphic[\"rock_type\"] = \"metamorphic\"\n</pre> # Combine all rock types components = [\"SiO2\", \"Al2O3\", \"Fe2O3\", \"CaO\", \"MgO\", \"Na2O\", \"K2O\", \"TiO2\"] igneous = pd.DataFrame(igneous_data, columns=components) igneous[\"rock_type\"] = \"igneous\" sedimentary = pd.DataFrame(sedimentary_data, columns=components) sedimentary[\"rock_type\"] = \"sedimentary\" metamorphic = pd.DataFrame(metamorphic_data, columns=components) metamorphic[\"rock_type\"] = \"metamorphic\" In\u00a0[\u00a0]: Copied! <pre>rocks = pd.concat([igneous, sedimentary, metamorphic], ignore_index=True)\n</pre> rocks = pd.concat([igneous, sedimentary, metamorphic], ignore_index=True) In\u00a0[\u00a0]: Copied! <pre># Shuffle the dataset\nrocks = rocks.sample(frac=1).reset_index(drop=True)\n</pre> # Shuffle the dataset rocks = rocks.sample(frac=1).reset_index(drop=True) In\u00a0[\u00a0]: Copied! <pre># Save the dataset\nrocks_type = rocks[\"rock_type\"]\nrocks_type.to_csv(\"data/rock_types.csv\", index=False)\nrocks_sample = rocks.drop(columns=\"rock_type\")\nrocks_sample.to_csv(\"data/rock_samples.csv\", index=False)\n# rocks.to_csv(\"data/rock_samples.csv\", index=False)\n</pre> # Save the dataset rocks_type = rocks[\"rock_type\"] rocks_type.to_csv(\"data/rock_types.csv\", index=False) rocks_sample = rocks.drop(columns=\"rock_type\") rocks_sample.to_csv(\"data/rock_samples.csv\", index=False) # rocks.to_csv(\"data/rock_samples.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>print(\"Synthetic rock sample dataset has been created and saved to 'data/rock_samples.csv'\")\n</pre> print(\"Synthetic rock sample dataset has been created and saved to 'data/rock_samples.csv'\") In\u00a0[\u00a0]: Copied! <pre># Visualize the data\nplt.figure(figsize=(12, 10))\nsns.pairplot(rocks, hue=\"rock_type\", vars=components, plot_kws={\"alpha\": 0.5})\nplt.tight_layout()\n# plt.show()\nplt.savefig(\"pairplot.png\")\nplt.close()\n</pre> # Visualize the data plt.figure(figsize=(12, 10)) sns.pairplot(rocks, hue=\"rock_type\", vars=components, plot_kws={\"alpha\": 0.5}) plt.tight_layout() # plt.show() plt.savefig(\"pairplot.png\") plt.close() In\u00a0[\u00a0]: Copied! <pre># Perform PCA\nX = rocks[components]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</pre> # Perform PCA X = rocks[components] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) In\u00a0[\u00a0]: Copied! <pre>pca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n</pre> pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=rocks[\"rock_type\"].astype(\"category\").cat.codes, alpha=0.6)\nplt.xlabel(\"First Principal Component\")\nplt.ylabel(\"Second Principal Component\")\nplt.title(\"PCA of Rock Samples\")\nplt.colorbar(scatter, label=\"Rock Type\")\n# plt.show()\nplt.savefig(\"pca_plot.png\")\nplt.close()\n</pre> plt.figure(figsize=(10, 8)) scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=rocks[\"rock_type\"].astype(\"category\").cat.codes, alpha=0.6) plt.xlabel(\"First Principal Component\") plt.ylabel(\"Second Principal Component\") plt.title(\"PCA of Rock Samples\") plt.colorbar(scatter, label=\"Rock Type\") # plt.show() plt.savefig(\"pca_plot.png\") plt.close() In\u00a0[\u00a0]: Copied! <pre># Perform K-means clustering\nkmeans = KMeans(n_clusters=3, random_state=19)\nkmeans_labels = kmeans.fit_predict(X_scaled)\n</pre> # Perform K-means clustering kmeans = KMeans(n_clusters=3, random_state=19) kmeans_labels = kmeans.fit_predict(X_scaled) In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, alpha=0.6)\nplt.xlabel(\"First Principal Component\")\nplt.ylabel(\"Second Principal Component\")\nplt.title(\"K-means Clustering of Rock Samples\")\nplt.colorbar(scatter, label=\"Cluster\")\n# plt.show()\nplt.savefig(\"kmeans_plot.png\")\nplt.close()\n</pre> plt.figure(figsize=(10, 8)) scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, alpha=0.6) plt.xlabel(\"First Principal Component\") plt.ylabel(\"Second Principal Component\") plt.title(\"K-means Clustering of Rock Samples\") plt.colorbar(scatter, label=\"Cluster\") # plt.show() plt.savefig(\"kmeans_plot.png\") plt.close() In\u00a0[\u00a0]: Copied! <pre>print(\"Visualizations have been saved as 'pairplot.png', 'pca_plot.png', and 'kmeans_plot.png'\")\n</pre> print(\"Visualizations have been saved as 'pairplot.png', 'pca_plot.png', and 'kmeans_plot.png'\")"},{"location":"ml4earth/","title":"Welcome to Machine Learning for Earth Science","text":""},{"location":"ml4earth/#syllabus","title":"Syllabus","text":""},{"location":"ml4earth/github/","title":"Github","text":""},{"location":"ml4earth/github/#tutorial-on-github-for-machine-learning","title":"Tutorial on GitHub for Machine Learning","text":""},{"location":"ml4earth/github/#introduction","title":"Introduction","text":"<p>In this tutorial, I'll assume you have basic knowledge in Python, Linear Algebra, and Statistics. There are many tutorials for different people, and I try to select some of them to give a picture of machine learning. </p>"},{"location":"ml4earth/github/#glossary-of-machine-learning-ai-101","title":"Glossary of Machine Learning--AI 101","text":"<p>Before we learn how to use something, we must know concepts and terms about it. Here I list some glossaries of machine learning:</p> <ul> <li>Machine Learning Glossary by Google</li> <li>Machine Learning Glossary by ml-glossary</li> </ul>"},{"location":"ml4earth/github/#frameworks","title":"Frameworks","text":"<p>The first step to learn machine learning is to learn how to use frameworks, because you must know how to implement your ideas. The most popular framework in academia now is PyTorch. So I'll only list some tutorials for <code>PyTorch</code>. There are also some other frameworks, such as <code>TensorFlow</code>, <code>Keras</code>, <code>Scikit-learn</code>, etc. <code>TensorFlow</code> is still popular in industry, but it's not so popular in academia now. <code>Keras</code> is a high-level framework based on <code>TensorFlow</code>, so it's easy to learn. <code>Scikit-learn</code> is a framework for traditional machine learning, such as SVM, Random Forest, etc. If you want to learn more, you can google them with keywords like <code>TensorFlow tutorial</code>.</p> <ul> <li>DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ (Official Tutorial)</li> <li>yunjey / pytorch-tutorial</li> </ul>"},{"location":"ml4earth/github/#machine-learning","title":"Machine Learning","text":"<p>This part is about basic machine learning. There are many machine learning algorithms, such as linear regression, SVM, decision tree, etc. I'll list some tutorials about them.</p> <ul> <li>Machine Learning by Andrew Ng (Course)</li> <li>Avik-Jain / 100-Days-Of-ML-Code</li> </ul>"},{"location":"ml4earth/github/#deep-learning","title":"Deep Learning","text":"<p>This part is about deep learning. Deep learning is a sub-field of machine learning. It's based on neural network models. There are many neural network models, such as CNN, RNN, Transformer, GNN, etc. I'll list some tutorials about them.</p> <ul> <li>deeplearningzerotoall / PyTorch (with Jupyter Notebook)</li> <li>Practical Deep Learning (with Jupyter Notebook)</li> </ul>"},{"location":"ml4earth/github/#physics-constrained-learning","title":"Physics-constrained Learning","text":"<ul> <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations (Paper)</li> <li>omniscientoctopus / Physics-Informed-Neural-Networks (Code)</li> </ul>"},{"location":"ml4earth/github/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>This part is about reinforcement learning. Reinforcement learning can be tabular or deep. I'll list some tutorials about them.</p> <ul> <li>Reinforcement Learning: An Introduction (Book)</li> <li>reinforcement-learning-an-introduction (Code)</li> <li>CS285 (Course)</li> </ul>"},{"location":"ml4earth/github/#collections","title":"Collections","text":"<p>In this part, I will introduce some collections of different areas in machine learning. It's almost impossible to know all the details of them, but if you are looking for something, just have a try.</p> <ul> <li>vinta / awesome-python</li> <li>bharathgs / Awesome-pytorch-list</li> <li>ujjwalkarn / Machine-Learning-Tutorials</li> <li>josephmisiti / awesome-machine-learning</li> <li>ChristosChristofidis / awesome-deep-learning</li> <li>aikorea / awesome-rl</li> </ul>"},{"location":"ml4earth/github/#epilogue","title":"Epilogue","text":"<p>Though I have listed many tutorials, there are actually more on the Internet. The point is not to collect as many resources as possible, or to pick the best ones. It's about starting now and insisting on any one of them. Then you can try to apply what you have learned in real problems. You'll learn more during the process. Good luck!</p>"},{"location":"ml4earth/practice/","title":"Practice","text":"<ul> <li>Collecte the dataset; host the dataset on HuggingFace Datasets</li> <li>Develop the neural network model using pytorch; Use wandb to track model training; Scale up training and tuning hyper-parameters using torchrun on cloud;</li> <li>Build a gradio app for streaming inference; Host the app on HuggingFace Spaces; Write a demo using gradio_client for API calls</li> <li>Containerize the model using docker for batch inference; Intergrate the model into QuakeFlow using Kubeflow Pipelines</li> <li>Test the model on multiple earthquake sequences</li> </ul>"},{"location":"ml4earth/seismic_event_format1/","title":"Standard Format for Seismic Event Data","text":""},{"location":"ml4earth/seismic_event_format1/#folder-structure","title":"Folder structure:","text":"<ul> <li>/SeismicEventData/<ul> <li>waveform.h5</li> <li>phase_picks.csv</li> <li>stations.json</li> <li>catalog.csv</li> <li>meta_info.txt</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format1/#waveformh5-format","title":"waveform.h5 format:","text":"<p>For simple explanation, we use the M6.4 Ridgecrest earthquake as an example. We recommand to store raw data without preprocessing such as filtering. In the dataset we are using, we put the first P pick of all stations at 30s and cut a window size of 120s. This convection can be changed according to different seismic networks.</p> <ul> <li>File name: waveform.h5<ul> <li>\"ci38443183\": (group)</li> <li>\"ci38443183\".attrs:<ul> <li>\"event_id\": ci38443183 (str)</li> <li>\"event_time\": 2019-07-04T17:33:490000+00:00 (str)</li> <li>\"event_time_index\"<sup>1</sup>: 2518 (int)</li> <li>\"begin_time\": 2019-07-04T17:33:190000+00:00 (str)</li> <li>\"end_time\": 2019-07-04T17:35:190000+00:00 (str)</li> <li>\"latitude\": 35.705 (float)</li> <li>\"longitude\": -117.504 (float)</li> <li>\"depth_km\": 10.5 (float)</li> <li>\"magnitude\": 6.4 (float)</li> <li>\"magnitude_type\": Mw (str)</li> <li>\"sampling_rate\": 100 (int)</li> <li>\"nt\": 12000 (int)</li> <li>\"nx\"<sup>2</sup>: 15 (int)</li> <li>\"source\": CI (str)</li> </ul> </li> <li>\"ci38443183/CI.RJOB..EH\":  (dataset; shape: 3 x nt, unit: \u03bcm/s; float32)</li> <li>\"ci38443183/CI.RJOB..EH\".attrs: <ul> <li>\"network\": CI (str)</li> <li>\"station\": RJOB (str)</li> <li>\"location\": \"\" (str)</li> <li>\"instrument\": \"EH\" (str)</li> <li>\"component\": \"ENZ\" (str)</li> <li>\"latitude\": 35.705 (float)</li> <li>\"longitude\": -117.504 (float)</li> <li>\"elevation_m\": 10.0 (float\uff09</li> <li>\u201clocal_depth_m\u201d<sup>3</sup>: -3.0 (float)</li> <li>\"distance_km\": 19.2 (float32)</li> <li>\"takeoff_angle: 13.0 (float32)</li> <li>\"azimuth\": 35.3 (float32)</li> <li>\"back_azimuth\": 152.1 (float32)</li> <li>\"dt_s\": 0.01 (float)</li> <li>\"unit\": 1e-6 m/s (str)</li> <li>\"snr\": [1.1, 2.3, 2.0] (list of float)</li> <li>\"p_phase_index\": 3000 (int)</li> <li>\"s_phase_index\": 3023 (int)</li> <li>\"p_phase_score\": 1.0 (float32)</li> <li>\"s_phase_score\": 0.9 (float32)</li> <li>\"p_phase_time\": \"2022-04-26T13:50:65.160000+00:00\" (str)</li> <li>\"s_phase_time\": \"2022-04-26T13:50:65.390000+00:00\" (str)</li> <li>\"p_phase_polarity\": U (str)</li> <li>\"s_phase_polarity\": N (str)</li> <li>\"p_phase_status\": automatic (str)</li> <li>\"s_phase_status\": manual (str)</li> <li>\"phase_type\": [P, S, \u2026] (list of str)</li> <li>\"phase_index\"<sup>4</sup>: [3000, 3023, \u2026] (list of int)</li> <li>\"phase_score\": [1.0, 0.9, \u2026] (list of float)</li> <li>\"phase_time\": [2022-04-26T13:50:65.160000+00:00, \u2026 ] (list of str)</li> <li>\"phase_polarity\": [U, D, N, \u2026]<sup>5</sup> (list of str)</li> <li>\"event_id\": [ci38443183, ci38443183, ...] (list of str; multiple events in a window)</li> </ul> </li> <li>\"ci38443183/...\" (next station)</li> <li>... (next group)</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format1/#phase_pickscsv-format","title":"phase_picks.csv format:","text":"<p>The phase_pick.csv file lists the attrs information of h5 files, which makes it easy to select training data. We recommand use comma (,) as the delimiter of the CSV file. </p> <ul> <li>Headers: event_id,station_id,phase_index,phase_time,phase_score,phase_type,phase_polarity</li> <li>dtype: str,str,int,str,float,str,str</li> <li>e.g.:ci38443183,CI.RJOB..EH,3000,2019-07-04T17:33:520000+00:00,0.98,P,U</li> </ul>"},{"location":"ml4earth/seismic_event_format1/#stationsjson-format","title":"stations.json format:","text":"<p>The stations.json file contains station location information</p> <pre><code>{\n    \"CI.CCC..BH\": {\n        \"longitude\": -117.36453,\n        \"latitude\": 35.52495,\n        \"elevation_m\": 670,\n        \"local_depth_m\": -3.0,\n        \"component\": [\"E\",\"N\",\"Z\"],\n        \"sensitivity\": [627368000.0, 627368000.0, 627368000.0],\n        \"unit\": \"m/s\"\n        },\n    .... (next station)\n}\n</code></pre>"},{"location":"ml4earth/seismic_event_format1/#catalogcsv-format","title":"catalog.csv format:","text":"<p>The catalog.csv file contains earthquake event information</p> <ul> <li>Headers: event_id,time, latitude, longitude,depth_km,magnitude,magnitude_type,source</li> <li>dtype: str,str,float,float,float,float,str,str</li> <li>e.g.:ci38443183,2019-07-04T17:33:490000+00:00,35.705,-117.504,10.5,6.4,Mw,CI</li> </ul>"},{"location":"ml4earth/seismic_event_format1/#meta_infotxt-format","title":"meta_info.txt format","text":"<p>This file contains other useful information about the dataset</p> <p>e.g.:</p> <p>Earthquake number: 145483</p> <p>Time range: 2019-06-01T00:00:00.000000+00:00 - 2020-06-01T00:00:00.000000+00:00</p> <p>Spatial range: (min_latitude, max_latitude, min_longitude, max_longitude) = (-122 -112, 30, 40)</p> <p>Magnitude range: (-1.0, 8.0)</p> <ol> <li> <p>which data point in the event origin time\u00a0\u21a9</p> </li> <li> <p>number of stations\u00a0\u21a9</p> </li> <li> <p>the depth of borehole data to the surface\u00a0\u21a9</p> </li> <li> <p>which data point is the picked phase time\u00a0\u21a9</p> </li> <li> <p>U: upgoing; D: downgoing; N: unknown\u00a0\u21a9</p> </li> </ol>"},{"location":"ml4earth/seismic_event_format2/","title":"Standard Event Format for Seismic Event Data","text":""},{"location":"ml4earth/seismic_event_format2/#folder-structure","title":"Folder structure:","text":"<ul> <li>/SeismicEventData/<ul> <li>data folder:<ul> <li>ci38443183.h5</li> <li>...</li> </ul> </li> <li>phase_picks.csv<ul> <li>ci38443183.csv</li> <li>...</li> </ul> </li> <li>stations.json</li> <li>catalog.csv</li> <li>meta_info.txt</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format2/#waveform-format-in-the-data-folder","title":"Waveform format in the data folder:","text":"<p>For simple explanation, we use the M6.4 Ridgecrest earthquake as an example. We recommand to store raw data without preprocessing such as filtering. In the dataset we are using, we put the first P pick of all stations at 30s and cut a window size of 120s. This convection can be changed according to different seismic networks.</p> <ul> <li>File name: ci38443183.h5<ul> <li>\"data\": (group)</li> <li>\"data\".attrs:<ul> <li>\"event_id\": ci38443183 (str)</li> <li>\"event_time\": 2019-07-04T17:33:490000+00:00 (str)</li> <li>\"event_time_index\"<sup>1</sup>: 2518 (int)</li> <li>\"begin_time\": 2019-07-04T17:33:190000+00:00 (str)</li> <li>\"end_time\": 2019-07-04T17:35:190000+00:00 (str)</li> <li>\"latitude\": 35.705 (float)</li> <li>\"longitude\": -117.504 (float)</li> <li>\"depth_km\": 10.5 (float)</li> <li>\"magnitude\": 6.4 (float)</li> <li>\"magnitude_type\": Mw (str)</li> <li>\"source\": CI (str)</li> </ul> </li> <li>\"data/CI.RJOB..EH\":  (dataset; shape: 3\\(\\times\\)nt; unit: \u03bcm/s; float32)</li> <li>\"data/CI.RJOB..EH\".attrs: <ul> <li>\"network\": CI (str)</li> <li>\"station\": RJOB (str)</li> <li>\"location\": \"\" (str)</li> <li>\"latitude\": 35.705 (float)</li> <li>\"longitude\": -117.504 (float)</li> <li>\"elevation_m\": 10.0 (float)</li> <li>\u201clocal_depth_m\u201d<sup>2</sup>: -3.0 (float)</li> <li>\"component\": [E,N,Z] (list of str)</li> <li>\"distance_km\": 19.2 (float32)</li> <li>\"takeoff_angle\": 12.0 (float32)</li> <li>\"azimuth\": 35.3 (float32)</li> <li>\"back_azimuth\": 152.1 (float32)</li> <li>\"dt_s\": 0.01 (float)</li> <li>\"unit\": 1e-6 m/s (str)</li> <li>\"snr\": [1.1, 2.3, 2.0] (list of float)</li> <li>\"phase_type\": [P, S, \u2026] (list of str)</li> <li>\"phase_index\"<sup>3</sup>: [3000,3023,\u2026] (list of int)</li> <li>\"phase_score\": [1.0, 0.9, \u2026] (list of float)</li> <li>\"phase_time\": [2022-04-26T13:50:65.160000+00:00, \u2026 ] (list of str)</li> <li>\"phase_polarity\": [U, D, N, \u2026]<sup>4</sup> (list of str)</li> <li>\"event_id\": [ci38443183, ci38443183, ...] (list of str; multiple events in a window)</li> </ul> </li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format2/#phase-pick-format-in-the-phase_picks-folder","title":"Phase pick format in the phase_picks folder:","text":"<p>Fhe file name should be the same as the hdf5 file. We recommand use comma (,) as the delimiter of the CSV file. </p> <ul> <li>File name:  ci38443183.csv</li> <li>Headers: station_id,phase_index,phase_time,phase_score,phase_type,phase_polarity</li> <li>dtype: str,int32,str,float32,str,str</li> <li>e.g.:CI.RJOB..EH,,3000,2019-07-04T17:33:520000+00:00,0.98,P,U</li> </ul>"},{"location":"ml4earth/seismic_event_format2/#stationsjson-format","title":"stations.json format:","text":"<p>The stations.json file contains station location information</p> <pre><code>{\n    \"CI.CCC..BH\": {\n        \"longitude\": -117.36453,\n        \"latitude\": 35.52495,\n        \"elevation_m\": 670,\n        \"local_depth_m\": -3,\n        \"component\": [\"E\",\"N\",\"Z\"],\n        \"sensitivity\": [627368000.0,627368000.0,627368000.0],\n        \"unit\": \"m/s\"\n        },\n    .... (next station)\n}\n</code></pre>"},{"location":"ml4earth/seismic_event_format2/#catalogcsv-format","title":"catalog.csv format:","text":"<p>The catalog.csv file contains earthquake event information</p> <ul> <li>Headers: event_id,time, latitude, longitude,depth_km,magnitude,magnitude_type,source</li> <li>dtype: str,str,float,float,float,float,str,str</li> <li>e.g.:ci38443183,2019-07-04T17:33:490000+00:00,35.705,-117.504,10.5,6.4,Mw,CI</li> </ul>"},{"location":"ml4earth/seismic_event_format2/#meta_infotxt-format","title":"meta_info.txt format","text":"<p>This file contains other useful information about the dataset</p> <p>e.g.:</p> <p>Earthquake number: 145483</p> <p>Time range: 2019-06-01T00:00:00.000000+00:00 - 2020-06-01T00:00:00.000000+00:00</p> <p>Spatial range: (min_latitude, max_latitude, min_longitude, max_longitude) = (-122 -112, 30, 40)</p> <p>Magnitude range: (-1.0, 8.0)</p> <ol> <li> <p>which data point in the event origin time\u00a0\u21a9</p> </li> <li> <p>the depth of borehole data\u00a0\u21a9</p> </li> <li> <p>which data point is the picked phase time\u00a0\u21a9</p> </li> <li> <p>U: upgoing; D: downgoing; N: unknown\u00a0\u21a9</p> </li> </ol>"},{"location":"ml4earth/seismic_event_format_das/","title":"Standard Event Format for DAS Data","text":""},{"location":"ml4earth/seismic_event_format_das/#folder-structure","title":"Folder structure:","text":"<ul> <li>/DASEventData/<ul> <li>data folder:<ul> <li>ci38443183.h5</li> <li>...</li> </ul> </li> <li>phase_picks folder:<ul> <li>ci38443183.csv</li> <li>...</li> </ul> </li> <li>das_info.csv</li> <li>catalog.csv</li> <li>meta_info.txt</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format_das/#waveform-format-in-the-data-folder","title":"Waveform format in the data folder:","text":"<p>For simple explanation, we use the M6.4 Ridgecrest earthquake as an example. We recommand to store raw data without preprocessing such as filtering. In the dataset we are using, we put the first P pick of all channels at 30s and cut a window size of 120s. This convection can be changed according to different DAS arrys and earthquake distributions.</p> <ul> <li>File name: ci38443183.h5<ul> <li>data: nch \\(\\times\\) nt (float32, unit: microstrain/s; float32)</li> <li>data.attrs:<ul> <li>\u201cevent_id\u201d: ci38443183 (str)</li> <li>\u201cevent_time\u201d: 2019-07-04T17:33:490000+00:00 (str)</li> <li>\"event_time_index\"<sup>1</sup>: 2518 (int)</li> <li>\u201cbegin_time\u201d: 2019-07-04T17:33:190000+00:00 (str)</li> <li>\u201cend_time\u201d: 2019-07-04T17:35:190000+00:00 (str)</li> <li>\u201clatitude\u201d: 35.705 (float)</li> <li>\u201clongitude\u201d: -117.504 (float)</li> <li>\u201cdepth_km\u201d: 10.5 (float)</li> <li>\u201cmagnitude\u201d: 6.4 (float)</li> <li>\u201cmagnitude_type\u201d: Mw (str)</li> <li>\u201cdt_s\u201d: 0.01 (float)</li> <li>\u201cdx_m\u201d: 8 (float)</li> <li>\u201cunit\u201d: microstrain/s (str)</li> <li>\u201csource\u201d: CI (str)</li> </ul> </li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format_das/#phase-pick-format-in-the-phase_picks-folder","title":"Phase pick format in the phase_picks folder:","text":"<p>Fhe file name should be the same as the hdf5 file. We recommand use comma (,) as the delimiter of the CSV file. </p> <ul> <li>File name: ci38443183.csv</li> <li>Headers: channel_index,phase_index,phase_time,phase_score,phase_type</li> <li>dtype: int32,int32,str,float32,str<ul> <li>e.g.:1000,3000,2019-07-04T17:33:520000+00:00,0.98,P</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format_das/#das_infocsv-format","title":"das_info.csv format:","text":"<p>The das_info file has the location information of the DAS array. We recommand use comma (,) as the delimiter of the CSV file. </p> <ul> <li>Headers: index,latitude,longitude,elevation_m</li> <li>dtype: int32,float,float,float<ul> <li>e.g.: 1,35.695,-117.494,121.1</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format_das/#catalogcsv-format","title":"catalog.csv format:","text":"<p>The cataloa.csv file lists the attrs information of h5 files, which makes it easy to select proper training data. We recommand use comma (,) as the delimiter of the CSV file. </p> <ul> <li>Headers: event_id,event_time, latitude, longitude,depth_km,magnitude,magnitude_type,source</li> <li>dtype: str,str,float,float,float,float,str,str<ul> <li>e.g.: ci38443183,2019-07-04T17:33:490000+00:00,35.705,-117.504,10.5,6.4,Mw,CI</li> </ul> </li> </ul>"},{"location":"ml4earth/seismic_event_format_das/#meta_infotxt-format","title":"meta_info.txt format","text":"<p>This file contains other useful information about the dataset</p> <p>e.g.:</p> <p>DAS location: ridgecrest</p> <p>DAS interrogator: quantx</p> <p>DAS manufacturer: optasense</p> <p>Earthquake number: 145483</p> <p>Time range: 2019-06-01T00:00:00.000000+00:00 - 2020-06-01T00:00:00.000000+00:00</p> <p>Spatial range: (min_latitude, max_latitude, min_longitude, max_longitude) = (-122 -112, 30, 40)</p> <p>Magnitude range: (-1.0, 8.0)</p> <p>Catalog source: SCEDC</p> <ol> <li> <p>which data point in the event origin time\u00a0\u21a9</p> </li> </ol>"},{"location":"ml4earth/seismic_template_format/","title":"Standard Format for Template Matching of DAS Data","text":"<p>For simple explanation, we use the M6.4 Ridgecrest earthquake as an example. </p> <p>File name: template.h5</p> <ul> <li>\"ci38443183\": (group, format: event_id)</li> <li>\"ci38443183\".attrs:<ul> <li>\u201cevent_id\u201d: ci38443183 (str)</li> <li>\u201cevent_time\u201d: 2019-07-04T17:33:490000+00:00 (str)</li> <li>\u201clatitude\u201d: 35.705 (float)</li> <li>\u201clongitude\u201d: -117.504 (float)</li> <li>\u201cdepth_km\u201d: 10.5 (float)</li> <li>\u201cmagnitude\u201d: 6.4 (float)</li> <li>\u201cmagnitude_type\u201d: Mw (str)</li> <li>\u201csource\u201d: CI (str)</li> </ul> </li> <li>\"ci38443183/P_Z\": (group, format: {phase}_{component})<ul> <li>\"data\": (dataset; shape: [nx, nt]; unit: \u03bcm/s; float32)</li> <li>\"data\".attrs:<ul> <li>\"nt\": 400 (int)</li> <li>\"nx\": 1250 (int)</li> <li>\"dt_s\": 0.01 (float)</li> <li>\"dx_m\": 8 (float)</li> <li>\"time_before_s\": 2 (int)</li> <li>\"time_after_s\": 2 (int)</li> <li>\"unit\": microstrain/s (str)</li> </ul> </li> <li>\"travel_time\": (dataset; shape: [nx,];  float32)</li> <li>\"travel_time_index\": (dataset; shape: [nx,]; int)</li> <li>\"travel_time_type\": (dataset; shape: [nx,]; int)</li> <li>\"travel_time_type\".attrs:<ul> <li>\"0\": \"predicted\"</li> <li>\"1\": \"auto_picked\"</li> <li>\"2\": \"manual_picked\"</li> </ul> </li> <li>\"station_id\": (dataset; shape: [nx,]; str)</li> <li>\"snr\": (dataset, shape: nx)</li> </ul> </li> <li>\"ci38443183/S_E\": (same formart for S phase)</li> <li>\"ci38443183/S_H\": (same formart for S phase)</li> </ul>"},{"location":"ml4earth/seismic_template_format_das/","title":"Standard Format for Template Matching of DAS Data","text":"<p>For simple explanation, we use the M6.4 Ridgecrest earthquake as an example. </p> <p>File name: 38443183.h5</p> <ul> <li>\"template\": (group)</li> <li>\"template\".attrs:<ul> <li>\u201cevent_id\u201d: 38443183 (str)</li> <li>\u201cevent_time\u201d: 2019-07-04T17:33:490000+00:00 (str)</li> <li>\u201clatitude\u201d: 35.705 (float)</li> <li>\u201clongitude\u201d: -117.504 (float)</li> <li>\u201cdepth_km\u201d: 10.5 (float)</li> <li>\u201cmagnitude\u201d: 6.4 (float)</li> <li>\u201cmagnitude_type\u201d: Mw (str)</li> <li>\u201csource\u201d: CI (str)</li> </ul> </li> <li>\"template/P\": (group)<ul> <li>\"data\": (dataset)</li> <li>\"data\".attrs:<ul> <li>\"nt\": 400 (int)</li> <li>\"nx\": 1250 (int)</li> <li>\"dt_s\": 0.01 (float)</li> <li>\"dx_m\": 8 (float)</li> <li>\u201ctime_reference\u201d: 2019-07-04T17:35:160000+00:00 (str)</li> <li>\"time_before\": 2 (int)</li> <li>\"time_after\": 2 (int)</li> <li>\"unit\": microstrain/s (str)</li> </ul> </li> <li>\"snr\": signal-to-noise ratio of each channel (dataset)</li> <li>\"shift_index\": shifted time index of each channel based on traveltime (dataset)</li> <li>\"travel_time\": phase traveltime in seconds (dataset, float32)</li> <li>\"travel_time\".attrs:<ul> <li>\"type\": 'phasenet' or 1D model name e.g., MAM (str)</li> <li>\"tref\": the minimal UTCTime of travel times (srt)</li> </ul> </li> </ul> </li> <li>\"template/S\": (same formart for S phase) </li> <li>\"template/event\": (same formart for the whole event waveform)</li> </ul>"},{"location":"ml4earth/syllabus/","title":"Class Syllabus","text":""},{"location":"ml4earth/syllabus/#python-linear-algebra-and-statistics-101","title":"Python, Linear Algebra, and Statistics 101","text":""},{"location":"ml4earth/syllabus/#supervised-learning","title":"Supervised Learning","text":""},{"location":"ml4earth/syllabus/#basic-machine-learning","title":"Basic Machine Learning","text":"<ul> <li>Linear regression<ul> <li>Case study: Earthquake magnitude; </li> <li>Case study: Gutenberg-Richter law and Omori\u2019s law</li> </ul> </li> <li>Support Vector Machine</li> <li>Decision Trees and Boosting</li> <li>Model evaluation, Precision-Recall, ROC, AUC </li> <li>Bias, variance, and regularization</li> </ul>"},{"location":"ml4earth/syllabus/#non-linear-optimization","title":"Non-linear Optimization","text":"<ul> <li>Automatic Differentiation for calculating gradients of complex functions.</li> <li>Gradient descent optimization; Quasi-Newton method (e.g., BFGS);<ul> <li>Case study: Earthquake location</li> </ul> </li> </ul>"},{"location":"ml4earth/syllabus/#deep-learning","title":"Deep Learning","text":"<ul> <li>Neural network models<ul> <li>Convolutional Neural Network</li> <li>Recurrent Neural Network</li> <li>Transformer Model</li> <li>Graph Neural Network</li> </ul> </li> <li>Classification problem<ul> <li>Case study: Earthquake detection</li> <li>Case study: Earthquake/explosion discrimination </li> </ul> </li> <li>Regression problem<ul> <li>Case study: Ground motion prediction</li> </ul> </li> <li>Semantic segmentation problem<ul> <li>Case study: Seismic phase picking</li> </ul> </li> <li>Generative models<ul> <li>Case study: Synthesizing seismic waveforms</li> </ul> </li> </ul>"},{"location":"ml4earth/syllabus/#unsupervised-learning","title":"Unsupervised Learning","text":""},{"location":"ml4earth/syllabus/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<ul> <li>PCA, ICA</li> <li>Auto-encoder<ul> <li>Case study: Denoising of seismic waveforms</li> </ul> </li> </ul>"},{"location":"ml4earth/syllabus/#clustering","title":"Clustering","text":"<ul> <li>K-Means<ul> <li>Case study: Earthquake swarm and background seismicity</li> </ul> </li> <li>Gaussiam Mixture Model<ul> <li>Case study: Earthquake phase association</li> </ul> </li> </ul>"},{"location":"ml4earth/syllabus/#physics-constrained-learning","title":"Physics-constrained Learning","text":""},{"location":"ml4earth/syllabus/#physics-informed-neural-networks","title":"Physics-informed Neural Networks","text":""},{"location":"ml4earth/syllabus/#fourier-neural-operator","title":"Fourier Neural Operator","text":""},{"location":"ml4earth/syllabus/#inverse-problem","title":"Inverse Problem","text":"<ul> <li>Well-determined, under-determined, and over-determined inverse problems</li> </ul>"},{"location":"ml4earth/syllabus/#baysesian-inference","title":"Baysesian Inference","text":"<ul> <li>Uncertainty Quantification</li> <li>Markov Chain Monte Carlo (MCMC)</li> <li>Variational Inference<ul> <li>Gaussian variational inference</li> <li>Stein variational inference</li> </ul> </li> </ul>"}]}